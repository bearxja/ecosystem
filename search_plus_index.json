{"./":{"url":"./","title":"FusionInsight生态地图","keywords":"","body":"﻿ FusionInsight生态地图 FusionInsight支持开源标准的Hadoop接口，可以与以下第三方工具进行对接 第三方工具 FusionInsight HD 涉及领域 工具名称 版本 C30 C50 C60 C70 C80 数据可视化 SAS Access for Hadoop SAS HPA SAS EP 9.4M3 通过 通过 通过 通过   IBM SPSS Analytic Server 3.1.0     通过     IBM Cognos 10.2.2fp4     通过     Tableau 10.0.0 通过 通过 通过     10.1.4     通过     10.3.2       通过   10.5.0         通过 QlikView 12     通过 通过 通过 QlikSense 3.2.4       通过   Oracle BIEE 11g     通过 通过   12c     通过 通过   Alteryx 11.0.2.25199     通过   通过 RapidMiner 8.2.001         通过 SmartBI 7.2.32464.17374       通过   永洪一站式大数据分析平台 7.1     通过     数据集成 IBM InfoSphere DataStage 11.3.1.0   通过       11.5.0.2     通过     IBM InfoSphere CDC 11.3.3.1   通过       Oracle GoldenGate 12.2.0.1.1     通过     12.3.1.1.1       通过 通过 Informatica BDM(native) 10.0.0   通过 通过     Informatica BDM(push down) 10.2.0       通过   Talend 6.3.1     通过     6.4.1       通过 通过 7.0.1       通过 通过 Apache NiFi 1.7.1         通过 Kettle 6.1     通过 通过 通过 8.0&8.1         通过 Pentaho 7.1       通过   8.0     通过     普元元数据管理 6.1   通过       杭州合众UTL 5.1   通过       集成开发环境 RStudio 1.0.153     通过 通过   Apache Zepplin 0.7.2     通过     0.7.3       通过 通过 0.8.0         通过 Jypyter Notebook         通过   DBeaver 4.0.8     通过     4.2.1       通过   DbVisualizer 9.5.7     通过     10.0.1       通过   Squirrel 3.7.1     通过     3.8.0       通过   HUE 4.0.1       通过   SQL分析 Apache Kylin 1.6.0     通过     2.1.0       通过   2.3.0       通过   2.3.1         通过 Kyligence 2.2     通过     2.3     通过     2.4       通过   2.5       通过   Presto 0.155     通过     0.184       通过   0.196         通过 数据库 SAP HANA 100_120_0-10009569   通过 通过 通过 通过 SAP VORA 2.0       通过   2.1       通过   杭州合众UDB 6.1   通过       其他 FUSE 2.8.3     通过     gis-tools-for-hadoop github     通过     IBM WAS 8.5.5.9   通过       Apache Livy 0.5.0         通过 NeoKylin 6.9       通过   7.2       通过   "},"Business_Intelligence/":{"url":"Business_Intelligence/","title":"数据可视化","keywords":"","body":" 数据可视化 SAS (TBD) IBM SPSS (TBD) IBM Cognos (TBD) 对接Alteryx 对接Tableau 对接QlikView 对接Oracle BIEE 对接RapidMiner 永洪BI (TBD) "},"Business_Intelligence/Using_Alteryx_with_FusionInsight.html":{"url":"Business_Intelligence/Using_Alteryx_with_FusionInsight.html","title":"对接Alteryx","keywords":"","body":"Alteryx对接FusionInsight 适用场景 Alteryx 2018.2.5.48994 FusionInsight HD V100R002C80SPC200 配置Windows的kerberos认证 下载并安装MIT Kerberos 下载网址：http://web.mit.edu/kerberos/dist/#kfw-4.0 版本与操作系统位数保持一致，本文版本kfw-4.1-amd64.msi。 确认客户端机器的时间与FusionInsight HD集群的时间一致，时间差要小于5分钟 设置Kerberos的配置文件 在FusionInsight Manager创建一个角色与“人机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。角色需要根据业务需要授予Spark，Hive，HDFS的访问权限，并将用户加入角色。例如，创建用户“developuser”并下载对应的keytab文件user.keytab以及krb5.conf文件，把krb5.conf文件重命名为krb5.ini，并放到C:\\ProgramData\\MIT\\Kerberos5目录中。 设置Kerberos票据的缓存文件 创建存放票据的目录，例如“C:\\temp”。 设置Windows的系统环境变量，变量名为“KRB5CCNAME”，变量值为“C:\\temp\\krb5cache”。 重启机器。 在Windows上进行认证 使用Kerbers认证的用户名密码登录，用户名的格式为：用户名@Kerberos域名。 打开MIT Kerberos，单击“get Ticket”，在弹出的“MIT Kerberos: Get Ticket”窗口中，“Pricipal”输入用户名，“Password”输入密码，单击“OK”。 配置Spark ODBC 连接 在操作系统中配置Spark ODBC驱动 下载并安装ODBC驱动：https://www.tableau.com/support/drivers 根据操作系统类型选择对应的ODBC版本，下载并安装。 创建DSN(Data Source Name)：选择 开始 -> Simba Spark ODBC Driver -> ODBC Administrator。 选择 System DSN -> Add -> Simba Spark ODBC Driver -> Finish 按实际配置相应的变量， Mechanism：Kerberos Host FQDN：hadoop.hadoop.com Service Name：spark2x Realm：留空 点击“Advanced Options”，勾选如下选项： 点击OK，保存配置。 点击Test进行测试连接，如果出现下图，则表示Spark ODBC连接成功。 在Alteryx使用Spark数据源 Alteryx启动后选择Options->Advanced Options->Manage In-DB Connections 在弹出的界面中填写配置： DataSource：Apache Spark ODBC COnnection Type：System Connections: 首次使用选new Connection Name: 自定义 Read->Driver：Apache Spark ODBC Write->Driver: 默认 Connection String：New database connection，选择Spark DSN填写用户名密码 新建一个workflow，拖入Input Data工具，在左侧Connect a file or database 中点击下拉菜单，选择Other Databases->ODBC->Simba Spark Data Source Name 选择在配置ODBC驱动时新建的Spark DSN：Simba Spark （System），填入用户名密码： 点击OK，Alteryx会连接至集群,在弹出的对话框中显示的是集群中Spark中的数据表，选择一个数据表作为输入，例如Customer 导入成功后显示如下,Refresh之后在左侧可以看到数据预览： 再添加一个数据源，执行join操作，成功后结果如下： 配置Hive ODBC数据源 下载并安装Hive的ODBC驱动 ODBC驱动下载地址：下载地址 创建DSN(Data Source Name)：选择 开始 -> Simba Spark ODBC Driver -> ODBC Administrator。 选择 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish 按实际配置相应的变量 Host(s): Hive Service主节点 Port：Hive Service端口21066 Mechanism：Kerberos Host FQDN：hadoop.hadoop.com Service Name：hive Realm：留空 如下图： Advanced Options不需要进行配置默认的参数即可连接成功。 点击Test进行测试连接，如果出现下图，则表示ODBC连接Hive成功。 Alteryx使用Hive数据源 Alteryx启动后选择Options->Advanced Options->Manage In-DB Connections 在弹出的界面中填写配置： DataSource：Hive Connection Type：System Connections: 首次使用选new Connection Name: 自定义 Read->Driver：Hive ODBC Write->Driver: Hive ODBC Connection String：New database connection，选择Hive DSN，填写用户名密码 在主界面新建一个workflow，拖入Input Data工具，在左侧Connect a file or database 中点击下拉菜单，选择Other Databases->ODBC Data Source Name 选择在配置ODBC驱动时新建的Hive DSN：Sample Cloudera Hive DSN(System)，填入用户名密码： 点击OK，Alteryx会连接至集群,在弹出的对话框中显示的是集群中Hive中的数据表，选择一个数据表作为输入，例如Customer： 导入成功后显示如下,Refresh之后在左侧可以看到数据预览： 再添加一个数据源，执行join操作，成功后结果如下： 配置HDFS数据源 HDFS是通过WebHDFS连接，前提条件是获取MIT Kerberos Ticket，并在Manager中修改HDFS的配置： dfs.http.policy 修改为HTTP_AND_HTTPS，重启HDFS。 在Alteryx主界面新建一个workflow，拖入Input Data工具，在左侧Connect a file or database 中点击下拉菜单，选择Hadoop 在弹出的界面中填写配置： Server：WebHDFS Host： HDFS所在服务器IP Port: 配置文件中dfs.namenode.http.port对应端口 User Name & Password： Kerberos 认证用户名及密码 Kerberos: Kerberos MIT 点击Test，出现Connection successful 表明连接成功。 弹出集群中的HDFS文件系统内容，目前支持Avro和CSV格式的文件，需上传至HDFS文件系统中。 选择相应文件，连接成功，Refresh之后左侧菜单显示文件内容预览： Join 操作成功后显示如下: FAQ 找不到C:\\ProgramData\\MIT\\Kerberos5文件夹 C:\\ProgramData一般属于隐藏文件夹，设置文件夹隐藏可见或者使用搜索功能即可解决问题。 连接成功无数据库权限 连接所使用的用户需要有数据库的权限，否则将导致ODBC连接成功却无法读取数据库内容。 测试连接时出现Default Kerberos ticket is expired Kerberos MIT ticket 过期，需要重新获得，获取一次有效期为10h. "},"Business_Intelligence/Using_Tableau_with_FusionInsight.html":{"url":"Business_Intelligence/Using_Tableau_with_FusionInsight.html","title":"对接Tableau","keywords":"","body":"Tableau对接FusionInsight 适用场景 Tableau 10.0.0 FusionInsight HD V100R002C30 Tableau 10.0.0 FusionInsight HD V100R002C50 Tableau 10.0.0 FusionInsight HD V100R002C60U10 Tableau 10.1.4 FusionInsight HD V100R002C60U20 Tableau 10.3.2 FusionInsight HD V100R002C70SPC200 Tableau 10.5.0 FusionInsight HD V100R002C80SPC100 配置Windows的kerberos认证 下载并安装MIT Kerberos 下载网址：http://web.mit.edu/kerberos/dist/#kfw-4.0 版本与操作系统位数保持一致，本文版本kfw-4.1-amd64.msi。 确认客户端机器的时间与FusionInsight HD集群的时间一致，时间差要小于5分钟 设置Kerberos的配置文件 在FusionInsight Manager创建一个角色与“人机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。角色需要根据业务需要授予Hive的访问权限，并将用户加入角色。例如，创建用户“tableau”并下载对应的keytab文件user.keytab以及krb5.conf文件，把krb5.conf文件重命名为krb5.ini，并放到C:\\ProgramData\\MIT\\Kerberos5目录中。 设置Kerberos票据的缓存文件 创建存放票据的目录，例如“C:\\temp”。 设置Windows的系统环境变量，变量名为“KRB5CCNAME”，变量值为“C:\\temp\\krb5cache”。 重启机器。 在Windows上进行认证 使用上述创建的用户名密码登录，用户名的格式为：用户名@Kerberos域名。 打开MIT Kerberos，单击“get Ticket”，在弹出的“MIT Kerberos: Get Ticket”窗口中，“Pricipal”输入用户名，“Password”输入密码，单击“OK”。 配置Hive数据源 Tableau中配置Hive数据源，对接Hive的ODBC接口。 下载并安装ODBC驱动：下载地址 根据操作系统类型选择对应的ODBC版本，下载并安装。 配置ODBC驱动 创建DSN(Data Source Name)：选择 开始 -> Simba Spark ODBC Driver -> ODBC Administrator。 选择 User DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish 按实际配置相应的变量， Host(s): Hive Service主节点 Port：Hive Service端口21066 Mechanism：Kerberos Host FQDN：hadoop.hadoop.com Service Name：hive Realm：留空 如下图 Advanced Options不需要进行配置默认的参数即可连接成功。 点击中的Test进行测试连接，如果出现下图，则表示ODBC连接Hive成功。 Tableau使用数据源 Tableau启动后会进入连接选择界面，点击“更多服务器”，再点击“其他数据库（ODBC）”； DSN选择hive_odbc（上一步中设置ODBC的名称），点击连接，如下图所示，点击“连接”，然后登陆。 查询百万级数据表数据 查询多表数据 配置Spark数据源 下载并安装spark的ODBC驱动 ODBC驱动下载地址：http://www.tableau.com/support/drivers 创建DSN（Data Source Name） 打开C:\\Program Files\\Simba Spark ODBC Driver\\lib\\DriverConfiguration64.exe 按实际配置相应的变量 Mechanism：Kerberos Host FQDN：hadoop.hadoop.com Service Name：spark Realm：留空 如下图： 点击“Advanced Options”，勾选如下选项： 点击OK，保存配置。 Tableau使用Spark数据源 Tableau启动后会进入连接选择界面，点击“更多服务器”，再点击“Spark SQL”，作如下配置： 其中服务器为JDBCServer(主)的业务IP。 端口为FusionInsight中Spark服务配置，导出服务配置文件，其中hive.server2.thrift.port对应值。 点击“登录”，进入tableau页面，选择架构和表，结果如下。 用Tableau做实时连接，打开工作簿，对该表进行图形化分析。 性能测试 查询包含百万条数据的表web_sales 多表关联查询：store_sales和item表做关联查询 增加customer_address表 查询结果： FAQ 找不到C:\\ProgramData\\MIT\\Kerberos5文件夹 C:\\ProgramData一般属于隐藏文件夹，设置文件夹隐藏可见或者使用搜索功能即可解决问题。 连接成功无数据库权限 连接所使用的用户需要有数据库的权限，否则将导致ODBC连接成功却无法读取数据库内容。 ODBC连接失败 常见情况是Host(s)、Port、Host FQDN等的输入数据有误，请根据实际情况进行输入 "},"Business_Intelligence/Using_QlikView_with_FusionInsight.html":{"url":"Business_Intelligence/Using_QlikView_with_FusionInsight.html","title":"对接QlikView","keywords":"","body":"QlikView对接FusionInsight 适用场景 QlikView 12 FusionInsight HD V100R002C60U20 QlikView 12 FusionInsight HD V100R002C70SPC200 QlikView 12 FusionInsight HD V100R002C80SPC100 配置Windows的kerberos认证 下载并安装MIT Kerberos，地址：http://web.mit.edu/kerberos/dist/#kfw-4.0 版本与操作系统位数保持一致，本文版本kfw-4.1-amd64.msi。 确认客户端机器的时间与FusionInsight HD集群的时间一致，时间差要小于5分钟 设置Kerberos的配置文件 在FusionInsight Manager创建一个角色与“机机”用户，具体请参见《FusionInsight HD管理员指南》的创建用户章节。角色需要根据业务需要授予Hive的访问权限，并将用户加入角色。例如，创建用户“sparkdemo”并下载对应的keytab文件user.keytab以及krb5.conf文件，把krb5.conf文件重命名为krb5.ini，并放到C:\\ProgramData\\MIT\\Kerberos5目录中。 设置Kerberos票据的缓存文件 创建存放票据的目录，例如C:\\temp。 设置Windows的系统环境变量，变量名为“KRB5CCNAME”，变量值为C:\\temp\\krb5cache 重启机器。 在Windows上进行认证 使用命令行进入到MIT Kerberos安装路径，找到可执行文件kinit.exe，例如本文路径为：C:\\Program Files\\MIT\\Kerberos\\bin 执行如下命令： kinit -k -t /path_to_userkeytab/user.keytab UserName 其中path_to_userkeytab为存放用户keytab文件的路径，user.keytab为用户的keytab，UserName为用户名。 配置Hive数据源 QlikView中配置Hive数据源，对接Hive的ODBC接口 下载安装Hive ODBC驱动 从以下地址下载驱动根据操作系统类型选择对应的ODBC版本，下载并安装： 下载地址 配置用户DSN 在OBDC数据源管理器页面的用户DSN标签页中，点击添加，配置用户数据源。 在 创建数据源 页面，找到 Cloudera ODBC Driver for Apache Hive，选中后点击 完成。 配置Hive数据源。 Data Source Name：为自定义参数 Host(s)： HiveServer的业务ip Port： Hive Service端口，21066 Mechanism： Kerberos Host FQDN： hadoop.hadoop.com Service Name： hive Realm： 留空 点击 Test 连接成功则表示配置成功，点击 OK 连接Hive数据源 打开QlikView 12，新建 一个文档 关闭弹出的入门向导 在工具栏中打开 编辑脚本 按钮 在弹出的 编辑脚本 页面下方，点击 数据 标签页，在 数据库 的下拉栏中找到 OCBC，点击 连接； 在连接到数据源页面，选择上面配置的数据源hive_odbc，然后点击确定； 在 编辑脚本 页面的 数据 标签页中，点击 选择 按钮 在 创建Select语句 页面中，选择想要导入的 数据库表格，在 字段 中选择，则导入完整表格，其余选项则导入其对应的表格，然后点击 确定（示例中选择）； 回到 编辑脚本 页面，点击 确定 回到QlikView工作表页面，点击 重新加载，则可以将数据库表格导入到QlikView中。 然后可以对数据进行制图制表分析等处理，具体步骤可以参考QlikView官网的使用指南。 配置Spark数据源 QlivView中配置Spark数据源，对接SparkSQL的thrift接口。 下载安装Spark的ODBC驱动 在Simba官网下载Spark ODBC驱动，根据用户自身操作系统选择32bit或64bit，Data Source选择Spark SQL，地址：http://www.tableau.com/support/drivers 根据安装客户端提示安装客户端。 配置用户DSN 在 OBDC数据源管理器 页面的 用户DSN 标签页中，点击 添加，配置用户数据源。 在 创建数据源 页面，找到 Simba Spark ODBC Driver，选中后点击 完成。 在 Simba Spark ODBC Driver DSN Setup 页面中配置Spark数据源。 Data Source Name： 自定义 Mechanism： Kerberos Host FQDN： hadoop.hadoop.com Service Name： spark Realm： 留空， Host(s)： JDBCServer(主)的业务ip， Port： SparkThriftServer客户端端口号23040。 设置完毕后点击 Advanced Options，在弹出的 Advanced Options 页面中，勾选 Use Native Query 和 Get Tables With Query，然后点击 OK 回到 Simba Spark ODBC Driver DSN Setup，点击 Test 连接成功，点击 OK 退出页面，否则将弹出失败对话框。 回到 Simba Spark ODBC Driver DSN Setup 页面，点击 OK，回到 ODBC数据源管理器 页面，点击 确定 完成并退出配置。 连接Spark数据源 打开QlikView 12，新建 一个文档 关闭弹出的入门向导 在工具栏中打开 编辑脚本 按钮 在弹出的 编辑脚本 页面下方，点击 数据 标签页，在 数据库 的下拉栏中找到 OCBC，点击 连接； 在 连接到数据源 页面，选择上面配置的数据源spark_odbc，然后点击 确定； 在 编辑脚本 页面的 数据 标签页中，点击 选择 按钮 在 创建Select语句 页面中，选择想要导入的 数据库表格，在 字段 中选择，则导入完整表格，其余选项则导入其对应的表格，然后点击 确定 （示例中选择）； 回到 编辑脚本 页面，点击 确定 回到QlikView工作表页面，点击 重新加载，则可以将数据库表格导入到QlikView中。 然后可以对数据进行制图制表分析等处理，具体步骤可以参考QlikView官网的使用指南。 FAQ 找不到C:\\ProgramData\\MIT\\Kerberos5文件夹 C:\\ProgramData一般属于隐藏文件夹，设置文件夹隐藏可见或者使用搜索功能即可解决问题。 连接成功无数据库权限 连接所使用的用户需要有数据库的权限，否则将导致ODBC连接成功却无法读取数据库内容。 ODBC连接失败 常见情况是 Host(s) 、 Port 、 Host FQDN 的输入数据有误，请根据实际情况进行录入 "},"Business_Intelligence/Using_Oracle_BIEE_with_FusionInsight.html":{"url":"Business_Intelligence/Using_Oracle_BIEE_with_FusionInsight.html","title":"对接Oracle BIEE","keywords":"","body":"Oracle BIEE对接FusionInsight 适用场景 Oracle BIEE 11g FusionInsight HD V100R002C60U20 Oracle BIEE 11g FusionInsight HD V100R002C70SPC200 Oracle BIEE 12c FusionInsight HD V100R002C60U20 Oracle BIEE 12c FusionInsight HD V100R002C70SPC200 Linux环境安装OBIEE 安装OS 安装RedHat6.5操作系统，desktop版 创建用户oracle 安装jdk8 获取jdk8安装包，执行安装 安装Weblogic 创建oracle home目录： umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ 上传weblogic安装包，解压 以oracle用户登录图形界面 安装BI Server 上传OBIEE安装包，解压 chmod 755 bi_platform-12.2.1.2.0_linux64.bin 以oracle用户登录图形界面 ./bi_platform-12.2.1.2.0_linux64.bin 补齐lib包 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel 取消当前安装，重新运行安装程序 安装Oracle Database 12c 安装数据库软件 创建数据库安装目录 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle 下载Oracle Database 12c安装包，解压得到database文件夹 chmod -R 755 database/ cd database/ su oracle ./runInstaller 只安装单实例数据库软件 创建数据库实例 cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca 字符集选择AL32UTF8，不勾选“create as container database” 配置环境变量vi ~/.bash_profile ORACLE_BASE=/Oracle/database ORACLE_HOME=$ORACLE_BASE/product/12.1.0/dbhome_1 ORACLE_SID=orcl ORACLE_TERM=xterm PATH=$PATH:$ORACLE_HOME/bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH 导入环境变量 source ~/.bash_profile 配置监听程序和网络服务名 netca Listener端口设为默认值1521 网络服务名配置为 ORCL 启动数据库和监听程序 主机重启后，需要重新执行以下命令启动数据库和监听程序 su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus界面执行startup 使用RCU创建Schema 启动rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu 配置BI Server 执行配置 cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh 安装BI Client 在Win7(64 bit)系统上安装BI Client 对接Hive 配置客户端系统DSN 配置Kerberos认证 从http://web.mit.edu/kerberos/下载安装kfw-4.1 安装配置Hive ODBC Driver 下载安装Hive ODBC Driver（Windows版本），下载地址 在BI客户端所在的Windows机器上配置系统DSN 测试ODBC连接 BI 管理工具新建RDP Client端打开Oracle BI 管理工具 选择上一步配置的DSN，用户名口令任意输入，但不能为空 禁用BI Server高速缓存 登录Weblogic域管理界面http://162.1.115.81:9500/em 配置中禁用高速缓存 上传RPD文件到服务端 客户端 cmd 切换到E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin目录 执行命令上传RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi 配置服务端系统DSN 配置Kerberos认证 mv /etc/krb5.conf /etc/krb5.conf.bak 将FusionInsight集群的krb5.conf上传到/etc目录下 kerberos认证 su oracle kinit test_cn 安装配置Cloudera Hive ODBC Driver yum install -y unixODBC 下载Hive ODBC Driver（Linux版本）下载地址 安装Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm 修改DSN配置，与Client端生成的RPD文件的DSN名称和配置保持一致 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini 修改odbc配置文件 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ 配置环境变量vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini 导入环境变量source /etc/profile 测试ODBC连接 su oracle isql -v 'Sample Cloudera Hive DSN' BI域配置系统ODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini 重启OBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh 服务端分析Hive数据 打开BI Analytics界面http://162.1.115.81:9502/analytics 创建分析 选择待分析的列拖到右侧区域 点击“结果”页签，检索所选列数据 点击右上角的保存按钮，保存查询结果 创建可视分析器项目 添加数据源 选取数据显示形式 添加计算 对接Spark SQL 配置客户端系统DSN Kerberos认证 Kerberos获取认证票据 安装配置Simba Spark ODBC Driver 下载安装 Simba Spark ODBC Driver：下载地址 配置DSN： 测试ODBC连接： BI管理工具新建RDP 新建obiee-spark.rdp，DSN选择上一步配置的 Sample Simba Spark DSN 上传RDP文件到服务端 上传RDP 配置服务端系统DSN Kerberos认证 su oracle kinit test_cn 安装配置Simba Spark ODBC Driver 下载Simba Spark ODBC Driver：下载地址 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm 修改DSN配置，增加Sample Simba Spark DSN，与Client端配置相同 vi /etc/odbc.ini 修改odbcinst.ini，vi /etc/odbcinist.ini 配置环境变量 vi /etc/profile 导入环境变量 source /etc/profile 测试ODBC连接 su oracle isql -v 'Sample Simba Spark DSN' BI域配置系统ODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini 重启OBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh 服务端分析Spark数据 参考服务端分析Hive数据 对接LibrA/ELK 配置LibrA与ELK的方式没有区别，以下以对接ELK为例进行操作 配置客户端系统DSN 配置obiee客户端的ODBC驱动 按照ELK的产品文档的指导安装配置ELK的windows驱动 配置DSN，测试ODBC连接，保存ODBC连接 BI管理工具新建RDP 新建obiee-elk.rdp，DSN选择上一步配置的 PostgreSQL35W 上传RDP文件到服务端 上传RDP 配置服务端系统DSN 参考LibrA/ELK的产品文档的Linux下配置数据源章节，完成obiee节点下的ODBC驱动的安装 测试ODBC连接，确保ODBC驱动安装成功 isql -v PostgreSQL35W BI域配置系统ODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini 在ODBC Data Sources部分增加PostgreSQL35W的DSN 在文件末尾增加PostgreSQL35W的DSN的详细配置 PostgreSQL35W的DSN的详细配置最后一行DriverUnicodeType=1需要加上，否则obiee查询的时候会报错[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) 重启OBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh 服务端分析Spark数据 参考服务端分析Hive数据 "},"Business_Intelligence/Using_RapidMiner_with_FusionInsight.html":{"url":"Business_Intelligence/Using_RapidMiner_with_FusionInsight.html","title":"对接RapidMiner","keywords":"","body":"RapidMiner对接FusionInsight 适用场景 Rapidminer Studio 8.2.001 FusionInsight HD V100R002C80SPC200 准备工作 下载安装RapidMiner Studio, 当前最新版本为8.2.001,下载地址 https://rapidminer.com/ 修改本地host文件，路径为C:\\Windows\\System32\\drivers\\etc，加入集群各个节点IP与主机名对应关系，保存文件。 设置Kerberos的配置文件 在FusionInsight Manager创建一个角色与“人机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。角色根据业务需要授予Spark，Hive，HDFS的访问权限，并将用户加入角色。例如，创建用户“developuser”并下载对应的keytab文件user.keytab以及krb5.conf文件。 准备FusionInsight客户端配置文件以及jar包 在集群的Manager中，选择服务->下载客户端->完整客户端 解压后，进入HDFS，Hive，Yarn组件的config目录，找到如下的配置文件，复制到一个文件夹里，例如命名为config。 打开hdfs-site.xml文件，将以下属性以及对应的value删除： dfs.client.failover.proxy.provider.hacluster 打开core-site.xml文件，修改以下属性的value fs.defaultFS 修改为 namenodeIP:dfs.namenode.rpc.port的形式，例如 172.21.3.116:25000 （可选）进入Spark组件的Jar包目录“\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars”，将所有jar包复制出来，保存在jars文件夹里。 集群配置 配置UDP端口绑定 下载安装UDP端口绑定工具uredir，下载地址https://github.com/troglobit/uredir 编译安装完成后，分别上传至KDC服务所在的主备节点，进入uredir执行文件所在目录，执行以下命令进行端口绑定,其中IP为所在节点IP./uredir IP 88 IP 21732 配置Radoop依赖jar包 在Radoop文档中心，下载Radoop依赖jar包，下载地址https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html,下载与安装的RapidMiner版本对应的jar包。 将jar包上传至集群每个节点相同的路径下，例如/usr/local/lib/radoop/ 在集群主节点和备节点，分别上传Radoop的jar包至以下路径 Hive服务端的lib路径\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib\"， Mapreduce服务端的lib路径：\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib\" 创建Radoop UDF函数 在主节点执行如下命令： #cd /opt/hadoopclient #source bigdata_env #kinit developuser 输入developuser用户密码，执行beeline，进入Hive Hive中创建数据库，例如创建数据库rapidminer,执行以下命令： create database rapidminer； use rapidminer； DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep'; RapidMiner配置 在RapidMiner中，菜单选择Connections->Manage Radoop Connections 在弹出的对话框中选择New Connections->Import Hadoop Configuration Files，选择配置文件所在文件夹，点击Import Configuration 导入成功后点击Next，进入连接配置窗口，根据左侧菜单栏，进行如下填写： Global： Hadoop Version：Other（Hadoop 2X line） Additional Libraries Directory：Spark组件的jars包 Client Principal： Kerberos用户名@HADOOP.com Keytab File: 从Manager下载的keytab文件 KDC Address： 集群KDC所在服务器IP REALM： HADOOP.COM Kerberos Config File: 从Manager下载的krb5配置文件 Hadoop： 在左上角搜索框中搜索split，在搜索结果中取消勾选mapreduce.input.fileinputformat.split.maxsize参数 搜索classpath，在搜索结果中取消勾选mapreduce.application.classpath参数 Spark： Spark Version：Spark2.1 Spark Archive（or libs）Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy：Static，Default Configuration Advanced Spark Parameters：添加spark.driver.extraJavaOptions和spark.executor.extraJavaOptions两个参数 参数value在Manager，Services->Spark2X Configuration->所有配置，搜索extraJavaOptions，选择Spark2x->SparkResource2x中的这两个参数值，将其中使用的所有“./”相对路径替换为服务端Spark配置文件所在的绝对路径，例如“/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc” Hive： Hive Version：Hive Server2 Hive Server Address：Hive 服务所在节点IP Hive Port： 21066 Database Name： 在Hive中创建的Radoop Function所在的数据库名称 Customer database for UDFs: 同Database Name 点击OK->Proced Anyway->Save 测试连接 点击Configure,在Global页面，点击Test，Test Results显示如下，表明Global测试成功 在Hadoop页面，点击Test，Test Results显示如下，表明Hadoop测试成功 在Spark页面，点击Test，Test Results显示如下，表明Spark测试成功 在Hive页面，点击Test，Test Results显示如下，表明Hive测试成功 在Manage Radoop Connections 窗口，选中所创建的连接，点击Full test进行完整测试，Test Results显示如下，表明完整测试通过 Radoop样例运行 在RapidMiner Studio 主页面，Help->Tutorials->User Hadoop->Rapidminer Radoop 根据Tutorials的指导运行样例，运行结果如下： FAQ 测试连接时，提示ICMP port unreachable/Error retrieving Hive object list问题 检查集群中端口绑定程序是否正常运行，绑定的端口是否正确。RapidMiner在测试时，会与集群的88端口连接进行Kerberos认证，而FusionInsight平台对端口进行了规划，Kerberos认证使用的端口是21732。 测试Spark时，提示GSS initiate failed 检查本地host文件是否添加了集群IP与主机名的对应关系。 测试Spark时，将各种版本都测试了一遍，最后提示Spark test failed 检查添加的两个Advanced Parameters是否填写正确，其value值中的绝对路径对于每个集群是不一样的，当集群重装后需要修改该值。 "},"Data_Integration/":{"url":"Data_Integration/","title":"数据集成","keywords":"","body":" 数据集成 IBM InfoSphere DataStage IBM InfoSphere CDC (TBD) 对接Oracle GoldenGate informatica (TBD) 对接Talend 对接 对接Apache NiFi 对接Kettle Kettle6.1 FusionInsight HD V100R002C60U20 Kettle8.0&8.1 FusionInsight HD V100R002C70SPC100 普元 (TBD) 杭州合众UTL (TBD) "},"Data_Integration/Using_IBM_InfoSphere_DataStage_with_FusionInsight.html":{"url":"Data_Integration/Using_IBM_InfoSphere_DataStage_with_FusionInsight.html","title":"IBM InfoSphere DataStage","keywords":"","body":"IBM InfoSphere DataStage对接FusionInsight 适用场景 IBM InfoSphere DataStage 11.3.1.0 FusionInsight HD V100R002C50 IBM InfoSphere DataStage 11.5.0.2 FusionInsight HD V100R002C60U20 前提条件 已完成IBM InfoSphere DataStage 11.5.0.2的安装部署（本文部署在Centos7.2上） 已完成FusionInsight集群的部署，版本FusionInsight HD V100R002C60U20 准备工作 配置域名解析 使用vi /etc/hosts命令修改DataStage Server和Client的hosts文件，添加FI集群节点信息，如： 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3 配置Kerberos认证 在FI管理界面创建DataStage对接用户，并赋予该用户所需权限，下载认证凭据 解压下载的tar文件，得到Kerberos配置文件krb5.conf和用户的keytab文件。 以root登录DataStage Server节点，将FI集群的krb5.conf文件复制到/etc目录。 将用户的user.keytab文件上传到DataStage Server节点的任意目录，如/home/dsadm。 安装FusionInsight客户端 参考FI产品文档，在FI服务管理界面下载完整客户端，上传到DataStageServer，安装至自定义目录，如/opt/ficlient。 对接HDFS 导入FI集群的SSL证书 浏览器导出FI集群的根证书 浏览器打开FI管理界面，查看证书，点击“证书路径”页签，选择根路径，查看根证书，在“详细信息”页签下，点击“复制到文件”，导出为cer格式 证书导入DataStage的keystore文件 将导出的FI根证书fi-root-ca.cer上传到DataStage服务端，如/home/dsadm路径下，将证书导入到keystore文件，命令参考： /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks 生成并保存加密后的keystore密码 使用vi /home/dsadm/authenticate.properties命令新建配置文件，保存上一步骤生成的密文： password={iisenc}SvtJ2f/uNTrvbuh26XDzag== 执行chown dsadm:dstage /home/dsadm/ authenticate.properties修改配置文件的属主 导出truststore环境变量 使用vi /opt/IBM/InformationServer/Server/DSEngine/dsenv编辑DSEngine的环境变量，在最后添加 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties 重启DSEngine，参考命令 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 读取HDFS文件 创建作业 新建并行作业，保存为hdfs2sf 添加File_Connector组件和Sequential File组件，以及File_Connector到Sequential File链接 参考下图修改配置 编译运行 保存配置后，编译，运行 在菜单 Tools -> Run Director 中打开Director客户端，查看作业日志 查看读取的数据 写入HDFS文件 创建作业 新建并行作业，保存为hdfswrite 添加Row Generator组件和File Connector组件，以及Row Generator到File Connector链接 参考下图修改配置 编译运行 保存 — 编译 — 运行 ，查看作业日志： 查看写入数据 对接Hive 使用Hive Connector 说明：Hive Connector官方认证过的Hive JDBC Driver只有DataDirect Hive Driver(IShive.jar)，用DataStage 11.5.0.2中自带的IShive.jar连接FusionInsight的hive时，会有thrift protocol报错，需要咨询IBM技术支持提供的最新的IShive.jar 设置JDBC Driver配置文件 在$DSHOME路径下创建isjdbc.config文件，CLASSPATH变量中添加DataDirect Hive Driver (IShive.jar)的路径，CLASS_NAMES变量中添加com.ibm.isf.jdbc.hive.HiveDriver，参考命令： su - dsadm cd $DSHOME vi isjdbc.config 在isjdbc.config中添加如下信息: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver 配置Kerberos认证信息： 在IShive.jar所在目录下创建JDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf 文件内容如下： JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; 读取Hive数据 创建作业 修改配置 URL参考如下进行配置： jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; 其中JDBC_DRIVER_test_keytab为上一步指定的鉴权信息 编译运行 保存 — 编译 — 运行 ，查看作业日志： 查看读取的数据 数据写入Hive表 创建作业 修改配置 编译运行 保存 — 编译 — 运行 ，查看作业日志，写入10条数据，用时2’11” 查看Hive表数据： Hive Connector向Hive表写数据使用Insert语句，每插入一条数据会起一个MR任务，效率特别低，不推荐使用这种方式。可以将数据直接写入HDFS文件。 使用JDBC Connector 如果要使用FusionInsight的Hive JDBC驱动， 用isjdbc.config文件CLASSPATH中添加jdbc驱动和依赖包的方式，在运行作业时会有如下报错，此时需要用导出CLASSPATH环境变量的方式加载 而且只能用JDBC Connector，不能用Hive Connector，否则会有如下报错 设置CLASSPATH环境变量 Hive jdbc驱动包及依赖包位于Hive客户端lib目录下/opt/ficlient/Hive/Beeline/lib，若未安装客户端，也可单独上传这些jar包到任意目录。 设置CLASSPATH环境变量，添加上述jar包的完整路径，参考命令： su - dsadm vi $DSHOME/dsenv 文件最后添加相关的jar包（具体路径根据实际环境调整） export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar 导入环境变量 source $DSHOME/dsenv 重启DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 读取Hive数据 创建作业 修改配置 其中URL为： jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; 编译运行 数据写入Hive表 创建作业 修改配置 编译运行 写入5条数据，用时1’49” 数据导入Hive表的HDFS文件 创建作业 修改配置 编译运行 查看写入数据 hive表数据增量100 增量数据定期自动导入Hive表的HDFS文件 增量数据可以新增HDFS文件的方式导入hive，如果要定期自动化执行，导入的文件名中需要包含可变参数进行设置和区分，然后以命令或脚本方式运行作业，给该参数赋值。 创建作业 设置作业参数 点击“job properties”按钮，设置参数如下 修改配置 File Connector配置导出文件的名称，以“#”引用设置的参数 dsjob命令运行作业 保存编译作业，在DataStage Server上执行dsjob -run命令，格式为： dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME 命令参考: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append 查看HDFS文件： 查看Hive数据增量为200条 对接SparkSQL 与使用FI Hive JDBC驱动类似，可以用SparkSQL JDBC驱动连接Hive，同样需要导出CLASSPATH环境变量来加载驱动包及依赖包。 SparkSQL jdbc不支持insert into语句，只能用来读hive数据，不能插入数据到hive表。 设置CLASSPATH环境变量 SparkSQL jdbc驱动包及依赖包位于Spark客户端lib目录下/opt/ficlient/Spark/spark/lib/，若未安装客户端，也可单独上传所需jar包到任意目录。 设置CLASSPATH环境变量，添加上述jar包的完整路径，以及spark客户端配置文件路径（SparkSQL jdbc连接hive时需要读取hive-site.xml中的配置）： su - dsadm vi $DSHOME/dsenv 配置如下内容： export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf 导入环境变量 source $DSHOME/dsenv 重启DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 读取Hive表数据 创建作业 修改配置 URL参考： jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; 编译运行 对接Phoenix 使用Phoenix以JDBC方式访问HBase表，也需要导出CLASSPATH环境变量来加载驱动包及依赖包。 设置CLASSPATH环境变量 Phoenix相关的jar包位于HBase客户端lib目录下/opt/ficlient/HBase/hbase/lib，若未安装客户端，也可单独上传所需jar包到任意目录。 设置CLASSPATH环境变量，添加上述jar包的完整路径，以及HBase客户端配置文件路径（phoenix连接时需要读取hbase-site.xml中的配置）： su - dsadm vi $DSHOME/dsenv 配置如下内容： export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf 导入环境变量 source $DSHOME/dsenv 重启DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 创建jaas配置文件 Phoenix连接需要查询zookeeper ，zookeeper的Kerberos认证需要指定jaas配置文件 su - admin vi /home/dsadm/jaas.conf 文件内容如下： Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; 读取Phoenix表数据 创建作业 修改配置 URL参考： jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab 配置JVM options为-Djava.security.auth.login.config=/home/dsadm/jaas.conf 编译运行 写入Phoenix表数据 Phoenix插入语句是upsert into，不支持Insert into 语句，所以不能用JDBC Connector在运行时自动生成SQL语句，需要自己填写，否则会报错： main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. 创建作业 修改配置 编译运行 对接Fiber 对接Fiber需要先安装FI客户端 修改JDBC Driver配置文件 修改$DSHOME路径的isjdbc.config文件，CLASSPATH变量中添加Fiber jdbc driver及依赖包的路径，CLASS_NAMES变量中添加com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver 参考命令： su - dsadm cd $DSHOME vi isjdbc.config 配置如下： CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver 修改Fiber配置文件 DataStage使用IBM jdk，需要新建Fiber配置文件给DataStage使用 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml 修改fiber_ibm.xml中phoenix,hive,spark各driver的以下两个参数： java.security.auth.login.config 修改为 /home/dsadm/jaas.conf zookeeper.kinit 修改为 /opt/IBM/InformationServer/jdk/jre/bin/kinit 文件/home/dsadm/jaas.conf的内容如下： Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; 其它配置项参考FI产品文档Fiber客户端配置指导修改。 使用Hive Driver读取数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive 编译运行 使用Hive Driver写入数据 创建作业 修改配置 编译运行 使用Spark Driver读取数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark 编译运行 使用Phoenix Driver读取数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix 编译运行 目前未能读取到数据，”The connector could not determine the value for the fetch size.”，问题正在确认中 使用Phoenix Driver写入数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix 编译运行 写入数据0行，问题正在确认中 对接Kafka 说明：kafka Connector不支持发送或者消费integer, float, double, numeric, decimal等数值类型的字段，需要转换成char, varchar, longvarchar等类型，否则会有如下报错： main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL). 安装kafka客户端 kafka Connector需要配置Kafka client Classpath，可以在DataStage节点安装kafka客户端来获取kafka-client jar包。安装步骤参考FusionInsight产品文档。 Kafka Client Classpath 需要提供kafka-client, log4j, slf4j-api 三个jar包的路径，如： /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar 发送消息到kafka 创建作业 修改配置 RowGenerator 生成数据 transformer数据类型转换： Kafka配置： 编译运行 读取Kafka消息 创建作业 修改配置 编译运行 查看读取的数据 对接MPPDB 获取MPPDB JDBC Driver 从MPPDB发布包中获取，包名为Gauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz 解压后得到gsjdbc4.jar，上传到DataStage Server 修改JDBC Driver配置文件 修改$DSHOME路径的isjdbc.config文件，CLASSPATH变量中添加MPPDB Driver 的路径，CLASS_NAMES变量中添加org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config 配置： CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver; 读取MPPDB表数据 创建作业 修改配置 URL格式为： jdbc:postgresql://host:port/database 编译运行 数据写入MPPDB表 创建作业 修改配置 URL格式为： jdbc:postgresql://host:port/database 编译运行 查看MPPDB表数据： "},"Data_Integration/Using_Oracle_GoldenGate_with_FusionInsight.html":{"url":"Data_Integration/Using_Oracle_GoldenGate_with_FusionInsight.html","title":"对接Oracle GoldenGate","keywords":"","body":"Oracle GoldenGate对接FusionInsight 适用场景 Oracle GoldenGate 12.2 FusionInsight HD V100R002C60U20 Oracle GoldenGate 12.3 FusionInsight HD V100R002C70SPC200 Oracle GoldenGate 12.3 FusionInsight HD V100R002C80SPC100 环境信息 软件信息 Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20 硬件信息 源端OGG VM: 162.1.115.68 Redhat6.5 （包含Oracle DB12c的数据库） 目标端OGG VM: 162.1.115.69 Redhat6.5（包含Hadoop的客户端） 拓朴结构 测试拓朴结构如下图所示： 测试表 源端测试表： 在源端Oracle的PDBORCL数据库的test用户下创建test1表，其中ID为主键 OGG for Oracle安装 前置条件：完成oracle12c数据库的安装（IP：162.1.115.68） 软件版本：linuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip 下载并安装OGG for Oracle 将fbo_ggs_Linux_x64_shiphome.zip上传至oracle客户端（ip：162.1.115.68）/home/oracle目录下，切换至oracle用户，解压生成bo_ggs_Linux_x64_shiphome目录。 在/home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1目录下，运行./runInstaller 安装成功，/home/orcle/OGG/是OGG for Oracle的安装目录。 配置环境变量 切换到oracle用户 su - oracle vi .bash_profile 文件.bash_profile内容如下： # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/bin export PATH PATH=$PATH:$HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib 运行OGG 打开数据库归档及开启最小附加日志 使用Sqlplus / as sysdba登陆Oracle源端数据库后打开Archive Log: shutdown immediate; startup mount; alter database archivelog; alter database open; archive log list; 源端数据库打开数据库级最小附加日志及force logging： SELECT supplemental_log_data_min, force_logging FROM v$database; alter database add supplemental log data; alter database force logging; 切换日志以使附加日志生效： ALTER SYSTEM switch logfile; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication; alter system set enable_goldengate_replication = true scope=both; 配置DB12c PDB的tnsname信息vi $ORACLE_HOME/network/admin/tnsnames.ora： 在数据库中创建ogg用户并赋予权限 使用sqlplus / as sysdba登陆数据库后创建ogg用户并赋予权限 create user c##ogg identified by welcome1; grant dba to c##ogg container=all; grant create session, connect, resource to c##ogg container=all; grant alter any table to c##ogg container=all; grant alter system to c##ogg container=all; exec dbms_goldengate_auth.grant_admin_privilege('c##ogg',container=>'all'); 配置GoldenGate 登陆数据库的别名 在GoldenGate中创建用户别名，用于登录Oracle数据库读取数据库日志： add credentialstore ALTER CREDENTIALSTORE ADD USER c##ogg PASSWORD welcome1 ALIAS ogg_src 这样就可以用别名ogg_src登陆数据库了： dblogin useridalias ogg_src C##ogg是Oracle DB12c的普通用户，可以访问多个数据库实例。 创建test用户和test1表 test用户是基于pdborcl数据库实例的： 登陆数据库 Sqlplus / as sysdba 创建用户 alter session set container=pdborcl; alter database open; create user test identified by welcome1; grant resource, connect to test; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500M UNIFORM SIZE 128k; alter user test quota unlimited on test; alter user test quota unlimited on users; 创建测试表 conn test/welcome1@pdborcl; create table test1(id number primary key, name varchar2(50)); 配置GoldenGate捕获进程 编辑eora.prm，在GGSCI命令行下运行edit param eora命令： GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume 编辑diroby/eora.oby文件，在GGSCI命令行下运行shell vi diroby/eora.oby命令：(shell之后接操作系统命令) 使用oracle用户创建diroby目录： cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby 注意进程名eora和数据文件dirdat/eo的对应关系 在GGSCI命令行下运行obey diroby/eora.oby命令，把捕获进程eora加入到管理者进程中： GGSCI> obey diroby/eora.oby 把捕获进程eora注册到pdborcl数据库中： GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) 为pdborcl.test下的所有表添加表级附加日志： GGSCI> add schematrandata pdborcl.test allcols 启动GoldenGate捕获进程eora: GGSCI> start eora 配置GoldenGate传输进程phdfs 配置GoldenGate传输进程phdfs，将OGG生成的数据文件传递给目标端GoldenGate HDFS处理。 编辑phdfs.prm，在GGSCI命令行下运行edit param phdfs命令： 编辑diroby/phdfs.oby文件，在GGSCI命令行下运行shell vi diroby/phdfs.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/phdfs.oby 注意进程名phdfs和数据文件dirdat/rs的对应关系 在GGSCI命令行下运行obey diroby/phdfs.oby命令，把捕获进程phdfs加入到管理者进程中： GGSCI> obey diroby/phdfs.oby 启动GoldenGate捕获进程phdfs: GGSCI> start phdfs 配置GoldenGate传输进程phbase 配置GoldenGate传输进程phbase，将OGG生成的数据文件传递给目标端GoldenGate HBASE处理。 编辑phbase.prm，在GGSCI命令行下运行edit param phbase命令： 编辑diroby/phbase.oby文件，在GGSCI命令行下运行shell vi diroby/phbase.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/phbase.oby 注意进程名phbase和数据文件dirdat/se的对应关系 在GGSCI命令行下运行obey diroby/phbase.oby命令，把捕获进程phbase加入到管理者进程中： GGSCI> obey diroby/phbase.oby 启动GoldenGate捕获进程phbase: GGSCI> start phbase 配置GoldenGate传输进程pflume 配置GoldenGate传输进程pflume，将OGG生成的数据文件传递给目标端GoldenGate FLUME处理。 编辑pflume.prm，在GGSCI命令行下运行edit param pflume命令： 编辑diroby/pflume.oby文件，在GGSCI命令行下运行shell vi diroby/pflume.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/pflume.oby 注意进程名pflume和数据文件dirdat/rf的对应关系 在GGSCI命令行下运行obey diroby/pflume.oby命令，把捕获进程pflume加入到管理者进程中： GGSCI> obey diroby/pflume.oby 启动GoldenGate捕获进程pflume: GGSCI> start pflume 配置GoldenGate传输进程pkafka 配置GoldenGate传输进程pkafka，将OGG生成的数据文件传递给目标端GoldenGate Kafka处理。 编辑pkafka.prm，在GGSCI命令行下运行edit param pkafka命令： 编辑diroby/pkafka.oby文件，在GGSCI命令行下运行shell vi diroby/pkafka.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/pkafka.oby 注意进程名pkafka和数据文件dirdat/rk的对应关系 在GGSCI命令行下运行obey diroby/pkafka.oby命令，把捕获进程pkafka加入到管理者进程中： GGSCI> obey diroby/ pkafka.oby 启动GoldenGate捕获进程pkafka: GGSCI> start pkafka 查看GoldenGate进程运行状态 查看GoldenGate进程状态：(EORCL是与ELK对接的进程) GGSCI> info all 查看某个进程的详细信息： GGSCI> info eora detail 查看GoldenGate的统计信息： GGSCI> stats eora, latest 查看GoldenGate进程报告，用于定位问题： GGSCI> view report eora OGG for Bigdata安装 环境准备 下载安装FusionInsight客户端 在Bigdata客户端机器上（ip：162.1.115.69）按照FusionInsight产品文档安装FusionInsight客户端。将客户端JDK替换成1.7版本。 下载并安装oracle JDK1.7 将krb5.conf放在/etc/目录下 下载并安装OGG for Bigdata 将122011_ggs_Adapters_Linux_x64.zip上传至客户端/opt目录下： unzip 122011_ggs_Adapters_Linux_x64.zip 将解压后的ggs_Adapters_Linux_x64.tar解压到/opt/OGG_HADOOP目录下： 配置环境变量 更改环境变量，编辑根目录下vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME=/usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib PATH=$JAVA_HOME/bin:$PATH:$HOME/bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH Source环境变量，source .bash_profile. 将/opt/OGG_HADOOP/AdapterExamples/big-data下的四个目录下的所有文件拷贝到/opt/OGG_HADOOP/dirprm目录下。 配置GoldenGate管理进程 编辑mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all 配置GoldenGate HDFS 复制进程 编辑rhdfs.prm，在GGSCI命令行下运行edit param rhdfs命令： GGSCI> edit param rhdfs 编辑hdfs.props, 在GGSCI命令行下运行shell vi dirprm/hdfs.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/hdfs.props 需要在HDFS中创建/ogg1目录。 将hdfs.keytab文件拷贝到/opt/OGG_HADOOP/dirprm目录中： 把GoldenGate复制进程rhdfs加入到GoldenGate管理者进程中： GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all 配置GoldenGate HBase 复制进程 编辑rhbase.prm，在GGSCI命令行下运行edit param rhbase命令： GGSCI> edit param rhbase 编辑hbase.props, 在GGSCI命令行下运行shell vi dirprm/hbase.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/hbase.props 拷贝hbase.keytab和jaas.conf到/opt/OGG_HADOOP/dirprm/下： jaas.conf 文件 把GoldenGate复制进程rhbase加入到GoldenGate管理者进程中： GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all 配置GoldenGate Kafka 复制进程 创建kafka消息，进入FusionInsight客户端/opt/hadoopclient/Kafka/kafka/bin Kafka创建消息： ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka查看消息： ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka给消息授权： ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test 编辑rkafka.prm，在GGSCI命令行下运行edit param rkafka命令： GGSCI> edit param rkafka 编辑kafka.props, 在GGSCI命令行下运行shell vi dirprm/kafka.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/kafka.props 其中 gg.handler.kafkahandler.BlockingSend 属性控制同步和异步，默认false，异步。 GGSCI> shell vi dirprm/custom_kafka_producer.properties 修改Kafka里的配置，将如下选项修改为True 把GoldenGate复制进程rkafka加入到GoldenGate管理者进程中： GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all 配置GoldenGate Flume 复制进程 安装Flume客户端，配置非加密传输 配置Server的配置文件properties.properties 导出的properties.properties文件，增加如下配置： 可以在HDFS中增加/ogg/flume目录 将此properties.properties文件上传至FusionInsight。 编辑rflume.prm，在GGSCI命令行下运行edit param rflume命令： GGSCI> edit param rflume 编辑flume.props, 在GGSCI命令行下运行shell vi dirprm/flume.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false 控制DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false 相同SCHAME打包 GGSCI> shell vi dirprm/custom-flume-rpc.properties 拷贝flume.keytab文件到/opt/OGG_HADOOP/dirprm/目录下 把GoldenGate复制进程rflume加入到GoldenGate管理者进程中： GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all 测试结果 Oracle端启动所有的传输进程 确保所有传输进程均已经正常启动 在Oracle数据库源端做Insert操作 su – oracle source .bash_profile sqlplus test/welcome1@pdborcl 查看HDFS同步情况，hadoop fs –ls /ogg1 查看HBase同步情况 hbase shell 查看kafka结果，进入kafka客户端/opt/hadoopclient/Kafka/kafka/bin 执行以下命令： ./kafka-console-consumer.sh --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning 在HDFS中查看flume运行结果：查看/ogg/flume/下数据文件： 在Oracle数据库源端做Update操作 执行以下命令 su – oracle source .bash_profile sqlplus test/welcome1@pdborcl 查看HDFS同步情况，hadoop fs –ls /ogg1 查看HBase同步情况 hbase shell 查看kafka结果，进入kafka客户端/opt/hadoopclient/Kafka/kafka/bin 执行以下命令： ./kafka-console-consumer.sh --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning 在HDFS中查看flume运行结果：查看/ogg/flume/下数据文件： 在Oracle数据库源端做Delete操作 执行以下命令 su – oracle source .bash_profile sqlplus test/welcome1@pdborcl 查看HDFS同步情况，hadoop fs –ls /ogg1 查看HBase同步情况 hbase shell 查看kafka结果，进入kafka客户端/opt/hadoopclient/Kafka/kafka/bin 执行以下命令： ./kafka-console-consumer.sh --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning 在HDFS中查看flume运行结果：查看/ogg/flume/下数据文件： "},"Data_Integration/Using_Talend_with_FusionInsight.html":{"url":"Data_Integration/Using_Talend_with_FusionInsight.html","title":"对接Talend","keywords":"","body":"Talend对接FusionInsight 适用场景 Talend 7.0.1 FusionInsight HD V100R002C80SPC200(HDFS,HBase组件) Talend 6.4.1 FusionInsight HD V100R002C80SPC200(hive组件) 注：因为Talend 7.0.1版本bug，HIve组件无法在版本7.0.1中通过，对接hive组件使用Talend 6.4.1版本 安装Talend 操作场景 安装Talend 7.0.1 前提条件 已完成FusionInsight HD客户端的安装(可参考产品文档->应用开发指南->安全模式->配置客户端文件) 操作步骤 配置环境变量JAVA_HOME,Path 配置Kerberos认证 向FusionInsight HD集群管理员获取集群Kerberos的krb5.conf文件,把相应的krb5.conf文件重命名为 krb5.ini,并放到C:\\ProgramData\\Kerberos目录中，同时将krb5.ini文件放到C:\\Windows目录下（Talend默认从此目录下查找） 下载TOS并修改TOS启动参数 在https://www.talend.com/products/big-data/big-data-open-studio/下载TOS，创建连接zookeeper的jaas配置文件（如C:\\developuser\\jaas.conf），内容格式如下： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; 启动TOS_BD，运行TOS_BD-win-x86_64.exe 安装必需的第三方库 Talend连接HDFS 操作场景 Talend中配置HDFS解析器，对的FI HD HDFS接口 前提条件 已经完成Talend 7.0.1的安装 已完成FusionInsight HD和客户端的安装，包含HDFS组件 HDFS Connection 操作步骤 添加tHDFSConnection组件，配置如下: 具体配置： 1: 选择Cloudera，版本为Cloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" 测试结果： HDFS Get 操作步骤 整个流程如图所示: tHDFSConnection组件配置不变 tHDFSGet组件配置如下： 注意：测试前在集群HDFS文件系统上 /tmp/talend_test路径下已经传入文件out.csv，C:/SOFT为本地输出文件路径 测试结果： 到本地路径C:/SOFT下查看测试结果 HDFS Put 操作步骤 整个流程如图所示: tHDFSConnection组件配置不变 tHDFSPut组件配置如下 注意：测试前在本地目录C:/SOFT下创建文件HDFSPut.txt, 内容如下： It is create on local PC. 测试结果： 登录到集群查看测试结果： Talend连接Hive 操作场景 Talend中配置JDBC解析器，对的FI HD Hive接口 前提条件 已经完成Talend 6.4.1的安装 已完成FusionInsight HD和客户端的安装，包含Hive组件 Hive Connection 操作步骤 对接Hive组件Talend版本需要6.4.1 整个流程如图所示: tHiveConnection组件配置如下 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" 注意：需要点击Distritution旁边的按钮来导入FusionInsight HD客户端Hive样例代码中的所有jar包，如果还有缺失的jar包，可用Talend自带的类库进行自动补全，或者也可以手动导入 测试结果： Hive Create Table & Load 操作步骤 tHiveConnection组件配置保持不变 tHiveCreateTable组件配置如下 注意：需要点击编辑架构旁边的按钮来配置需要导入hive表的结构 tHiveCreateTable组件配置如下： 注意：提前需要向hdfs文件存储系统/tmp/talend_test/路径下传入文件out.csv out.csv文件内容如下： 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq tHiveClose组件配置如下: 测试结果： 在集群上检查传入的表createdTableTalend Hive Input 操作步骤 整个流程如图所示: tHiveConnection组件配置保持不变 tHiveInput组件配置如下： 注意：需要点击编辑架构旁边的按钮来配置hive表的结构 tLogRow组件使用默认配置 tHiveClose组件配置如下 测试结果： Hive Row 操作步骤 整个流程如图所示: tHiveConnection组件配置保持不变 tHiveRow组件配置如下 注意：需要点击编辑架构旁边的按钮来配置hive表的结构 测试结果： 连接到集群查看测试结果 Talend连接HBase 操作场景 Talend中配置HBase解析器，对的FI HD HBase接口 前提条件 已经完成Talend 7.0.1的安装 已完成FusionInsight HD和客户端的安装，包含HBase组件 HBase Connection 操作步骤 整个流程如图所示: 用eclipse导出FusionInsight HD客户端中Hbase样例代码中的LoginUtil类（样例代码路径如C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example） 在Talend里插入tHbaseConnection组件，点击组件进行设置 首先点击tHBaseConnection图标下面的组件按钮，选择版本为Custom - Unsupported和Hadoop 2，再点击版本旁边的按钮导入jar包，需要导入的是上一步导出的hbase_loginUtil.jar以及FusionInsight HD客户端中Hbase样例代码hbase-example中引用的所有jar包，如果还有缺失的jar包，可用Talend自带的类库进行自动补全，或者也可以手动导入 hbase-example样例代码中lib目录下所有的jar包如下： 使用tLibraryLoad组件导入hbase_loginUtil.jar 点击 Advanced settings在Import中增加import com.huawei.hadoop.security.LoginUtil; tHBaseConnection配置如下: 引入tJava组件用定制代码替代Connection组件 代码内容如下： org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); 测试结果 HBase Input Output 操作步骤 整个流程如图所示: tLibraryLoad，tHBaseConnection，tJava配置不变 加入tFileInputDelimited组件配置如下： 注意需要点击编辑架构旁边的按钮，根据需要存入文件(out.csv)的格式定义列和类型 out.csv测试数据如下： 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq 加入tHBaseOutput组件配置如下： 注意需要点击编辑架构旁边的按钮编辑表的架构： tHBaseInput组件配置如下，需要注意的是同样需要点击编辑架构旁边的按钮配置表的结构 tLogRow组件使用默认配置 测试结果 检查集群创建的HBase表hbaseInputOutputTest 在集群上使用代码 hbase shell scan 'hbaseInputOutputTest' "},"Data_Integration/Using_Kettle_with_FusionInsight.html":{"url":"Data_Integration/Using_Kettle_with_FusionInsight.html","title":"对接Kettle","keywords":"","body":"Kettle对接FusionInsight HD Kettle6.1 FusionInsight HD V100R002C60U10 Kettle8.0&8.1 FusionInsight HD V100R002C80SPC200 "},"Data_Integration/Using_Kettle_6.1_with_FusionInsight_HD_C60U10.html":{"url":"Data_Integration/Using_Kettle_6.1_with_FusionInsight_HD_C60U10.html","title":"Kettle6.1 <-> FusionInsight HD V100R002C60U20","keywords":"","body":"Kettle对接FusionInsight 适用场景 Kettle 6.1 FusionInsight HD V100R002C60U10 Kettle 6.1 FusionInsight HD V100R002C60U10 Kettle 6.1 FusionInsight HD V100R002C60U10 环境准备 Linux平台 安装操作系统 安装CentOS6.5 Desktop 禁用防火墙，SELinux 添加本地主机名解析 使用vi /etc/hosts添加本地主机名解析 162.1.115.89 kettle 安装FusionInsight HD客户端 下载完整客户端，安装至目录/opt/hadoopclient 使用vi /etc/profile编辑以下内容插入到文件末尾 source /opt/hadoopclient/bigdata_env 将krb5.conf放在/etc目录下 cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Windows平台 安装JDK8 配置系统环境变量 JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 在PATH环境变量添加 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 获取Kerberos配置文件 在FI管理界面下载用户的认证凭据 解压后得到Kerberos配置文件krb5.conf和用户密钥文件user.keytab 将krb5.conf文件复制C:\\Windows目录下，重命名为krb5.ini 添加系统环境变量KRB5_CONFIG（可选步骤） KRB5_CONFIG=C:\\Windows 配置并启动Kettle 从以下地址 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ 下载Kettle6.1版本 解压得到data-integration目录 替换pentaho-big-data-plugin下的配置文件 下载FusionInsightHD客户端并解压 用解压目录下Hive/jdbc-examples/conf/core-site.xml文件 替换data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23目录下的core-site.xml文件 替换Hive相关jar包 将data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib下的hive相关的jar包 替换成Hive客户端下jdbc-examples/lib目录下的以下jar包 获取用户keytab文件 在FI管理界面下载用户的keytab文件到本地 Kerberos认证（可选步骤） 在对接Hive时，可以使用本地缓存的认证票据，或者在jdbc URL中指定principal和keytab文件进行认证（对接HDFS时，只能使用本地缓存的票据） 如果使用本地缓存的票据，需要在启动kettle前先完成认证。 使用本地缓存票据存在以下问题：kettle只在启动时读取一次票据，而不是连接时实时读取当前票据信息，所以当kettle启动时获取的票据过期以后，连接Hive会失败，必须重启kettle。 启动kettle Linux平台 VNC登录CentOS桌面，打开Terminal cd /opt/data-integration/ ./spoon.sh Windows平台 双击data-integration目录下的Spoon.bat 对接Hive 创建Hive连接 选择 文件 -> 新建 -> 转换 点击 主对象树 页签，在页签中选择 转换 -> DB连接，右键选择 新建 连接类型选择Hive 2，填写主机名、端口号、数据库名 点击左侧 选项，如果使用本地缓存票据，填写以下参数： 如果要在连接Hive时使用keytab文件认证，增加user.principal和user.keytab两个参数： 测试连接时，Hadoop版本选用HDP2.3 连接测试成功后，点击 确认 保存连接 读取Hive数据 以hive -> postgresql为例 将上面创建的转换保存为hive2postgres.ktr 创建postgresql连接 添加转换步骤 在 核心对象 页签下，拖动 输入 -> 表输入，和 输出 -> 表输出 两个步骤到工作区，并连接这两个步骤。 修改Hive表输入配置 双击 表输入 步骤， 数据库连接 选择前面创建的hive连接，点击 获取SQL查询语句 ，选择需要导入的hive表 修改postgresql表输出配置 双击 表输出 步骤，数据库连接中 选择前面创建的postgresql连接，点击 获取目标表配置 如下（需要先在postgresql数据库创建目标表） 运行转换 保存配置，点击 执行 按钮，选择 本地执行 执行结果： postgresql数据库查看： 写入Hive数据 以oracle -> hive为例 添加Oracle JDBC Driver 从http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html 下载对应版本的jdbc Driver，放到data-integration/lib目录下，重启kettle 新建转换，保存为oracle2hive.ktr 创建Oracle连接 参考上面章节创建hive连接 创建待导入的Hive表 CREATE TABLE IF NOT EXISTS kettle_export ( id int, name string ); 添加转换步骤 修改步骤配置 Oracle表输入配置 Hive表输出配置 运行转换 保存配置，点击 执行 按钮，选择 本地执行 执行结果：向Hive表写入13条数据，用时4min+ 查看Hive表数据： 说明：向Hive表中写入数据，每插入一条数据会起一个MR任务，所以效率特别低，不推荐用这种方式，可以将数据写入HDFS文件 对接HDFS 创建Hadoop Cluster 选择 文件 -> 新建 -> 转换，点击 主对象树 页签，在 Hadoop Clusters 右键选择 New Cluster HDFS的Hostname填写NameNode主节点的IP，端口号是25000，如果NaneNode发生主备切换，需要修改IP JobTracker的Hostname 填写 Yarn ResourceManager主节点的IP，端口号是26004，如果ResourceManager发生主备切换，需要修改IP。 点击 测试 kettle6.1不支持HDFS NameNode和Yarn ResourceManager的HA配置 导入HDFS文件 以postgresql -> HDFS为例 将上面创建的转换保存为postgres2hdfs.ktr 参考前面章节创建postgresql连接 添加转换步骤 在 核心对象 页签下，拖动 输入 -> 表输入 ，和 Big Data -> Hadoop File Output 两个步骤到工作区，并连接这两个步骤。 创建待导入的Hive表 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string, description string, total_emp int, salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES (\"field.delim\"=\"[,]\") STORED AS TEXTFILE; 如果数据中含有”,”，列分隔符不可以使用默认的”,”，本样例使用多字节分隔符”[,]” 修改postgresql表输入配置 双击 表输入 步骤，数据库连接 选择前面创建的postgresql连接，点击 获取SQL查询语句，选择需要导入的表 修改Hadoop File Output配置 双击 Hadoop File Output 步骤，在 文件 页签下，Hadoop Cluster 选择前面创建的集群，Folder/File 选择到hive表对应的hdfs目录，文件名可以任意指定 点击 内容 页签，分隔符设置与前面创建的Hive表相同，勾选 快速数据存储（无格式）（否则保存的文件中会按字段长度填充空格） 点击 字段 页签，获取字段 运行转换 保存配置，点击 执行 按钮，选择 本地执行 。 执行结果： 查看导入的HDFS文件： 查看Hive表数据： 读取HDFS文件 以HDFS -> Excel为例 新建转换，保存为hdfs2excel.ktr 添加转换步骤 在 核心对象 页签下，拖动 Big Data -> Hadoop File Input 和 输出 -> Microsoft Excel 输出，两个步骤到工作区，并连接这两个步骤。 修改 Hadoop File Input配置 双击 Hadoop File Input 步骤，文件 页签，选择待导出的文件，文件类型支持CSV（txt也可以）和Fixed（固定列宽） 点击 内容 页签，选择文件类型、分隔符、编码方式等 点击 字段 页签，获取字段 kettle会自动扫描文件中的字段类型和长度 可以手动修改字段名称和长度 点击 确定 按钮，保存配置 修改Microsoft Excel输出配置 双击 Microsoft Excel 输出 步骤，选择文件保存位置和文件名 点击 内容 页签，获取字段 运行转换 保存配置，点击 执行 按钮，选择 本地执行 执行结果 查看导出的excel文件 "},"Data_Integration/Using_Kettle_8.0&8.1_with_FusionInsight_HD_C80SPC200.html":{"url":"Data_Integration/Using_Kettle_8.0&8.1_with_FusionInsight_HD_C80SPC200.html","title":"Kettle8.0&8.1 <-> FusionInsight HD V100R002C70SPC100","keywords":"","body":"Kettle对接FusionInsight 适用场景 Pentaho8.0.0 FusionInsight HD V100R002C80SPC200 Pentaho8.1.0 FusionInsight HD V100R002C80SPC200 Windows平台 环境准备 安装JDK8 配置系统环境变量 JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 在PATH环境变量添加 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 获取Kerberos配置文件 在FI管理界面下载集群用户的认证凭据，例如用户为developuser; 解压后得到Kerberos配置文件krb5.conf和用户密钥文件user.keytab 将krb5.conf文件复制到本地某目录下，例如C: 配置并启动Kettle 软件获取 打开以下地址[https://github.com/pentaho/pentaho-kettle/tree/8.0] (https://github.com/pentaho/pentaho-kettle/tree/8.0), 选择DownloadZip下载Kettle8.0版本 解压得到pdi-ce-8.0.0.0-28; 获取Fi28的适配包，放入入\\data-integration\\plugins\\pentaho-big-data-plugin\\hadoop-configurations 目录下; 该适配包请联系华为FusionInsight Openlab小组获取 获取FusionInsightHD客户端配置文件 下载FusionInsightHD客户端并解压 解压后，进入HDFS，Hive，Hbase等组件的config目录，找到如下的配置文件，复制到Fi28适配包的文件夹里; 修改core-site.xml文件中以下字段： fs.defaultFS hdfs://hacluster 获取用户keytab文件及配置 在FI管理界面下载用户的keytab文件到本地 修改Fi28适配包中config.properties文件: pentaho.authentication.default.kerberos.keytabLocation=C:/kerberos/user.keytab pentaho.authentication.default.kerberos.conf=C:/kerberos/krb5.conf pentaho.authentication.default.kerberos.principal=developuser@HADOOP.COM 启动kettle Windows平台 双击data-integration目录下的Spoon.bat,进入界面后，在上方菜单栏选择工具->Hadoop Distribution,选择FusionInsight HD C80 200 对接Hive 创建Hive连接 选择 文件 -> 新建 -> 转换 点击 主对象树 页签，在页签中选择 转换 -> DB连接，右键选择 新建 为连接命名，连接类型选择Hive 2，填写主机名、端口号、数据库名 点击左侧 选项，填写以下参数： 点击测试，显示以下窗口，表明测试成功 连接测试成功后，点击 确认 保存连接 读取Hive数据 添加转换步骤 在 核心对象 页签下，拖动 表输入 和 文本文件输出 两个步骤到工作区，并连接这两个步骤; 修改Hive表输入配置 双击 表输入 步骤， 数据库连接 选择前面创建的hive连接，点击 获取SQL查询语句 ，选择需要导入的hive表 选择是,该表的字段将会包含在SQL语句中， 可以点击预览，并选择行数，预览Hive表中的数据 修改文本文件输出配置 在文件选项卡中，设置输出文件名称，扩展名： 在内容选项卡中，设置文件输出时属性 在字段选项卡中，点击获取字段，获得文件字段内容，可以点击最小宽度,使字段宽度最小 点击确定,保存设置。 运行转换，在主界面点击工具栏左侧的三角形运行按钮 执行结果： 写入Hive数据 以本地文本文件 -> hive为例 新建转换，保存为hive_out.ktr 添加转换步骤，将文本文件输入和表输出两个步骤拖入工作区，连接两个步骤; 双击文本文件输入，在文件选项卡中，点击浏览，选择需要上传的本地文件，点击添加，文件被添加至下方选中的文件; 在内容选项卡中，设置文件类型、分隔符、限定符、编码等等 在字段选项卡中，点击获取字段，获得字段后，可以点击Minimal Width使字段宽度最小 点击确定,保存配置。 双击表输出，参考上面章节创建hive连接 设置目标表，该表需要已经在Hive中创建好，并且字段与本地文件保持一致; 运行转换 保存配置，点击 执行 按钮， 执行结果：向Hive表写入10条数据，用时2min+ 查看Hive表数据： 说明：向Hive表中写入数据，每插入一条数据会起一个MR任务，所以效率特别低，不推荐用这种方式，可以将数据写入HDFS文件之后再载入Hive表 对接HDFS 创建Hadoop Cluster 选择 文件 -> 新建 -> 转换，点击 主对象树 页签，在 Hadoop Clusters 右键选择 New Cluster HDFS的Hostname填写hacluster; JobTracker的Hostname 填写 Yarn ResourceManager主节点的IP，端口号是21066,如果ResourceManager发生主备切换，需要修改IP; ZooKeeper的Hostname 填写ZooKeeper的主节点IP，端口号是24002，如果ResourceManager发生主备切换，需要修改IP; Oozie的URL填写oozie WebUI的地址. 点击 测试 导入HDFS文件 以本地文件 -> HDFS为例 添加转换步骤 在 核心对象 页签下，拖动 输入 -> 文本文件输入 ，和 Big Data -> Hadoop File Output 两个步骤到工作区，并连接这两个步骤。 文本文件输入配置参考上面章节配置 修改Hadoop File Output配置 双击 Hadoop File Output 步骤，在 文件 页签下，Hadoop Cluster 选择前面创建的集群，Folder/File 选择hdfs目录，文件名可以任意指定 点击 内容 页签，设置分隔符，勾选 快速数据存储（无格式）（否则保存的文件中会按字段长度填充空格） 点击 字段 页签，获取字段，并设置最小宽度 运行转换 保存配置，点击 执行 按钮。 执行结果： 查看HDFS文件 读取HDFS文件 以HDFS -> Excel为例 新建转换，保存为hdfs2excel.ktr 添加转换步骤 在 核心对象 页签下，拖动 Big Data -> Hadoop File Input 和 输出 -> Microsoft Excel 输出，两个步骤到工作区，并连接这两个步骤。 修改 Hadoop File Input配置 双击 Hadoop File Input 步骤，文件 页签，选择待导出的文件，文件类型支持CSV（txt也可以）和Fixed（固定列宽） 点击 内容 页签，选择文件类型、分隔符、编码方式等 点击 字段 页签，获取字段 点击 确定 按钮，保存配置 修改Microsoft Excel输出配置 双击 Microsoft Excel 输出 步骤，选择文件保存位置和文件名 点击 字段 页签，获取字段 运行转换 保存配置，点击 执行 按钮，启动转换 执行结果 查看导出的excel文件 对接HBase 导入HBASE文件 以本地文件 -> HBase为例 添加转换步骤 在 核心对象 页签下，拖动 输入 -> 文本文件输入 ，和 Big Data -> HBase Output 两个步骤到工作区，并连接这两个步骤。 文本文件输入配置参考上面章节配置，注意在集群HBase中要有和导入的表相同的空表，指明字段和列簇. 修改 HBase Output 配置 双击 HBase Output 步骤，在 Configure connection 页签下，选择已经配置好的Hadoop集群，点击Get table name，获取要输出的表,点击Get mapping for specified table获取该表对应的mapping. 若该表没有创建mapping,在 Create/Edit Mappings 页签创建mapping,指定各项属性 点击 确定 ，保存配置 运行转换 保存配置，点击 执行 按钮,启动转换 执行结果： 查看集群中的HBase文件 执行 hbase shell count 'customer' 读取HBASE文件 以HBase -> 文本文件为例 新建转换，保存为hbase.ktr 添加转换步骤 在 核心对象 页签下，拖动 Big Data -> HBase Input 和 输出 -> 文本文件输出，两个步骤到工作区，并连接这两个步骤。 修改 HBase Input配置 双击 HBase Input 步骤，在 Configure query 页签，选择已经连接好的Hadoop集群，若无已经连接的集群，点击new,参照上面章节Hadoop集群配置，配置连接集群; 在 Create/Edit Mappings 页签，点击Get table names,获取集群中的Hbase表，选择要读取的表，在Mapping name下拉选择与该表关联的map，若没有，自定义一个map的名字，填写字段和列簇，并指定字段是否为key，字段类型. 回到 Configure query 页签,点击Get mapped table names,选择要读取的表，点击Get mappings for the specified table获取该表对应的mapping，点击右下角Get Key/Feilds Info，获取对应的表的信息. 点击 确定 按钮，保存配置 修改文本文件输出配置 双击 文本文件输出 步骤，在 文件 页签，填写文件名和扩展名; 点击 字段 页签，点击获取字段,设置最小宽度(可选) 点击 确定 按钮，保存配置 运行转换 保存配置，点击 执行 按钮，启动转换 执行结果 查看导出的文件 Linux平台 环境准备 安装操作系统 安装RedHat 6.5 禁用防火墙，SELinux 添加本地主机名解析 使用vi /etc/hosts添加本地主机名解析 节点IP host1 节点IP host2 节点IP host3 若是桌面版操作系统，Kettle对接参照上面章节Windows系统下的对接方式. 参考上面章节，在有图形界面的操作系统下，配置好Kettle与Fi集群的连接，测试连通性,将Kettle的data-integration目录以及其下所有文件上传至Linux系统的opt目录下. 安装FusionInsight HD客户端 下载完整客户端，安装至目录/opt/hadoopclient 使用vi /etc/profile编辑以下内容插入到文件末尾 source /opt/hadoopclient/bigdata_env 将krb5.conf放在/etc目录下 cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Hive对接 导出Hive表 以Hive->文本文件为例 在有图形界面的操作系统中，新建一个转换，在工作区中放入 表输入 和 文本文件输出 ，保存为hive.ktr; 点击表输入，参考上面章节中关于Hive连接的配置，只需修改连接选项中user.keytab文件所在路径，修改为/etc/user.keytab 将hive.ktr转换文件上传至Linux系统下Kettle的data-integration文件夹下，执行以下脚本清除cache（参见FAQ1） sed -i \"s/^org.pentaho\\.clean\\.karaf\\.cache=false/org\\.pentaho\\.clean\\.karaf\\.cache=true/g\" /opt/data-integration/system/karaf/etc/custom.properties 可将其保存为脚本文件，每次执行命令前先执行该脚本 根据Kettle版本执行以下命令 cd /opt/data-integration/ 对于Kettle-8.0版本,执行 ./kitchen.sh -file=hive.ktr 对于Kettle-8.1版本,执行 ./pan.sh -file=hive.ktr 执行结果如下 导出的表在data-integration/目录下 上传文件至Hive 同Windows操作系统下创建ktr文件操作，在选择需要上传的文件时，修改本地文件的路径，在Hive连接选项配置修改中user.keytab文件的路径为/etc/user.keytab即可，将ktr文件置于Linux系统中data-integration文件夹下，执行命令同上小节中操作。 HDFS & HBase文件输出 将上面章节创建的ktr转换文件上传至Linux系统下Kettle的data-integration文件夹下，根据Kettle版本执行命令(同hive)即可 执行结果如下 导出的表在data-integration/目录下 上传文件至HDFS & HBase 同上传文件至Hive操作，修改本地文件路径即可。 FAQ 1.在Linux系统中，每执行一次转换或者任务，Kettle都会生成一些Cache文件，在执行下一次转换/任务之前，需要清除这些Cache，否在HDFS Hive 和HBase进行连接传输时会出错 "},"Data_Integration/Using_Nifi_1.7.1_with_FusionInsight_HD_C80spc200.html":{"url":"Data_Integration/Using_Nifi_1.7.1_with_FusionInsight_HD_C80spc200.html","title":"对接Apache NiFi","keywords":"","body":"Apache NiFi对接FusionInsight 适用场景 Apache NiFi 1.7.1 FusionInsight HD V100R002C80SPC200 安装Apache NiFi 操作场景 安装Apache NiFi 1.7.1 前提条件 已完成FusionInsight HD和客户端的安装。 操作步骤 执行source命令到客户端，获取java配置信息 source /opt/hadoopclient/bigdata_env echo $JAVA_HOME 安装NiFi，在网址https://nifi.apache.org/download.html下载安装包，使用WinSCP导入主机并用命令unzip nifi-1.7.1-bin.zip解压安装生成nifi-1.7.1目录，安装目录为/usr/nifi/nifi-1.7.1 执行vi /usr/nifi/nifi-1.7.1/conf/nifi.properties配置NiFi服务器ip和端口如下： nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 启动和停止NiFi cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop 运行NiFi bin/nifi.sh start NiFi配置Kerberos认证 操作场景 NiFi配置并保存Kerberos认证信息，供以后使用 前提条件 已经完成Nifi 1.7.1的安装 已完成FusionInsight HD和客户端的安装并创建测试用户developuser (参考产品文档->应用开发指南->安全模式->安全认证) 操作步骤 在FusionInsight HD Manager上下载认证用户的配置文件user.keytab，krb5.conf，并一起存入路径/opt/developuser 执行命令vi /usr/nifi/nifi-1.7.1/conf/nifi.properties配置Kerberos认证 具体配置： nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab 登录NiFi网页界面，右键选择Configure 点击加号按钮添加服务 选择KeytabCredentialsService，点击ADD添加 点击齿轮图标进行配置 点击闪电图标生效并保存KeytabCredentialsService 完成 NiFi连接HDFS 操作场景 NiFi中配置HDFS相关处理器，对接HDFS 前提条件 已经完成NiFi 1.7.1的安装 已完成FusionInsight HD和客户端的安装，包含HDFS组件 已完成 NiFi Kerberos认证配置 PutHDFS 操作步骤 将FusionInsight HD客户端中关于HDFS的配置文件hdfs-site.xml，core-site.xml导入路径/usr/nifi/nifi-1.7.1/conf 修改hdfs-site.xml内容，删除如下配置项 dfs.client.failover.proxy.provider.hacluster org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider 修改core-site.xml内容，修改如下配置项中hacluster改为节点ip加端口号 fs.defaultFS hdfs://172.21.3.102:25000 整个过程的流程图所示： 处理器GetFile的配置如下： 具体配置： 1: /home/dataset 处理器PutHDFS配置如下 具体配置： 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: 选择NiFi配置Kerberos认证这一节中创建的 KeytabCredentialsService 3: /tmp/nifitest 两个处理器的连接配置如下： 测试前将测试文件nifiHDFS.csv放入路径/home/dataset 文件内容如下： 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq 测试后 登录集群HDFS文件系统查看测试结果 hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS 操作步骤 整个过程的流程如图所示： 处理器GetHDFS配置如下 具体配置： 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: 选择NiFi配置Kerberos认证这一节中创建的 KeytabCredentialsService 3: /tmp/nifitest/HDFS 处理器PutFile配置如下 具体配置： 1: /home/dataset/HDFS 测试前将测试文件nifiHDFS.csv放到HDFS文件系统的/tmp/nifitest/HDFS路径下 测试后 登录安装 FusionInsight HD客户端主机路径/home/dataset/HDFS查看结果 ListHDFS & FetchHDFS 操作步骤 整个过程的流程如图所示： 处理器ListHDFS配置如下： 具体配置为： 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest 处理器RouteOnAttribute具体配置如下： 注意：需要点击加号图标增加一条配置，Property 配置为requiredfilenames，Value 配置为${filename:matches('sanguo.*')} 具体配置为： 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} 处理器RouteOnAttribute和上、下处理器FetchHDFS的连接配置分别对应为requiredfilenames和unmatched，如图： 两个处理器FetchHDFS的配置如下： 具体配置为： 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 上、下处理器PutFile配置分别如下： 测试前，执行命令hdfs dfs -ls /tmp/nifitest登录集群HDFS文件系统/tmp/nifitest查看文件 测试后 登录FusionInsight HD客户端主机路径/home/dataset/HDFS/matchedFiles和/home/dataset/HDFS/unmatchedFiles分别查看结果： NiFi连接Hive 操作场景 NiFi中配置JDBC解析器，对的FI HD Hive接口 前提条件 已经完成NiFi 1.7.1的安装 已完成FusionInsight HD和客户端的安装，包含Hive组件 已完成 NiFi Kerberos认证配置 HiveConnectionPool 配置操作步骤 登录NiFi网页界面，右键选择Configure 点击加号按钮添加服务 选择HiveConnectionPool，点击ADD添加 点击齿轮图标进行配置 具体配置为 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService 点击闪电图标点击闪电图标生效并保存HiveConnectionPool 完成 在路径/usr/nifi/nifi-1.7.1/conf下创建jaas.conf文件内容如下： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; 执行命令vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf配置bootstrap.conf文件如下: java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true 执行命令vi /usr/nifi/nifi-1.7.1/conf/nifi.properties配置nifi.properties文件如下： nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true 执行命令cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies到NiFi Hive类库中，将原有的zookeeper-3.4.6.jar替换为FusionInsight HD客户端中的zookeeper-3.5.1.jar SelectHiveQL 读取Hive表 操作步骤 整个过程的流程如图所示： 处理器SelectHiveQL配置如下： 具体配置如下： 1: HiveConnectionPool 2: select * from default.t2 3. CSV 处理器PutFile配置如下： 运行前登录集群查看hive表t2: 运行后： 登录路径/home/dataset/HIVE查看结果： PutHiveQL 整表导入 操作步骤 整个过程的流程如图所示： 处理器GetFile的配置如下 具体配置如下： 1： /home/dataset/ 2: iris.txt 数据文件iris.txt的内容如下: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa 处理器PutHDFS的配置如下 具体配置如下： 1： /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2： KeytabCredentialsService 3: /tmp/nifitest/loadhive 处理器ReplaceText配置如下 具体配置如下： 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; 处理器PutHiveQL配置如下 运行前将数据文件iris.txt导入路径/home/dataset/ 运行后： 登录HIVE查看测试结果: PutHiveQL 单行导入 操作步骤 整个过程的流程如图所示： 处理器GetFile的配置如下: 具体配置如下： 1： /home/dataset/ 2： iris_add.txt 数据文件iris_add.txt的内容如下: \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" 处理器SplitText配置如下： 处理器ExtractText配置保持默认配置 处理器ReplaceText配置如下: 处理器PutHiveQL配置如下: 运行前将数据文件iris_add.txt导入路径/home/dataset/ 运行后： 登录HIVE查看测试结果： NiFi连接HBase 操作场景 NiFi中配置HBase解析器，对的FI HD HBase接口 前提条件 已经完成NiFi 1.7.1的安装 已完成FusionInsight HD和客户端的安装，包含HBase组件 已完成 NiFi Kerberos认证配置 HBase_1_1_2_ClientService 配置操作步骤 将FusionInsight HD客户端中关于HBase的配置文件hbase-site.xml导入路径/usr/nifi/nifi-1.7.1/conf 更换路径/usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies下面的zookeeper-3.4.6.jar为FusionInsight HD客户端自带的zookeeper-3.5.1.jar 登录NiFi网页界面，右键选择Configure 点击加号按钮添加服务 选择HBase_1_1_2_ClientService，点击ADD添加 点击齿轮图标进行配置 具体配置如下： 1： /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2： KeytabCredentialsService 点击闪电图标点击闪电图标生效并保存HiveConnectionPool 完成 PutHBaseJSON 向HBase导入表 整个过程的流程如图所示： 处理器GetFile的配置如下: 数据文件hbase_test.csv的内容如下: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica 处理器InverAvroSchema配置如下： 具体配置如下： 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data 处理器ConvertCSVToAvro配置如下: 处理器ConvertAvroToJSON配置如下： 处理器SplitJson配置如下： 处理器PutHBaseJSON配置如下： 具体配置如下: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data 测试前需要将数据文件hbase_test.csv导入路径/home/dataset/HBASE 并且需要在集群里面建一个hbase表，执行命令 hbase shell create 'HBase_test','data' 运行后： 登录集群查看结果： GetHbase 操作步骤 整个过程的流程如图所示： 驱动器GetHBase的配置如下： 驱动器PutFile的配置如下： 测试后 登录到路径/home/dataset/GetHBase_test查看结果： NiFi连接Spark 操作场景 NiFi中配置Livy解析器，对的FI HD HBase接口 前提条件 已经完成NiFi 1.7.1的安装 已完成FusionInsight HD和客户端的安装，包含Spark2x组件 已完成 NiFi Kerberos认证配置 已完成Apache Livy 0.5.0的安装 （Livy可安装在FI HD客户端主机，也可以安装在其他主机但是需要保证安装Livy主机能够和FI HD客户端主机以及集群网络互通） 可参考《Apache Livy对接FusionInsight》对接文档完成Apache Livy的安装 配置LivySessionController操作步骤 登录NiFi网页界面，右键选择Configure 点击加号按钮添加服务 选择LivySessionController，点击ADD添加 点击齿轮图标进行配置 具体配置如下： 1: 172.21.3.43 (已安装Apache Livy的主机ip) 2: 8998 (Livy默认端口，可更改) 3: spark 4：KeytabCredentialsService 继续点击加号按钮添加服务 选择LivySessionController，点击ADD添加 点击齿轮图标进行配置 更改Controller名字为 LivySessionController_PySpark 具体配置如下： 1: 172.21.3.43 (已安装Apache Livy的主机ip) 2: 8998 (Livy默认端口，可更改) 3: pysaprk 4：KeytabCredentialsService 继续点击加号按钮添加服务 选择LivySessionController，点击ADD添加 点击齿轮图标进行配置 更改Controller名字为 LivySessionController_SparkR 具体配置如下： 1: 172.21.3.43 (已安装Apache Livy的主机ip) 2: 8998 (Livy默认端口，可更改) 3: sparkr 4：KeytabCredentialsService 点击闪电图标选择Service and referencing components生效并保存LivySessionController,LivySessionController_PySpark,LivySessionController_SparkR 完成 运行Spark样例操作步骤 整个过程的流程如图所示： 处理器GetFile配置如下： 具体配置如下： 1: /home/dataset/sparkTest 2: code1.txt 代码内容文件code1.txt的内容如下： 1+2 处理器ExtractText配置如下： 需要点击加号按钮，Property项命名为code1，Value项赋值为$ 处理器ExecuteSparkInteractive配置为： 具体配置如下： 1: LivySessionController 2: ${code1} 测试前将代码文件code1.txt上传至安装nifi主机的路径/home/dataset/sparkTest下： 在已安装Livy的主机上启动Livy： 测试后： 登录Livy sever查看测试结果 运行PySpark样例操作步骤 整个过程的流程如图所示： 处理器GetFile配置如下： 具体配置如下： 1: /home/dataset/sparkTest 2: code2.txt 代码内容文件code2.txt的内容如下： import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y 处理器ExtractText配置如下： 需要点击加号按钮，Property项命名为code2，Value项赋值为$ 处理器ExecuteSparkInteractive配置为： 具体配置如下： 1: LivySessionController_PySpark 2: ${code2} 测试前将代码文件code1.txt上传至安装nifi主机的路径/home/dataset/sparkTest下： 在已安装Livy的主机上启动Livy 测试后 登录Livy sever查看测试结果 运行SparkR样例操作步骤 整个过程的流程如图所示： 注意：在测试过程中如果与Spark，PySpark样例不完全一样 处理器GetFile配置如下： 具体配置如下： 1: /home/dataset/sparkTest 2: code3.txt 代码内容文件code3.txt的内容如下： piR 处理器ExecuteSparkInteractive配置为： 具体配置如下： 1: /home/dataset/sparkTest 2: code3.txt里的代码内容 测试前将代码文件code3.txt上传至安装nifi主机的路径/home/dataset/sparkTest下： 在已安装Livy的主机上启动Livy 测试后 登录Livy sever查看测试结果 NiFi连接Kafka 操作场景 NiFi中配置kafka解析器，对的FI HD kafka接口 前提条件 已经完成NiFi 1.7.1的安装 已完成FusionInsight HD和客户端的安装，包含kafka组件 已完成 NiFi Kerberos认证配置 GetHTTP & PutKafka 操作步骤 整个过程的流程如图所示： 驱动器GetHTTP的配置如下： 具体配置如下： 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv 驱动器PutKafka配置如下： 具体配置如下： 1： 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2： nifi-kafka-test-demo 3： nifi 测试前： 登录FI客户端kafak组件，创建Topic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 测试后： 登录FI客户端kafak组件，查看结果： cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning ConsumeKafka_0_11 操作步骤 整个过程的流程如图所示： 处理器ConsumeKafka_0_11配置如下： 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer 处理器PutFile配置如下： 测试前： 用eclipse打开客户端自带的kafka样例代码kafka-examples，调试使得样例代码能够正常运行NewProducer.java 由于ConsumeKafka是实时获取日志信息的，所以在测试的时候需要先运行NewProducer.java往Kafka上传日志文件，再同时开启nifi的驱动器ConsumeKafka_0_11进行读取日志的测试 测试后： 登录路径/home/dataset/Kafka查看测试结果： "},"Integrated_Development_Environment/":{"url":"Integrated_Development_Environment/","title":"集成开发环境","keywords":"","body":" 集成开发环境 R语言 对接RStudio Notebook类开发IDE 对接Apache Zeppelin Zeppelin0.7.2 FusionInsight HD V100R002C60U20 Zeppelin0.7.3 FusionInsight HD V100R002C70SPC100 Zeppelin0.8.0 FusionInsight HD V100R002C80SPC200 对接Jupyter Notebook SQL类开发IDE 对接DBeaver 对接DbVisualizer 对接Squirrel "},"Integrated_Development_Environment/Using_RStudio_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_RStudio_with_FusionInsight.html","title":"对接RStudio","keywords":"","body":"RSutdio对接FusionInsight Spark 适用场景 R-3.4.1 FusionInsight HD V100R002C60U10 R-3.4.1 FusionInsight HD V100R002C70SPC100 对接方式 RStudio与Spark集成有两种方式： 通过RStudio官方发布的sparklyr与Spark进行集成 通过Apache Spark社区发布的SparkR进行集成 本文档包含了两种方式对接的步骤, 相关对接步骤如下： 安装R 安装RStudio Server 安装FusionInsight客户端 使用SparkR与RStudio集成进行分析 在RStudio中使用SparkR进行数据分析 使用RStudio Sparklyr和Spark集成进行分析 使用sparklyr结合spark进行数据分析babynames数据集 使用sparklyr结合spark进行数据分析航空公司飞行数据(必须配套Spark2x) 安装R 由于Spark的Executor上也需要执行R，所以除了在RStudio的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 本文使用的RStudio节点为Redhat7.1，FusionInsight集群节点为Redhat6.6 配置redhat的yum源，国内可以配置aliyun的源或者163的源 配置EPEL的源 安装R-3.4.1 配置aliyun的源 配置好Redhat7.1的yum源 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache 配置163的源 配置好Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 配置EPEL的源 安装EPEL源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm 更新cache yum clean all yum makecache 安装R-3.4.1 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 安装RStudio Server 下载并安装RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm 使用 vi /etc/rstudio/rserver.conf 修改RStudio的配置文件，指定RStudio Server使用的R的路径 rsession-which-r=/usr/bin/R 重启rstudio-server后，查看服务是否正常 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server 服务正常启动如下 由于RStudio Server不允许使用root用户登陆，需要新建一个普通用户用于Web界面的登陆 useradd -d /home/test -m test passwd test 用户新建完成后，关闭防火墙，然后使用本机ip:8787端口访问RStudio Server，使用新建的test用户登陆即可进入RStudio的Web开发界面 sudo systemctl stop firewalld 安装FusionInsight客户端 登录FusionInsight Manager系统，单击 服务管理 ，在菜单栏中单击 下载客户端, 客户端类型勾选 完整客户端, 是否在集群的节点中生成客户端文件选择 否 使用WinSCP工具将下载下来的软件包上传到Linux服务器的目录，例如 /tmp/client 切换到新建的test用户 su test 解压软件包。进入安装包所在目录，例如 /tmp/client 。执行如下命令解压安装包到本地目录 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar 进入安装包所在目录，执行如下命令安装客户端到指定目录（绝对路径），例如安装到 /home/test/hadoopclient 目录 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient 客户端将被安装到 /home/test/hadoopclient 目录中 检查客户端节点与FusionInsight集群时间同步（差距不能超过5分钟） 检查SparkR是否可用 使用sparkuser进行Kerberos认证(sparkuser为FusionInsight中创建的拥有Spark访问权限的人机用户) cd /home/test/hadoopclient source bigdata_env kinit sparkuser 执行sparkR启动SparkR, 正常启动出现以下界面 使用SparkR与RStudio集成进行分析 使用新建的用户登陆即可进入RStudio的Web开发界面 选择 Tools 菜单下的 Shell 进入登陆用户的shell进行kerberos认证 cd /home/test/hadoopclient source bigdata_env kinit sparkuser 在RStudio界面中配置环境变量，初始化SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc 初始化成功后如下图 在Yarn的ResourceManager界面可以看到sparkuser在集群启动了一个SparkR的应用 在RStudio中使用SparkR进行数据分析 R DataFrame 转化为SparkR DataFrame df 通过JSON文件加载数据进行分析处理 将测试数据put到HDFS中 wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ 执行文件加载分析 people 从Hive表中加载数据进行分析 hiveContext DataFrame Operations Selecting rows, columns df Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts Operating on Columns df$waiting_secs Running SQL Queries from SparkR people = 13 AND age Machine Learning df 使用RStudio Sparklyr和Spark集成进行分析 选择 Tools 菜单下的 Shell 进入登陆用户的shell进行kerberos认证 cd /home/test/hadoopclient source bigdata_env kinit sparkuser 在RStudio中执行下面的命令，安装所需的library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") 通过spark_connect连接spark集群 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc 这里如果SPARK_HOME指向/home/test/hadoopclient/Spark/spark，同时设置version为1.6.1，则会对接上1.5.1的Spark sparklyr官方支持是1.6.1以上的Spark，这里强制指定version为1.6.1，主要功能均正常，部分Spark1.6.1支持而1.5.1不支持的特性执行会失败 启动成功后，在FusionInsgiht的Yarn的ResourceManager页面可以看到sparklyr的任务已经启动 在RStudio的Spark面板刷新一下，可以看到所有hive的表 选择hive表右边的数据图表可以预览表中的数据 使用sparklyr结合spark进行数据分析babynames数据集 Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl % mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl % filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl % filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl % group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl % filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank % arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly % inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl % filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank % arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly % inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName % mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE) 使用sparklyr结合spark进行数据分析航空公司飞行数据(必须配套Spark2x) Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data % filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay % filter(arrdelay > -60 & arrdelay % filter(year >= 2003 & year % left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition % sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 % ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) Assess model performance Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles % mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles % ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 % filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay % filter(arrdelay > -60 & arrdelay % filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier % group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights. FAQ FusionInsight集群不允许访问网络，如何安装R 在集群外同版本的Redhat版本下按照配置EPEL的源安装R进行操作，最后一步不要执行yum install R 执行yum install yum-utils安装yumdownloader 执行yumdownloader R --resolve --destdir=/tmp/packages把所有的rpm安装包下载到/tmp/packages中 将/tmp/packages中的所有rpm包复制到集群每个节点的/tmp/packages中 切换到集群每个节点的/tmp/packages中，执行yum localinstall *.rpm完成安装 安装sparklyr报错configuration failed for package ‘openssl’ 操作系统需要执行yum install openssl-devel安装openssl-devel 如何获取本文中使用sparklyr分析的源数据 执行以下shell脚本获取待分析的数据 # Make download directory mkdir /tmp/flights # Download flight data by year for i in {2006..2008} do echo \"$(date) $i Download\" fnam=$i.csv.bz2 wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam echo \"$(date) $i Unzip\" bunzip2 /tmp/flights/$fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat 将下载下来的/tmp/flights目录以及/tmp/airlines.csv，/tmp/airports.csv文件上传到HDFS的/user/sparkuser目录中，然后在Hive中创建三张表，将数据加载到对应的表中 hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int, month int, dayofmonth int, dayofweek int, deptime int, crsdeptime int, arrtime int, crsarrtime int, uniquecarrier string, flightnum int, tailnum string, actualelapsedtime int, crselapsedtime int, airtime string, arrdelay int, depdelay int, origin string, dest string, distance int, taxiin string, taxiout string, cancelled int, cancellationcode string, diverted int, carrierdelay string, weatherdelay string, nasdelay string, securitydelay string, lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES(\"skip.header.line.count\"=\"1\"); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string, Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,', \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties(\"skip.header.line.count\"=\"1\"); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string, name string, city string, country string, faa string, icao string, lat double, lon double, alt int, tz_offset double, dst string, tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,', \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports; "},"Integrated_Development_Environment/Using_Zeppelin_with_FusionInsight_HD.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_with_FusionInsight_HD.html","title":"对接Apache Zeppelin","keywords":"","body":"Zeppelin对接FusionInsight HD Zeppelin0.7.2 FusionInsight HD V100R002C60U20 Zeppelin0.7.3 FusionInsight HD V100R002C70SPC100 Zeppelin0.8.0 FusionInsight HD V100R002C80SPC200 "},"Integrated_Development_Environment/Using_Zeppelin_0.7.2_with_FusionInsight_HD_C60U20.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_0.7.2_with_FusionInsight_HD_C60U20.html","title":"Zeppelin0.7.2 <-> FusionInsight HD V100R002C60U20","keywords":"","body":"Zeppelin对接FusionInsight HD 适用场景 Zeppelin 0.7.2 FusionInsight HD V100R002C60U20 安装Zeppelin 操作场景 安装Zeppelin0.7.2 前提条件 已完成FusionInsight HD客户端的安装。 操作步骤 将软件包zeppelin-0.7.2-bin-all.tgz上传至/opt目录下，解压生成zeppelin-0.7.2-bin-all目录。 tar -zxvf zeppelin-0.7.2-bin-all.tgz 启动和停止Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop 配置Zeppelin环境变量，在profile文件中加入如下变量 vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh 加入如下内容： export JAVA_HOME=/opt/jdk1.7.0_51/ 编辑zeppelin-site.xml文件，位置/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml 将zeppelin-site.xml中端口8080替换成18081（可自定义，也可以不改） sed -i 's/8080/18081/' zeppelin-site.xml 运行zeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start 在浏览器中输入地址zeppelin_ip:18081登陆，zeppelin_ip为安装zeppelin的虚拟机IP。 根据产品文档创建用户test，并赋予足够权限，下载用户test的keytab文件user.keytab，上传至/opt/目录下。 编辑zeppelin-site.xml文件，将zeppelin.anonymous.allowed参数的true修改为false。 编辑shiro.ini文件，位置/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc表示对任何url访问都需要验证 [users]下增加用户test，密码Huawei@123 重启zeppelin。 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart 使用test用户登陆Zeppelin Zeppelin连接Hive 操作场景 Zeppelin中配置JDBC解析器，对接Hive的JDBC接口。 前提条件 已经完成Zeppelin 0.7.2的安装； 已完成FusionInsight HD客户端的安装，包含Hive组件。 操作步骤 将/opt/hadoopclient/Hive/Beeline/lib/下的jar包拷贝至/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/目录下。 将从新拷贝过来的jar包的属主和权限修改为和/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/下原有的jar包相同 chown 501:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.2-bin-all/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择JDBC，点击 edit 编辑，修改default.driver和default.url参数，点击 save 保存 default.driver：org.apache.hive.jdbc.HiveDriver default.url：jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hive 编辑note，点击右侧“执行”按钮。 %jdbc Show tables; Select * from workers_info; 查看结果 Zeppelin连接HBase 操作场景 Zeppelin中配置Hbase解析器，对接Hbase 前提条件 已经完成Zeppelin 0.7.2的安装； 已完成FusionInsight HD客户端的安装，包含HBase组件。 操作步骤 将/opt/hadoopclient/HBase/hbase/lib/以下的jar包拷贝至/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/目录下，overwrite选择n 在/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/下新建目录zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar 将/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/下与FusionInsight冲突的38个jar包移动到zeppelin_hbase_jar目录中 commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar 最终/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/有152个jar包 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.2-bin-all/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择hbase，点击 edit 编辑，修改hbase.home参数，点击 save 保存 hbase.home：/opt/hadoopclient/HBase/hbase 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hbase 编辑note，点击右侧 执行 按钮 %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' 在FusionInsight的客户端下可以看到创建的hbase表test2和数据 Zeppelin连接Spark 操作场景 Zeppelin中配置Spark解析器 前提条件 完成Zeppelin0.7.2的安装； 已完成FusionInsight HD V100R002C60U20和客户端的安装，包含Spark组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 将/opt/zeppelin-0.7.2-bin-all/lib/目录下的原有的相关的jar包删除 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar 将/opt/hadoopclient/Spark/adapter/dev_lib/下的以下jar包拷贝到/opt/zeppelin-0.7.2-bin-all/lib/目录下 hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar 将/opt/zeppelin-0.7.2-bin-all/lib/下的jackson的相关jar包删除 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar 将/opt/hadoopclient/Spark/adapter/dev_lib/下的jackson相关的jar包拷贝到/opt/zeppelin-0.7.2-bin-all/lib/下 jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar 将步骤1和步骤2所有从spark客户端拷贝过来的jar包的属主和权限修改为和/opt/zeppelin-0.7.2-bin-all/lib/下原有的jar包相同 chown 501:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf，加入以下内容 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择Spark，点击 edit 编辑，将 Master 参数改为 yarn-client，点击 save 保存 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart 执行zeppelin的spark样例代码zeppelin Tutorial -> Basic Features(Spark) 样例代码需要访问Internet上的资源，所以保证zeppelin所在的节点可以联网，检测是否能打开以下链接 执行zeppelin的spark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) 安装python-matplotlib yum install python-matplotlib 安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh 配置环境变量PATH，将python换成安装Anaconda安装目录中的python export PATH=/root/anaconda2/bin/:$PATH 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.pyspark.python 参数改为Anaconda安装目录中的python，点击 save 保存 执行zeppelin的pyspark样例代码Zeppelin Tutorial -> Matplotlib Zeppelin连接SparkR 操作场景 Zeppelin中配置Spark解析器，连接SparkR 前提条件 完成Zeppelin0.7.2的安装； 已完成FusionInsight HD V100R002C60U20和客户端的安装，包含Spark组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 由于Spark的Executor上也需要执行R，所以除了在Zeppelin的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 不同OS配置yum源时下载的文件路径有所不同，下面以Redhat6.6安装R为例 如果安装R的节点无法访问互联网，参考FAQ进行R的安装 配置Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 配置EPEL的源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm 更新cache yum clean all yum makecache 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 FusionInsight客户端下测试是否可以使用sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR 正常启动如下图所示 参考http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter 在R的命令行中安装sparkR样例需要的R的libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.R.cmd 参数改为R的启动文件，点击 save 保存 重启zeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart 在Zeppelin中执行Zeppelin Tutorial -> R (SparkR)样例 FAQ FusionInsight集群不允许访问网络，如何安装R 在集群外同版本的Redhat版本下按照本文中yum源的方式进行安装R的操作，最后一步不要执行yum install R 执行yum install yum-utils安装yumdownloader 执行yumdownloader R --resolve --destdir=/tmp/packages把所有的rpm安装包下载到/tmp/packages中 将/tmp/packages中的所有rpm包复制到集群每个节点的/tmp/packages中 切换到集群每个节点的/tmp/packages中，执行yum localinstall *.rpm完成安装 连接hbase出现AuthFialed for /hwbackup/hbase 原因：zeppelin的原理hbase的jar包与从FusionInsight客户端下拷贝过来的jar冲突。 解决：将zeppelin中原有的重名jar包移走或删除，全部用FusionInsight客户端下的相关jar包。 Zeppelin连接spark是报如下NoSuchMethodError 原因：jar包冲突 解决：删除/opt/zeppelin-0.7.2-bin-all/lib/下原有jar包scala-reflect-2.11.7.jar，替换为FusionInsight客户端下的jar包，重启zeppelin Zeppelin执行Spark样例代码时报GC overhead limit exceeded 原因：内存不够 解决：安装Zeppelin的节点的内存需要16G以上 执行zeppelin的样例代码Zeppelin Tutorial/Matplotlib (Python PySpark)报如下错误 原因：python版本问题 解决：安装Anaconda2-4.4 "},"Integrated_Development_Environment/Using_Zeppelin_0.7.3_with_FusionInsight_HD_C70SPC100.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_0.7.3_with_FusionInsight_HD_C70SPC100.html","title":"Zeppelin0.7.3 <-> FusionInsight HD V100R002C70SPC100","keywords":"","body":"Zeppelin对接FusionInsight HD 适用场景 Zeppelin 0.7.3 FusionInsight HD V100R002C70SPC100 (Spark2.x) Zeppelin 0.7.3 FusionInsight HD V100R002C80SPC200 (Spark2.x) 编译Zeppelin 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v 安装git yum install -y git 安装nodejs： wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin 导入环境变量 source /etc/profile 执行npm -v 安装bower npm install -g bower 配置bower允许root用户执行 echo '{ \"allow_root\": true }' > /root/.bowerrc 执行bower -v 获取Zeppelin0.7.3的版本 git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 修改scala版本，适配FusionInsight_HD_V100R002C70SPC100的Hadoop版本 在zeppelin代码根目录执行vi ./dev/change_scala_version.sh，修改下图的SCALA_LIB_VERSION为2.11.8 执行命令完成scala版本的修改 ./dev/change_scala_version.sh 2.11 执行vi pom.xml文件的修改为0.9.3 执行vi hbase/pom.xml修改hbase版本和hadoop版本 编译Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests 编译完成后在zeppelin-distribution/target目录下生成zeppelin-0.7.3.tar.gz文件 安装Zeppelin 操作场景 安装Zeppelin0.7.3 前提条件 已完成FusionInsight HD客户端的安装。 操作步骤 将编译好的zeppelin-0.7.3.tar.gz上传放到/opt目录下，解压生成zeppelin-0.7.3目录。 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz 配置Zeppelin环境变量，在profile文件中加入如下变量 vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh 加入如下内容： export JAVA_HOME=/opt/hadoopclient/JDK/jdk 编辑zeppelin-site.xml文件，位置/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml 将zeppelin-site.xml中端口8080替换成18081（可自定义，也可以不改） sed -i 's/8080/18081/' zeppelin-site.xml 运行zeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start 在浏览器中输入地址zeppelin_ip:18081登陆，zeppelin_ip为安装zeppelin的虚拟机IP。 根据产品文档创建用户test，并赋予足够权限，下载用户test的keytab文件user.keytab，上传至/opt/目录下。 编辑zeppelin-site.xml文件，将zeppelin.anonymous.allowed参数的true修改为false。 编辑shiro.ini文件，位置/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc表示对任何url访问都需要验证 [users]下增加用户test，密码Huawei@123 重启zeppelin。 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart 使用test用户登陆Zeppelin Zeppelin连接Hive 操作场景 Zeppelin中配置JDBC解析器，对接Hive的JDBC接口。 前提条件 已经完成Zeppelin 0.7.3的安装； 已完成FusionInsight HD客户端的安装，包含Hive组件。 操作步骤 将/opt/hadoopclient/Hive/Beeline/lib/下的jar包拷贝至/opt/zeppelin-0.7.3/interpreter/jdbc/目录下。 将从新拷贝过来的jar包的属主和权限修改为和/opt/zeppelin-0.7.3/ interpreter/jdbc/下原有的jar包相同 chown 501:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.3/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择JDBC，点击 edit 编辑，修改default.driver和default.url参数，点击 save 保存 default.driver：org.apache.hive.jdbc.HiveDriver default.url：jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hive 编辑note，点击右侧“执行”按钮。 %jdbc Show tables; Select * from workers_info; 查看结果 Zeppelin连接HBase 操作场景 Zeppelin中配置Hbase解析器，对接Hbase 前提条件 已经完成Zeppelin 0.7.3的安装； 已完成FusionInsight HD客户端的安装，包含HBase组件。 操作步骤 将/opt/zeppelin-0.7.3/interpreter/hbase/目录下旧的jar包移走cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar 将/opt/hadoopclient/HBase/hbase/lib/以下的jar包拷贝至/opt/zeppelin-0.7.3/interpreter/hbase/目录下 cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.3/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择hbase，点击 edit 编辑，修改hbase.home参数，点击 save 保存 hbase.home：/opt/hadoopclient/HBase/hbase 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hbase 编辑note，点击右侧 执行 按钮 %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' 在FusionInsight的客户端下可以看到创建的hbase表test2和数据 Zeppelin连接Spark 操作场景 Zeppelin中配置Spark解析器 前提条件 完成Zeppelin0.7.3的安装； 已完成FusionInsight HD V100R002C70SPC100和客户端的安装，包含Spark2x组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下内容 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择Spark，点击 edit 编辑，将 Master 参数改为 yarn-client，点击 save 保存 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart 执行zeppelin的sparkSQL语句 执行zeppelin的spark样例代码zeppelin Tutorial -> Basic Features(Spark) 样例代码需要访问Internet上的资源，所以保证zeppelin所在的节点可以联网，检测是否能打开以下链接 执行zeppelin的spark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) 安装python-matplotlib yum install python-matplotlib 安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh 配置环境变量PATH，将python换成安装Anaconda安装目录中的python export PATH=/root/anaconda2/bin/:$PATH 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.pyspark.python 参数改为Anaconda安装目录中的python，点击 save 保存 执行zeppelin的pyspark样例代码Zeppelin Tutorial -> Matplotlib Zeppelin连接SparkR 操作场景 Zeppelin中配置Spark解析器，连接SparkR 前提条件 完成Zeppelin0.7.3的安装； 已完成FusionInsight HD V100R002C70SPC100和客户端的安装，包含Spark组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 由于Spark的Executor上也需要执行R，所以除了在Zeppelin的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 不同OS配置yum源时下载的文件路径有所不同，下面以Redhat6.6安装R为例 如果安装R的节点无法访问互联网，参考FAQ进行R的安装 配置Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 配置EPEL的源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm 更新cache yum clean all yum makecache 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 FusionInsight客户端下测试是否可以使用sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR 正常启动如下图所示 参考http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter 在R的命令行中安装sparkR样例需要的R的libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.R.cmd 参数改为R的启动文件，点击 save 保存 重启zeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart 在Zeppelin中执行Zeppelin Tutorial -> R (SparkR)样例 FAQ FusionInsight集群不允许访问网络，如何安装R 在集群外同版本的Redhat版本下按照本文中yum源的方式进行安装R的操作，最后一步不要执行yum install R 执行yum install yum-utils安装yumdownloader 执行yumdownloader R --resolve --destdir=/tmp/packages把所有的rpm安装包下载到/tmp/packages中 将/tmp/packages中的所有rpm包复制到集群每个节点的/tmp/packages中 切换到集群每个节点的/tmp/packages中，执行yum localinstall *.rpm完成安装 连接hbase出现AuthFialed for /hwbackup/hbase 原因：zeppelin的原理hbase的jar包与从FusionInsight客户端下拷贝过来的jar冲突。 解决：将zeppelin中原有的重名jar包移走或删除，全部用FusionInsight客户端下的相关jar包。 Zeppelin连接spark是报如下NoSuchMethodError 原因：jar包冲突 解决：删除/opt/zeppelin-0.7.3/lib/下原有jar包scala-reflect-2.11.7.jar，替换为FusionInsight客户端下的jar包，重启zeppelin Zeppelin执行Spark样例代码时报GC overhead limit exceeded 原因：内存不够 解决：安装Zeppelin的节点的内存需要16G以上 执行zeppelin的样例代码Zeppelin Tutorial/Matplotlib (Python PySpark)报如下错误 原因：python版本问题 解决：安装Anaconda2-4.4 "},"Integrated_Development_Environment/Using_Zeppelin_0.8.0_with_FusionInsight_HD_C80SPC200.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_0.8.0_with_FusionInsight_HD_C80SPC200.html","title":"Zeppelin0.8.0 <-> FusionInsight HD V100R002C80SPC200","keywords":"","body":"Zeppelin对接FusionInsight HD 适用场景 Zeppelin 0.8.0 FusionInsight HD V100R002C80SPC200 (Spark2.x) 安装Zeppelin 操作场景 安装Zeppelin0.8.0 前提条件 已完成FusionInsight HD和客户端的安装。 操作步骤 安装Zeppelin 0.8.0,在网址https://zeppelin.apache.org/download.html下载安装包，使用WinSCP导入主机并用tar -zxvf zeppelin-0.8.0-bin-all.tgz安装生成zeppelin-0.8.0-bin-all目录。 启动和停止Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop 执行source命令到客户端，获取java配置信息 source /opt/hadoopclient/bigdata_env echo $JAVA_HOME 配置Zeppelin环境变量，在profile文件中加入如下变量 vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH 编辑zeppelin-env.sh文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh 加入如下内容： export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 编辑zeppelin-site.xml文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml 将zeppelin-site.xml中端口8080替换成18081（可自定义，也可以不改） sed -i 's/8080/18081/' zeppelin-site.xml 运行zeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start 在浏览器中输入地址zeppelin_ip:18081登陆，zeppelin_ip为安装zeppelin的虚拟机IP 根据产品文档创建用户developuser，并赋予足够权限，下载用户developuser的keytab文件user.keytab，上传至/usr/zeppelin/zeppelin-0.8.0-bin-all目录下 编辑zeppelin-site.xml文件，将zeppelin.anonymous.allowed参数的true修改为false 编辑shiro.ini文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc表示对任何url访问都需要验证 [users]下增加用户developuser，密码Huawei@123，权限admin 重启zeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 使用账户developuser登陆zeppelin Zeppelin连接Hive 操作场景 Zeppelin中配置JDBC解析器，对接Hive的JDBC接口。 前提条件 已经完成Zeppelin 0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Hive组件。 操作步骤 将/opt/hadoopclient/Hive/Beeline/lib/下的jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/目录下。 将从新拷贝过来的jar包的属主和权限修改为和/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/下原有的jar包相同 chown 502:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/，加入以下三个配置内容export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 从FusionInsight客户端下载用户developuser的user.keytab和krb5.conf文件，将krb5.conf文件放在/opt/developuser/下 在/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/路径下新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的developuser用户，将developuser的keytab文件user.key放在/opt/developuser/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择JDBC，点击 edit 编辑，修改default.driver和default.url参数，点击 save 保存 default.driver：org.apache.hive.jdbc.HiveDriver default.url：jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hive_test 编辑note，点击右侧“执行”按钮。 %jdbc Show tables; %jdbc select * from t2 查看结果 Zeppelin连接HBase 操作场景 Zeppelin中配置Hbase解析器，对接Hbase 前提条件 已经完成Zeppelin 0.8.0的安装 已完成FusionInsight HD和客户端的安装，包含HBase组件 操作步骤 将/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/目录下旧的jar包移走cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar 将/opt/hadoopclient/HBase/hbase/lib/以下的jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/目录下 cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase 编辑zeppelin-env.sh文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase 从FusionInsight客户端下载用户developuser的user.keytab和krb5.conf文件，将krb5.conf文件放在/opt/developuser下 在/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/路径下新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的developuser用户，将developuser的keytab文件user.key放在/opt/developuser/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择hbase，点击 edit 编辑，修改hbase.home参数，点击 save 保存 hbase.home：/opt/hadoopclient/HBase/hbase 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hbase_test 编辑note，点击右侧“执行”按钮 %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' 在FusionInsight的客户端下可以看到创建的hbase表test4和数据 Zeppelin连接Spark 操作场景 Zeppelin中配置Spark解析器 前提条件 完成Zeppelin0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Spark2x组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 将/opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars路径下所有的jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark 将/opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars路径下libfb303-0.9.3.jar和libthrift-0.9.3.jar两个jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep路径下 确保/usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter路径下有且仅有libthrift-0.9.3.jar这个版本的jar包 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下内容 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择Spark，点击 edit 编辑，将 master 参数改为 yarn-client，并且检查zeppelin.spark.useHiveContext项，使其值为false，点击 save 保存 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 执行zeppelin的spark样例代码，参考网址 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ 样例代码需要访问Internet上的资源，所以保证zeppelin所在的节点可以联网，检测是否能打开以下链接 执行zeppelin的spark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) 安装python-matplotlib yum install python-matplotlib 安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh 配置环境变量PATH，将python换成安装Anaconda安装目录中的python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.pyspark.python 参数改为Anaconda安装目录中的python，点击 save 保存 执行zeppelin的pyspark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) Zeppelin连接SparkR 操作场景 Zeppelin中配置Spark解析器，连接SparkR 前提条件 完成Zeppelin0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Spark2x组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 由于Spark的Executor上也需要执行R，所以除了在Zeppelin的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 不同OS配置yum源时下载的文件路径有所不同，下面以Redhat6.6安装R为例 配置Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 如果遇到源yum-plugin-fastestmirror无法下载时，可在网址https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror下选择相应的版本代替下载安装 配置EPEL的源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm 更新cache yum clean all yum makecache 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 FusionInsight客户端下测试是否可以使用sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR 参考http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter 在R的命令行中安装sparkR样例需要的R的libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.R.cmd 参数改为R的启动文件，点击 save 保存 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 在Zeppelin中执行Zeppelin Tutorial -> R (SparkR)样例 Zeppelin连接Apache Livy 操作场景 Zeppelin中配置Livy解析器，连接Livy 前提条件 完成Zeppelin0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Spark2x组件。 完成Apache Livy 0.5.0的安装 可参考《Apache Livy对接FusionInsight》对接文档完成Apache Livy的安装 操作步骤 用如下命令启动Livy服务 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择livy，点击 edit 编辑zeppelin.livy.url的值为http://172.21.3.43:8998（可以不更改），点击 save 保存 页面选择Notebook -> Create new note 自定义note名称，例如livy_connection_test 在Zeppelin中执行Spark样例代码 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y 在Zeppelin中执行PySpark样例代码 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y 在Zeppelin中执行SparkR样例代码 %livy.sparkr hello "},"Integrated_Development_Environment/Using_Jupyter_Notebook_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_Jupyter_Notebook_with_FusionInsight.html","title":"对接Jypyter Notebook","keywords":"","body":"Jupyter_Notebook对接FusionInsight 安装Jupyter notebook Jupyter notebook的安装依赖于Python，且涉及到许多工具的依赖包，相互之间还存在版本依赖关系，比较麻烦，通常可以直接安装Anaconda包，里面包含了Python、Jupyter Notebook，以及众多的科学工具包，这里我们直接安装Anaconda 从Anaconda官网下载并安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh 生成Jupyter notebook的配置文件 jupyter notebook --generate-config --allow-root 修改Jupyter notebook的配置IPc.NotebookApp.ip为本机IP地址 vi /root/.jupyter/jupyter_notebook_config.py 启动Jupyter notebook:: jupyter notebook --allow-root 出现如下提示表示Jupyter notebook启动成功 [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 使用 Ctrl+C 可以退出Jupyter notebook 安装FusionInsight Client 参考FusionInsight的产品文档完成Linux下的FusionInsight客户端的安装，安装到/opt/hadoopclient目录 完成Kerberos认证 使用sparkuser进行Kerberos认证(sparkuser为FusionInsight中创建的拥有Spark访问权限的人机用户)cd /opt/hadoopclient/ source bigdata_env kinit sparkuser 导入ipython相关环境变量 执行以下命令导入环境变量，或者将下面两行添加到/opt/hadoopclient/bigdata_env文件，后续source bigdata_env时可以自动将环境变量导入export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" Jupyter notebook中使用pyspark进行分析 执行pyspark会自动启动Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 打开上述链接，可以进行数据分析 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc % summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10) Jupyter notebook中使用R语言进行分析 TBD "},"Integrated_Development_Environment/Using_DBeaver_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_DBeaver_with_FusionInsight.html","title":"对接DBeaver","keywords":"","body":"DBeaver对接FusionInsight 适用场景 DBeaver 4.0.8 FusionInsight HD V100R002C60U20 DBeaver 4.2.1 FusionInsight HD V100R002C70SPC200 说明 SQL开发工具，如DbVisualizer、DBeaver、Squirrel是数据库开发的常用选择，虽然这些工具大多不提供原生Hive、SparkSQL、Phoenix的支持，但是通过它们支持的自定义JDBC的能力，我们可以与FusionInsignt提供的Fiber组件的JDBC接口进行对接，实现这Hive、SparkSQL、Phoenix组件的统一SQL查询。 Fiber架构图 本文介绍了DBeaver与FusionInsight的Fiber对接的操作步骤 Linux下DBeaver连接Fiber 操作场景 以安全模式为例，使用DBeaver通过Fiber访问Hive、Spark、Phoenix 前提条件 已经安装好Linux（Redhat Linux Enterprise 6.5 64bit）Desktop操作系统； 已经安装好的Linux机器的时间与FusionInsight HD集群的时间要保持一致，时间差小于5分钟。 已完成FusionInsight HD V100R002C60U20安全集群的安装，已安装好Fiber客户端。 操作步骤 安装jdk1.8，DBeaver4.0.8需要jdk1.8以上版本 tar -xvf jdk-8u112-linux-x64.tar.gz 配置环境变量/etc/profile，加入如下内容，source环境变量 #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH 下载地址：http://dbeaver.jkiss.org/download/, 软件dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz，安装DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz 安装FusionInsight客户端，具体请参见《FusionInsight HD 产品文档》的 安装客户端 章节，客户端安装目录为/opt/hadoopclient/，其中Fiber客户端目录/opt/hadoopclient/Fiber/。 修改Fiber的配置文件/opt/hadoopclient/Fiber/conf/fiber.xml，将其中hive、spark、phoenix的认证方式改为安全模式keytab认证方式，具体配置方法参考 产品文档 -> 管理员指南 -> 业务操作指南 -> 统一SQL(Fiber) -> 客户端配置 章节。 Hive JDBC连接配置 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback /opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab java.security.krb5.conf /opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf java.security.auth.login.config /opt/jaas.conf zookeeper.server.principal {HIVE_CLIENT_ZK_PRINCIPAL} zookeeper.kinit /opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit Spark连接配置 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback /opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab java.security.krb5.conf /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf java.security.auth.login.config /opt/jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit /opt/hadoopclient/KrbClient/kerberos/bin/kinit Phoenix连接配置 phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback /opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab java.security.krb5.conf /opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf java.security.auth.login.config /opt/jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit /opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit jaas.conf文件： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 打开DBeaver，进入DBeaver的安装目录执行./dbeaver，启动dbeaver 进入DBeaver界面，菜单选择 Database -> 新建DriverManager，在弹出的对话框中点击 New 填写如下信息，点击 OK Driver Name：Fiber（自定义） Class Name：com.huawei.fiber.FiberDriver URL Template：jdbc:fiber:// Default Port：2345（可随便写） Category：Hadoop 点击 Add File 按钮，将Fiber客户端/opt/hadoopclient/Fiber/lib/下的jar包添加进来 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar 在Connection Properties中加入以下属性： 菜单栏选择 File -> New -> Database Connection, 类型选择Fiber User name和Password可不填写 配置Driver properties里面的defaultDirver，可按需求填写hive或spark或phoenix，点击next Network页面保持默认，点击 next 输入自定义Connection name后，点击 finish, 连接建立完成 测试hive链接 查看Hive表中数据 测试spark链接, 把driver切换为spark，连接右键选择 Edit Connection 使用spark driver查看表中数据 测试phoenix连接，把driver切换为phoenix，连接右键选择 Edit Connection 查看phoenix表中数据 Windows下DBeaver连接Fiber 操作场景 以安全模式为例，使用DBeaver通过Fiber访问Hive、Spark、Phoenix 前提条件 Windows上已经安装好jdk1.8以上版本，并完成jdk环境变量配置 客户端机器的时间与FusionInsight HD集群的时间要保持一致，时间差小于5分钟。 从http://dbeaver.jkiss.org/download/下载DBeaver软件，完成windows上的安装 已完成FusionInsight HD V100R002C60U20安全集群的安装，已安装好Fiber客户端。 已将集群的节点主机名与IP的映射关系加入到windows的hosts文件中C:\\Windows\\System32\\drivers\\etc\\hosts 操作步骤 Fiber的安全认证可以用kinit和keytab两种方式，具体参数配置说明可参考 产品文档 -> 管理员指南 -> 业务操作指南 -> 统一SQL(Fiber) -> 客户端配置 章节。kinit认证的有效期是24小时，keytab认证方式长期有效 使用kinit认证方式配置 使用keytab认证方式配置 使用kinit认证方式配置 下载对应操作系统架构的MIT Kerberos，并安装 http://web.mit.edu/kerberos/dist/#kfw-4.0 确认客户端机器的时间与FusionInsight HD集群的时间一致，时间差要小于5分钟 设置Kerberos的配置文件 在FusionInsight Manager创建角色和人机用户，具体请参见 产品文档 -> 管理员指南 -> 系统设置 -> 权限管理 -> 用户管理 -> 创建用户 章节。角色需要根据业务需要授予Hive的访问权限，并将用户加入角色，创建用户“test” 下载对应的keytab文件user.keytab以及krb5.conf文件，把krb5.conf文件重命名为krb5.ini，并放到C:\\ProgramData\\MIT\\Kerberos5目录中 设置Kerberos票据的缓存文件 创建存放票据的目录，例如C:\\temp 设置Windows的系统环境变量，变量名为KRB5CCNAME，变量值为C:\\temp\\krb5cache 在Windows上进行认证 打开MIT Kerberos，单击 get Ticket ，在弹出的MIT Kerberos: Get Ticket窗口中，Pricipal 输入用户名(如：test@HADOOP.COM)，Password 输入密码，单击 OK 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; 修改fiber.xml文件，位置C:\\Fiber\\conf\\fiber.xml Hive的JDBC连接 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接 phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe DBeaver连接前确认kerberos认证有效 使用keytab认证方式配置 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下，principal和keytab按实际填写 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 修改fiber.xml文件配置，位置C:\\Fiber\\conf\\fiber.xml Hive的JDBC连接 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接，需要增加属性hbase.myclient.keytab和hbase.myclient.principal phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf hbase.myclient.keytab C:\\\\Fiber\\\\conf\\\\user.keytab hbase.myclient.principal test zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe DBeaver连接Fiber 将Hive、Spark、Phoenix的JDBC配置中classPath中的文件拷贝至Fiber文件夹中 进入DBeaver界面，菜单选择 Database -> 新建DriverManager，在弹出的对话框中点击 New 填写如下信息，点击 OK Driver Name：Fiber（自定义） Class Name：com.huawei.fiber.FiberDriver URL Template：jdbc:fiber:// Default Port：2345（可随便写） Category：Hadoop 点击 Add File 按钮，将Fiber客户端（/opt/hadoopclient/Fiber/lib/）下的jar包添加进来 在Connection Properties中加入以下属性 菜单栏选择 File -> New -> Database Connection User name和Password可不填写 确认defaultDirver，可按需求填写hive或spark或phoenix。 Network保持默认，点击 next 自定义Connection name，点击finish 连接建立完成 测试hive连接 查看Hive表中数据 测试spark连接, 把driver切换为spark，连接右键选择 Edit Connection 使用spark driver查看表中数据 测试phoenix连接，把driver切换为phoenix，连接右键选择 Edit Connection 查看phoenix表中数据 DBeaver对接Fiber功能验证 Hive增加查看数据 将JDBC的defaultDrive切换至Hive Hive查询数据：菜单栏选择 SQL Editor -> New SQL Editor，编辑脚本，点击左上角执行按钮。 SELECT * FROM workers_info Hive增加数据： 编辑数据文件data_input.txt，上传至集群的hdfs目录中，例如/tmp/下，文本内容如下： 编辑脚本，点击左上角执行按钮。 查看更新后数据： Spark增加查看数据 将JDBC 的defaultDriver切换至Spark Spark查询数据：编辑脚本，点击左上角执行按钮。 SELECT * FROM workers_info Spark增加数据： 编辑数据文件data_input.txt，上传至Spark的JDBCServer(主)实例所在的节点的/opt/目录下 文本内容如下： 编辑脚本，点击左上角执行按钮。 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info 查看结果： Phoenix增删改查数据 将JDBC 的defaultDrive切换至Phoenix Phoenix增加数据 菜单栏选择 SQL Editor -> New SQL Editor，编辑脚本，点击左上角 执行 按钮。 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') 查看增加的数据： Phoenix删除数据 页面上删除：选择待删除的列，然后点击下方 删除 按钮，然后点击 save 按钮： 脚本删除：编辑脚本，点击左上方 执行 按钮 delete from TB_PHOENIX where ID=104; 查看输出后的数据 Phoenix更新数据, 编辑更新脚本，点击左上方 执行 按钮 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') 查看更新后的数据： 查看数据：编辑查询脚本，点击左上方 执行 按钮。 SELECT * FROM TB_PHOENIX "},"Integrated_Development_Environment/Using_DbVisualizer_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_DbVisualizer_with_FusionInsight.html","title":"对接DbVisualizer","keywords":"","body":"DbVisualizer对接FusionInsight 适用场景 DbVisualizer 9.5.7 FusionInsight HD V100R002C60U20 DbVisualizer 10.0.1 FusionInsight HD V100R002C70SPC200 说明 SQL开发工具，如DbVisualizer、DBeaver、Squirrel是数据库开发的常用选择，虽然这些工具大多不提供原生Hive、SparkSQL、Phoenix的支持，但是通过它们支持的自定义JDBC的能力，我们可以与FusionInsignt提供的Fiber组件的JDBC接口进行对接，实现这Hive、SparkSQL、Phoenix组件的统一SQL查询。 Fiber架构图 本文介绍了DbVisualizer与FusionInsight的Fiber对接的操作步骤 DbVisualizer安装 DbVisualizer9.5.7需要jdk1.8，下载安装jdk1.8，配置环境变量。 参考FusionInsight产品文档安装FusionInsight客户端，位置/opt/hadoopclient 修改C:\\Windows\\System32\\drivers\\etc\\hosts文件，加入FusionInsight集群信息 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber。 参考FusionInsight产品文档创建用户test，并赋予足够的权限，下载test的keytab文件user.keytab，拷贝到C:\\Fiber\\conf\\文件夹下。 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下，principal和keytab按实际填写： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 修改fiber.xml文件配置，位置C:\\Fiber\\conf\\fiber.xml Hive的JDBC连接 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接，需要增加属性 hbase.myclient.keytab 和 hbase.myclient.principal phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf hbase.myclient.keytab C:\\\\Fiber\\\\conf\\\\user.keytab hbase.myclient.principal test zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe 将Hive、Spark、Phoenix的JDBC配置中classPath中的文件拷贝至Fiber文件夹中。 下载DbVisualizer，地址：http://www.dbvis.com/download/，下载软件dbvis_windows-x64_9_5_7_jre.exe 双击dbvis_windows-x64_9_5_7_jre.exe安装 DbVisualizer连接Fiber 配置DbVisualizer通过Fiber连接FusionInsight的Hive、Spark、Phoenix组件。 打开DbVisualizer9.5.7，点击 Cancel 菜单栏选择 ToolsDriver Manager 新建driver Name：Fiber(自定义) URL Format：jdbc:fiber:// User Specified：将C:\\Fiber\\lib\\下所有的jar包加入 Driver Class：加入jar包后选择com.huawei.fiber.FiberDriver 菜单栏 Database -> Create Database Connection 选择 Use Wizard {width=\"4.2in\" height=\"1.4in\"} 自定义连接名称，例如Fiber 选择Driver Fiber 填写URL：jdbc:fiber:// 点击 Finish 查询Hive表数据 打开 Properties 面板，填写defaultDriver和fiberconfig属性，点击 Apply 。 打开 Connection 面板，点击 Connect 按钮，可以在左侧看到hive数据表。 菜单栏选择 File -> New SQL Commander ，编辑SQL，点击 执行 按钮，查看查询结果。 查询SparkSQL中的数据 将defaultDriver切换为spark：将 Properties 中的defaultDriver值改为spark，点击 Apply 。 打开Connection面板，点击 Reconnect ，连接成功，可以看到SparkSQL中的数据表。 菜单栏选择 File -> New SQL Commander，编辑SQL，点击 执行 按钮，查看查询结果。 查询Phoenix中的数据 将defaultDriver切换为phoenix，将 Properties 中的defaultDriver值改为phoenix，点击 Apply 。 打开 Connection 面板，点击 Reconnect，连接成功，可以看到phoenix数据表 查看phoenix表TB_PHOENIX中的数据。 菜单栏选择 File -> New SQL Commander，编辑SQL，点击 执行 按钮，查看查询结果。 Phoenix的增加删除更新数据 Phoenix的增加删除更新数据，需要在Fiber中hbase的配置文件hbase-site.xml中加入如下参数，否则不会自动Commit 修改Hbase-site.xml文件，位置C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml，然后重启DbVisualizer。 phoenix.connection.autoCommit true Phoenix表增加数据 UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix表删除数据 delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix表更新数据 UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix; "},"Integrated_Development_Environment/Using_Squirrel_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_Squirrel_with_FusionInsight.html","title":"对接Squirrel","keywords":"","body":"Squirrel对接FusionInsight 适用场景 Squirrel 3.7.1 FusionInsight HD V100R002C60U20 Squirrel 3.8.0 FusionInsight HD V100R002C70SPC200 说明 SQL开发工具，如DbVisualizer、DBeaver、Squirrel是数据库开发的常用选择，虽然这些工具大多不提供原生Hive、SparkSQL、Phoenix的支持，但是通过它们支持的自定义JDBC的能力，我们可以与FusionInsignt提供的Fiber组件的JDBC接口进行对接，实现这Hive、SparkSQL、Phoenix组件的统一SQL查询。 Fiber架构图 本文介绍了Squirrel与FusionInsight的Fiber对接的操作步骤 Squirrel安装 安装jdk1.8，配置环境变量。 参考FusionInsight产品文档安装FusionInsight客户端，位置/opt/hadoopclient。 修改C:\\Windows\\System32\\drivers\\etc\\hosts文件，加入FusionInsight集群信息。 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber。 参考FusionInsight产品文档创建用户test，并赋予足够的权限，下载test的keytab文件user.keytab，拷贝到C:\\Fiber\\conf\\文件夹下。 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下，principal和keytab按实际填写： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 修改fiber.xml文件配置，位置C:\\Fiber\\conf\\fiber.xml。 Hive的JDBC连接： hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接： spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接，需要增加属性 hbase.myclient.keytab 和 hbase.myclient.principal： phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf hbase.myclient.keytab C:\\\\Fiber\\\\conf\\\\user.keytab hbase.myclient.principal test zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe 将Hive、Spark、Phoenix的JDBC配置中classPath中的文件拷贝至Fiber文件夹中。 下载Squirrel，地址：http://www.squirrelsql.org/#installation，选择Install jar of SQuirreL 3.7.1 for Windows/Linux/others，下载软件squirrel-sql-3.7.1-standard.jar 双击squirrel-sql-3.7.1-standard.jar安装 在这里可以选择要安装哪些环境，使用的数据库插件，语言包。 Squirrel连接Fiber 使用SQuirreL SQL Client通过Fiber连接FusionInsight的Hive、SparkSQL、Phoenix组件。 打开SQuirreL SQL Client，选择Drivers，点击 +。 填写Driver信息，点击 OK。 Name：Fiber（自定义） Example URL：jdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path：将Fiber/lib下的jar包都添加进来 ClassName：com.huawei.fiber.FiberDriver 可以看到添加完成的Driver Fiber。 对接Hive 点击 Aliases，点击 + 在弹出框中填写信息 Name：Fiber（自定义） Driver：选择Fiber User Name：test Password：密码 点击 Connect 连接成功，点击 OK 点击 Connect 查看hive中数据表 点击 SQL面板，编辑SQL语句，点击 执行 按钮，在下方可以看到查询结果。 Hive增加数据： 编辑数据文件data_input.txt，上传至集群的hdfs目录中，例如/tmp/下，文本内容如下： 编辑脚本，点击 执行 按钮： load data inpath ‘/tmp/data_input.txt’ overwrite into table workers_info 查看结果： 对接SparkSQL 将defaultDriver切换为spark，点击 Test 点击 Connect 连接成功，点击 OK 双击Fiber，点击 Connet，将driver切换为spark 可以看到数据表 点击 SQL面板，编辑SQL语句，点击 执行 按钮，在下方可以看到查询结果。 Spark增加数据 编辑数据文件data_input.txt，上传至集群的hdfs目录中，例如/tmp/下，文本内容如下： 编辑脚本，点击 执行 按钮： load data inpath ‘/tmp/data_input.txt’ overwrite into table workers_info 查看结果： 对接Phoenix 将defaultDriver切换为phoenix，点击 Test 点击 Connect 连接成功，点击 OK 双击 Fiber，点击 Connect，将driver切换为phoenix 可以看到数据phoenix表 点击 SQL面板 ，编辑SQL语句，点击 执行 按钮，在下方可以看到查询结果。 select * from tb_phoenix 点击 SQL面板，编辑SQL语句，向phoenix表中增加一条数据，点击 执行 按钮。 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') 查询结果： select * from tb_phoenix 点击 SQL面板，编辑SQL语句，删除一条数据，点击 执行 按钮。 delete from TB_PHOENIX where ID=109; 查看结果： select * from tb_phoenix 点击 SQL面板，编辑SQL语句，更新一条数据，点击 执行 按钮。 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') 查看结果 "},"SQL_Analytics_Engine/":{"url":"SQL_Analytics_Engine/","title":"SQL引擎","keywords":"","body":"SQL分析引擎 对接Apache Kylin Apache Kylin 1.6.0 FusionInsight HD V100R002C60U20 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC200 Apache Kylin 2.3.1 FusionInsight HD V100R002C80SPC200 对接Kyligence 对接Presto Presto0.155 FusionInsight HD V100R002C60U20 Presto0.184 FusionInsight HD V100R002C70SPC100 "},"SQL_Analytics_Engine/Using_Kylin_with_FusionInsight.html":{"url":"SQL_Analytics_Engine/Using_Kylin_with_FusionInsight.html","title":"对接Apache Kylin","keywords":"","body":"Apache Kylin对接FusionInsight HD Apache Kylin 1.6.0 FusionInsight HD V100R002C60U20 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC200 Apache Kylin 2.3.1 FusionInsight HD V100R002C80SPC200 "},"SQL_Analytics_Engine/Using_Kylin1.6.0_with_FusionInsight_HD_C60U20.html":{"url":"SQL_Analytics_Engine/Using_Kylin1.6.0_with_FusionInsight_HD_C60U20.html","title":"Kylin 1.6.0 <-> C60U20","keywords":"","body":"Apache Kylin对接FusionInsight 适用场景 Apache Kylin 1.6.0 (基于HBase1.0.2的分支版本：yang21-hbase102) FusionInsight HD V100R002C60U20 说明 Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 Apache Kylin主要与FusionInsight的Hive和HBase进行对接 环境准备 修改/etc/hosts 添加本机主机名解析 162.1.115.89 kylin 配置NTP服务 使用vi /etc/ntp.conf增加NTP服务的配置 server 162.2.200.200 nomodify notrap nopeer noquery 启动NTP服务 service ntpd start chkconfig ntpd on 安装Hadoop client 在FusionInsight Manager服务管理页面下载客户端，上传到kylin安装主机，解压 cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient 安装JDK rpm -Uvh jdk-8u112-linux-x64.rpm 编译Kylin 可直接下载的二进制文件的Kylin-1.6.0主版本是基于HBase1.1.1编译的，而FusionInsight使用的HBase版本是1.0.2，这两个版本部分类和方法不兼容，需要配套1.0.2的HBase重新编译Kylin。 下载Kylin-1.6.0基于HBase1.0.2版本的源码https://github.com/apache/kylin/tree/yang21-hbase102得到kylin-yang21-hbase102.zip 安装编译工具 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v 安装git yum install -y git 安装nodejs： wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin 导入环境变量 source /etc/profile 执行npm -v 编译打包 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh 等待编译完成，得到Kylin二进制安装包 启动Kylin 解压二进制包 解压上一步骤生成的安装包tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt 配置环境变量 配置环境变量：vi /etc/profile，增加以下配置 export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin 导入环境变量 source /etc/profile Kylin启动还需要配置HIVE_CONF、HCAT_HOME，使用vi /opt/hadoopclient/Hive/component_env，在文件最后增加 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog 导入环境变量 source /opt/hadoopclient/bigdata_env 进行kerberos认证 kinit test_cn Kylin检查环境设置： cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh 修改Kylin配置 修改kylin.properties： vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties 配置Hive client使用beeline： kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' 配置获取任务状态时使用kerberos 鉴权： kylin.job.status.with.kerberos=true 去掉不允许修改的配置 FusionInsight不允许修改dfs.replication, mapreduce.job.split.metainfo.maxsize的参数，需要注释掉Kylin所有配置文件中的相关参数，否则构建Cube时会报如下错误： 修改以下文件： kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase配置 将/opt/hadoopclient/Hive/config/hivemetastore-site.xml中的配置合并到hive-site.xml 将/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml中的配置合并到/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib路径 kylin的find-hive-dependency.sh默认Hive lib路径为大数据集群中Hive的安装路径，若Kylin安装在集群节点上不会有问题，否则需要修改为客户端路径。 编辑find-hive-dependency.sh：vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'` 启动Kylin 使用./kylin.sh start启动Kylin 输入默认用户名密码：ADMIN/KYLIN登陆 Demo测试 导入Demo数据 执行以下命令导入sample数据 cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh 选择菜单 System -> Actions -> Reload Metadata 构建Cube 选择learn_kylin工程，构建默认的kylin_sales_cube Cube构建成功，状态变为READY 查询表数据 在Insight页面执行查询 "},"SQL_Analytics_Engine/Using_Kylin2.1.0_with_FusionInsight_HD_C70.html":{"url":"SQL_Analytics_Engine/Using_Kylin2.1.0_with_FusionInsight_HD_C70.html","title":"Kylin 2.1.0 <-> C70","keywords":"","body":"Apache Kylin2.1.0对接FusionInsight_HD_C70 适用场景 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC200 说明 Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 Apache Kylin主要与FusionInsight的Hive和HBase进行对接 环境准备 修改/etc/hosts 添加本机主机名解析 172.16.52.86 kylin 配置NTP服务 使用vi /etc/ntp.conf增加NTP服务的配置 server 172.18.0.18 nomodify notrap nopeer noquery 启动NTP服务 service ntpd start chkconfig ntpd on 安装Hadoop client 在FusionInsight Manager服务管理页面下载客户端，上传到kylin安装主机，解压 ./install.sh /opt/hadoopclient 安装JDK rpm -Uvh jdk-8u112-linux-x64.rpm 编译Kylin 可直接下载的二进制文件的Kylin-2.1.0主版本是基于HBase1.1.1编译的，而FusionInsight使用的HBase版本是1.0.2，这两个版本部分类和方法不兼容，需要重新编译Kylin。 下载Kylin-2.1.0基于HBase1.1.1版本的源码码https://github.com/apache/kylin/tree/2.1.x得到kylin-2.1.x.zip 安装编译工具 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v 安装git yum install -y git 安装nodejs： wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin 导入环境变量 source /etc/profile 执行npm -v 修改kylin源码 修改HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java 修改pom.xml vi /opt/kylin-2.1.x/pom.xml 将HBase、Hive、Hadoop版本改成与FusionInsight HD对应的版本 编译打包 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh 等待编译完成，得到Kylin二进制安装包 启动Kylin 解压二进制包 解压上一步骤生成的安装包tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt 配置环境变量 配置环境变量：vi /etc/profile，增加以下配置 export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin 导入环境变量 source /etc/profile Kylin启动还需要配置HIVE_CONF、HCAT_HOME，使用vi /opt/hadoopclient/Hive/component_env，在文件最后增加 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog 导入环境变量 source /opt/hadoopclient/bigdata_env 进行kerberos认证 kinit test Kylin检查环境设置： cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh 修改Kylin配置 修改kylin.properties： vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties 配置Hive client使用beeline： kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' 配置获取任务状态时使用kerberos 鉴权： kylin.job.status.with.kerberos=true 去掉不允许修改的配置 FusionInsight不允许修改dfs.replication, mapreduce.job.split.metainfo.maxsize的参数，需要注释掉Kylin所有配置文件中的相关参数，否则构建Cube时会报如下错误： 需要修改以下文件： kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase配置 将/opt/hadoopclient/Hive/config/hivemetastore-site.xml中的配置合并到hive-site.xml 将/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml中的配置合并到/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib路径 kylin的/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh默认Hive lib路径为大数据集群中Hive的安装路径，若Kylin安装在集群节点上不会有问题，否则需要修改为客户端路径。 启动Kylin 使用./kylin.sh start启动Kylin 输入默认用户名密码：ADMIN/KYLIN登陆 Demo测试 导入Demo数据 执行以下命令导入sample数据 cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh 选择菜单 System -> Actions -> Reload Metadata 选择菜单 System -> Model 构建Cube 构建默认的kylin_sales_cube 选择End Data（Exclude）时间： 点击Monitor可以查看build状态： Build完成： Cube构建成功，状态变为READY 查询表数据 在Insight页面执行查询 "},"SQL_Analytics_Engine/Using_Kylin2.3.1_with_FusionInsight_HD_C80.html":{"url":"SQL_Analytics_Engine/Using_Kylin2.3.1_with_FusionInsight_HD_C80.html","title":"Kylin 2.3.1 <-> C80","keywords":"","body":"Apache Kylin2.3.1对接FusionInsight_HD_C80 适用场景 Apache Kylin 2.3.1 FusionInsight HD V100R002C80SPC100 说明 Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 Apache Kylin主要与FusionInsight的Hive和HBase进行对接 环境准备 修改/etc/hosts 添加本机主机名解析 172.16.52.86 kylin 配置NTP服务 使用vi /etc/ntp.conf增加NTP服务的配置,时间与FusionInsight集群同步 server 172.18.0.18 nomodify notrap nopeer noquery 启动NTP服务 service ntpd start chkconfig ntpd on 参考FusionInsight产品文档在Kylin节点安装FusionInsight客户端 在FusionInsight Manager服务管理页面下载客户端，上传到kylin节点安装FusionInsight客户端到/opt/hadoopclient目录 ./install.sh /opt/hadoopclient 安装JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm 下载Kylin Fusioninsight配套的HBase是1.3.0，Apache Kylin可直接下载apache-kylin-2.3.1-hbase1x-bin.tar.gz主版本二进制包，无需编译Apache kylin 下载解压Kylin 下载Kylin-2.3.1基于HBase1.x版本的二进制包， http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz 上传apache-kylin-2.3.1-hbase1x-bin.tar.gz到Apache kylin节点的/opt目录 解压上一步骤的安装包 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt 配置Kylin 配置环境变量 配置环境变量：vi /etc/profile，增加以下配置 export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin 导入环境变量 source /etc/profile Kylin启动还需要配置HIVE_CONF、HCAT_HOME，使用vi /opt/hadoopclient/Hive/component_env，在文件最后增加 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog 导入环境变量 source /opt/hadoopclient/bigdata_env 进行kerberos认证 kinit test Kylin检查环境设置： cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh 修改FusionInsight的Hive配置项 在hive.security.authorization.sqlstd.confwhitelist.append参数最后追加一下参数配置，保存配置，重启影响的服务 |mapreduce\\.job\\..*|dfs\\..* 修改Kylin配置 获取Hive的JDBC字符串 执行Beeline查看Hive的JDBC字符串 source bigdata_env kinit test beeline 修改kylin.properties： vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties 配置Hive client使用beeline： kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC字符串使用上一步骤获取的字符串 注意：kylin.source.hive.beeline-params参数里面原有的 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' 要去掉 修改Hive/HBase配置 将/opt/hadoopclient/Hive/config/hivemetastore-site.xml中的配置合并到hive-site.xml 将/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml中的配置合并到/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib路径 kylin的/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh默认Hive lib路径为大数据集群中Hive的安装路径，需要修改为客户端路径 启动Kylin 使用./kylin.sh start启动Kylin 输入默认用户名密码：ADMIN/KYLIN登陆 Demo测试 导入Demo数据 执行以下命令导入sample数据 cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh 选择菜单 System -> Actions -> Reload Metadata 选择菜单 System -> Model 构建Cube 构建默认的kylin_sales_cube 选择End Data（Exclude）时间： 点击Monitor可以查看build状态： Build完成： Cube构建成功，状态变为READY 查询表数据 在Insight页面执行查询 "},"SQL_Analytics_Engine/Using_Kyligence_with_FusionInsight.html":{"url":"SQL_Analytics_Engine/Using_Kyligence_with_FusionInsight.html","title":"对接Kyligence","keywords":"","body":"Kyligence Analytics Platform对接FusionInsight 适用场景 Kyligence Analytics Platform v2.2 FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.3 FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.4 FusionInsight HD V100R002C70SPC200 Kyligence Analytics Platform v2.5 FusionInsight HD V100R002C70SPC200 说明 参考 官方产品文档 的 快速安装 章节 "},"SQL_Analytics_Engine/Using_Presto_with_FusionInsight.html":{"url":"SQL_Analytics_Engine/Using_Presto_with_FusionInsight.html","title":"对接Presto","keywords":"","body":"Presto对接FusionInsight HD Presto0.155 FusionInsight HD V100R002C60U20 Presto0.184 FusionInsight HD V100R002C70SPC100 "},"SQL_Analytics_Engine/Using_Presto0.155_with_FusionInsight_HD_C60U20.html":{"url":"SQL_Analytics_Engine/Using_Presto0.155_with_FusionInsight_HD_C60U20.html","title":"Presto0.146 <-> C60U20","keywords":"","body":"Apache Presto对接FusionInsight 适用场景 Presto0.155 FusionInsight HD V100R002C60U20 说明 Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。 Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题 Presto主要与FusionInsight的Hive和HDFS进行对接 配置Hive Connector Presto集群包括coordinator节点和不限数量的worker节点(coordinator节点也可同时为worker节点)，其中只需要在coordinator节点上配置Hive Connector即可。 本文档中配置coordinator节点同时也是worker节点。 从该链接下载presto-server的安装包，并上传到presto coordinator的节点 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz 将该压缩包解压缩后得到目录/opt/presto-server-0.155。 在presto节点上安装华为FusionInsight HD的客户端，默认安装目录/opt/hadoopclient presto该0.155版本要求jdk至少在1.8u60+以上，修改/etc/profile文件方式配置系统默认的java为FusionInsight HD客户端的jdk，并source环境变量，命令参考如下 在/etc/profile中增加以下行 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source 环境变量 source /etc/profile 创建Java keystore File for TLS，(后续步骤默认都在presto节点上执行)参考如下命令 source /opt/hadoopclient/bigdata_env keytool –genkeypair –alias testuser –keyalg RSA –keystore /opt/presto.jks alias后的值必须要跟后面创建的用户名称一致 first and last name必须写成presto节点的主机名 通过FusionInsight HD的管理页面创建一个“机机”用户，具体请参见《FusionInsight HD管理员指南》的 创建用户 章节。例如，创建用户testuser，并选择hadoop和hive用户组，下载对应的秘钥文件user.keytab以及krb5.conf文件，并上传到presto节点的/opt/hadoopclient目录下，将user.keytab改名为testuser.keytab。 参考如下命令在Huawei FusionInsight HD的Kerberos中创建一个新的principal，其名称为“testuser/presto-server”，其中presto-server为presto的coordinator节点的主机名，导出该principal的秘钥文件为/opt/presto.keytab。 执行kadmin –p kadmin/admin命令时初始密码Admin@123，修改后需严格牢记新密码。 创建目录/opt/presto-server-0.155/etc，在该目录下创建如下文件 config.properties参考如下 coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config参考如下内容 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties参考如下内容 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties参考如下内容 com.facebook.presto=INFO 创建目录/opt/presto-server-0.155/etc/catalog，在该目录下创建hive.properties文件 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml 其中hive.metastore.uri的值从/opt/hadoopclient/Hive/config/hive-site.xml中查找 修改/etc/hosts文件，将本机的IP与主机名解析以及Huawei FusionInsight HD集群节点的IP与主机名解析添加进去，例如 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v可以正确输出mvn版本 安装git yum install -y git 参考如下命令，下载presto-server-0.155的源码 git clone https://github.com/prestodb/presto.git git checkout 0.155 修改presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java的代码，将代码中\"Sasl.QOP=auth\"修改为\"Sasl.QOP=auth-conf\" 重新编译presto cd presto-hive mvn clean install -DskipTests 将编译后target目录下的presto-hive-0.155.jar文件替换/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar文件 启动presto server，跟踪/var/presto/data/var/log/server.log查看启动日志 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log 检查FusionInsight Manager中HDFS服务配置中hadoop.rpc.protection的配置，必须设置为authentacation。 通过Presto CLI连接Hive 使用Presto CLI连接Huawei FusionInsight HD的Hive，使用presto自带的命令行工具执行SQL语句。 通过如下链接下载presto cli启动的jar包 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar 并将该jar包上传到可与presto节点网络互通的节点上(也可将presto coordinator节点作为cli使用节点)。 配置cli节点的jdk为1.8u60+以上版本 配置cli节点的/etc/hosts文件，将FI集群和presto coordinator节点的IP与主机名关系配置到cli节点 从presto节点拷贝presto.jks、presto.keytab、krb5.conf以及连接HDFS所需的core-site.xml和hdfs-site.xml文件到cli节点 将presto-cli-0.155-executable.jar包改为可执行文件 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h 创建presto cli启动脚本，类似如下，注意将相关文件的路径按实际位置替换 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog后面的hive是和presto coordinator节点配置的hive.properties的文件名匹配的，如果hive.properties改名为hivetest.properties，则这里改为hivetest 通过cli执行SQL语句，其他SQL语法请参考https://prestodb.io/docs/0.155/sql.html 查询表workers_info中数据： 百万记录数表web_sales查询： 通过Presto JDBC连接Hive 使用Presto JDBC接口连接Huawei FusionInsight HD Hive 从如下链接下载jdbc的驱动包 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar 参考https://prestodb.io/docs/0.155/installation/jdbc.html设置JDBC URL，用户名为任意字符，密码为空，在eclipse中调通的示例如下: import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; public class PrestoTest { public static void main(String[] args) throws SQLException, ClassNotFoundException { Class.forName(\"com.facebook.presto.jdbc.PrestoDriver\"); Connection connection =DriverManager.getConnection (\"jdbc:presto://162.1.115.71:8080/hive/default\",\"root\",null); Statement stmt =connection.createStatement(); ResultSet rs = stmt.executeQuery(\"select * from workers_info limit 10\"); int col = rs.getMetaData().getColumnCount(); while(rs.next()) { for (int i = 1; i 测试结果： 百万记录数表web_sales查询： "},"SQL_Analytics_Engine/Using_Presto0.184_with_FusionInsight_HD_C70SPC100.html":{"url":"SQL_Analytics_Engine/Using_Presto0.184_with_FusionInsight_HD_C70SPC100.html","title":"Presto0.184 <-> C70SPC100","keywords":"","body":"Apache Presto对接FusionInsight 适用场景 Presto0.184 FusionInsight HD V100R002C70SPC100 Presto0.196 FusionInsight HD V100R002C80SPC100 说明 Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。 Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题 Presto主要与FusionInsight的Hive和HDFS进行对接 配置Hive Connector Presto集群包括coordinator节点和不限数量的worker节点(coordinator节点也可同时为worker节点)，其中只需要在coordinator节点上配置Hive Connector即可。 本文档中配置coordinator节点同时也是worker节点。 从该链接下载presto-server的安装包，并上传到presto coordinator的节点 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz 将该压缩包解压缩后得到目录/opt/presto-server-0.184。 在presto节点上安装华为FusionInsight HD V100R002C70SPC100的客户端，默认安装目录/opt/hadoopclient presto该0.184版本要求jdk至少在1.8u60+以上，修改/etc/profile文件方式配置系统默认的java为FusionInsight HD客户端的jdk，并source环境变量，命令参考如下 在/etc/profile中增加以下行 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source 环境变量 source /etc/profile 创建Java keystore File for TLS，(后续步骤默认都在presto节点上执行)参考如下命令 source /opt/hadoopclient/bigdata_env keytool –genkeypair –alias testuser –keyalg RSA –keystore /opt/presto.jks alias后的值必须要跟后面创建的用户名称一致 first and last name必须写成presto节点的主机名 通过FusionInsight HD的管理页面创建一个“机机”用户，具体请参见《FusionInsight HD管理员指南》的 创建用户 章节。例如，创建用户testuser，并选择hadoop和hive用户组，下载对应的秘钥文件user.keytab以及krb5.conf文件，并上传到presto节点的/opt/hadoopclient目录下，将user.keytab改名为testuser.keytab。 参考如下命令在Huawei FusionInsight HD的Kerberos中创建一个新的principal，其名称为“testuser/presto-server”，其中presto-server为presto的coordinator节点的主机名，导出该principal的秘钥文件为/opt/presto.keytab。 执行kadmin –p kadmin/admin命令时初始密码Admin@123，修改后需严格牢记新密码。 创建目录/opt/presto-server-0.184/etc，在该目录下创建如下文件 config.properties参考如下 coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config参考如下内容 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties参考如下内容 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties参考如下内容 com.facebook.presto=INFO 创建目录/opt/presto-server-0.184/etc/catalog，在该目录下创建hive.properties文件 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml 其中hive.metastore.uri的值从/opt/hadoopclient/Hive/config/hive-site.xml中查找 将FusionInsight HD客户端中的core-site.xml和hdfs-site.xml复制到/opt/presto-server-0.184/etc/catalog中 cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ 按照下图修改hdfs-site.xml文件中的dfs.client.failover.proxy.provider.hacluster属性为org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml 修改/etc/hosts文件，将本机的IP与主机名解析以及Huawei FusionInsight HD集群节点的IP与主机名解析添加进去，例如 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v可以正确输出mvn版本 安装git yum install -y git 参考如下命令，下载presto-server-0.184的源码 git clone https://github.com/prestodb/presto.git git checkout 0.184 修改presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java的代码，将代码中\"Sasl.QOP=auth\"修改为\"Sasl.QOP=auth-conf\" 重新编译presto cd presto-hive mvn clean install -DskipTests 将编译后target目录下的presto-hive-0.184.jar文件替换/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar文件 启动presto server，跟踪/var/presto/data/var/log/server.log查看启动日志 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log 通过Presto CLI连接Hive 使用Presto CLI连接Huawei FusionInsight HD的Hive，使用presto自带的命令行工具执行SQL语句。 通过如下链接下载presto cli启动的jar包 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar 并将该jar包上传到可与presto节点网络互通的节点上(也可将presto coordinator节点作为cli使用节点)。 配置cli节点的jdk为1.8u60+以上版本 配置cli节点的/etc/hosts文件，将FI集群和presto coordinator节点的IP与主机名关系配置到cli节点 从presto节点拷贝presto.jks、presto.keytab、krb5.conf以及连接HDFS所需的core-site.xml和hdfs-site.xml文件到cli节点 将presto-cli-0.184-executable.jar包改为可执行文件 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h 创建presto cli启动脚本，类似如下，注意将相关文件的路径按实际位置替换 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog后面的hive是和presto coordinator节点配置的hive.properties的文件名匹配的，如果hive.properties改名为hivetest.properties，则这里改为hivetest 通过cli执行SQL语句，其他SQL语法请参考https://prestodb.io/docs/0.184/sql.html 查询表workers_info中数据： 百万记录数表web_sales查询： 通过Presto JDBC连接Hive 使用Presto JDBC接口连接Huawei FusionInsight HD Hive 从如下链接下载jdbc的驱动包 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar 参考https://prestodb.io/docs/0.184/installation/jdbc.html设置JDBC URL，用户名为任意字符，密码为空，在eclipse中调通的示例如下: import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; public class PrestoTest { public static void main(String[] args) throws SQLException, ClassNotFoundException { Class.forName(\"com.facebook.presto.jdbc.PrestoDriver\"); Connection connection =DriverManager.getConnection (\"jdbc:presto://162.1.115.71:8080/hive/default\",\"root\",null); Statement stmt =connection.createStatement(); ResultSet rs = stmt.executeQuery(\"select * from workers_info limit 10\"); int col = rs.getMetaData().getColumnCount(); while(rs.next()) { for (int i = 1; i 测试结果： 百万记录数表web_sales查询： "},"Database/":{"url":"Database/","title":"数据库","keywords":"","body":" 数据库 SAP HANA (TBD) "},"Other/":{"url":"Other/","title":"其他","keywords":"","body":" 其他 对接FUSE 对接Gis-Tools-For-Hadoop 对接Apache Livy IBM WAS (TBD) "},"Other/Using_FUSE_with_FusionInsight.html":{"url":"Other/Using_FUSE_with_FusionInsight.html","title":"对接FUSE","keywords":"","body":"FUSE对接FusionInsight HDFS 适用场景 fuse 2.8.3 FusionInsight HD V100R002C60U20（非安全模式） 说明 通过使用FUSE组件，可以使用将远端的HDFS文件系统mount到本端的Linux系统中使用 配置对接 安装jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz 配置环境变量/etc/profile，加入如下内容，source环境变量 export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH source /etc/profile 安装rpm包 yum install fuse fuse-devel fuse-libs 安装FusionInsight客户端，具体请参见产品文档的 安装客户端 章节，例如客户端安装目录为/opt/hadoopclient/ 下载Hadoop-2.7.2源码hadoop-2.7.2-src.tar.gz，编译fuse_dfs，将编译好的fuse_dfs拷贝到/opt目录下 将源码下的fuse_dfs_wrapper.sh脚本拷贝至/opt目录下，并根据实际情况作如下修改(修改HADOOP_PREFIX和JAVA_HOME的配置)： export HADOOP_PREFIX=/opt/hadoopclient/HDFS/hadoop if [ \"$OS_ARCH\" = \"\" ]; then export OS_ARCH=amd64 fi if [ \"$JAVA_HOME\" = \"\" ]; then export JAVA_HOME=/opt/jdk1.8.0_112 fi if [ \"$LD_LIBRARY_PATH\" = \"\" ]; then export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:$HADOOP_PREFIX/lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX/share ]; then export HADOOP_PREFIX=$HADOOP_PREFIX for f in ${HADOOP_PREFIX}/share/hadoop/hdfs/*.jar ; do export CLASSPATH=$CLASSPATH:$f done for f in $HADOOP_PREFIX/share/hadoop/hdfs/lib/*.jar; do export CLASSPATH=$CLASSPATH:$f done for f in ${HADOOP_PREFIX}/share/hadoop/common/lib/*.jar; do export CLASSPATH=$CLASSPATH:$f done for f in ${HADOOP_PREFIX}/share/hadoop/common/*.jar; do export CLASSPATH=$CLASSPATH:$f done export PATH=/opt:$PATH export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:$HADOOP_PREFIX/lib/native:/usr/local/lib:$LD_LIBRARY_PATH fi fuse_dfs $@ 更改文件权限 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source环境变量 source /opt/hadoopclient/bigdata_env 本地创建mount目录 mkdir –p /mnt/hdfs 执行挂载脚本，其中162-1-95-196是HDFS的NameNode(hacluster,主)的主机名 ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ 查看/mnt/hdfs目录 如果需要卸载挂载点，执行umount /mnt/hdfs即可 验证对接 hdfsclient写 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient读 hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse写 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse读 dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null "},"Other/Using_GIS_Tools_for_Hadoop_with_FusionInsight.html":{"url":"Other/Using_GIS_Tools_for_Hadoop_with_FusionInsight.html","title":"对接Gis-Tools-For-Hadoop","keywords":"","body":"GIS Tools for Hadoop对接FusionInsight 适用场景 GIS Tools for Hadoop FusionInsight HD V100R002C60U20 aggregation-hive 参考GIS说明https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive中关于集成Hive的示例，在华为FusionInsight HD中执行该示例。 获取gis源代码https://github.com/Esri/gis-tools-for-hadoop/ 完成FusionInsight HD V100R002C60U20的安装，包含Hive组件。 在FusionInsight Manager创建一个HiveAdmin角色，具体请参加《FusionInsight HD 管理员指南》的 创建Hive角色 章节。 在FusionInsight Manager创建一个“机机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。将用户加入上面创建的角色HiveAdmin。例如，创建用户 testuser 并下载对应的keytab文件user.keytab以及krb5.conf文件 安装FusionInsight HD的客户端，具体请参见《FusionInsight HD 管理员指南》的安装和使用客户端章节。 将下载的gis tools源码通过WinSCP工具上传到安装有FusionInsight HD客户端所在节点的/opt目录下，上传源码目录为gis-tools-for-hadoop-master 将下载的gis tools源码通过FusionInsight HD的客户端上传到HDFS文件系统中，将目录gis-tools-for-hadoop-master直接放到HDFS的根目录下，命令参考 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master 修改执行hive示例的sql文件，修改后的文件如下 set role admin; add jar hdfs:///gis-tools-for-hadoop-master/samples/lib/esri-geometry-api.jar; add jar hdfs:///gis-tools-for-hadoop-master/samples/lib/spatial-sdk-hadoop.jar; reload function; DROP TABLE earthquakes; DROP TABLE counties; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point'; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains'; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING, latitude DOUBLE, longitude DOUBLE, depth DOUBLE, magnitude DOUBLE, magtype string, mbstations string, gap string, distance string, rms string, source string, eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data'; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string, Perimeter string, State string, County string, Name string, BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data'; SELECT counties.name, count(*) cnt FROM counties JOIN earthquakes WHERE ST_Contains(counties.boundaryshape, ST_Point(earthquakes.longitude, earthquakes.latitude)) GROUP BY counties.name ORDER BY cnt desc; 使用FusionInsight HD客户端执行修改后的sql文件，命令参考 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql 执行结果如下，与GIS开源网站描述一致 aggregation-mr 参考GIS说明https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr中关于集成MR的示例，在华为FusionInsight HD中执行该示例。 获取gis源代码https://github.com/Esri/gis-tools-for-hadoop/ 完成FusionInsight HD V100R002C60U20的安装，包含Hive组件。 在FusionInsight Manager创建一个“机机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。将用户加入上面创建的角色HiveAdmin。例如，创建用户“testuser”并下载对应的keytab文件user.keytab以及krb5.conf文件 安装FusionInsight HD的客户端，具体请参见《FusionInsight HD 管理员指南》的安装和使用客户端章节。 将下载的gis tools源码通过WinSCP工具上传到安装有FusionInsight HD客户端所在节点的/opt目录下，上传源码目录为gis-tools-for-hadoop-master 修改/opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh如下，其中26004为yarn配置的yarn.resourcemanager.port端口 #!/bin/bash NAME_NODE_URL=hdfs://hacluster JOB_TRACKER_URL=162.1.93.103:26004 SAMPLE_DIR=/tmp/gistest JOB_DIR=$SAMPLE_DIR/job LIB_DIR=$SAMPLE_DIR/lib DATA_DIR=$SAMPLE_DIR/data OUTPUT_DIR=$SAMPLE_DIR/output 修改/opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh的执行权限，并执行 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh 执行完毕得到如下结果文件result.txt "},"Other/Using_Livy_with_FusionInsight.html":{"url":"Other/Using_Livy_with_FusionInsight.html","title":"对接Apache Livy","keywords":"","body":"Apache Livy对接FusionInsight 适用场景 Apache Livy 0.5.0-incubating FusionInsight HD V100R002C80SPC200 (Spark2.x) 安装Livy 操作场景 安装 Apache Livy 0.5.0 前提条件 已完成FusionInsight HD和客户端的安装。 操作步骤 安装Apache Livy 0.5.0-incubating，在网址https://livy.incubator.apache.org/download/下载安装包，使用WinSCP导入主机并用unzip livy-0.5.0-incubating-bin.zip解压生成livy-0.5.0-incubating-bin目录 执行source命令到客户端，获取java配置信息 source /opt/hadoopclient/bigdata_env echo $JAVA_HOME 根据产品文档创建用户developuser，并赋予足够权限，下载用户developuser的keytab文件user.keytab，上传至/opt/developuser目录下 在/usr/livy/livy-0.5.0-incubating-bin/conf路径下新建livy的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的developuser用户，将developuser的keytab文件user.key放在/opt/developuser/目录下 配置Livy环境变量，在profile文件中加入如下变量 vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH 编辑livy.conf文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf 加入如下内容： livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true livy.server.auty.type=kerberos livy.server.auth.kerberos.keytab=/opt/developuser/user.keytab livy.server.auth.kerberos.principal=developuser@HADOOP.COM livy.server.launch.kerberos.keytab=/opt/developuser/user.keytab livy.server.launch.kerberos.principal=developuser@HADOOP.COM 编辑livy-client.conf文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf 加入如本机ip地址： livy.rsc.rpc.server.address =172.16.52.190 编辑livy-env.sh文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh 加入如下内容： export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 编辑spark-blacklist.conf文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf 注销掉如下内容： spark.master spark.submit.deployMode 启动和停止Livy，在路径/usr/livy/livy-0.5.0-incubating-bin下 bin/livy-server start 启动成功后可以在http://172.16.52.190:8998访问到Livy服务器： 测试运行Livy样例代码 操作场景 测试运行Livy样例代码，包括Spark Shell，PySpark，SparkR 样例代码参考网址https://livy.incubator.apache.org/examples/ 前提条件 已完成FusionInsight HD和客户端的安装。 已完成Anaconda和R在客户端主机上的安装。 若没有安装Anaconda和R，请参考Zeppelin0.8.0对接FusionInsight HD V100R002C80SPC200 (Spark2.x)指导文档中连接Spark和SparkR部分相关内容 运行Spark样例操作步骤 输入命令python启动Anaconda 输入如下python代码启动一个Livy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) 当一个session完成启动后， 它将会变为闲置状态 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() 下面通过传递一个简单JSON命令行的方式来执行Scala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) 可以在Session0状态栏看到之前运行的样例代码以及结果 也可以在终端看到以JSON格式返回的结果 更新Scala再次运行 data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y 可以在Session0状态栏看到之前运行的样例代码以及结果 关闭session0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers) 运行PySpark样例操作步骤 继续接着上面的步骤，更改类型为pyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() 可以在Session状态栏看到新启动的Session1 通过传递JSON命令的方式执行Python样例代码，注意要更改statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y 可以在Session1状态栏看到之前运行的样例代码以及结果 关闭session1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers) 运行SparkR样例操作步骤 继续接着上面的步骤，更改类型为sparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() 可以在Session状态栏看到新启动的Session2 通过传递JSON命令的方式执行R样例代码，注意要更改statements_url data = { 'code': textwrap.dedent(\"\"\" hello 可以在Session2状态栏看到之前运行的样例代码以及结果 关闭session2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers) "}}