{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FusionData Ecosystem \u00b6 FusionInsight support opensource Hadoop interface, it can integrate with the following thridparty tools Thrid Party Tools FusionInsight Category Name Version C50 C60 C70 C80 6.5 Business Intelligence QlikView 12 Hive SparkSQL 12 Hive SparkSQL 12 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Data Analysis Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark Data Integration Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka Informatica PowerCenter 10.2.0 HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Talend 6.4.1 HDFS HBase Hive 7.0.1 HDFS HBase","title":"Home"},{"location":"#fusiondata-ecosystem","text":"FusionInsight support opensource Hadoop interface, it can integrate with the following thridparty tools Thrid Party Tools FusionInsight Category Name Version C50 C60 C70 C80 6.5 Business Intelligence QlikView 12 Hive SparkSQL 12 Hive SparkSQL 12 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Data Analysis Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark Data Integration Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka Informatica PowerCenter 10.2.0 HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Talend 6.4.1 HDFS HBase Hive 7.0.1 HDFS HBase","title":"FusionData Ecosystem"},{"location":"Business_Intelligence/","text":"Business Intelligence \u00b6 QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80","title":"Home"},{"location":"Business_Intelligence/#business-intelligence","text":"QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80","title":"Business Intelligence"},{"location":"Business_Intelligence/QlikView/","text":"QlikView and FusionInsight Instruction \u00b6 Case \u00b6 QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Configuring kerberos authentication for Windows \u00b6 Download and install MIT Kerberos at: http://web.mit.edu/kerberos/dist/#kfw-4.0 The version is consistent with the number of operating system bits. This article is version kfw-4.1-amd64.msi. Verify that the time of the client machine is the same as that of the FusionInsight HD cluster. The time difference is less than 5 minutes. Set the Kerberos configuration file Create a role and machine user on the FusionInsight Manager. For details, see the chapter \"Creating Users\" in the FusionInsight HD Administrator Guide. The role needs to grant Hive access rights based on business needs and add users to the role. For example, create the user \"sparkdemo\" and download the corresponding keytab file user.keytab and krb5.conf file, rename the krb5.conf file to krb5.ini, and put it in the C:\\ProgramData\\MIT\\Kerberos5 directory. Set the cache file for the Kerberos ticket Create a directory to store the tickets, such as C:\\temp . Set the system environment variable of Windows, the variable name is \"KRB5CCNAME\", and the variable value is C:\\temp\\krb5cache Restart the machine. Authenticate on Windows Use the command line to go to the MIT Kerberos installation path and find the executable kinit.exe. For example, the path to this article is: C:\\Program Files\\MIT\\Kerberos\\bin Execute the following command: kinit -k -t /path_to_userkeytab/user.keytab UserName Path_to_userkeytab is the path where the user keytab file is stored, user.keytab is the user's keytab, and UserName is the user name. Configuring a Hive data source \u00b6 Hive data source in QlikView, docking Hive's ODBC interface Download and install the Hive ODBC driver \u00b6 Download the driver from the following address and select the corresponding ODBC version according to the operating system type, download and install: address Configuring user DSN \u00b6 * On the User DSN tab of the OBDC Data Source Manager page, click Add to configure the user data source. On the Create Data Source page, find the Cloudera ODBC Driver for Apache Hive , select it and click Finish . Configure the Hive data source. Data Source Name: is a custom parameter Host(s): HiveServer business ip Port: Hive Service port, 21066 Mechanism: Kerberos Host FQDN: hadoop.hadoop.com Service Name: hive Realm: Leave blank Click Test If the connection is successful, the configuration is successful. Click OK Connect to the Hive data source \u00b6 Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar * In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Source page, select the data source hive_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button * In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website. Configuring a Spark data source \u00b6 Configure the Spark data source in QlivView and connect to the thrift interface of SparkSQL. Download and install Spark's ODBC driver \u00b6 Download the Spark ODBC driver on Simba's official website, select 32bit or 64bit according to the user's own operating system, and select Spark for Data Source. SQL, address: http://www.tableau.com/support/drivers * Install the client as prompted by the install client. Configuring user DSN \u00b6 On the OBDC Data Source Manager page, on the User DSN tab, click Add to configure the user data source. On the Create Data Source page, find Simba Spark ODBC Driver , select it and click Finish . Configure the Spark data source on the Simba Spark ODBC Driver DSN Setup page. Data Source Name\uff1a Custom Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a Leave blank, Host(s)\uff1a JDBCServer (main) service ip, Port\uff1a SparkThriftServer client port number 23040. After setting, click Advanced Options , in the pop-up Advanced Options page, check Use Native Query and Get Tables With Query , then click OK Go back to the Simba Spark ODBC Driver DSN Setup , click Test to connect successfully, click OK to exit the page, otherwise a failure dialog will pop up. Go back to the Simba Spark ODBC Driver DSN Setup page, click OK , go back to the ODBC Data Source Manager page, click OK to complete and exit the configuration. Connect to a Spark data source \u00b6 Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Sources page, select the data source spark_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website. FAQ \u00b6 Cannot find C:\\ProgramData\\MIT\\Kerberos5 folder C:\\ProgramData is generally a hidden folder, setting the folder to be hidden or using the search function to solve the problem. Connection succeeded without database permissions The user used for the connection needs to have the permissions of the database, otherwise the ODBC connection will succeed but the database content cannot be read. ODBC connection failed The common situation is that the input data of Host(s) , Port , Host FQDN is incorrect. Please enter according to the actual situation.","title":"12 <--> C80"},{"location":"Business_Intelligence/QlikView/#qlikview-and-fusioninsight-instruction","text":"","title":"QlikView and FusionInsight Instruction"},{"location":"Business_Intelligence/QlikView/#case","text":"QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL)","title":"Case"},{"location":"Business_Intelligence/QlikView/#configuring-kerberos-authentication-for-windows","text":"Download and install MIT Kerberos at: http://web.mit.edu/kerberos/dist/#kfw-4.0 The version is consistent with the number of operating system bits. This article is version kfw-4.1-amd64.msi. Verify that the time of the client machine is the same as that of the FusionInsight HD cluster. The time difference is less than 5 minutes. Set the Kerberos configuration file Create a role and machine user on the FusionInsight Manager. For details, see the chapter \"Creating Users\" in the FusionInsight HD Administrator Guide. The role needs to grant Hive access rights based on business needs and add users to the role. For example, create the user \"sparkdemo\" and download the corresponding keytab file user.keytab and krb5.conf file, rename the krb5.conf file to krb5.ini, and put it in the C:\\ProgramData\\MIT\\Kerberos5 directory. Set the cache file for the Kerberos ticket Create a directory to store the tickets, such as C:\\temp . Set the system environment variable of Windows, the variable name is \"KRB5CCNAME\", and the variable value is C:\\temp\\krb5cache Restart the machine. Authenticate on Windows Use the command line to go to the MIT Kerberos installation path and find the executable kinit.exe. For example, the path to this article is: C:\\Program Files\\MIT\\Kerberos\\bin Execute the following command: kinit -k -t /path_to_userkeytab/user.keytab UserName Path_to_userkeytab is the path where the user keytab file is stored, user.keytab is the user's keytab, and UserName is the user name.","title":"Configuring kerberos authentication for Windows"},{"location":"Business_Intelligence/QlikView/#configuring-a-hive-data-source","text":"Hive data source in QlikView, docking Hive's ODBC interface","title":"Configuring a Hive data source"},{"location":"Business_Intelligence/QlikView/#download-and-install-the-hive-odbc-driver","text":"Download the driver from the following address and select the corresponding ODBC version according to the operating system type, download and install: address","title":"Download and install the Hive ODBC driver"},{"location":"Business_Intelligence/QlikView/#configuring-user-dsn","text":"* On the User DSN tab of the OBDC Data Source Manager page, click Add to configure the user data source. On the Create Data Source page, find the Cloudera ODBC Driver for Apache Hive , select it and click Finish . Configure the Hive data source. Data Source Name: is a custom parameter Host(s): HiveServer business ip Port: Hive Service port, 21066 Mechanism: Kerberos Host FQDN: hadoop.hadoop.com Service Name: hive Realm: Leave blank Click Test If the connection is successful, the configuration is successful. Click OK","title":"Configuring user DSN"},{"location":"Business_Intelligence/QlikView/#connect-to-the-hive-data-source","text":"Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar * In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Source page, select the data source hive_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button * In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website.","title":"Connect to the Hive data source"},{"location":"Business_Intelligence/QlikView/#configuring-a-spark-data-source","text":"Configure the Spark data source in QlivView and connect to the thrift interface of SparkSQL.","title":"Configuring a Spark data source"},{"location":"Business_Intelligence/QlikView/#download-and-install-sparks-odbc-driver","text":"Download the Spark ODBC driver on Simba's official website, select 32bit or 64bit according to the user's own operating system, and select Spark for Data Source. SQL, address: http://www.tableau.com/support/drivers * Install the client as prompted by the install client.","title":"Download and install Spark's ODBC driver"},{"location":"Business_Intelligence/QlikView/#configuring-user-dsn_1","text":"On the OBDC Data Source Manager page, on the User DSN tab, click Add to configure the user data source. On the Create Data Source page, find Simba Spark ODBC Driver , select it and click Finish . Configure the Spark data source on the Simba Spark ODBC Driver DSN Setup page. Data Source Name\uff1a Custom Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a Leave blank, Host(s)\uff1a JDBCServer (main) service ip, Port\uff1a SparkThriftServer client port number 23040. After setting, click Advanced Options , in the pop-up Advanced Options page, check Use Native Query and Get Tables With Query , then click OK Go back to the Simba Spark ODBC Driver DSN Setup , click Test to connect successfully, click OK to exit the page, otherwise a failure dialog will pop up. Go back to the Simba Spark ODBC Driver DSN Setup page, click OK , go back to the ODBC Data Source Manager page, click OK to complete and exit the configuration.","title":"Configuring user DSN"},{"location":"Business_Intelligence/QlikView/#connect-to-a-spark-data-source","text":"Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Sources page, select the data source spark_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website.","title":"Connect to a Spark data source"},{"location":"Business_Intelligence/QlikView/#faq","text":"Cannot find C:\\ProgramData\\MIT\\Kerberos5 folder C:\\ProgramData is generally a hidden folder, setting the folder to be hidden or using the search function to solve the problem. Connection succeeded without database permissions The user used for the connection needs to have the permissions of the database, otherwise the ODBC connection will succeed but the database content cannot be read. ODBC connection failed The common situation is that the input data of Host(s) , Port , Host FQDN is incorrect. Please enter according to the actual situation.","title":"FAQ"},{"location":"Business_Intelligence/Tableau/","text":"Connection Instruction between Tableau and FusionInsight \u00b6 Succeeded Case \u00b6 Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Configure the Kerberos on Windows \u00b6 Download and install MIT Kerberos from the following URL http://web.mit.edu/kerberos/dist/#kfw-4.0 Make sure the time differences between FusionInsight clusters and Tableau client is no longer than 5 minutes. Configure required Kerberos filesystem Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf files on the Tableau client, rename the krb5.conf file into krb5.ini and save it into the following directory C:\\ProgramData\\MIT\\Kerberos5 Configure Kerberos cache file Create a directory to save the Kerberos cache file, for example, C:\\temp Configure the Environment Variables, Variable name is KRB5CCNAME , Variable value is C:\\temp\\krb5cache restart the Tableau client Start Kerberos Authentication on Windows use the created username and password to log in, the Principal is equal to username@Kerberos Realm name Open MIT Kerberos , click on Get Ticket, and type in the right Principal and Password for authentication Connecting Tableau to Hive \u00b6 Configure the ODBC interface to connect FusionInsight HiveDriver Download and install the ODBC driver Download URL: http://www.cloudera.com/content/cloudera/en/downloads/connectors/hive/odbc/hive-odbc-v2-5-15.html and choose the right one depends on the OS and bit version Configure ODBC drivers Open ODBC Data Sources(64-bit) by searching the keyword ODBC on Windows click on User DSN tab, click on Add button, choose Cloudera ODBC Driver for Apache Hive and click on Finish to start to configure In detail: 1: Hive Server 2 2: No Service Discovery 3: 172.21.3.101 4: 21006 5: default 6: Kerberos 7: hadoop.hadoop.com 8: hive 9: SASL click on Test button to test the connection Open Tableau Click on More option, and choose ODBC by search in keyword Connection Configuration shown as bellow: click on Connect and then click on Sign In Search the Data Search the data from multiple tables Connecting Tableau to Spark \u00b6 Download and install the ODBC driver for Spark Download url http://www.tableau.com/support/drivers Created DSN \uff08Data Source Name\uff09 Open ODBC Data Sources(64-bit) Click on System DSN tab, click on Add, choose Simba Spark ODBC Driver and click on Finish Open the installed Driver directory, for example, C:\\Program Files\\Simba Spark ODBC Driver\\lib and open the DriverConfiguration64.exe to Configure In detail: 1\uff1aSparkThriftServer (Spark 1.1 and later) 2: Kerberos 3: hadoop.hadoop.com 4: spark 5: SASL click on Advanced Options and choose \"Driver Config Take Precedence\" Click on ok to save the configuration Open Tableau Click on More option, and choose Spark SQL by search in keyword Connection Configuration shown as bellow: Server info can be got from FusionInsight Manager Web UI Port info can be got from FusionInsight Manager Web UI as well Click on Sign In , to come into a new Page, choose Schema and Table shown as bellow Open Sheet to visulize the data Performance test Search the table web_sales which contains millions of records Search the table by multiple tables whose names are store_sales and item Add customer_address table Test outcome\uff1a FAQ \u00b6 Cannot find C:\\ProgramData\\MIT\\Kerberos5 This Folder is hidden, configure the windows can solve it Connection succeeded but permission denied Use the user who has the privilege to DATABASE","title":"10.5.0 <--> C80"},{"location":"Business_Intelligence/Tableau/#connection-instruction-between-tableau-and-fusioninsight","text":"","title":"Connection Instruction between Tableau and FusionInsight"},{"location":"Business_Intelligence/Tableau/#succeeded-case","text":"Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL)","title":"Succeeded Case"},{"location":"Business_Intelligence/Tableau/#configure-the-kerberos-on-windows","text":"Download and install MIT Kerberos from the following URL http://web.mit.edu/kerberos/dist/#kfw-4.0 Make sure the time differences between FusionInsight clusters and Tableau client is no longer than 5 minutes. Configure required Kerberos filesystem Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf files on the Tableau client, rename the krb5.conf file into krb5.ini and save it into the following directory C:\\ProgramData\\MIT\\Kerberos5 Configure Kerberos cache file Create a directory to save the Kerberos cache file, for example, C:\\temp Configure the Environment Variables, Variable name is KRB5CCNAME , Variable value is C:\\temp\\krb5cache restart the Tableau client Start Kerberos Authentication on Windows use the created username and password to log in, the Principal is equal to username@Kerberos Realm name Open MIT Kerberos , click on Get Ticket, and type in the right Principal and Password for authentication","title":"Configure the Kerberos on Windows"},{"location":"Business_Intelligence/Tableau/#connecting-tableau-to-hive","text":"Configure the ODBC interface to connect FusionInsight HiveDriver Download and install the ODBC driver Download URL: http://www.cloudera.com/content/cloudera/en/downloads/connectors/hive/odbc/hive-odbc-v2-5-15.html and choose the right one depends on the OS and bit version Configure ODBC drivers Open ODBC Data Sources(64-bit) by searching the keyword ODBC on Windows click on User DSN tab, click on Add button, choose Cloudera ODBC Driver for Apache Hive and click on Finish to start to configure In detail: 1: Hive Server 2 2: No Service Discovery 3: 172.21.3.101 4: 21006 5: default 6: Kerberos 7: hadoop.hadoop.com 8: hive 9: SASL click on Test button to test the connection Open Tableau Click on More option, and choose ODBC by search in keyword Connection Configuration shown as bellow: click on Connect and then click on Sign In Search the Data Search the data from multiple tables","title":"Connecting Tableau to Hive"},{"location":"Business_Intelligence/Tableau/#connecting-tableau-to-spark","text":"Download and install the ODBC driver for Spark Download url http://www.tableau.com/support/drivers Created DSN \uff08Data Source Name\uff09 Open ODBC Data Sources(64-bit) Click on System DSN tab, click on Add, choose Simba Spark ODBC Driver and click on Finish Open the installed Driver directory, for example, C:\\Program Files\\Simba Spark ODBC Driver\\lib and open the DriverConfiguration64.exe to Configure In detail: 1\uff1aSparkThriftServer (Spark 1.1 and later) 2: Kerberos 3: hadoop.hadoop.com 4: spark 5: SASL click on Advanced Options and choose \"Driver Config Take Precedence\" Click on ok to save the configuration Open Tableau Click on More option, and choose Spark SQL by search in keyword Connection Configuration shown as bellow: Server info can be got from FusionInsight Manager Web UI Port info can be got from FusionInsight Manager Web UI as well Click on Sign In , to come into a new Page, choose Schema and Table shown as bellow Open Sheet to visulize the data Performance test Search the table web_sales which contains millions of records Search the table by multiple tables whose names are store_sales and item Add customer_address table Test outcome\uff1a","title":"Connecting Tableau to Spark"},{"location":"Business_Intelligence/Tableau/#faq","text":"Cannot find C:\\ProgramData\\MIT\\Kerberos5 This Folder is hidden, configure the windows can solve it Connection succeeded but permission denied Use the user who has the privilege to DATABASE","title":"FAQ"},{"location":"Data_Analysis/","text":"Data Analysis \u00b6 Rapidminer Studio 8.2.001 \u2194 C80","title":"Home"},{"location":"Data_Analysis/#data-analysis","text":"Rapidminer Studio 8.2.001 \u2194 C80","title":"Data Analysis"},{"location":"Data_Analysis/RapidMiner/","text":"Connection between RapidMiner with FusionInsightHD \u00b6 Succeeded Case \u00b6 Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark) preparation \u00b6 Download and install RapidMiner Studio, download site https://rapidminer.com/ Start rapidminer, on the top of the main menu, choose Extensions->Marketplace ,type radoop ,install it and restart rapidminer Configure the local host file\uff0cfile path is C:\\Windows\\System32\\drivers\\etc \uff0cadd the cluster node ip and host name and save the file. Configure Kerberos file Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive, Spark,HDFS privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf of the user and save them in your computer. Prepare the FusionInsight client configuration filesystem and jar files In the Manager GUI\uff0cchoose Service->Download Client->Only Configuration File Unzip the file,find the following files,copy them into a directory,like ../config . Open yarn-site.xml ,delete the following property: <property> <name>audit.service.name</name> <value>Yarn</value> </property> Login to one of the cluster nodes, go to the following path \\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars ,download the file directory /jars ,save it in your computer,like C:/jars \u3002 Configure the cluster \u00b6 Bind the UDP port Download the UDP port bind tool uredir ,website is https://github.com/troglobit/uredir After building and installing, we get the executing file uredir ,upload it to the KDC server nodes in the cluster,and run the following command,here IP refers to the node ip. ./uredir IP:88 IP:21732 Configure Radoop Jars Download Radoop jars in this address https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,get the correct version\u3002 Upload the jar files to each node of the cluster,eg, /usr/local/lib/radoop/ In the HiveServer node of the cluster,uplaod the Radoop jar files to the following path and change their owner and execution authority ``` cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar `` * In the FusionInsight Manager GUI, choose Service->Hive->Service Configuration`add the following configuration radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name - Notice that there should be a | as seperater Save the configuration\uff0crestart HiveServer Create Radoop UDF functions Run the following command in the client node, login to the Hive database source /opt/hadoopclient\u3001bigdata_env kinit developuser beeline create a database in Hive, for example rapidminer , and create functions,run the following commands in beeline mode create database rapidminer; use rapidminer; DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep'; RapidMiner Configuration \u00b6 In RapidMiner sStadio\uff0cchoose Connections->Manage Radoop Connections in the top menu. choose New Connections->Import Hadoop Configuration Files ,choose the configuration files downloaded from the cluster,click Import Configuration After the import, click Next , go to the Connection settings window,configure ad following: Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory: Spark jar files downloaded from the cluster Client Principal: Kerberos user name @HADOOP.com Keytab File: the keytab file downloaded from manager KDC Address: the KDC server IP(see the krb5.conf file) REALM: HADOOP.COM Kerberos Config File: the krb5 file downloaded from manager Hadoop\uff1a At the filter in upper right corner, search split , uncheck mapreduce.input.fileinputformat.split.maxsize Search classpath ,uncheck mapreduce.application.classpath Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive(or libs)Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1aadd the following two parameters for spark: park.driver.extraJavaOptions and spark.executor.extraJavaOptions The value can be found in manager GUI, choose Services->Spark2X Configuration->type all \uff0csearch extraJavaOptions in the search bar, choose the parameters in Spark2x->SparkResource2x Copy the values into a text file, replace the relative path ./ in the value with absolute path in the cluster, like /opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc , then copy the values into rapidminer spark configuration Hive: Hive Version: Hive Server2 Hive Server Address\uff1aHive Server IP Hive Port: 21066 Database Name: the database name created in Hive,here is rapidminer Customer database for UDFs: same as before click OK->Proced Anyway->Save Test the Connection \u00b6 Click Configure, in Global tab, click Test\uff0cTest Results show as following: In Hadoop tab\uff0cclick Test,Test Results show as following: In Spark tab,click Test,Test Results show as following: In Hive tab, click Test, Test Results show as following: Click Full test,Test Results show as following: Radoop Demo \u00b6 In RapidMiner Studio main menu,choose Help->Tutorials->User Hadoop->Rapidminer Radoop Run the demo accordding to the Tutorials, get the follwing results","title":"8.2.001 <--> C80"},{"location":"Data_Analysis/RapidMiner/#connection-between-rapidminer-with-fusioninsighthd","text":"","title":"Connection between RapidMiner with FusionInsightHD"},{"location":"Data_Analysis/RapidMiner/#succeeded-case","text":"Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark)","title":"Succeeded Case"},{"location":"Data_Analysis/RapidMiner/#preparation","text":"Download and install RapidMiner Studio, download site https://rapidminer.com/ Start rapidminer, on the top of the main menu, choose Extensions->Marketplace ,type radoop ,install it and restart rapidminer Configure the local host file\uff0cfile path is C:\\Windows\\System32\\drivers\\etc \uff0cadd the cluster node ip and host name and save the file. Configure Kerberos file Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive, Spark,HDFS privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf of the user and save them in your computer. Prepare the FusionInsight client configuration filesystem and jar files In the Manager GUI\uff0cchoose Service->Download Client->Only Configuration File Unzip the file,find the following files,copy them into a directory,like ../config . Open yarn-site.xml ,delete the following property: <property> <name>audit.service.name</name> <value>Yarn</value> </property> Login to one of the cluster nodes, go to the following path \\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars ,download the file directory /jars ,save it in your computer,like C:/jars \u3002","title":"preparation"},{"location":"Data_Analysis/RapidMiner/#configure-the-cluster","text":"Bind the UDP port Download the UDP port bind tool uredir ,website is https://github.com/troglobit/uredir After building and installing, we get the executing file uredir ,upload it to the KDC server nodes in the cluster,and run the following command,here IP refers to the node ip. ./uredir IP:88 IP:21732 Configure Radoop Jars Download Radoop jars in this address https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,get the correct version\u3002 Upload the jar files to each node of the cluster,eg, /usr/local/lib/radoop/ In the HiveServer node of the cluster,uplaod the Radoop jar files to the following path and change their owner and execution authority ``` cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar `` * In the FusionInsight Manager GUI, choose Service->Hive->Service Configuration`add the following configuration radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name - Notice that there should be a | as seperater Save the configuration\uff0crestart HiveServer Create Radoop UDF functions Run the following command in the client node, login to the Hive database source /opt/hadoopclient\u3001bigdata_env kinit developuser beeline create a database in Hive, for example rapidminer , and create functions,run the following commands in beeline mode create database rapidminer; use rapidminer; DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep';","title":"Configure the cluster"},{"location":"Data_Analysis/RapidMiner/#rapidminer-configuration","text":"In RapidMiner sStadio\uff0cchoose Connections->Manage Radoop Connections in the top menu. choose New Connections->Import Hadoop Configuration Files ,choose the configuration files downloaded from the cluster,click Import Configuration After the import, click Next , go to the Connection settings window,configure ad following: Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory: Spark jar files downloaded from the cluster Client Principal: Kerberos user name @HADOOP.com Keytab File: the keytab file downloaded from manager KDC Address: the KDC server IP(see the krb5.conf file) REALM: HADOOP.COM Kerberos Config File: the krb5 file downloaded from manager Hadoop\uff1a At the filter in upper right corner, search split , uncheck mapreduce.input.fileinputformat.split.maxsize Search classpath ,uncheck mapreduce.application.classpath Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive(or libs)Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1aadd the following two parameters for spark: park.driver.extraJavaOptions and spark.executor.extraJavaOptions The value can be found in manager GUI, choose Services->Spark2X Configuration->type all \uff0csearch extraJavaOptions in the search bar, choose the parameters in Spark2x->SparkResource2x Copy the values into a text file, replace the relative path ./ in the value with absolute path in the cluster, like /opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc , then copy the values into rapidminer spark configuration Hive: Hive Version: Hive Server2 Hive Server Address\uff1aHive Server IP Hive Port: 21066 Database Name: the database name created in Hive,here is rapidminer Customer database for UDFs: same as before click OK->Proced Anyway->Save","title":"RapidMiner Configuration"},{"location":"Data_Analysis/RapidMiner/#test-the-connection","text":"Click Configure, in Global tab, click Test\uff0cTest Results show as following: In Hadoop tab\uff0cclick Test,Test Results show as following: In Spark tab,click Test,Test Results show as following: In Hive tab, click Test, Test Results show as following: Click Full test,Test Results show as following:","title":"Test the Connection"},{"location":"Data_Analysis/RapidMiner/#radoop-demo","text":"In RapidMiner Studio main menu,choose Help->Tutorials->User Hadoop->Rapidminer Radoop Run the demo accordding to the Tutorials, get the follwing results","title":"Radoop Demo"},{"location":"Data_Integration/","text":"Data Integration \u00b6 Apache NiFi 1.7.1 \u2194 C80 Informatica PowerCenter 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 6.5 Talend 6.4.1 \u2194 C80 7.0.1 \u2194 C80","title":"Home"},{"location":"Data_Integration/#data-integration","text":"Apache NiFi 1.7.1 \u2194 C80 Informatica PowerCenter 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 6.5 Talend 6.4.1 \u2194 C80 7.0.1 \u2194 C80","title":"Data Integration"},{"location":"Data_Integration/Apache_NiFi/","text":"Connection Instruction between Apache NiFi and FusionInsight \u00b6 Succeeded Case \u00b6 Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka) Installing Apache NiFi \u00b6 Purpose \u00b6 Installing Apache NiFi 1.7.1 Prerequisites \u00b6 Installing FusionInsight HD cluster and its client completed Procedure \u00b6 Get JAVA_HOME configuration by execute source command on client side source /opt/hadoopclient/bigdata_env echo $JAVA_HOME Download NiFi installation file from https://nifi.apache.org/download.html , move the file to client side by using tool WinSCP , execute command unzip nifi-1.7.1-bin.zip to unzip the installation file to the following directory /usr/nifi/nifi-1.7.1 Configure NiFi server IP address and port by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 Start and Stop NiFi server cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop Start NiFi Server bin/nifi.sh start Configuring Kerberos authentication within NiFi \u00b6 Purpose \u00b6 Configuring Kerberos authentication within NiFi server for the later connection usage Prerequisites \u00b6 Installing Apache NiFi completed Installing FusionInsight HD cluster and its client completed Create a developuser for connection Procedure \u00b6 Download the required Kerberos authentication files user.keytab and krb5.conf from FusionInsight HD Manager site, save the files into the following directory /opt/developuser Configure Kerberos authentication by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file Detailed Configuration\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find KeytabCredentialsService and click ADD Click on **gear** icon to configure ![](assets/Apache_NiFi/markdown-img-paste-20180912174747644.png) ![](assets/Apache_NiFi/markdown-img-paste-2018091217482271.png) Click on **lightning** icon to enable and save the KeytabCredentialsService ![](assets/Apache_NiFi/markdown-img-paste-20180912174904147.png) ![](assets/Apache_NiFi/markdown-img-paste-20180912175037790.png) Completed Connecting NiFi to HDFS \u00b6 Purpose \u00b6 Configuring NiFi related HDFS processor to connect FusionInsight HD HDFS Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed PutHDFS Procedure \u00b6 Find and Copy the hdfs-site.xml \uff0c core-site.xml files which located in FusionInsight HD client to the following directory /usr/nifi/nifi-1.7.1/conf Make an adjustment to the content of hdfs-site.xml that is to delete the following property <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> Make an adjustment to the content of core-site.xml that is to change halcluster into detailed namenode ip with its port <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset The configuration of processor PutHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest The configuration of the connection between two former processors Move the file nifiHDFS.csv into the following directory /home/dataset before test start Content of nifiHDFS.csv \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - Test completed Log into FusionInsight HDFS to check the test outcome by using the following command hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest/HDFS The configuration of processor PutFile In detail\uff1a 1: /home/dataset/HDFS Move the file nifiHDFS.csv into HDFS directory /tmp/nifitest/HDFS Test completed Log into the FusionInsight HD client side to check the outcome with the directory /home/dataset/HDFS ListHDFS & FetchHDFS Procedure \u00b6 The whole process shown as the following pic: The configuration of processor ListHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest The configuration of processor RouteOnAttribute Note: Add one customized property requiredfilenames with the value ${filename:matches('sanguo.*')} by clicking on plus icon In detail\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} The relationship configuration between processor RouteOnAttribute and upper processor FetchHDFS shown as the following pic The relationship configuration between processor RouteOnAttribute and lower processor FetchHDFS shown as the following pic The configuration of processor FetchHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService The configuration of upper processor PutFile The configuration of lower processor PutFile Check the files on FusionInsight HDFS by executing command hdfs dfs -ls /tmp/nifitest Test completed Log into FusionInsight HD client side to check the outcomes separately Connecting NiFi to Hive \u00b6 Purpose \u00b6 Configuring NiFi Hive processor to connect FusionInsight HD Hive Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed HiveConnectionPool Procedure \u00b6 Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HiveConnectionPool and click ADD Click on gear icon to configure In detail 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService Click on lightning icon to enable and save the HiveConnectionPool Completed Create jaas.conf file which located at directory /usr/nifi/nifi-1.7.1/conf wit the following content Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; Make an adjustment to the bootstrap.conf file by executing following command vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true Make an adjustment to the nifi.properties file by executing following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true Execute the following command to come into the directory of NiFi Hive related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar SelectHiveQL read Hive table Procedure \u00b6 The whole process shown as the following pic: The configuration of processor SelectHiveQL In detail\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV The configuration of processor PutFile Log into FusionInsight cluster to check table t2 on hive Completed Check the outcome by log into the following directory /home/dataset/HIVE PutHiveQL load whole table Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2: iris.txt Content of iris.txt \uff1a 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa The configuration of processor PutHDFS In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive The configuration of processor ReplaceText In detail\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; The configuration of processor PutHiveQL Move the data file iris.txt into the following directory /home/dataset/ before test Completed: Login the HIVE to check the test outcome PutHiveQL Load the table by rows Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt Content of iris_add.txt \uff1a \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" The configuration of processor SplitText There is no change for the configuration of processor ExtractText The configuration of processor ReplaceText The configuration of processor PutHiveQL Move the data file iris_add.txt into the following directory /home/dataset/ before test Completed\uff1a Login the HIVE to check the test outcome\uff1a Connecting NiFi to HBase \u00b6 Purpose \u00b6 Configuring NiFi HBase processor to connect FusionInsight HD HBase Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed HBase_1_1_2_ClientService Procedure \u00b6 Move the hbase related configuration file hbase-site.xml which is within the FusionInsight HD client side into the following directory /usr/nifi/nifi-1.7.1/conf Execute the following command to come into the directory of NiFi HBase related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HBase_1_1_2_ClientService and click ADD Click on gear icon to configure In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService Click on lightining icon to enable and save the HBase_1_1_2_ClientService Completed PutHBaseJSON load the table Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile Content of hbase_test.csv \uff1a 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica The configuration of processor InverAvroSchema In detail\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data The configuration of processor ConvertCSVToAvro The configuration of processor ConvertAvroToJSON The configuration of processor SplitJson The configuration of processor PutHBaseJSON In detail: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data Move the data file hbase_test.csv into the following directory /home/dataset/HBASE before test In addition, execute following command to create a HBase table hbase shell create 'HBase_test','data' Completed\uff1a Login into the FusionInsight HD cluster to check the outcome: GetHbase Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetHBase The configuration of processor PutFile Completed Login into the following directory /home/dataset/GetHBase_test to check the test outcome Connecting NiFi to Spark \u00b6 Purpose \u00b6 Configuring NiFi Livy Session processor to connect FusionInsight HD Spark Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed Installing and configuring Apache Livy 0.5.0 (Apache Livy can be installed on test host or any other host as long as they can connect to each other including FusionInsight HD cluster) There exist connection instruction between Apache Livy and FusionInsight, please check the FusionInsight ecosystem LivySessionController Procedure \u00b6 Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: spark 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_PySpark In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: pysaprk 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_SparkR In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: sparkr 4\uff1aKeytabCredentialsService Click on lightining icon to enable and save the LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR Completed Spark Sample Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code1.txt Content of code1.txt \uff1a 1+2 The configuration of processor ExtractText Click plus icon to add a Property code1 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController 2: ${code1} Move the code file code1.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed\uff1a Log into the Livy server to check the outcome PySpark Sample Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code2.txt Content of code2.txt \uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) The configuration of processor ExtractText Click plus icon to add a Property code2 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController_PySpark 2: ${code2} Move the code file code2.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome SparkR Sample Procedure \u00b6 The whole process shown as the following pic: Note: It's different by comparing to example of former Spark and PySpark The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code3.txt Content of code3.txt \uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: /home/dataset/sparkTest 2: code content of code3.txt Move the code file code3.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome Connecting NiFi to Kafka \u00b6 Purpose \u00b6 Configuring NiFi Kafka processor to connect FusionInsight HD Kafka Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed GetHTTP & PutKafka Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetHTTP In detail\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv The configuration of processor PutKafka In detail\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi Before test\uff1a Log into the Kafka component within FusionInsightHD client side and create a Topic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 Completed\uff1a Log into the kafka component within FusionInsightHD client side to check the outcome cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning ConsumeKafka_0_11 Procedure \u00b6 The whole process shown as the following pic: The configuration of processor ConsumeKafka_0_11 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer The configuration of processor PutFile Before test\uff1a Open the kafka-examples which provided by FusionInsightHD client in eclipse, configure the kafka-examples so that it can be successfully ran and produce messages to kafka Note: There must be a producer when testing the NiFi ConsumeKafka_0_11 processor, run NewProducer.java within kafka-examples at first and then start to test NiFi ConsumeKafka_0_11 Completed\uff1a Log into the follow directory /home/dataset/Kafka to check the test outcome","title":"1.7.1 <--> C80"},{"location":"Data_Integration/Apache_NiFi/#connection-instruction-between-apache-nifi-and-fusioninsight","text":"","title":"Connection Instruction between Apache NiFi and FusionInsight"},{"location":"Data_Integration/Apache_NiFi/#succeeded-case","text":"Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka)","title":"Succeeded Case"},{"location":"Data_Integration/Apache_NiFi/#installing-apache-nifi","text":"","title":"Installing Apache NiFi"},{"location":"Data_Integration/Apache_NiFi/#purpose","text":"Installing Apache NiFi 1.7.1","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites","text":"Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#procedure","text":"Get JAVA_HOME configuration by execute source command on client side source /opt/hadoopclient/bigdata_env echo $JAVA_HOME Download NiFi installation file from https://nifi.apache.org/download.html , move the file to client side by using tool WinSCP , execute command unzip nifi-1.7.1-bin.zip to unzip the installation file to the following directory /usr/nifi/nifi-1.7.1 Configure NiFi server IP address and port by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 Start and Stop NiFi server cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop Start NiFi Server bin/nifi.sh start","title":"Procedure"},{"location":"Data_Integration/Apache_NiFi/#configuring-kerberos-authentication-within-nifi","text":"","title":"Configuring Kerberos authentication within NiFi"},{"location":"Data_Integration/Apache_NiFi/#purpose_1","text":"Configuring Kerberos authentication within NiFi server for the later connection usage","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_1","text":"Installing Apache NiFi completed Installing FusionInsight HD cluster and its client completed Create a developuser for connection","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#procedure_1","text":"Download the required Kerberos authentication files user.keytab and krb5.conf from FusionInsight HD Manager site, save the files into the following directory /opt/developuser Configure Kerberos authentication by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file Detailed Configuration\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find KeytabCredentialsService and click ADD Click on **gear** icon to configure ![](assets/Apache_NiFi/markdown-img-paste-20180912174747644.png) ![](assets/Apache_NiFi/markdown-img-paste-2018091217482271.png) Click on **lightning** icon to enable and save the KeytabCredentialsService ![](assets/Apache_NiFi/markdown-img-paste-20180912174904147.png) ![](assets/Apache_NiFi/markdown-img-paste-20180912175037790.png) Completed","title":"Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-hdfs","text":"","title":"Connecting NiFi to HDFS"},{"location":"Data_Integration/Apache_NiFi/#purpose_2","text":"Configuring NiFi related HDFS processor to connect FusionInsight HD HDFS","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_2","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#puthdfs-procedure","text":"Find and Copy the hdfs-site.xml \uff0c core-site.xml files which located in FusionInsight HD client to the following directory /usr/nifi/nifi-1.7.1/conf Make an adjustment to the content of hdfs-site.xml that is to delete the following property <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> Make an adjustment to the content of core-site.xml that is to change halcluster into detailed namenode ip with its port <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset The configuration of processor PutHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest The configuration of the connection between two former processors Move the file nifiHDFS.csv into the following directory /home/dataset before test start Content of nifiHDFS.csv \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - Test completed Log into FusionInsight HDFS to check the test outcome by using the following command hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv","title":"PutHDFS Procedure"},{"location":"Data_Integration/Apache_NiFi/#gethdfs-procedure","text":"The whole process shown as the following pic: The configuration of processor GetHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest/HDFS The configuration of processor PutFile In detail\uff1a 1: /home/dataset/HDFS Move the file nifiHDFS.csv into HDFS directory /tmp/nifitest/HDFS Test completed Log into the FusionInsight HD client side to check the outcome with the directory /home/dataset/HDFS","title":"GetHDFS Procedure"},{"location":"Data_Integration/Apache_NiFi/#listhdfs-fetchhdfs-procedure","text":"The whole process shown as the following pic: The configuration of processor ListHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest The configuration of processor RouteOnAttribute Note: Add one customized property requiredfilenames with the value ${filename:matches('sanguo.*')} by clicking on plus icon In detail\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} The relationship configuration between processor RouteOnAttribute and upper processor FetchHDFS shown as the following pic The relationship configuration between processor RouteOnAttribute and lower processor FetchHDFS shown as the following pic The configuration of processor FetchHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService The configuration of upper processor PutFile The configuration of lower processor PutFile Check the files on FusionInsight HDFS by executing command hdfs dfs -ls /tmp/nifitest Test completed Log into FusionInsight HD client side to check the outcomes separately","title":"ListHDFS &amp; FetchHDFS Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-hive","text":"","title":"Connecting NiFi to Hive"},{"location":"Data_Integration/Apache_NiFi/#purpose_3","text":"Configuring NiFi Hive processor to connect FusionInsight HD Hive","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_3","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#hiveconnectionpool-procedure","text":"Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HiveConnectionPool and click ADD Click on gear icon to configure In detail 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService Click on lightning icon to enable and save the HiveConnectionPool Completed Create jaas.conf file which located at directory /usr/nifi/nifi-1.7.1/conf wit the following content Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; Make an adjustment to the bootstrap.conf file by executing following command vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true Make an adjustment to the nifi.properties file by executing following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true Execute the following command to come into the directory of NiFi Hive related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar","title":"HiveConnectionPool Procedure"},{"location":"Data_Integration/Apache_NiFi/#selecthiveql-read-hive-table-procedure","text":"The whole process shown as the following pic: The configuration of processor SelectHiveQL In detail\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV The configuration of processor PutFile Log into FusionInsight cluster to check table t2 on hive Completed Check the outcome by log into the following directory /home/dataset/HIVE","title":"SelectHiveQL read Hive table Procedure"},{"location":"Data_Integration/Apache_NiFi/#puthiveql-load-whole-table-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2: iris.txt Content of iris.txt \uff1a 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa The configuration of processor PutHDFS In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive The configuration of processor ReplaceText In detail\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; The configuration of processor PutHiveQL Move the data file iris.txt into the following directory /home/dataset/ before test Completed: Login the HIVE to check the test outcome","title":"PutHiveQL load whole table Procedure"},{"location":"Data_Integration/Apache_NiFi/#puthiveql-load-the-table-by-rows-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt Content of iris_add.txt \uff1a \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" The configuration of processor SplitText There is no change for the configuration of processor ExtractText The configuration of processor ReplaceText The configuration of processor PutHiveQL Move the data file iris_add.txt into the following directory /home/dataset/ before test Completed\uff1a Login the HIVE to check the test outcome\uff1a","title":"PutHiveQL Load the table by rows Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-hbase","text":"","title":"Connecting NiFi to HBase"},{"location":"Data_Integration/Apache_NiFi/#purpose_4","text":"Configuring NiFi HBase processor to connect FusionInsight HD HBase","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_4","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#hbase_1_1_2_clientservice-procedure","text":"Move the hbase related configuration file hbase-site.xml which is within the FusionInsight HD client side into the following directory /usr/nifi/nifi-1.7.1/conf Execute the following command to come into the directory of NiFi HBase related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HBase_1_1_2_ClientService and click ADD Click on gear icon to configure In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService Click on lightining icon to enable and save the HBase_1_1_2_ClientService Completed","title":"HBase_1_1_2_ClientService Procedure"},{"location":"Data_Integration/Apache_NiFi/#puthbasejson-load-the-table-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile Content of hbase_test.csv \uff1a 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica The configuration of processor InverAvroSchema In detail\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data The configuration of processor ConvertCSVToAvro The configuration of processor ConvertAvroToJSON The configuration of processor SplitJson The configuration of processor PutHBaseJSON In detail: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data Move the data file hbase_test.csv into the following directory /home/dataset/HBASE before test In addition, execute following command to create a HBase table hbase shell create 'HBase_test','data' Completed\uff1a Login into the FusionInsight HD cluster to check the outcome:","title":"PutHBaseJSON load the table Procedure"},{"location":"Data_Integration/Apache_NiFi/#gethbase-procedure","text":"The whole process shown as the following pic: The configuration of processor GetHBase The configuration of processor PutFile Completed Login into the following directory /home/dataset/GetHBase_test to check the test outcome","title":"GetHbase Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-spark","text":"","title":"Connecting NiFi to Spark"},{"location":"Data_Integration/Apache_NiFi/#purpose_5","text":"Configuring NiFi Livy Session processor to connect FusionInsight HD Spark","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_5","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed Installing and configuring Apache Livy 0.5.0 (Apache Livy can be installed on test host or any other host as long as they can connect to each other including FusionInsight HD cluster) There exist connection instruction between Apache Livy and FusionInsight, please check the FusionInsight ecosystem","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#livysessioncontroller-procedure","text":"Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: spark 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_PySpark In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: pysaprk 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_SparkR In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: sparkr 4\uff1aKeytabCredentialsService Click on lightining icon to enable and save the LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR Completed","title":"LivySessionController Procedure"},{"location":"Data_Integration/Apache_NiFi/#spark-sample-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code1.txt Content of code1.txt \uff1a 1+2 The configuration of processor ExtractText Click plus icon to add a Property code1 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController 2: ${code1} Move the code file code1.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed\uff1a Log into the Livy server to check the outcome","title":"Spark Sample Procedure"},{"location":"Data_Integration/Apache_NiFi/#pyspark-sample-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code2.txt Content of code2.txt \uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) The configuration of processor ExtractText Click plus icon to add a Property code2 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController_PySpark 2: ${code2} Move the code file code2.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome","title":"PySpark Sample Procedure"},{"location":"Data_Integration/Apache_NiFi/#sparkr-sample-procedure","text":"The whole process shown as the following pic: Note: It's different by comparing to example of former Spark and PySpark The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code3.txt Content of code3.txt \uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: /home/dataset/sparkTest 2: code content of code3.txt Move the code file code3.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome","title":"SparkR Sample Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-kafka","text":"","title":"Connecting NiFi to Kafka"},{"location":"Data_Integration/Apache_NiFi/#purpose_6","text":"Configuring NiFi Kafka processor to connect FusionInsight HD Kafka","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_6","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#gethttp-putkafka-procedure","text":"The whole process shown as the following pic: The configuration of processor GetHTTP In detail\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv The configuration of processor PutKafka In detail\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi Before test\uff1a Log into the Kafka component within FusionInsightHD client side and create a Topic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 Completed\uff1a Log into the kafka component within FusionInsightHD client side to check the outcome cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning","title":"GetHTTP &amp; PutKafka Procedure"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_0_11-procedure","text":"The whole process shown as the following pic: The configuration of processor ConsumeKafka_0_11 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer The configuration of processor PutFile Before test\uff1a Open the kafka-examples which provided by FusionInsightHD client in eclipse, configure the kafka-examples so that it can be successfully ran and produce messages to kafka Note: There must be a producer when testing the NiFi ConsumeKafka_0_11 processor, run NewProducer.java within kafka-examples at first and then start to test NiFi ConsumeKafka_0_11 Completed\uff1a Log into the follow directory /home/dataset/Kafka to check the test outcome","title":"ConsumeKafka_0_11 Procedure"},{"location":"Data_Integration/Informatica_PWX_CDC/","text":"Connection Instruction Between Informatica PowerExchange CDC and FusionInsight \u00b6 Succeeded Case \u00b6 Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD 6.5 (Kafka) Environment Information \u00b6 Informatica PowerExchange CDC 10.2.0 Linux & Windows version Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka client Architecture \u00b6 A data source, oracle database One Linux machine, installed with Informatica PWX CDC, start the listener and logger service, then install the PWX Publisher which can transfer the log data captured by PWX CDC to the kafka topic. One Linux machine, installed with FusionInsight HD Kafka client, consume the data transferred from PWX Publisher (optional) One Windows machine, installed with PWX CDC, start the listener service, use navigator to see the data captured by PWX CDC. database configuration \u00b6 >This part can refer to the Informatica PowerExchange CDC user guide https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html login to the system as oracle user, use Sqlplus / as sysdba login to Oracle database, open Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >Tips:Back up your database after both SHUTDOWN commands.. Set Up Oracle Minimal Global Supplemental Logging SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; ALTER SYSTEM switch logfile ; Copy the Oracle Catalog to the Archived Logs EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Define a CDC User and Grant User Privileges create a test table and insert some data Install Informatica PWX CDC & PWX Publisher \u00b6 Install Informatica PWX CDC in Linux \u00b6 Get the installation package pwx1020_linux_em64t.tar . untar the package and run ./install.sh , configure the installation path here is /opt/PowerExchange/10.2.0 . Configure the environment \u00b6 open environment file vi ~/.bash_profile add the following configuration export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * run source ~/.bash_profile * run dtlinfo ,check the installation Configure dbmover.cfg and pwxccl.cfg file \u00b6 Configure dbmover.cfg as following nodeln is the self defined listener node name the second ORCL in ORACLEID is the database name to be listened. CAPT_PATH is the CDC control file path, the path should be created previously define the SVCNODE and CMDNODE name Configure pwxccl.cfg as following CONDENSENAME should be the same as SVCNODE in dbmover.cfg DBID is the database SID CAPTURE_NODE is the capture node name CAPTURE_NODE_UID is the database user name CAPTURE_NODE_PWD is the database user password Start listener and logger services Use PWX CDC capture ORACLE log data \u00b6 ### install Informatica PWX CDC in Windows machine Get the installation package and double click to install, add environment variable PWX_CONFIG ,configured as the dbmover.cfg file in PWX Configure dbmover.cfg file set listener name, add listener Information in server side - set the listened database name - set the control file path * start the listener * start Navigator In Navigator create a new registeration group as following: NEXT chick next,we can see the test table created in oracle, double click the table name, choose all columns chick next, change state to active , check box run DDL immediately , click finish In Extraction Groups, double click the orcl11 created before, right click, add Extract Defination, set the map name and table name click next, can see the capture created before click add, finish click the icon, run row test, the captured data is shown as following Use PWX CDC publisher to connect Kafka \u00b6 ### Change kafka configuration file * Configure producer.properties , add the following configuration sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * Configure jaas.conf as following ![](assets/Informatica_PWX_CDC/14cae.png) create a kafka topic, named pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### Install Informatica PWX Publisher * Get the package pwxcdcpub120_linux_x64.tar.gz ,untar it Login as root\uff0cadd the following configuration in ~/.bash_profile file export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key source the environment, kerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser Copy all the files in directory samples to instanceA/config > Configuration for PWX Publisher can refer to the Informatica user guide https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html Configure cdcPublisherAvro.cfg - Configure cdcPublisherCommon.cfg - Configure cdcPublisherKafka.cfg , set kafka topic name and the properties file path - Configure cdcPowerExchange.cfg * Extract.pwxCapiConnectionName is the CAPI_CONNECTION in dbmover.cfg file * Extract.pwxExtractionMapSchemaName is the schema name in pwx extraction, here is u8orcl * Extract.pwxNodeLocation is pwx node name * Extract.pwxNodeUserId/Extract.pwxNodePwd and Extract.pwxXmapUserId/Extract.pwxXmappassword is database user name and pasword Change the PwxCDCPublisher.sh file in installation path bin,add the following RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" Start pwx CDC Publisher,run sh PwxCDCPublisher.sh Start kafka consumer \u00b6 In FusionInsight HD Kafka client, run the following command to start consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties Insert data in oracle, the captured data in kafka is the following Update data in oracle, the captured data in kafka is the following Delete data in oracle, the captured data in kafka is the following Q&A \u00b6 1.Failed to start pwxccl A:Run the following script in oracle exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Then grant C##PWX sysdba right grant sysdba to C##PWX","title":"10.2.0 <--> 6.5"},{"location":"Data_Integration/Informatica_PWX_CDC/#connection-instruction-between-informatica-powerexchange-cdc-and-fusioninsight","text":"","title":"Connection Instruction Between Informatica PowerExchange CDC and FusionInsight"},{"location":"Data_Integration/Informatica_PWX_CDC/#succeeded-case","text":"Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD 6.5 (Kafka)","title":"Succeeded Case"},{"location":"Data_Integration/Informatica_PWX_CDC/#environment-information","text":"Informatica PowerExchange CDC 10.2.0 Linux & Windows version Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka client","title":"Environment Information"},{"location":"Data_Integration/Informatica_PWX_CDC/#architecture","text":"A data source, oracle database One Linux machine, installed with Informatica PWX CDC, start the listener and logger service, then install the PWX Publisher which can transfer the log data captured by PWX CDC to the kafka topic. One Linux machine, installed with FusionInsight HD Kafka client, consume the data transferred from PWX Publisher (optional) One Windows machine, installed with PWX CDC, start the listener service, use navigator to see the data captured by PWX CDC.","title":"Architecture"},{"location":"Data_Integration/Informatica_PWX_CDC/#database-configuration","text":">This part can refer to the Informatica PowerExchange CDC user guide https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html login to the system as oracle user, use Sqlplus / as sysdba login to Oracle database, open Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >Tips:Back up your database after both SHUTDOWN commands.. Set Up Oracle Minimal Global Supplemental Logging SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; ALTER SYSTEM switch logfile ; Copy the Oracle Catalog to the Archived Logs EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Define a CDC User and Grant User Privileges create a test table and insert some data","title":"database configuration"},{"location":"Data_Integration/Informatica_PWX_CDC/#install-informatica-pwx-cdc-pwx-publisher","text":"","title":"Install Informatica PWX CDC &amp; PWX Publisher"},{"location":"Data_Integration/Informatica_PWX_CDC/#install-informatica-pwx-cdc-in-linux","text":"Get the installation package pwx1020_linux_em64t.tar . untar the package and run ./install.sh , configure the installation path here is /opt/PowerExchange/10.2.0 .","title":"Install Informatica PWX CDC in Linux"},{"location":"Data_Integration/Informatica_PWX_CDC/#configure-the-environment","text":"open environment file vi ~/.bash_profile add the following configuration export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * run source ~/.bash_profile * run dtlinfo ,check the installation","title":"Configure the environment"},{"location":"Data_Integration/Informatica_PWX_CDC/#configure-dbmovercfg-and-pwxcclcfg-file","text":"Configure dbmover.cfg as following nodeln is the self defined listener node name the second ORCL in ORACLEID is the database name to be listened. CAPT_PATH is the CDC control file path, the path should be created previously define the SVCNODE and CMDNODE name Configure pwxccl.cfg as following CONDENSENAME should be the same as SVCNODE in dbmover.cfg DBID is the database SID CAPTURE_NODE is the capture node name CAPTURE_NODE_UID is the database user name CAPTURE_NODE_PWD is the database user password Start listener and logger services","title":"Configure dbmover.cfg and pwxccl.cfg file"},{"location":"Data_Integration/Informatica_PWX_CDC/#use-pwx-cdc-capture-oracle-log-data","text":"### install Informatica PWX CDC in Windows machine Get the installation package and double click to install, add environment variable PWX_CONFIG ,configured as the dbmover.cfg file in PWX Configure dbmover.cfg file set listener name, add listener Information in server side - set the listened database name - set the control file path * start the listener * start Navigator In Navigator create a new registeration group as following: NEXT chick next,we can see the test table created in oracle, double click the table name, choose all columns chick next, change state to active , check box run DDL immediately , click finish In Extraction Groups, double click the orcl11 created before, right click, add Extract Defination, set the map name and table name click next, can see the capture created before click add, finish click the icon, run row test, the captured data is shown as following","title":"Use PWX CDC capture ORACLE log data"},{"location":"Data_Integration/Informatica_PWX_CDC/#use-pwx-cdc-publisher-to-connect-kafka","text":"### Change kafka configuration file * Configure producer.properties , add the following configuration sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * Configure jaas.conf as following ![](assets/Informatica_PWX_CDC/14cae.png) create a kafka topic, named pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### Install Informatica PWX Publisher * Get the package pwxcdcpub120_linux_x64.tar.gz ,untar it Login as root\uff0cadd the following configuration in ~/.bash_profile file export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key source the environment, kerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser Copy all the files in directory samples to instanceA/config > Configuration for PWX Publisher can refer to the Informatica user guide https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html Configure cdcPublisherAvro.cfg - Configure cdcPublisherCommon.cfg - Configure cdcPublisherKafka.cfg , set kafka topic name and the properties file path - Configure cdcPowerExchange.cfg * Extract.pwxCapiConnectionName is the CAPI_CONNECTION in dbmover.cfg file * Extract.pwxExtractionMapSchemaName is the schema name in pwx extraction, here is u8orcl * Extract.pwxNodeLocation is pwx node name * Extract.pwxNodeUserId/Extract.pwxNodePwd and Extract.pwxXmapUserId/Extract.pwxXmappassword is database user name and pasword Change the PwxCDCPublisher.sh file in installation path bin,add the following RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" Start pwx CDC Publisher,run sh PwxCDCPublisher.sh","title":"Use PWX CDC publisher to connect Kafka"},{"location":"Data_Integration/Informatica_PWX_CDC/#start-kafka-consumer","text":"In FusionInsight HD Kafka client, run the following command to start consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties Insert data in oracle, the captured data in kafka is the following Update data in oracle, the captured data in kafka is the following Delete data in oracle, the captured data in kafka is the following","title":"Start kafka consumer"},{"location":"Data_Integration/Informatica_PWX_CDC/#qa","text":"1.Failed to start pwxccl A:Run the following script in oracle exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Then grant C##PWX sysdba right grant sysdba to C##PWX","title":"Q&amp;A"},{"location":"Data_Integration/Informatica_PowerCenter/","text":"Connection Instruction Between Informatica PowerCenter and FusionInsight HD \u00b6 Succeeded Case \u00b6 Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive) Environment Information \u00b6 Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD client Architecture \u00b6 One Linux machine, installed with Informatica Server and FusionInsight HD client One Windows machine, installed with Informatica PowerCenter Client Install and config FusionInsight HD client \u00b6 Install the FusionInsight client\uff0cinstallation path is /opt/hadoopclient Create a user from FusionInsight HD manager\uff0crefer to . For example, create a user named developuser\uff0cand assign him all rights for HDFS and Hive. Download the keytab file, put krb5.conf file to the /opt/ path of client node. Install Oracle database and Informatica Server \u00b6 create user oracle\uff0cinstall oracle database create user infa,login to oracle use sqlplus / as sysdba , run the following sql create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; Get Informatica Server installation package and upload to server node, run ./install.sh as user infa, the installation path is /home/infa/Informatica/10.2.0 . Visit ip:6008 in a browser, open the Administrator tool, input the user name and password. Informatica Server configuration \u00b6 Create PowerCenter Repository Service In Services and Nodes, right click domain, Create a PowerCenter Repository Service - Set Name and node, next - Set database information, finish Enable the Repository Service,and create contents In repository Properties, set the Operating Mode to Mormal, and recycle the service Create PowerCenter Integration Service In Services and Nodes, right click domain, Create a PowerCenter Integration Service - Set Name and node, next - Set Repository information, finish\uff0cenable the service Create developuser in infa server InSecurity tab, create a user, named as developuser\uff0cthe same as user in Hadoop cluster Edit the user privileges and groups Infa Server configuration for Hadoop Copy krb5.conf file in /opt to /etc and ${INFA_HOME}java/jre/lib/security/ , and give the read right to user infa. Login to node as user infa, create a directory for the configuration file, such as /opt/pwx-hadoop/conf Get the follwoing configuration fie from FusionInsight HD client, put them into /opt/pwx-hadoop/conf and change the file right to 775 - Do Kerberos authentication, and set cache,the infa user should have read and write rights for the cache file source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - Edit the core-site.xml file in /opt/pwx-hadoop/conf add the following property <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - In Administrator tool, add an Environment variable for pwc_DIS , recycle the service delete jar files related to hive in /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ and copy jar files related to hive in /opt/hadoopclient/Hive/Beeline/lib to the path, change the file rights rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* PowerCenter Client configuration \u00b6 PowerCenter Repository Manager configuration \u00b6 Get PowerCenter Client installation package,install PowerCenter Client,start PowerCenter Repository Manager, in tool bar, choose Repository->Configure domain ,input the domain information, then we can see the repository created before. Double click the repository, input user name and password, connect In folder, create a folder PowerCenter Designer configuration \u00b6 Open PowerCenter Designer, right click the folder, click open - Click tool bar, Sources->import from databases\uff0ccreate a system DSN in ODBC source,choose Oracle Driver,input database information. - Choose the data source created just now, input database user name and password, connect, get the table in the database - Choose target designer\uff0cdrag in the table in sources - Double click the table, set database type to Flat File In mapping configuration, create a new mapping, drag in the source and target table and link them PowerCenter Workflow Manager \u00b6 In tool bar Task, create a new task,name it and choose the mapping created just now Create a workflow, drag in the task, link them In tool bar connection, create a application connection, choose Hadoop HDFS Connection HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser Double click the task created, in mapping tab, click Targets , set the Writers to HDFS Flat Write , set connection value to the connection created just now In properties, config as following Save the workflow, right click, start the workflow Open PowerCenter Workflow Monitor, the run information is shown. In HDFS, check if the data is uploaded. In the task properties, choose Generate And Load Hive Table , Overwrite Hive Table ,input the table created in hive, start workflow Open PowerCenter Workflow Monitor, the run information is shown. In Hive, check if the data is loaded into the table","title":"10.2.0 <--> 6.5"},{"location":"Data_Integration/Informatica_PowerCenter/#connection-instruction-between-informatica-powercenter-and-fusioninsight-hd","text":"","title":"Connection Instruction Between Informatica PowerCenter and FusionInsight HD"},{"location":"Data_Integration/Informatica_PowerCenter/#succeeded-case","text":"Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive)","title":"Succeeded Case"},{"location":"Data_Integration/Informatica_PowerCenter/#environment-information","text":"Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD client","title":"Environment Information"},{"location":"Data_Integration/Informatica_PowerCenter/#architecture","text":"One Linux machine, installed with Informatica Server and FusionInsight HD client One Windows machine, installed with Informatica PowerCenter Client","title":"Architecture"},{"location":"Data_Integration/Informatica_PowerCenter/#install-and-config-fusioninsight-hd-client","text":"Install the FusionInsight client\uff0cinstallation path is /opt/hadoopclient Create a user from FusionInsight HD manager\uff0crefer to . For example, create a user named developuser\uff0cand assign him all rights for HDFS and Hive. Download the keytab file, put krb5.conf file to the /opt/ path of client node.","title":"Install and config FusionInsight HD client"},{"location":"Data_Integration/Informatica_PowerCenter/#install-oracle-database-and-informatica-server","text":"create user oracle\uff0cinstall oracle database create user infa,login to oracle use sqlplus / as sysdba , run the following sql create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; Get Informatica Server installation package and upload to server node, run ./install.sh as user infa, the installation path is /home/infa/Informatica/10.2.0 . Visit ip:6008 in a browser, open the Administrator tool, input the user name and password.","title":"Install Oracle database and Informatica Server"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-server-configuration","text":"Create PowerCenter Repository Service In Services and Nodes, right click domain, Create a PowerCenter Repository Service - Set Name and node, next - Set database information, finish Enable the Repository Service,and create contents In repository Properties, set the Operating Mode to Mormal, and recycle the service Create PowerCenter Integration Service In Services and Nodes, right click domain, Create a PowerCenter Integration Service - Set Name and node, next - Set Repository information, finish\uff0cenable the service Create developuser in infa server InSecurity tab, create a user, named as developuser\uff0cthe same as user in Hadoop cluster Edit the user privileges and groups Infa Server configuration for Hadoop Copy krb5.conf file in /opt to /etc and ${INFA_HOME}java/jre/lib/security/ , and give the read right to user infa. Login to node as user infa, create a directory for the configuration file, such as /opt/pwx-hadoop/conf Get the follwoing configuration fie from FusionInsight HD client, put them into /opt/pwx-hadoop/conf and change the file right to 775 - Do Kerberos authentication, and set cache,the infa user should have read and write rights for the cache file source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - Edit the core-site.xml file in /opt/pwx-hadoop/conf add the following property <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - In Administrator tool, add an Environment variable for pwc_DIS , recycle the service delete jar files related to hive in /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ and copy jar files related to hive in /opt/hadoopclient/Hive/Beeline/lib to the path, change the file rights rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive*","title":"Informatica Server configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-client-configuration","text":"","title":"PowerCenter Client configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-repository-manager-configuration","text":"Get PowerCenter Client installation package,install PowerCenter Client,start PowerCenter Repository Manager, in tool bar, choose Repository->Configure domain ,input the domain information, then we can see the repository created before. Double click the repository, input user name and password, connect In folder, create a folder","title":"PowerCenter Repository Manager configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-designer-configuration","text":"Open PowerCenter Designer, right click the folder, click open - Click tool bar, Sources->import from databases\uff0ccreate a system DSN in ODBC source,choose Oracle Driver,input database information. - Choose the data source created just now, input database user name and password, connect, get the table in the database - Choose target designer\uff0cdrag in the table in sources - Double click the table, set database type to Flat File In mapping configuration, create a new mapping, drag in the source and target table and link them","title":"PowerCenter Designer configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-workflow-manager","text":"In tool bar Task, create a new task,name it and choose the mapping created just now Create a workflow, drag in the task, link them In tool bar connection, create a application connection, choose Hadoop HDFS Connection HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser Double click the task created, in mapping tab, click Targets , set the Writers to HDFS Flat Write , set connection value to the connection created just now In properties, config as following Save the workflow, right click, start the workflow Open PowerCenter Workflow Monitor, the run information is shown. In HDFS, check if the data is uploaded. In the task properties, choose Generate And Load Hive Table , Overwrite Hive Table ,input the table created in hive, start workflow Open PowerCenter Workflow Monitor, the run information is shown. In Hive, check if the data is loaded into the table","title":"PowerCenter Workflow Manager"},{"location":"Data_Integration/Talend/","text":"Connection Instruction between Talend and FusionInsight \u00b6 Succeeded Case \u00b6 Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) Note: Because of the version bug of Talend 7.0.1, Hive cannot be successfully connected. Using Talend 6.4.1 for substitution. Installing Talend \u00b6 Purpose \u00b6 Installing Talend 7.0.1 Prerequisites \u00b6 Installing FusionInsight HD cluster and its client completed Procedure \u00b6 Configure the JAVA_HOME into Path Environment Variables Configure Kerberos Get Kerberos related userkeytab and krb5.conf files by login into the FusionInsight HD manager web UI and put them into the following directory C:\\ProgramData\\Kerberos . In addition, create a new file named krb5.ini with the same content of krb5.conf, put the krb5.ini file into the following directory C:\\Windows Download TOS from the following web pages https://www.talend.com/products/big-data/big-data-open-studio/ , create the jaas.conf file for zookeeper connection with its content shown as follows Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; Sart TOS_BD by clicking TOS_BD-win-x86_64.exe Installing additional Talend Packages Connecting Talend to HDFS \u00b6 Purpose \u00b6 Configuring Talend related HDFS processor to connect FusionInsight HD HDFS Prerequisites \u00b6 Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed HDFS Connection Procedure \u00b6 Add the tHDFSConnection component with its configuration shown as follows: In detail\uff1a 1: Cloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - Test completed\uff1a HDFS Get Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSGet component shown as follows Note: Put the out.csv into the HDFS filesystem with the following directory /tmp/talend_test , C:/SOFT is the local folder for file output TEST completed\uff1a Check the test outcome by coming into the local directory C:/SOFT HDFS Put Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSPut component shown as follows Note: Before the test starts, create HDFSPut.txt located at the directory C:/SOFT with its content shown as follows It is created on a local PC. Test Completed\uff1a Login into the cluster to check the test outcome: Connecting Talend to Hive \u00b6 Purpose \u00b6 Configuring Talend related Hive processor to connect FusionInsight HD Hive Prerequisites \u00b6 Installing Talend 6.4.1 completed Installing FusionInsight HD cluster and its client completed Hive Connection Procedure \u00b6 The Talend version for Hive connection is 6.4.1 The whole process is shown as the following pic: The configuration of tHiveConnection component shown as follows 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" Note: Need to click the button which besides the Distribution to import the required jar files of FusionInsight HD. If there still need to add extra jar files, you can complete this step either by Talend itself or manually add these jar files. Test Completed\uff1a Hive Create Table & Load Procedure \u00b6 The configuration of tHiveConnection component does not change The configuration of tHiveCreateTable component shown as follows Note: It is required to Edit schema of the table The configuration of tHiveLoad component shown as follows Note: Before the test starts, the file out.csv need to be uploaded into the hdfs filesystem directory /tmp/talend_test/ The content of out.csv shown as follows \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHiveClose component shown as follows Test Completed\uff1a Check the table createdTableTalend by login into the cluster Hive Input Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveInput component shown as follows Note: It is required to Edit schema of the hive table The configuration of tLogRow keeps by default The configuration of tHiveClose component shown as follows Test Completed\uff1a Hive Row Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveRow component shown as follows Note: It is required to Edit schema of hive table Test Completed\uff1a Check the cluster outcome by login into the FusionInsight Cluster Connecting Talend to HBase \u00b6 Purpose \u00b6 Configuring Talend related HBase processor to connect FusionInsight HD HBase Prerequisites \u00b6 Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed HBase Connection Procedure \u00b6 The whole process is shown as the following pic: Using eclipse to export the LoginUtil which from HBase sample project code of FusionInsight HD client (Sample project code in this time can be found by following directory C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example ) Find the tHbaseConnection component by Palette The configuration of tHbaseConnection shown as the following pic: Note: It is required to import the jar files of HBase sample project and the exported hbase_loginUtil.jar hbase-example required jar faile can be located by the following directory C:\\FusionInsight_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.0.2.tar.gz\\hbase\\lib The configuration of tLibraryLoad shown as folloing pic: Click on Advanced settings and add the java code import com.huawei.hadoop.security.LoginUtil; shown as follows: Use tJava component to customize the tHBaseConnection component The content of the Java code shown as follows\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - Test Completed HBase Input Output Procedure \u00b6 The content of the Java code shown as follows\uff1a The configuration of tLibraryLoad \uff0c tHBaseConnection \uff0c tJava , tHBaseClose do not change The configuration of tFileInputDelimited shown as following pic: Note: It is required to Edit schema of out.csv The content of out.csv shown as follows: 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHBaseOutput shown as folloing pic: Note: It is required to Edit Schema of table: The configuration of tHBaseInput shown as folloing pic: The configuration of tLogRow keeps by default Test Completed: Login into the FusinInsight HD cluster and check the HBase table hbaseInputOutputTest by using following comands: hbase shell scan 'hbaseInputOutputTest'","title":"7.0.1 <--> C80"},{"location":"Data_Integration/Talend/#connection-instruction-between-talend-and-fusioninsight","text":"","title":"Connection Instruction between Talend and FusionInsight"},{"location":"Data_Integration/Talend/#succeeded-case","text":"Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) Note: Because of the version bug of Talend 7.0.1, Hive cannot be successfully connected. Using Talend 6.4.1 for substitution.","title":"Succeeded Case"},{"location":"Data_Integration/Talend/#installing-talend","text":"","title":"Installing Talend"},{"location":"Data_Integration/Talend/#purpose","text":"Installing Talend 7.0.1","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites","text":"Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#procedure","text":"Configure the JAVA_HOME into Path Environment Variables Configure Kerberos Get Kerberos related userkeytab and krb5.conf files by login into the FusionInsight HD manager web UI and put them into the following directory C:\\ProgramData\\Kerberos . In addition, create a new file named krb5.ini with the same content of krb5.conf, put the krb5.ini file into the following directory C:\\Windows Download TOS from the following web pages https://www.talend.com/products/big-data/big-data-open-studio/ , create the jaas.conf file for zookeeper connection with its content shown as follows Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; Sart TOS_BD by clicking TOS_BD-win-x86_64.exe Installing additional Talend Packages","title":"Procedure"},{"location":"Data_Integration/Talend/#connecting-talend-to-hdfs","text":"","title":"Connecting Talend to HDFS"},{"location":"Data_Integration/Talend/#purpose_1","text":"Configuring Talend related HDFS processor to connect FusionInsight HD HDFS","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites_1","text":"Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#hdfs-connection-procedure","text":"Add the tHDFSConnection component with its configuration shown as follows: In detail\uff1a 1: Cloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - Test completed\uff1a","title":"HDFS Connection Procedure"},{"location":"Data_Integration/Talend/#hdfs-get-procedure","text":"The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSGet component shown as follows Note: Put the out.csv into the HDFS filesystem with the following directory /tmp/talend_test , C:/SOFT is the local folder for file output TEST completed\uff1a Check the test outcome by coming into the local directory C:/SOFT","title":"HDFS Get Procedure"},{"location":"Data_Integration/Talend/#hdfs-put-procedure","text":"The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSPut component shown as follows Note: Before the test starts, create HDFSPut.txt located at the directory C:/SOFT with its content shown as follows It is created on a local PC. Test Completed\uff1a Login into the cluster to check the test outcome:","title":"HDFS Put Procedure"},{"location":"Data_Integration/Talend/#connecting-talend-to-hive","text":"","title":"Connecting Talend to Hive"},{"location":"Data_Integration/Talend/#purpose_2","text":"Configuring Talend related Hive processor to connect FusionInsight HD Hive","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites_2","text":"Installing Talend 6.4.1 completed Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#hive-connection-procedure","text":"The Talend version for Hive connection is 6.4.1 The whole process is shown as the following pic: The configuration of tHiveConnection component shown as follows 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" Note: Need to click the button which besides the Distribution to import the required jar files of FusionInsight HD. If there still need to add extra jar files, you can complete this step either by Talend itself or manually add these jar files. Test Completed\uff1a","title":"Hive Connection Procedure"},{"location":"Data_Integration/Talend/#hive-create-table-load-procedure","text":"The configuration of tHiveConnection component does not change The configuration of tHiveCreateTable component shown as follows Note: It is required to Edit schema of the table The configuration of tHiveLoad component shown as follows Note: Before the test starts, the file out.csv need to be uploaded into the hdfs filesystem directory /tmp/talend_test/ The content of out.csv shown as follows \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHiveClose component shown as follows Test Completed\uff1a Check the table createdTableTalend by login into the cluster","title":"Hive Create Table &amp; Load Procedure"},{"location":"Data_Integration/Talend/#hive-input-procedure","text":"The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveInput component shown as follows Note: It is required to Edit schema of the hive table The configuration of tLogRow keeps by default The configuration of tHiveClose component shown as follows Test Completed\uff1a","title":"Hive Input Procedure"},{"location":"Data_Integration/Talend/#hive-row-procedure","text":"The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveRow component shown as follows Note: It is required to Edit schema of hive table Test Completed\uff1a Check the cluster outcome by login into the FusionInsight Cluster","title":"Hive Row Procedure"},{"location":"Data_Integration/Talend/#connecting-talend-to-hbase","text":"","title":"Connecting Talend to HBase"},{"location":"Data_Integration/Talend/#purpose_3","text":"Configuring Talend related HBase processor to connect FusionInsight HD HBase","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites_3","text":"Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#hbase-connection-procedure","text":"The whole process is shown as the following pic: Using eclipse to export the LoginUtil which from HBase sample project code of FusionInsight HD client (Sample project code in this time can be found by following directory C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example ) Find the tHbaseConnection component by Palette The configuration of tHbaseConnection shown as the following pic: Note: It is required to import the jar files of HBase sample project and the exported hbase_loginUtil.jar hbase-example required jar faile can be located by the following directory C:\\FusionInsight_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.0.2.tar.gz\\hbase\\lib The configuration of tLibraryLoad shown as folloing pic: Click on Advanced settings and add the java code import com.huawei.hadoop.security.LoginUtil; shown as follows: Use tJava component to customize the tHBaseConnection component The content of the Java code shown as follows\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - Test Completed","title":"HBase Connection Procedure"},{"location":"Data_Integration/Talend/#hbase-input-output-procedure","text":"The content of the Java code shown as follows\uff1a The configuration of tLibraryLoad \uff0c tHBaseConnection \uff0c tJava , tHBaseClose do not change The configuration of tFileInputDelimited shown as following pic: Note: It is required to Edit schema of out.csv The content of out.csv shown as follows: 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHBaseOutput shown as folloing pic: Note: It is required to Edit Schema of table: The configuration of tHBaseInput shown as folloing pic: The configuration of tLogRow keeps by default Test Completed: Login into the FusinInsight HD cluster and check the HBase table hbaseInputOutputTest by using following comands: hbase shell scan 'hbaseInputOutputTest'","title":"HBase Input Output Procedure"},{"location":"Database/","text":"Database \u00b6","title":"Home"},{"location":"Database/#database","text":"","title":"Database"},{"location":"Development/","text":"Development \u00b6","title":"Home"},{"location":"Development/#development","text":"","title":"Development"},{"location":"Other/","text":"Other \u00b6","title":"Home"},{"location":"Other/#other","text":"","title":"Other"},{"location":"SQL_Analytics/","text":"SQL Analytics \u00b6","title":"Home"},{"location":"SQL_Analytics/#sql-analytics","text":"","title":"SQL Analytics"}]}