{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u667a\u80fd\u6570\u636e\u751f\u6001\u5730\u56fe \u00b6 FusionInsight\u652f\u6301\u5f00\u6e90\u6807\u51c6\u7684Hadoop\u63a5\u53e3\uff0c\u53ef\u4ee5\u4e0e\u4ee5\u4e0b\u7b2c\u4e09\u65b9\u5de5\u5177\u8fdb\u884c\u5bf9\u63a5 \u7b2c\u4e09\u65b9\u5de5\u5177 FusionInsight \u6d89\u53ca\u9886\u57df \u5de5\u5177\u540d\u79f0 \u7248\u672c C50 C60 C70 C80 6.5 \u6570\u636e\u53ef\u89c6\u5316 Tableau 10.0.0 Hive SparkSQL Hive SparkSQL \u3000 \u3000 \u3000 10.1.4 \u3000 Hive SparkSQL \u3000 \u3000 \u3000 10.3.2 \u3000 \u3000 Hive SparkSQL \u3000 \u3000 10.5.0 \u3000 \u3000 \u3000 Hive SparkSQL \u3000 QlikView 12 \u3000 Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL QlikSense 3.2.4 \u3000 \u3000 Hive SparkSQL \u3000 Hive SparkSQL SmartBI 7.2.32464.17374 \u3000 \u3000 Hive SparkSQL \u3000 \u3000 Oracle BIEE 11g \u3000 Hive SparkSQL Hive SparkSQL ELK GaussDB \u3000 \u3000 12c \u3000 Hive SparkSQL Hive SparkSQL ELK GaussDB \u3000 Hive SparkSQL ELK GaussDB IBM Cognos 10.2.2fp4 \u3000 Hive \u3000 \u3000 \u3000 \u6570\u636e\u5206\u6790 SAS Access for Hadoop SAS HPA SAS EP 9.4M3 HDFS Hive HDFS Hive HDFS Hive HDFS Hive \u3000 IBM SPSS Analytic Server 3.1.0 \u3000 HDFS Spark \u3000 \u3000 \u3000 Alteryx 11.0.2.25199 \u3000 HDFS Hive SparkSQL \u3000 HDFS Hive SparkSQL HDFS Hive SparkSQL RapidMiner 8.2.001 \u3000 \u3000 \u3000 HDFS Hive MapReduce Spark \u3000 Splunk 7.2.4 \u3000 \u3000 \u3000 HDFS Hive HDFS \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u3000 Hive SparkSQL \u3000 \u3000 \u3000 \u6570\u636e\u96c6\u6210 IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL \u3000 \u3000 \u3000 \u3000 11.5.0.2 \u3000 HDFS Phoenix Hive SparkSQL Kafka GaussDB \u3000 \u3000 \u3000 IBM InfoSphere CDC 11.3.3.1 HDFS \u3000 \u3000 \u3000 \u3000 Oracle GoldenGate 12.2.0.1.1 \u3000 HDFS HBase Flume Kafka \u3000 \u3000 HDFS HBase Flume Kafka 12.3.1.1.1 \u3000 \u3000 HDFS HBase Flume Kafka HDFS HBase Flume Kafka \u3000 Informatica BDM(native) 10.0.0 HDFS HBase Hive HDFS HBase Hive \u3000 HDFS Hive HDFS Hive Informatica BDM(push down) 10.2.0 \u3000 \u3000 HDFS HBase Hive Yarn \u3000 \u3000 Informatica PWX CDC 10.2.0 \u3000 \u3000 \u3000 Kafka \u3000 Informatica PowerCenter 10.2.0 \u3000 \u3000 \u3000 HDFS Hive \u3000 Talend 6.3.1 \u3000 HDFS HBase Hive \u3000 \u3000 \u3000 6.4.1 \u3000 \u3000 HDFS HBase Hive HDFS HBase Hive \u3000 7.0.1 \u3000 \u3000 HDFS HBase Hive HDFS HBase Hive \u3000 Apache NiFi 1.7.1 \u3000 \u3000 \u3000 HDFS HBase Hive Spark Kafka \u3000 Kettle 6.1 \u3000 HDFS Hive HDFS Hive HDFS Hive \u3000 Pentaho 7.1 \u3000 \u3000 HDFS Hive \u3000 \u3000 8.0 \u3000 HDFS Hive \u3000 \u3000 \u3000 Knime 3.6.1 \u3000 \u3000 \u3000 HDFS Hive Spark HDFS Hive H2O.ai 3.24.0.2 \u3000 \u3000 \u3000 HDFS GaussDB \u3000 Kafka Manager 1.3.3.21 \u3000 \u3000 \u3000 Kafka \u3000 \u676d\u5dde\u5408\u4f17UTL 5.1 HDFS HBase Hive Kafka \u3000 \u3000 \u3000 \u3000 \u96c6\u6210\u5f00\u53d1\u73af\u5883 RStudio 1.0.153 \u3000 SparkR SparkR \u3000 \u3000 Apache Zepplin 0.7.2 \u3000 HBase Hive Spark SparkR \u3000 \u3000 \u3000 0.7.3 \u3000 \u3000 HBase Hive Spark SparkR HBase Hive Spark SparkR \u3000 0.8.0 \u3000 \u3000 \u3000 HBase Hive Spark SparkR \u3000 Jypyter Notebook \u3000 \u3000 \u3000 pySpark \u3000 \u3000 DBeaver 4.0.8 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 \u3000 4.2.1 \u3000 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 DbVisualizer 9.5.7 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 \u3000 10.0.1 \u3000 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 Squirrel 3.7.1 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 \u3000 3.8.0 \u3000 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 HUE 4.0.1 \u3000 \u3000 HDFS HBase Hive Spark \u3000 \u3000 SQL\u5206\u6790 Apache Kylin 1.6.0 \u3000 Hive HBase \u3000 \u3000 \u3000 2.1.0 \u3000 \u3000 Hive HBase \u3000 \u3000 2.3.1 \u3000 \u3000 \u3000 Hive HBase \u3000 2.6.1 \u3000 \u3000 \u3000 \u3000 Hive HBase Kyligence 2.2 \u3000 Hive HBase \u3000 \u3000 \u3000 2.3 \u3000 Hive HBase \u3000 \u3000 \u3000 2.4 \u3000 \u3000 Hive HBase \u3000 \u3000 2.5 \u3000 \u3000 Hive HBase \u3000 \u3000 3.0 \u3000 \u3000 \u3000 Hive HBase Yarn \u3000 Presto 0.155 \u3000 HDFS Hive \u3000 \u3000 \u3000 0.184 \u3000 \u3000 HDFS Hive \u3000 \u3000 0.210 \u3000 \u3000 \u3000 HDFS Hive ElasticSearch HDFS Hive \u6570\u636e\u5e93 SAP HANA 100_120_0-10009569 Hive Hive Hive Hive \u3000 SAP VORA 2.0 \u3000 \u3000 Spark \u3000 \u3000 2.1 \u3000 \u3000 Spark \u3000 \u3000 \u676d\u5dde\u5408\u4f17UDB 6.1 GaussDB \u3000 \u3000 \u3000 \u3000 \u5176\u4ed6 FUSE 2.8.3 \u3000 HDFS \u3000 \u3000 \u3000 gis-tools-for-hadoop github \u3000 Hive MapReduce \u3000 \u3000 \u3000 IBM WAS 8.5.5.9 IBM_JDK \u3000 \u3000 \u3000 \u3000 Apache Livy 0.5.0 \u3000 \u3000 \u3000 Spark \u3000 Logstash 6.4.2 \u3000 \u3000 \u3000 ElasticSearch \u3000 Kibana 6.1.3 \u3000 \u3000 \u3000 ElasticSearch \u3000 elasticsearch-head 1.0 \u3000 \u3000 \u3000 ElasticSearch \u3000 beats 6.5.1 \u3000 \u3000 \u3000 ElasticSearch \u3000 NeoKylin 6.9 \u3000 \u3000 OS \u3000 \u3000 7.2 \u3000 \u3000 OS","title":"Home"},{"location":"#_1","text":"FusionInsight\u652f\u6301\u5f00\u6e90\u6807\u51c6\u7684Hadoop\u63a5\u53e3\uff0c\u53ef\u4ee5\u4e0e\u4ee5\u4e0b\u7b2c\u4e09\u65b9\u5de5\u5177\u8fdb\u884c\u5bf9\u63a5 \u7b2c\u4e09\u65b9\u5de5\u5177 FusionInsight \u6d89\u53ca\u9886\u57df \u5de5\u5177\u540d\u79f0 \u7248\u672c C50 C60 C70 C80 6.5 \u6570\u636e\u53ef\u89c6\u5316 Tableau 10.0.0 Hive SparkSQL Hive SparkSQL \u3000 \u3000 \u3000 10.1.4 \u3000 Hive SparkSQL \u3000 \u3000 \u3000 10.3.2 \u3000 \u3000 Hive SparkSQL \u3000 \u3000 10.5.0 \u3000 \u3000 \u3000 Hive SparkSQL \u3000 QlikView 12 \u3000 Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL QlikSense 3.2.4 \u3000 \u3000 Hive SparkSQL \u3000 Hive SparkSQL SmartBI 7.2.32464.17374 \u3000 \u3000 Hive SparkSQL \u3000 \u3000 Oracle BIEE 11g \u3000 Hive SparkSQL Hive SparkSQL ELK GaussDB \u3000 \u3000 12c \u3000 Hive SparkSQL Hive SparkSQL ELK GaussDB \u3000 Hive SparkSQL ELK GaussDB IBM Cognos 10.2.2fp4 \u3000 Hive \u3000 \u3000 \u3000 \u6570\u636e\u5206\u6790 SAS Access for Hadoop SAS HPA SAS EP 9.4M3 HDFS Hive HDFS Hive HDFS Hive HDFS Hive \u3000 IBM SPSS Analytic Server 3.1.0 \u3000 HDFS Spark \u3000 \u3000 \u3000 Alteryx 11.0.2.25199 \u3000 HDFS Hive SparkSQL \u3000 HDFS Hive SparkSQL HDFS Hive SparkSQL RapidMiner 8.2.001 \u3000 \u3000 \u3000 HDFS Hive MapReduce Spark \u3000 Splunk 7.2.4 \u3000 \u3000 \u3000 HDFS Hive HDFS \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u3000 Hive SparkSQL \u3000 \u3000 \u3000 \u6570\u636e\u96c6\u6210 IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL \u3000 \u3000 \u3000 \u3000 11.5.0.2 \u3000 HDFS Phoenix Hive SparkSQL Kafka GaussDB \u3000 \u3000 \u3000 IBM InfoSphere CDC 11.3.3.1 HDFS \u3000 \u3000 \u3000 \u3000 Oracle GoldenGate 12.2.0.1.1 \u3000 HDFS HBase Flume Kafka \u3000 \u3000 HDFS HBase Flume Kafka 12.3.1.1.1 \u3000 \u3000 HDFS HBase Flume Kafka HDFS HBase Flume Kafka \u3000 Informatica BDM(native) 10.0.0 HDFS HBase Hive HDFS HBase Hive \u3000 HDFS Hive HDFS Hive Informatica BDM(push down) 10.2.0 \u3000 \u3000 HDFS HBase Hive Yarn \u3000 \u3000 Informatica PWX CDC 10.2.0 \u3000 \u3000 \u3000 Kafka \u3000 Informatica PowerCenter 10.2.0 \u3000 \u3000 \u3000 HDFS Hive \u3000 Talend 6.3.1 \u3000 HDFS HBase Hive \u3000 \u3000 \u3000 6.4.1 \u3000 \u3000 HDFS HBase Hive HDFS HBase Hive \u3000 7.0.1 \u3000 \u3000 HDFS HBase Hive HDFS HBase Hive \u3000 Apache NiFi 1.7.1 \u3000 \u3000 \u3000 HDFS HBase Hive Spark Kafka \u3000 Kettle 6.1 \u3000 HDFS Hive HDFS Hive HDFS Hive \u3000 Pentaho 7.1 \u3000 \u3000 HDFS Hive \u3000 \u3000 8.0 \u3000 HDFS Hive \u3000 \u3000 \u3000 Knime 3.6.1 \u3000 \u3000 \u3000 HDFS Hive Spark HDFS Hive H2O.ai 3.24.0.2 \u3000 \u3000 \u3000 HDFS GaussDB \u3000 Kafka Manager 1.3.3.21 \u3000 \u3000 \u3000 Kafka \u3000 \u676d\u5dde\u5408\u4f17UTL 5.1 HDFS HBase Hive Kafka \u3000 \u3000 \u3000 \u3000 \u96c6\u6210\u5f00\u53d1\u73af\u5883 RStudio 1.0.153 \u3000 SparkR SparkR \u3000 \u3000 Apache Zepplin 0.7.2 \u3000 HBase Hive Spark SparkR \u3000 \u3000 \u3000 0.7.3 \u3000 \u3000 HBase Hive Spark SparkR HBase Hive Spark SparkR \u3000 0.8.0 \u3000 \u3000 \u3000 HBase Hive Spark SparkR \u3000 Jypyter Notebook \u3000 \u3000 \u3000 pySpark \u3000 \u3000 DBeaver 4.0.8 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 \u3000 4.2.1 \u3000 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 DbVisualizer 9.5.7 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 \u3000 10.0.1 \u3000 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 Squirrel 3.7.1 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 \u3000 3.8.0 \u3000 \u3000 Phoenix Hive SparkSQL \u3000 \u3000 HUE 4.0.1 \u3000 \u3000 HDFS HBase Hive Spark \u3000 \u3000 SQL\u5206\u6790 Apache Kylin 1.6.0 \u3000 Hive HBase \u3000 \u3000 \u3000 2.1.0 \u3000 \u3000 Hive HBase \u3000 \u3000 2.3.1 \u3000 \u3000 \u3000 Hive HBase \u3000 2.6.1 \u3000 \u3000 \u3000 \u3000 Hive HBase Kyligence 2.2 \u3000 Hive HBase \u3000 \u3000 \u3000 2.3 \u3000 Hive HBase \u3000 \u3000 \u3000 2.4 \u3000 \u3000 Hive HBase \u3000 \u3000 2.5 \u3000 \u3000 Hive HBase \u3000 \u3000 3.0 \u3000 \u3000 \u3000 Hive HBase Yarn \u3000 Presto 0.155 \u3000 HDFS Hive \u3000 \u3000 \u3000 0.184 \u3000 \u3000 HDFS Hive \u3000 \u3000 0.210 \u3000 \u3000 \u3000 HDFS Hive ElasticSearch HDFS Hive \u6570\u636e\u5e93 SAP HANA 100_120_0-10009569 Hive Hive Hive Hive \u3000 SAP VORA 2.0 \u3000 \u3000 Spark \u3000 \u3000 2.1 \u3000 \u3000 Spark \u3000 \u3000 \u676d\u5dde\u5408\u4f17UDB 6.1 GaussDB \u3000 \u3000 \u3000 \u3000 \u5176\u4ed6 FUSE 2.8.3 \u3000 HDFS \u3000 \u3000 \u3000 gis-tools-for-hadoop github \u3000 Hive MapReduce \u3000 \u3000 \u3000 IBM WAS 8.5.5.9 IBM_JDK \u3000 \u3000 \u3000 \u3000 Apache Livy 0.5.0 \u3000 \u3000 \u3000 Spark \u3000 Logstash 6.4.2 \u3000 \u3000 \u3000 ElasticSearch \u3000 Kibana 6.1.3 \u3000 \u3000 \u3000 ElasticSearch \u3000 elasticsearch-head 1.0 \u3000 \u3000 \u3000 ElasticSearch \u3000 beats 6.5.1 \u3000 \u3000 \u3000 ElasticSearch \u3000 NeoKylin 6.9 \u3000 \u3000 OS \u3000 \u3000 7.2 \u3000 \u3000 OS","title":"\u667a\u80fd\u6570\u636e\u751f\u6001\u5730\u56fe"},{"location":"Business_Intelligence/","text":"\u6570\u636e\u53ef\u89c6\u5316 \u00b6 \u5bf9\u63a5Tableau \u5bf9\u63a5QlikView \u5bf9\u63a5Oracle BIEE","title":"Home"},{"location":"Business_Intelligence/#_1","text":"\u5bf9\u63a5Tableau \u5bf9\u63a5QlikView \u5bf9\u63a5Oracle BIEE","title":"\u6570\u636e\u53ef\u89c6\u5316"},{"location":"Business_Intelligence/Oracle_BIEE/","text":"Oracle BIEE\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Oracle BIEE 11g <-> FusionInsight HD V100R002C60U20 Oracle BIEE 11g <-> FusionInsight HD V100R002C70SPC200 Oracle BIEE 12c <-> FusionInsight HD V100R002C60U20 Oracle BIEE 12c <-> FusionInsight HD V100R002C70SPC200 Oracle BIEE 12c <-> FusionInsight HD V100R002C80SPC200 Oracle BIEE 12c <-> FusionInsight HD 6.5.0 Linux\u73af\u5883\u5b89\u88c5OBIEE \u00b6 \u5b89\u88c5OS \u00b6 \u5b89\u88c5RedHat6.5\u64cd\u4f5c\u7cfb\u7edf\uff0cdesktop\u7248 \u521b\u5efa\u7528\u6237oracle \u5b89\u88c5jdk8 \u00b6 \u83b7\u53d6jdk8\u5b89\u88c5\u5305\uff0c\u6267\u884c\u5b89\u88c5 \u5b89\u88c5Weblogic \u00b6 \u521b\u5efaoracle home\u76ee\u5f55\uff1a umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ \u4e0a\u4f20weblogic\u5b89\u88c5\u5305\uff0c\u89e3\u538b \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 \u5b89\u88c5BI Server \u00b6 \u4e0a\u4f20OBIEE\u5b89\u88c5\u5305\uff0c\u89e3\u538b chmod 755 bi_platform-12.2.1.2.0_linux64.bin \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 ./bi_platform-12.2.1.2.0_linux64.bin \u8865\u9f50lib\u5305 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel \u53d6\u6d88\u5f53\u524d\u5b89\u88c5\uff0c\u91cd\u65b0\u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f \u5b89\u88c5Oracle Database 12c \u00b6 \u5b89\u88c5\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b89\u88c5\u76ee\u5f55 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle \u4e0b\u8f7dOracle Database 12c\u5b89\u88c5\u5305\uff0c\u89e3\u538b\u5f97\u5230database\u6587\u4ef6\u5939 chmod -R 755 database/ cd database/ su oracle ./runInstaller \u53ea\u5b89\u88c5\u5355\u5b9e\u4f8b\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b9e\u4f8b cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca \u5b57\u7b26\u96c6\u9009\u62e9AL32UTF8\uff0c\u4e0d\u52fe\u9009\u201ccreate as container database\u201d \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi ~/.bash_profile ORACLE_BASE = /Oracle/database ORACLE_HOME = $ORACLE_BASE /product/12.1.0/dbhome_1 ORACLE_SID = orcl ORACLE_TERM = xterm PATH = $PATH : $ORACLE_HOME /bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source ~/.bash_profile \u914d\u7f6e\u76d1\u542c\u7a0b\u5e8f\u548c\u7f51\u7edc\u670d\u52a1\u540d netca Listener\u7aef\u53e3\u8bbe\u4e3a\u9ed8\u8ba4\u503c1521 \u7f51\u7edc\u670d\u52a1\u540d\u914d\u7f6e\u4e3a ORCL \u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f \u4e3b\u673a\u91cd\u542f\u540e\uff0c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus\u754c\u9762\u6267\u884c startup \u4f7f\u7528RCU\u521b\u5efaSchema \u00b6 \u542f\u52a8rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu \u914d\u7f6eBI Server \u00b6 \u6267\u884c\u914d\u7f6e cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh \u5b89\u88c5BI Client \u00b6 \u5728Win7(64 bit)\u7cfb\u7edf\u4e0a\u5b89\u88c5BI Client \u5bf9\u63a5Hive \u00b6 \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u4ece http://web.mit.edu/kerberos/ \u4e0b\u8f7d\u5b89\u88c5kfw-4.1 \u5b89\u88c5\u914d\u7f6eHive ODBC Driver \u4e0b\u8f7d\u5b89\u88c5Hive ODBC Driver\uff08Windows\u7248\u672c\uff09\uff0c \u4e0b\u8f7d\u5730\u5740 \u5728BI\u5ba2\u6237\u7aef\u6240\u5728\u7684Windows\u673a\u5668\u4e0a\u914d\u7f6e\u7cfb\u7edfDSN \u6d4b\u8bd5ODBC\u8fde\u63a5 BI \u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 Client\u7aef\u6253\u5f00Oracle BI \u7ba1\u7406\u5de5\u5177 \u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684DSN\uff0c\u7528\u6237\u540d\u53e3\u4ee4\u4efb\u610f\u8f93\u5165\uff0c\u4f46\u4e0d\u80fd\u4e3a\u7a7a \u7981\u7528BI Server\u9ad8\u901f\u7f13\u5b58 \u00b6 \u767b\u5f55Weblogic\u57df\u7ba1\u7406\u754c\u9762 http://162.1.115.81:9500/em \u914d\u7f6e\u4e2d\u7981\u7528\u9ad8\u901f\u7f13\u5b58 \u4e0a\u4f20RPD\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u5ba2\u6237\u7aef cmd \u5207\u6362\u5230 E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin \u76ee\u5f55 \u6267\u884c\u547d\u4ee4\u4e0a\u4f20RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 mv /etc/krb5.conf /etc/krb5.conf.bak \u5c06FusionInsight\u96c6\u7fa4\u7684krb5.conf\u4e0a\u4f20\u5230/etc\u76ee\u5f55\u4e0b kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eCloudera Hive ODBC Driver yum install -y unixODBC \u4e0b\u8f7dHive ODBC Driver\uff08Linux\u7248\u672c\uff09 \u4e0b\u8f7d\u5730\u5740 \u5b89\u88c5Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u4e0eClient\u7aef\u751f\u6210\u7684RPD\u6587\u4ef6\u7684DSN\u540d\u79f0\u548c\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini \u4fee\u6539odbc\u914d\u7f6e\u6587\u4ef6 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Cloudera Hive DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e \u00b6 \u6253\u5f00BI Analytics\u754c\u9762 http://162.1.115.81:9502/analytics \u521b\u5efa\u5206\u6790 \u9009\u62e9\u5f85\u5206\u6790\u7684\u5217\u62d6\u5230\u53f3\u4fa7\u533a\u57df \u70b9\u51fb\u201c\u7ed3\u679c\u201d\u9875\u7b7e\uff0c\u68c0\u7d22\u6240\u9009\u5217\u6570\u636e \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u4fdd\u5b58\u6309\u94ae\uff0c\u4fdd\u5b58\u67e5\u8be2\u7ed3\u679c \u521b\u5efa\u53ef\u89c6\u5206\u6790\u5668\u9879\u76ee \u6dfb\u52a0\u6570\u636e\u6e90 \u9009\u53d6\u6570\u636e\u663e\u793a\u5f62\u5f0f \u6dfb\u52a0\u8ba1\u7b97 \u5bf9\u63a5Spark SQL \u00b6 \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 Kerberos\u8ba4\u8bc1 Kerberos\u83b7\u53d6\u8ba4\u8bc1\u7968\u636e \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7d\u5b89\u88c5 Simba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6eDSN\uff1a \u6d4b\u8bd5ODBC\u8fde\u63a5\uff1a BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 \u65b0\u5efaobiee-spark.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 Sample Simba Spark DSN \u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u4e0a\u4f20RDP \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 Kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7dSimba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u589e\u52a0Sample Simba Spark DSN\uff0c\u4e0eClient\u7aef\u914d\u7f6e\u76f8\u540c vi /etc/odbc.ini \u4fee\u6539odbcinst.ini\uff0c vi /etc/odbcinist.ini \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Simba Spark DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e \u00b6 \u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e \u5bf9\u63a5LibrA/ELK \u00b6 \u914d\u7f6eLibrA\u4e0eELK\u7684\u65b9\u5f0f\u6ca1\u6709\u533a\u522b\uff0c\u4ee5\u4e0b\u4ee5\u5bf9\u63a5ELK\u4e3a\u4f8b\u8fdb\u884c\u64cd\u4f5c \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eobiee\u5ba2\u6237\u7aef\u7684ODBC\u9a71\u52a8 \u6309\u7167ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684\u6307\u5bfc\u5b89\u88c5\u914d\u7f6eELK\u7684windows\u9a71\u52a8 \u914d\u7f6eDSN\uff0c\u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u4fdd\u5b58ODBC\u8fde\u63a5 BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 \u65b0\u5efaobiee-elk.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 PostgreSQL35W \u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u4e0a\u4f20RDP \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 \u53c2\u8003LibrA/ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684Linux\u4e0b\u914d\u7f6e\u6570\u636e\u6e90\u7ae0\u8282\uff0c\u5b8c\u6210obiee\u8282\u70b9\u4e0b\u7684ODBC\u9a71\u52a8\u7684\u5b89\u88c5 \u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u786e\u4fddODBC\u9a71\u52a8\u5b89\u88c5\u6210\u529f isql -v PostgreSQL35W BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u5728ODBC Data Sources\u90e8\u5206\u589e\u52a0PostgreSQL35W\u7684DSN \u5728\u6587\u4ef6\u672b\u5c3e\u589e\u52a0PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e\u6700\u540e\u4e00\u884cDriverUnicodeType=1\u9700\u8981\u52a0\u4e0a\uff0c\u5426\u5219obiee\u67e5\u8be2\u7684\u65f6\u5019\u4f1a\u62a5\u9519[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e \u00b6 \u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"\u5bf9\u63a5Oracle BIEE"},{"location":"Business_Intelligence/Oracle_BIEE/#oracle-bieefusioninsight","text":"","title":"Oracle BIEE\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/Oracle_BIEE/#_1","text":"Oracle BIEE 11g <-> FusionInsight HD V100R002C60U20 Oracle BIEE 11g <-> FusionInsight HD V100R002C70SPC200 Oracle BIEE 12c <-> FusionInsight HD V100R002C60U20 Oracle BIEE 12c <-> FusionInsight HD V100R002C70SPC200 Oracle BIEE 12c <-> FusionInsight HD V100R002C80SPC200 Oracle BIEE 12c <-> FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Oracle_BIEE/#linuxobiee","text":"","title":"Linux\u73af\u5883\u5b89\u88c5OBIEE"},{"location":"Business_Intelligence/Oracle_BIEE/#os","text":"\u5b89\u88c5RedHat6.5\u64cd\u4f5c\u7cfb\u7edf\uff0cdesktop\u7248 \u521b\u5efa\u7528\u6237oracle","title":"\u5b89\u88c5OS"},{"location":"Business_Intelligence/Oracle_BIEE/#jdk8","text":"\u83b7\u53d6jdk8\u5b89\u88c5\u5305\uff0c\u6267\u884c\u5b89\u88c5","title":"\u5b89\u88c5jdk8"},{"location":"Business_Intelligence/Oracle_BIEE/#weblogic","text":"\u521b\u5efaoracle home\u76ee\u5f55\uff1a umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ \u4e0a\u4f20weblogic\u5b89\u88c5\u5305\uff0c\u89e3\u538b \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762","title":"\u5b89\u88c5Weblogic"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server","text":"\u4e0a\u4f20OBIEE\u5b89\u88c5\u5305\uff0c\u89e3\u538b chmod 755 bi_platform-12.2.1.2.0_linux64.bin \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 ./bi_platform-12.2.1.2.0_linux64.bin \u8865\u9f50lib\u5305 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel \u53d6\u6d88\u5f53\u524d\u5b89\u88c5\uff0c\u91cd\u65b0\u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f","title":"\u5b89\u88c5BI Server"},{"location":"Business_Intelligence/Oracle_BIEE/#oracle-database-12c","text":"\u5b89\u88c5\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b89\u88c5\u76ee\u5f55 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle \u4e0b\u8f7dOracle Database 12c\u5b89\u88c5\u5305\uff0c\u89e3\u538b\u5f97\u5230database\u6587\u4ef6\u5939 chmod -R 755 database/ cd database/ su oracle ./runInstaller \u53ea\u5b89\u88c5\u5355\u5b9e\u4f8b\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b9e\u4f8b cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca \u5b57\u7b26\u96c6\u9009\u62e9AL32UTF8\uff0c\u4e0d\u52fe\u9009\u201ccreate as container database\u201d \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi ~/.bash_profile ORACLE_BASE = /Oracle/database ORACLE_HOME = $ORACLE_BASE /product/12.1.0/dbhome_1 ORACLE_SID = orcl ORACLE_TERM = xterm PATH = $PATH : $ORACLE_HOME /bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source ~/.bash_profile \u914d\u7f6e\u76d1\u542c\u7a0b\u5e8f\u548c\u7f51\u7edc\u670d\u52a1\u540d netca Listener\u7aef\u53e3\u8bbe\u4e3a\u9ed8\u8ba4\u503c1521 \u7f51\u7edc\u670d\u52a1\u540d\u914d\u7f6e\u4e3a ORCL \u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f \u4e3b\u673a\u91cd\u542f\u540e\uff0c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus\u754c\u9762\u6267\u884c startup","title":"\u5b89\u88c5Oracle Database 12c"},{"location":"Business_Intelligence/Oracle_BIEE/#rcuschema","text":"\u542f\u52a8rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu","title":"\u4f7f\u7528RCU\u521b\u5efaSchema"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server_1","text":"\u6267\u884c\u914d\u7f6e cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh","title":"\u914d\u7f6eBI Server"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-client","text":"\u5728Win7(64 bit)\u7cfb\u7edf\u4e0a\u5b89\u88c5BI Client","title":"\u5b89\u88c5BI Client"},{"location":"Business_Intelligence/Oracle_BIEE/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn","text":"\u914d\u7f6eKerberos\u8ba4\u8bc1 \u4ece http://web.mit.edu/kerberos/ \u4e0b\u8f7d\u5b89\u88c5kfw-4.1 \u5b89\u88c5\u914d\u7f6eHive ODBC Driver \u4e0b\u8f7d\u5b89\u88c5Hive ODBC Driver\uff08Windows\u7248\u672c\uff09\uff0c \u4e0b\u8f7d\u5730\u5740 \u5728BI\u5ba2\u6237\u7aef\u6240\u5728\u7684Windows\u673a\u5668\u4e0a\u914d\u7f6e\u7cfb\u7edfDSN \u6d4b\u8bd5ODBC\u8fde\u63a5","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-rdp","text":"Client\u7aef\u6253\u5f00Oracle BI \u7ba1\u7406\u5de5\u5177 \u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684DSN\uff0c\u7528\u6237\u540d\u53e3\u4ee4\u4efb\u610f\u8f93\u5165\uff0c\u4f46\u4e0d\u80fd\u4e3a\u7a7a","title":"BI \u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server_2","text":"\u767b\u5f55Weblogic\u57df\u7ba1\u7406\u754c\u9762 http://162.1.115.81:9500/em \u914d\u7f6e\u4e2d\u7981\u7528\u9ad8\u901f\u7f13\u5b58","title":"\u7981\u7528BI Server\u9ad8\u901f\u7f13\u5b58"},{"location":"Business_Intelligence/Oracle_BIEE/#rpd","text":"\u5ba2\u6237\u7aef cmd \u5207\u6362\u5230 E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin \u76ee\u5f55 \u6267\u884c\u547d\u4ee4\u4e0a\u4f20RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi","title":"\u4e0a\u4f20RPD\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_1","text":"\u914d\u7f6eKerberos\u8ba4\u8bc1 mv /etc/krb5.conf /etc/krb5.conf.bak \u5c06FusionInsight\u96c6\u7fa4\u7684krb5.conf\u4e0a\u4f20\u5230/etc\u76ee\u5f55\u4e0b kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eCloudera Hive ODBC Driver yum install -y unixODBC \u4e0b\u8f7dHive ODBC Driver\uff08Linux\u7248\u672c\uff09 \u4e0b\u8f7d\u5730\u5740 \u5b89\u88c5Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u4e0eClient\u7aef\u751f\u6210\u7684RPD\u6587\u4ef6\u7684DSN\u540d\u79f0\u548c\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini \u4fee\u6539odbc\u914d\u7f6e\u6587\u4ef6 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Cloudera Hive DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#hive_1","text":"\u6253\u5f00BI Analytics\u754c\u9762 http://162.1.115.81:9502/analytics \u521b\u5efa\u5206\u6790 \u9009\u62e9\u5f85\u5206\u6790\u7684\u5217\u62d6\u5230\u53f3\u4fa7\u533a\u57df \u70b9\u51fb\u201c\u7ed3\u679c\u201d\u9875\u7b7e\uff0c\u68c0\u7d22\u6240\u9009\u5217\u6570\u636e \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u4fdd\u5b58\u6309\u94ae\uff0c\u4fdd\u5b58\u67e5\u8be2\u7ed3\u679c \u521b\u5efa\u53ef\u89c6\u5206\u6790\u5668\u9879\u76ee \u6dfb\u52a0\u6570\u636e\u6e90 \u9009\u53d6\u6570\u636e\u663e\u793a\u5f62\u5f0f \u6dfb\u52a0\u8ba1\u7b97","title":"\u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e"},{"location":"Business_Intelligence/Oracle_BIEE/#spark-sql","text":"","title":"\u5bf9\u63a5Spark SQL"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_2","text":"Kerberos\u8ba4\u8bc1 Kerberos\u83b7\u53d6\u8ba4\u8bc1\u7968\u636e \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7d\u5b89\u88c5 Simba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6eDSN\uff1a \u6d4b\u8bd5ODBC\u8fde\u63a5\uff1a","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#birdp","text":"\u65b0\u5efaobiee-spark.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 Sample Simba Spark DSN","title":"BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#rdp","text":"\u4e0a\u4f20RDP","title":"\u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_3","text":"Kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7dSimba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u589e\u52a0Sample Simba Spark DSN\uff0c\u4e0eClient\u7aef\u914d\u7f6e\u76f8\u540c vi /etc/odbc.ini \u4fee\u6539odbcinst.ini\uff0c vi /etc/odbcinist.ini \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Simba Spark DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#spark","text":"\u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"\u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e"},{"location":"Business_Intelligence/Oracle_BIEE/#libraelk","text":"\u914d\u7f6eLibrA\u4e0eELK\u7684\u65b9\u5f0f\u6ca1\u6709\u533a\u522b\uff0c\u4ee5\u4e0b\u4ee5\u5bf9\u63a5ELK\u4e3a\u4f8b\u8fdb\u884c\u64cd\u4f5c","title":"\u5bf9\u63a5LibrA/ELK"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_4","text":"\u914d\u7f6eobiee\u5ba2\u6237\u7aef\u7684ODBC\u9a71\u52a8 \u6309\u7167ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684\u6307\u5bfc\u5b89\u88c5\u914d\u7f6eELK\u7684windows\u9a71\u52a8 \u914d\u7f6eDSN\uff0c\u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u4fdd\u5b58ODBC\u8fde\u63a5","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#birdp_1","text":"\u65b0\u5efaobiee-elk.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 PostgreSQL35W","title":"BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#rdp_1","text":"\u4e0a\u4f20RDP","title":"\u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_5","text":"\u53c2\u8003LibrA/ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684Linux\u4e0b\u914d\u7f6e\u6570\u636e\u6e90\u7ae0\u8282\uff0c\u5b8c\u6210obiee\u8282\u70b9\u4e0b\u7684ODBC\u9a71\u52a8\u7684\u5b89\u88c5 \u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u786e\u4fddODBC\u9a71\u52a8\u5b89\u88c5\u6210\u529f isql -v PostgreSQL35W BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u5728ODBC Data Sources\u90e8\u5206\u589e\u52a0PostgreSQL35W\u7684DSN \u5728\u6587\u4ef6\u672b\u5c3e\u589e\u52a0PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e\u6700\u540e\u4e00\u884cDriverUnicodeType=1\u9700\u8981\u52a0\u4e0a\uff0c\u5426\u5219obiee\u67e5\u8be2\u7684\u65f6\u5019\u4f1a\u62a5\u9519[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#spark_1","text":"\u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"\u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e"},{"location":"Business_Intelligence/QlikView/","text":"QlikView\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 QlikView 12 <-> FusionInsight HD V100R002C60U20 QlikView 12 <-> FusionInsight HD V100R002C70SPC200 QlikView 12 <-> FusionInsight HD V100R002C80SPC100 QlikView 12 <-> FusionInsight HD 6.5.0 \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\uff0c\u5730\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684**\u521b\u5efa\u7528\u6237**\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201csparkdemo\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230C:\\ProgramData\\MIT\\Kerberos5\u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u547d\u4ee4\u884c\u8fdb\u5165\u5230MIT Kerberos\u5b89\u88c5\u8def\u5f84\uff0c\u627e\u5230\u53ef\u6267\u884c\u6587\u4ef6kinit.exe\uff0c\u4f8b\u5982\u672c\u6587\u8def\u5f84\u4e3a\uff1a C:\\Program Files\\MIT\\Kerberos\\bin \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a kinit -k -t /path_to_userkeytab/user.keytab UserName \u5176\u4e2dpath_to_userkeytab\u4e3a\u5b58\u653e\u7528\u6237keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0cuser.keytab\u4e3a\u7528\u6237\u7684keytab\uff0cUserName\u4e3a\u7528\u6237\u540d\u3002 \u914d\u7f6eHive\u6570\u636e\u6e90 \u00b6 QlikView\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3 \u4e0b\u8f7d\u5b89\u88c5Hive ODBC\u9a71\u52a8 \u00b6 \u4ece\u4ee5\u4e0b\u5730\u5740\u4e0b\u8f7d\u9a71\u52a8\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6e\u7528\u6237DSN \u00b6 \u5728OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668\u9875\u9762\u7684\u7528\u6237DSN\u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb\u6dfb\u52a0\uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Cloudera ODBC Driver for Apache Hive \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u914d\u7f6eHive\u6570\u636e\u6e90\u3002 Data Source Name\uff1a\u4e3a\u81ea\u5b9a\u4e49\u53c2\u6570 Host(s)\uff1a HiveServer\u7684\u4e1a\u52a1ip Port\uff1a Hive Service\u7aef\u53e3\uff0c21066 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a hive Realm\uff1a \u7559\u7a7a \u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\u5219\u8868\u793a\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb OK \u8fde\u63a5Hive\u6570\u636e\u6e90 \u00b6 \u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728**\u8fde\u63a5\u5230\u6570\u636e\u6e90**\u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90hive_odbc\uff0c\u7136\u540e\u70b9\u51fb**\u786e\u5b9a**\uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002 \u914d\u7f6eSpark\u6570\u636e\u6e90 \u00b6 QlivView\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\uff0c\u5bf9\u63a5SparkSQL\u7684thrift\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5b89\u88c5Spark\u7684ODBC\u9a71\u52a8 \u00b6 \u5728Simba\u5b98\u7f51\u4e0b\u8f7dSpark ODBC\u9a71\u52a8\uff0c\u6839\u636e\u7528\u6237\u81ea\u8eab\u64cd\u4f5c\u7cfb\u7edf\u9009\u62e932bit\u621664bit\uff0cData Source\u9009\u62e9Spark SQL\uff0c\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u63d0\u793a\u5b89\u88c5\u5ba2\u6237\u7aef\u3002 \u914d\u7f6e\u7528\u6237DSN \u00b6 \u5728 OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\u7684 \u7528\u6237DSN \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Simba Spark ODBC Driver \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728 Simba Spark ODBC Driver DSN Setup \u9875\u9762\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\u3002 Data Source Name\uff1a \u81ea\u5b9a\u4e49 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a \u7559\u7a7a\uff0c Host(s)\uff1a JDBCServer(\u4e3b)\u7684\u4e1a\u52a1ip\uff0c Port\uff1a SparkThriftServer\u5ba2\u6237\u7aef\u7aef\u53e3\u53f723040\u3002 \u8bbe\u7f6e\u5b8c\u6bd5\u540e\u70b9\u51fb Advanced Options \uff0c\u5728\u5f39\u51fa\u7684 Advanced Options \u9875\u9762\u4e2d\uff0c\u52fe\u9009 Use Native Query \u548c Get Tables With Query \uff0c\u7136\u540e\u70b9\u51fb OK \u56de\u5230 Simba Spark ODBC Driver DSN Setup \uff0c\u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u9000\u51fa\u9875\u9762\uff0c\u5426\u5219\u5c06\u5f39\u51fa\u5931\u8d25\u5bf9\u8bdd\u6846\u3002 \u56de\u5230 Simba Spark ODBC Driver DSN Setup \u9875\u9762\uff0c\u70b9\u51fb OK \uff0c\u56de\u5230 ODBC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u5e76\u9000\u51fa\u914d\u7f6e\u3002 \u8fde\u63a5Spark\u6570\u636e\u6e90 \u00b6 \u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728 \u8fde\u63a5\u5230\u6570\u636e\u6e90 \u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90spark_odbc\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002 FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662f Host(s) \u3001 Port \u3001 Host FQDN \u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u5f55\u5165","title":"\u5bf9\u63a5QlikView"},{"location":"Business_Intelligence/QlikView/#qlikviewfusioninsight","text":"","title":"QlikView\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/QlikView/#_1","text":"QlikView 12 <-> FusionInsight HD V100R002C60U20 QlikView 12 <-> FusionInsight HD V100R002C70SPC200 QlikView 12 <-> FusionInsight HD V100R002C80SPC100 QlikView 12 <-> FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/QlikView/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\uff0c\u5730\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684**\u521b\u5efa\u7528\u6237**\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201csparkdemo\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230C:\\ProgramData\\MIT\\Kerberos5\u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u547d\u4ee4\u884c\u8fdb\u5165\u5230MIT Kerberos\u5b89\u88c5\u8def\u5f84\uff0c\u627e\u5230\u53ef\u6267\u884c\u6587\u4ef6kinit.exe\uff0c\u4f8b\u5982\u672c\u6587\u8def\u5f84\u4e3a\uff1a C:\\Program Files\\MIT\\Kerberos\\bin \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a kinit -k -t /path_to_userkeytab/user.keytab UserName \u5176\u4e2dpath_to_userkeytab\u4e3a\u5b58\u653e\u7528\u6237keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0cuser.keytab\u4e3a\u7528\u6237\u7684keytab\uff0cUserName\u4e3a\u7528\u6237\u540d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/QlikView/#hive","text":"QlikView\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3","title":"\u914d\u7f6eHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#hive-odbc","text":"\u4ece\u4ee5\u4e0b\u5730\u5740\u4e0b\u8f7d\u9a71\u52a8\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\uff1a \u4e0b\u8f7d\u5730\u5740","title":"\u4e0b\u8f7d\u5b89\u88c5Hive ODBC\u9a71\u52a8"},{"location":"Business_Intelligence/QlikView/#dsn","text":"\u5728OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668\u9875\u9762\u7684\u7528\u6237DSN\u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb\u6dfb\u52a0\uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Cloudera ODBC Driver for Apache Hive \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u914d\u7f6eHive\u6570\u636e\u6e90\u3002 Data Source Name\uff1a\u4e3a\u81ea\u5b9a\u4e49\u53c2\u6570 Host(s)\uff1a HiveServer\u7684\u4e1a\u52a1ip Port\uff1a Hive Service\u7aef\u53e3\uff0c21066 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a hive Realm\uff1a \u7559\u7a7a \u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\u5219\u8868\u793a\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb OK","title":"\u914d\u7f6e\u7528\u6237DSN"},{"location":"Business_Intelligence/QlikView/#hive_1","text":"\u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728**\u8fde\u63a5\u5230\u6570\u636e\u6e90**\u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90hive_odbc\uff0c\u7136\u540e\u70b9\u51fb**\u786e\u5b9a**\uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002","title":"\u8fde\u63a5Hive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#spark","text":"QlivView\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\uff0c\u5bf9\u63a5SparkSQL\u7684thrift\u63a5\u53e3\u3002","title":"\u914d\u7f6eSpark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#sparkodbc","text":"\u5728Simba\u5b98\u7f51\u4e0b\u8f7dSpark ODBC\u9a71\u52a8\uff0c\u6839\u636e\u7528\u6237\u81ea\u8eab\u64cd\u4f5c\u7cfb\u7edf\u9009\u62e932bit\u621664bit\uff0cData Source\u9009\u62e9Spark SQL\uff0c\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u63d0\u793a\u5b89\u88c5\u5ba2\u6237\u7aef\u3002","title":"\u4e0b\u8f7d\u5b89\u88c5Spark\u7684ODBC\u9a71\u52a8"},{"location":"Business_Intelligence/QlikView/#dsn_1","text":"\u5728 OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\u7684 \u7528\u6237DSN \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Simba Spark ODBC Driver \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728 Simba Spark ODBC Driver DSN Setup \u9875\u9762\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\u3002 Data Source Name\uff1a \u81ea\u5b9a\u4e49 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a \u7559\u7a7a\uff0c Host(s)\uff1a JDBCServer(\u4e3b)\u7684\u4e1a\u52a1ip\uff0c Port\uff1a SparkThriftServer\u5ba2\u6237\u7aef\u7aef\u53e3\u53f723040\u3002 \u8bbe\u7f6e\u5b8c\u6bd5\u540e\u70b9\u51fb Advanced Options \uff0c\u5728\u5f39\u51fa\u7684 Advanced Options \u9875\u9762\u4e2d\uff0c\u52fe\u9009 Use Native Query \u548c Get Tables With Query \uff0c\u7136\u540e\u70b9\u51fb OK \u56de\u5230 Simba Spark ODBC Driver DSN Setup \uff0c\u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u9000\u51fa\u9875\u9762\uff0c\u5426\u5219\u5c06\u5f39\u51fa\u5931\u8d25\u5bf9\u8bdd\u6846\u3002 \u56de\u5230 Simba Spark ODBC Driver DSN Setup \u9875\u9762\uff0c\u70b9\u51fb OK \uff0c\u56de\u5230 ODBC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u5e76\u9000\u51fa\u914d\u7f6e\u3002","title":"\u914d\u7f6e\u7528\u6237DSN"},{"location":"Business_Intelligence/QlikView/#spark_1","text":"\u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728 \u8fde\u63a5\u5230\u6570\u636e\u6e90 \u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90spark_odbc\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002","title":"\u8fde\u63a5Spark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662f Host(s) \u3001 Port \u3001 Host FQDN \u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u5f55\u5165","title":"FAQ"},{"location":"Business_Intelligence/Tableau/","text":"Tableau\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Tableau 10.0.0 <-> FusionInsight HD V100R002C30 Tableau 10.0.0 <-> FusionInsight HD V100R002C50 Tableau 10.0.0 <-> FusionInsight HD V100R002C60U10 Tableau 10.1.4 <-> FusionInsight HD V100R002C60U20 Tableau 10.3.2 <-> FusionInsight HD V100R002C70SPC200 Tableau 10.5.0 <-> FusionInsight HD V100R002C80SPC100 \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctableau\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002 \u914d\u7f6eHive\u6570\u636e\u6e90 \u00b6 Tableau\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a \u4e0b\u8f7d\u5730\u5740 \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb\u4e2d\u7684Test\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Tableau\u4f7f\u7528\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201c\u5176\u4ed6\u6570\u636e\u5e93\uff08ODBC\uff09\u201d\uff1b DSN\u9009\u62e9hive_odbc\uff08\u4e0a\u4e00\u6b65\u4e2d\u8bbe\u7f6eODBC\u7684\u540d\u79f0\uff09\uff0c\u70b9\u51fb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\uff0c\u7136\u540e\u767b\u9646\u3002 \u67e5\u8be2\u767e\u4e07\u7ea7\u6570\u636e\u8868\u6570\u636e \u67e5\u8be2\u591a\u8868\u6570\u636e \u914d\u7f6eSpark\u6570\u636e\u6e90 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5spark\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u521b\u5efaDSN\uff08Data Source Name\uff09 \u6253\u5f00 C:\\Program Files\\Simba Spark ODBC Driver\\lib\\DriverConfiguration64.exe \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 Tableau\u4f7f\u7528Spark\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201cSpark SQL\u201d\uff0c\u4f5c\u5982\u4e0b\u914d\u7f6e\uff1a \u5176\u4e2d\u670d\u52a1\u5668\u4e3aJDBCServer(\u4e3b)\u7684\u4e1a\u52a1IP\u3002 \u7aef\u53e3\u4e3aFusionInsight\u4e2dSpark\u670d\u52a1\u914d\u7f6e\uff0c\u5bfc\u51fa\u670d\u52a1\u914d\u7f6e\u6587\u4ef6\uff0c\u5176\u4e2d hive.server2.thrift.port \u5bf9\u5e94\u503c\u3002 \u70b9\u51fb\u201c\u767b\u5f55\u201d\uff0c\u8fdb\u5165tableau\u9875\u9762\uff0c\u9009\u62e9\u67b6\u6784\u548c\u8868\uff0c\u7ed3\u679c\u5982\u4e0b\u3002 \u7528Tableau\u505a\u5b9e\u65f6\u8fde\u63a5\uff0c\u6253\u5f00\u5de5\u4f5c\u7c3f\uff0c\u5bf9\u8be5\u8868\u8fdb\u884c\u56fe\u5f62\u5316\u5206\u6790\u3002 \u6027\u80fd\u6d4b\u8bd5 \u67e5\u8be2\u5305\u542b\u767e\u4e07\u6761\u6570\u636e\u7684\u8868web_sales \u591a\u8868\u5173\u8054\u67e5\u8be2\uff1astore_sales\u548citem\u8868\u505a\u5173\u8054\u67e5\u8be2 \u589e\u52a0customer_address\u8868 \u67e5\u8be2\u7ed3\u679c\uff1a FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662fHost(s)\u3001Port\u3001Host FQDN\u7b49\u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u8f93\u5165","title":"\u5bf9\u63a5Tableau"},{"location":"Business_Intelligence/Tableau/#tableaufusioninsight","text":"","title":"Tableau\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/Tableau/#_1","text":"Tableau 10.0.0 <-> FusionInsight HD V100R002C30 Tableau 10.0.0 <-> FusionInsight HD V100R002C50 Tableau 10.0.0 <-> FusionInsight HD V100R002C60U10 Tableau 10.1.4 <-> FusionInsight HD V100R002C60U20 Tableau 10.3.2 <-> FusionInsight HD V100R002C70SPC200 Tableau 10.5.0 <-> FusionInsight HD V100R002C80SPC100","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Tableau/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctableau\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/Tableau/#hive","text":"Tableau\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a \u4e0b\u8f7d\u5730\u5740 \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb\u4e2d\u7684Test\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Tableau\u4f7f\u7528\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201c\u5176\u4ed6\u6570\u636e\u5e93\uff08ODBC\uff09\u201d\uff1b DSN\u9009\u62e9hive_odbc\uff08\u4e0a\u4e00\u6b65\u4e2d\u8bbe\u7f6eODBC\u7684\u540d\u79f0\uff09\uff0c\u70b9\u51fb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\uff0c\u7136\u540e\u767b\u9646\u3002 \u67e5\u8be2\u767e\u4e07\u7ea7\u6570\u636e\u8868\u6570\u636e \u67e5\u8be2\u591a\u8868\u6570\u636e","title":"\u914d\u7f6eHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/Tableau/#spark","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5spark\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u521b\u5efaDSN\uff08Data Source Name\uff09 \u6253\u5f00 C:\\Program Files\\Simba Spark ODBC Driver\\lib\\DriverConfiguration64.exe \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 Tableau\u4f7f\u7528Spark\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201cSpark SQL\u201d\uff0c\u4f5c\u5982\u4e0b\u914d\u7f6e\uff1a \u5176\u4e2d\u670d\u52a1\u5668\u4e3aJDBCServer(\u4e3b)\u7684\u4e1a\u52a1IP\u3002 \u7aef\u53e3\u4e3aFusionInsight\u4e2dSpark\u670d\u52a1\u914d\u7f6e\uff0c\u5bfc\u51fa\u670d\u52a1\u914d\u7f6e\u6587\u4ef6\uff0c\u5176\u4e2d hive.server2.thrift.port \u5bf9\u5e94\u503c\u3002 \u70b9\u51fb\u201c\u767b\u5f55\u201d\uff0c\u8fdb\u5165tableau\u9875\u9762\uff0c\u9009\u62e9\u67b6\u6784\u548c\u8868\uff0c\u7ed3\u679c\u5982\u4e0b\u3002 \u7528Tableau\u505a\u5b9e\u65f6\u8fde\u63a5\uff0c\u6253\u5f00\u5de5\u4f5c\u7c3f\uff0c\u5bf9\u8be5\u8868\u8fdb\u884c\u56fe\u5f62\u5316\u5206\u6790\u3002 \u6027\u80fd\u6d4b\u8bd5 \u67e5\u8be2\u5305\u542b\u767e\u4e07\u6761\u6570\u636e\u7684\u8868web_sales \u591a\u8868\u5173\u8054\u67e5\u8be2\uff1astore_sales\u548citem\u8868\u505a\u5173\u8054\u67e5\u8be2 \u589e\u52a0customer_address\u8868 \u67e5\u8be2\u7ed3\u679c\uff1a","title":"\u914d\u7f6eSpark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/Tableau/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662fHost(s)\u3001Port\u3001Host FQDN\u7b49\u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u8f93\u5165","title":"FAQ"},{"location":"Data_Analysis/","text":"\u6570\u636e\u53ef\u89c6\u5316 \u00b6 \u5bf9\u63a5Alteryx \u5bf9\u63a5RapidMiner \u5bf9\u63a5Splunk","title":"Home"},{"location":"Data_Analysis/#_1","text":"\u5bf9\u63a5Alteryx \u5bf9\u63a5RapidMiner \u5bf9\u63a5Splunk","title":"\u6570\u636e\u53ef\u89c6\u5316"},{"location":"Data_Analysis/Alteryx/","text":"Alteryx\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Alteryx 2018.2.5.48994 <-> FusionInsight HD V100R002C80SPC200 Alteryx 2018.2.5.48994 <-> FusionInsight HD 6.5.0 \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528Kerbers\u8ba4\u8bc1\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002 \u914d\u7f6eSpark ODBC \u8fde\u63a5 \u00b6 \u5728\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u914d\u7f6eSpark ODBC\u9a71\u52a8 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a https://www.tableau.com/support/drivers \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Simba Spark ODBC Driver -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aSpark ODBC\u8fde\u63a5\u6210\u529f\u3002 \u5728Alteryx\u4f7f\u7528Spark\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aApache Spark ODBC COnnection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aApache Spark ODBC Write->Driver: \u9ed8\u8ba4 Connection String\uff1aNew database connection\uff0c\u9009\u62e9Spark DSN\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC->Simba Spark Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Spark DSN\uff1aSimba Spark \uff08System\uff09\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dSpark\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a \u914d\u7f6eHive ODBC\u6570\u636e\u6e90 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Hive\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a \u4e0b\u8f7d\u5730\u5740 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Alteryx\u4f7f\u7528Hive\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aHive Connection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aHive ODBC Write->Driver: Hive ODBC Connection String\uff1aNew database connection\uff0c\u9009\u62e9Hive DSN\uff0c\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u5728\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Hive DSN\uff1aSample Cloudera Hive DSN(System)\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dHive\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer\uff1a \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a \u914d\u7f6eHDFS\u6570\u636e\u6e90 \u00b6 HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6MIT Kerberos Ticket\uff0c\u5e76\u5728Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS\u3002 \u5728Alteryx\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Hadoop \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a Server\uff1aWebHDFS Host\uff1a HDFS\u6240\u5728\u670d\u52a1\u5668IP Port: \u914d\u7f6e\u6587\u4ef6\u4e2ddfs.namenode.http.port\u5bf9\u5e94\u7aef\u53e3 User Name & Password\uff1a Kerberos \u8ba4\u8bc1\u7528\u6237\u540d\u53ca\u5bc6\u7801 Kerberos: Kerberos MIT \u70b9\u51fbTest\uff0c\u51fa\u73b0Connection successful \u8868\u660e\u8fde\u63a5\u6210\u529f\u3002 \u5f39\u51fa\u96c6\u7fa4\u4e2d\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5185\u5bb9\uff0c\u76ee\u524d\u652f\u6301Avro\u548cCSV\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u9700\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002 \u9009\u62e9\u76f8\u5e94\u6587\u4ef6\uff0c\u8fde\u63a5\u6210\u529f\uff0cRefresh\u4e4b\u540e\u5de6\u4fa7\u83dc\u5355\u663e\u793a\u6587\u4ef6\u5185\u5bb9\u9884\u89c8\uff1a Join \u64cd\u4f5c\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b: FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 \u6d4b\u8bd5\u8fde\u63a5\u65f6\u51fa\u73b0Default Kerberos ticket is expired Kerberos MIT ticket \u8fc7\u671f\uff0c\u9700\u8981\u91cd\u65b0\u83b7\u5f97\uff0c\u83b7\u53d6\u4e00\u6b21\u6709\u6548\u671f\u4e3a10h.","title":"\u5bf9\u63a5Alteryx"},{"location":"Data_Analysis/Alteryx/#alteryxfusioninsight","text":"","title":"Alteryx\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/Alteryx/#_1","text":"Alteryx 2018.2.5.48994 <-> FusionInsight HD V100R002C80SPC200 Alteryx 2018.2.5.48994 <-> FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/Alteryx/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528Kerbers\u8ba4\u8bc1\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Data_Analysis/Alteryx/#spark-odbc","text":"\u5728\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u914d\u7f6eSpark ODBC\u9a71\u52a8 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a https://www.tableau.com/support/drivers \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Simba Spark ODBC Driver -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aSpark ODBC\u8fde\u63a5\u6210\u529f\u3002 \u5728Alteryx\u4f7f\u7528Spark\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aApache Spark ODBC COnnection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aApache Spark ODBC Write->Driver: \u9ed8\u8ba4 Connection String\uff1aNew database connection\uff0c\u9009\u62e9Spark DSN\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC->Simba Spark Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Spark DSN\uff1aSimba Spark \uff08System\uff09\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dSpark\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a","title":"\u914d\u7f6eSpark ODBC \u8fde\u63a5"},{"location":"Data_Analysis/Alteryx/#hive-odbc","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Hive\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a \u4e0b\u8f7d\u5730\u5740 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Alteryx\u4f7f\u7528Hive\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aHive Connection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aHive ODBC Write->Driver: Hive ODBC Connection String\uff1aNew database connection\uff0c\u9009\u62e9Hive DSN\uff0c\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u5728\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Hive DSN\uff1aSample Cloudera Hive DSN(System)\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dHive\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer\uff1a \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a","title":"\u914d\u7f6eHive ODBC\u6570\u636e\u6e90"},{"location":"Data_Analysis/Alteryx/#hdfs","text":"HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6MIT Kerberos Ticket\uff0c\u5e76\u5728Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS\u3002 \u5728Alteryx\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Hadoop \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a Server\uff1aWebHDFS Host\uff1a HDFS\u6240\u5728\u670d\u52a1\u5668IP Port: \u914d\u7f6e\u6587\u4ef6\u4e2ddfs.namenode.http.port\u5bf9\u5e94\u7aef\u53e3 User Name & Password\uff1a Kerberos \u8ba4\u8bc1\u7528\u6237\u540d\u53ca\u5bc6\u7801 Kerberos: Kerberos MIT \u70b9\u51fbTest\uff0c\u51fa\u73b0Connection successful \u8868\u660e\u8fde\u63a5\u6210\u529f\u3002 \u5f39\u51fa\u96c6\u7fa4\u4e2d\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5185\u5bb9\uff0c\u76ee\u524d\u652f\u6301Avro\u548cCSV\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u9700\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002 \u9009\u62e9\u76f8\u5e94\u6587\u4ef6\uff0c\u8fde\u63a5\u6210\u529f\uff0cRefresh\u4e4b\u540e\u5de6\u4fa7\u83dc\u5355\u663e\u793a\u6587\u4ef6\u5185\u5bb9\u9884\u89c8\uff1a Join \u64cd\u4f5c\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b:","title":"\u914d\u7f6eHDFS\u6570\u636e\u6e90"},{"location":"Data_Analysis/Alteryx/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 \u6d4b\u8bd5\u8fde\u63a5\u65f6\u51fa\u73b0Default Kerberos ticket is expired Kerberos MIT ticket \u8fc7\u671f\uff0c\u9700\u8981\u91cd\u65b0\u83b7\u5f97\uff0c\u83b7\u53d6\u4e00\u6b21\u6709\u6548\u671f\u4e3a10h.","title":"FAQ"},{"location":"Data_Analysis/RapidMiner/","text":"RapidMiner\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Rapidminer Studio 8.2.001 <-> FusionInsight HD V100R002C80SPC200 \u51c6\u5907\u5de5\u4f5c \u00b6 \u4e0b\u8f7d\u5b89\u88c5RapidMiner Studio, \u5f53\u524d\u6700\u65b0\u7248\u672c\u4e3a8.2.001,\u4e0b\u8f7d\u5730\u5740 https://rapidminer.com/ \u5b89\u88c5\u5b8c\u6210\u540e\u5728\u4e3b\u754c\u9762\u9876\u90e8\u83dc\u5355\u680f\u9009\u62e9 Extensions->Marketplace ,\u641c\u7d22 radoop ,\u5b89\u88c5\u540e\u91cd\u542frapidminer \u4fee\u6539\u672c\u5730host\u6587\u4ef6\uff0c\u8def\u5f84\u4e3aC:\\Windows\\System32\\drivers\\etc\uff0c\u52a0\u5165\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9IP\u4e0e\u4e3b\u673a\u540d\u5bf9\u5e94\u5173\u7cfb\uff0c\u4fdd\u5b58\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002 \u51c6\u5907FusionInsight\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4ee5\u53cajar\u5305 \u5728\u96c6\u7fa4\u7684Manager\u4e2d\uff0c\u9009\u62e9\u670d\u52a1->\u4e0b\u8f7d\u5ba2\u6237\u7aef->\u5b8c\u6574\u5ba2\u6237\u7aef \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cYarn\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u5939\u91cc\uff0c\u4f8b\u5982\u547d\u540d\u4e3aconfig\u3002 \u6253\u5f00 yarn-site.xml ,\u5220\u9664\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e <property> <name>audit.service.name</name> <value>Yarn</value> </property> \u8fdb\u5165Spark\u7ec4\u4ef6\u7684Jar\u5305\u76ee\u5f55\u201c\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars\u201d\uff0c\u5c06\u6240\u6709jar\u5305\u590d\u5236\u51fa\u6765\uff0c\u4fdd\u5b58\u5728\u672c\u673a\u67d0\u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:/jars \u3002 \u96c6\u7fa4\u914d\u7f6e \u00b6 \u914d\u7f6eUDP\u7aef\u53e3\u7ed1\u5b9a \u4e0b\u8f7d\u5b89\u88c5UDP\u7aef\u53e3\u7ed1\u5b9a\u5de5\u5177uredir\uff0c\u4e0b\u8f7d\u5730\u5740 https://github.com/troglobit/uredir \u7f16\u8bd1\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5206\u522b\u4e0a\u4f20\u81f3KDC\u670d\u52a1\u6240\u5728\u7684\u4e3b\u5907\u8282\u70b9(\u53ef\u5728krb5.conf\u6587\u4ef6\u4e2d\u67e5\u770b)\uff0c\u8fdb\u5165uredir\u6267\u884c\u6587\u4ef6\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a,\u5176\u4e2dIP\u4e3a\u6240\u5728\u8282\u70b9IP ./uredir IP:88 IP:21732 \u914d\u7f6eRadoop\u4f9d\u8d56jar\u5305 \u5728Radoop\u6587\u6863\u4e2d\u5fc3\uff0c\u4e0b\u8f7dRadoop\u4f9d\u8d56jar\u5305\uff0c\u4e0b\u8f7d\u5730\u5740 https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,\u4e0b\u8f7d\u4e0e\u5b89\u88c5\u7684RapidMiner\u7248\u672c\u5bf9\u5e94\u7684jar\u5305\u3002 \u5c06jar\u5305\u4e0a\u4f20\u81f3\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u76f8\u540c\u7684\u8def\u5f84\u4e0b\uff0c\u4f8b\u5982/usr/local/lib/radoop/ \u5728\u96c6\u7fa4HiveServer\u6240\u5728\u8282\u70b9\uff0c\u5206\u522b\u4e0a\u4f20Radoop\u7684jar\u5305\u81f3\u4ee5\u4e0b\u8def\u5f84\uff0c\u5e76\u4fee\u6539\u6240\u6709\u8005\u548c\u6267\u884c\u6743\u9650 Hive\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib\"\uff0c Mapreduce\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\uff1a\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib\" cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar * \u5728FusionInsight Manager \u754c\u9762\u6dfb\u52a0Hive\u767d\u540d\u5355\u914d\u7f6e radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name \u9700\u8981\u4ee5 | \u5206\u5272 * \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u91cd\u542fHiveServer \u521b\u5efaRadoop UDF\u51fd\u6570 \u5728\u4e3b\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a #cd /opt/hadoopclient #source bigdata_env #kinit developuser \u8f93\u5165developuser\u7528\u6237\u5bc6\u7801\uff0c\u6267\u884cbeeline\uff0c\u8fdb\u5165Hive Hive\u4e2d\u521b\u5efa\u6570\u636e\u5e93\uff0c\u4f8b\u5982\u521b\u5efa\u6570\u636e\u5e93rapidminer,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a create database rapidminer\uff1b use rapidminer\uff1b DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep'; RapidMiner\u914d\u7f6e \u00b6 \u5728RapidMiner\u4e2d\uff0c\u83dc\u5355\u9009\u62e9Connections->Manage Radoop Connections \u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u9009\u62e9New Connections->Import Hadoop Configuration Files\uff0c\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939\uff0c\u70b9\u51fbImport Configuration \u5bfc\u5165\u6210\u529f\u540e\u70b9\u51fbNext\uff0c\u8fdb\u5165\u8fde\u63a5\u914d\u7f6e\u7a97\u53e3\uff0c\u6839\u636e\u5de6\u4fa7\u83dc\u5355\u680f\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199\uff1a Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory\uff1aSpark\u7ec4\u4ef6\u7684jars\u5305 Client Principal\uff1a Kerberos\u7528\u6237\u540d@HADOOP.com Keytab File: \u4eceManager\u4e0b\u8f7d\u7684keytab\u6587\u4ef6 KDC Address\uff1a \u96c6\u7fa4KDC\u6240\u5728\u670d\u52a1\u5668IP REALM\uff1a HADOOP.COM Kerberos Config File: \u4eceManager\u4e0b\u8f7d\u7684krb5\u914d\u7f6e\u6587\u4ef6 Hadoop\uff1a \u5728\u5de6\u4e0a\u89d2\u641c\u7d22\u6846\u4e2d\u641c\u7d22split\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.input.fileinputformat.split.maxsize\u53c2\u6570 \u641c\u7d22classpath\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.application.classpath\u53c2\u6570 Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive\uff08or libs\uff09Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1a\u6dfb\u52a0spark.driver.extraJavaOptions\u548cspark.executor.extraJavaOptions\u4e24\u4e2a\u53c2\u6570 \u53c2\u6570value\u5728Manager\uff0cServices->Spark2X Configuration->\u6240\u6709\u914d\u7f6e\uff0c\u641c\u7d22extraJavaOptions\uff0c\u9009\u62e9Spark2x->SparkResource2x\u4e2d\u7684\u8fd9\u4e24\u4e2a\u53c2\u6570\u503c\uff0c\u5c06\u5176\u4e2d\u4f7f\u7528\u7684\u6240\u6709\u201c./\u201d\u76f8\u5bf9\u8def\u5f84\u66ff\u6362\u4e3a\u670d\u52a1\u7aefSpark\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4f8b\u5982\u201c/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc\u201d Hive\uff1a Hive Version\uff1aHive Server2 Hive Server Address\uff1aHive \u670d\u52a1\u6240\u5728\u8282\u70b9IP Hive Port\uff1a 21066 Database Name\uff1a \u5728Hive\u4e2d\u521b\u5efa\u7684Radoop Function\u6240\u5728\u7684\u6570\u636e\u5e93\u540d\u79f0 Customer database for UDFs: \u540cDatabase Name \u70b9\u51fbOK->Proced Anyway->Save \u6d4b\u8bd5\u8fde\u63a5 \u00b6 \u70b9\u51fbConfigure,\u5728Global\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eGlobal\u6d4b\u8bd5\u6210\u529f \u5728Hadoop\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHadoop\u6d4b\u8bd5\u6210\u529f \u5728Spark\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eSpark\u6d4b\u8bd5\u6210\u529f \u5728Hive\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHive\u6d4b\u8bd5\u6210\u529f \u5728Manage Radoop Connections \u7a97\u53e3\uff0c\u9009\u4e2d\u6240\u521b\u5efa\u7684\u8fde\u63a5\uff0c\u70b9\u51fbFull test\u8fdb\u884c\u5b8c\u6574\u6d4b\u8bd5\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u5b8c\u6574\u6d4b\u8bd5\u901a\u8fc7 Radoop\u6837\u4f8b\u8fd0\u884c \u00b6 \u5728RapidMiner Studio \u4e3b\u9875\u9762\uff0cHelp->Tutorials->User Hadoop->Rapidminer Radoop \u6839\u636eTutorials\u7684\u6307\u5bfc\u8fd0\u884c\u6837\u4f8b\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a FAQ \u00b6 \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0c\u63d0\u793aICMP port unreachable/Error retrieving Hive object list\u95ee\u9898 \u68c0\u67e5\u96c6\u7fa4\u4e2d\u7aef\u53e3\u7ed1\u5b9a\u7a0b\u5e8f\u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff0c\u7ed1\u5b9a\u7684\u7aef\u53e3\u662f\u5426\u6b63\u786e\u3002RapidMiner\u5728\u6d4b\u8bd5\u65f6\uff0c\u4f1a\u4e0e\u96c6\u7fa4\u768488\u7aef\u53e3\u8fde\u63a5\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight\u5e73\u53f0\u5bf9\u7aef\u53e3\u8fdb\u884c\u4e86\u89c4\u5212\uff0cKerberos\u8ba4\u8bc1\u4f7f\u7528\u7684\u7aef\u53e3\u662f21732\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u63d0\u793aGSS initiate failed \u68c0\u67e5\u672c\u5730host\u6587\u4ef6\u662f\u5426\u6dfb\u52a0\u4e86\u96c6\u7fa4IP\u4e0e\u4e3b\u673a\u540d\u7684\u5bf9\u5e94\u5173\u7cfb\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u5c06\u5404\u79cd\u7248\u672c\u90fd\u6d4b\u8bd5\u4e86\u4e00\u904d\uff0c\u6700\u540e\u63d0\u793aSpark test failed \u68c0\u67e5\u6dfb\u52a0\u7684\u4e24\u4e2aAdvanced Parameters\u662f\u5426\u586b\u5199\u6b63\u786e\uff0c\u5176value\u503c\u4e2d\u7684\u7edd\u5bf9\u8def\u5f84\u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5f53\u96c6\u7fa4\u91cd\u88c5\u540e\u9700\u8981\u4fee\u6539\u8be5\u503c\u3002","title":"\u5bf9\u63a5RapidMiner"},{"location":"Data_Analysis/RapidMiner/#rapidminerfusioninsight","text":"","title":"RapidMiner\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/RapidMiner/#_1","text":"Rapidminer Studio 8.2.001 <-> FusionInsight HD V100R002C80SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/RapidMiner/#_2","text":"\u4e0b\u8f7d\u5b89\u88c5RapidMiner Studio, \u5f53\u524d\u6700\u65b0\u7248\u672c\u4e3a8.2.001,\u4e0b\u8f7d\u5730\u5740 https://rapidminer.com/ \u5b89\u88c5\u5b8c\u6210\u540e\u5728\u4e3b\u754c\u9762\u9876\u90e8\u83dc\u5355\u680f\u9009\u62e9 Extensions->Marketplace ,\u641c\u7d22 radoop ,\u5b89\u88c5\u540e\u91cd\u542frapidminer \u4fee\u6539\u672c\u5730host\u6587\u4ef6\uff0c\u8def\u5f84\u4e3aC:\\Windows\\System32\\drivers\\etc\uff0c\u52a0\u5165\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9IP\u4e0e\u4e3b\u673a\u540d\u5bf9\u5e94\u5173\u7cfb\uff0c\u4fdd\u5b58\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002 \u51c6\u5907FusionInsight\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4ee5\u53cajar\u5305 \u5728\u96c6\u7fa4\u7684Manager\u4e2d\uff0c\u9009\u62e9\u670d\u52a1->\u4e0b\u8f7d\u5ba2\u6237\u7aef->\u5b8c\u6574\u5ba2\u6237\u7aef \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cYarn\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u5939\u91cc\uff0c\u4f8b\u5982\u547d\u540d\u4e3aconfig\u3002 \u6253\u5f00 yarn-site.xml ,\u5220\u9664\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e <property> <name>audit.service.name</name> <value>Yarn</value> </property> \u8fdb\u5165Spark\u7ec4\u4ef6\u7684Jar\u5305\u76ee\u5f55\u201c\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars\u201d\uff0c\u5c06\u6240\u6709jar\u5305\u590d\u5236\u51fa\u6765\uff0c\u4fdd\u5b58\u5728\u672c\u673a\u67d0\u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:/jars \u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Analysis/RapidMiner/#_3","text":"\u914d\u7f6eUDP\u7aef\u53e3\u7ed1\u5b9a \u4e0b\u8f7d\u5b89\u88c5UDP\u7aef\u53e3\u7ed1\u5b9a\u5de5\u5177uredir\uff0c\u4e0b\u8f7d\u5730\u5740 https://github.com/troglobit/uredir \u7f16\u8bd1\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5206\u522b\u4e0a\u4f20\u81f3KDC\u670d\u52a1\u6240\u5728\u7684\u4e3b\u5907\u8282\u70b9(\u53ef\u5728krb5.conf\u6587\u4ef6\u4e2d\u67e5\u770b)\uff0c\u8fdb\u5165uredir\u6267\u884c\u6587\u4ef6\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a,\u5176\u4e2dIP\u4e3a\u6240\u5728\u8282\u70b9IP ./uredir IP:88 IP:21732 \u914d\u7f6eRadoop\u4f9d\u8d56jar\u5305 \u5728Radoop\u6587\u6863\u4e2d\u5fc3\uff0c\u4e0b\u8f7dRadoop\u4f9d\u8d56jar\u5305\uff0c\u4e0b\u8f7d\u5730\u5740 https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,\u4e0b\u8f7d\u4e0e\u5b89\u88c5\u7684RapidMiner\u7248\u672c\u5bf9\u5e94\u7684jar\u5305\u3002 \u5c06jar\u5305\u4e0a\u4f20\u81f3\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u76f8\u540c\u7684\u8def\u5f84\u4e0b\uff0c\u4f8b\u5982/usr/local/lib/radoop/ \u5728\u96c6\u7fa4HiveServer\u6240\u5728\u8282\u70b9\uff0c\u5206\u522b\u4e0a\u4f20Radoop\u7684jar\u5305\u81f3\u4ee5\u4e0b\u8def\u5f84\uff0c\u5e76\u4fee\u6539\u6240\u6709\u8005\u548c\u6267\u884c\u6743\u9650 Hive\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib\"\uff0c Mapreduce\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\uff1a\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib\" cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar * \u5728FusionInsight Manager \u754c\u9762\u6dfb\u52a0Hive\u767d\u540d\u5355\u914d\u7f6e radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name \u9700\u8981\u4ee5 | \u5206\u5272 * \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u91cd\u542fHiveServer \u521b\u5efaRadoop UDF\u51fd\u6570 \u5728\u4e3b\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a #cd /opt/hadoopclient #source bigdata_env #kinit developuser \u8f93\u5165developuser\u7528\u6237\u5bc6\u7801\uff0c\u6267\u884cbeeline\uff0c\u8fdb\u5165Hive Hive\u4e2d\u521b\u5efa\u6570\u636e\u5e93\uff0c\u4f8b\u5982\u521b\u5efa\u6570\u636e\u5e93rapidminer,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a create database rapidminer\uff1b use rapidminer\uff1b DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep';","title":"\u96c6\u7fa4\u914d\u7f6e"},{"location":"Data_Analysis/RapidMiner/#rapidminer","text":"\u5728RapidMiner\u4e2d\uff0c\u83dc\u5355\u9009\u62e9Connections->Manage Radoop Connections \u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u9009\u62e9New Connections->Import Hadoop Configuration Files\uff0c\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939\uff0c\u70b9\u51fbImport Configuration \u5bfc\u5165\u6210\u529f\u540e\u70b9\u51fbNext\uff0c\u8fdb\u5165\u8fde\u63a5\u914d\u7f6e\u7a97\u53e3\uff0c\u6839\u636e\u5de6\u4fa7\u83dc\u5355\u680f\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199\uff1a Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory\uff1aSpark\u7ec4\u4ef6\u7684jars\u5305 Client Principal\uff1a Kerberos\u7528\u6237\u540d@HADOOP.com Keytab File: \u4eceManager\u4e0b\u8f7d\u7684keytab\u6587\u4ef6 KDC Address\uff1a \u96c6\u7fa4KDC\u6240\u5728\u670d\u52a1\u5668IP REALM\uff1a HADOOP.COM Kerberos Config File: \u4eceManager\u4e0b\u8f7d\u7684krb5\u914d\u7f6e\u6587\u4ef6 Hadoop\uff1a \u5728\u5de6\u4e0a\u89d2\u641c\u7d22\u6846\u4e2d\u641c\u7d22split\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.input.fileinputformat.split.maxsize\u53c2\u6570 \u641c\u7d22classpath\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.application.classpath\u53c2\u6570 Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive\uff08or libs\uff09Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1a\u6dfb\u52a0spark.driver.extraJavaOptions\u548cspark.executor.extraJavaOptions\u4e24\u4e2a\u53c2\u6570 \u53c2\u6570value\u5728Manager\uff0cServices->Spark2X Configuration->\u6240\u6709\u914d\u7f6e\uff0c\u641c\u7d22extraJavaOptions\uff0c\u9009\u62e9Spark2x->SparkResource2x\u4e2d\u7684\u8fd9\u4e24\u4e2a\u53c2\u6570\u503c\uff0c\u5c06\u5176\u4e2d\u4f7f\u7528\u7684\u6240\u6709\u201c./\u201d\u76f8\u5bf9\u8def\u5f84\u66ff\u6362\u4e3a\u670d\u52a1\u7aefSpark\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4f8b\u5982\u201c/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc\u201d Hive\uff1a Hive Version\uff1aHive Server2 Hive Server Address\uff1aHive \u670d\u52a1\u6240\u5728\u8282\u70b9IP Hive Port\uff1a 21066 Database Name\uff1a \u5728Hive\u4e2d\u521b\u5efa\u7684Radoop Function\u6240\u5728\u7684\u6570\u636e\u5e93\u540d\u79f0 Customer database for UDFs: \u540cDatabase Name \u70b9\u51fbOK->Proced Anyway->Save","title":"RapidMiner\u914d\u7f6e"},{"location":"Data_Analysis/RapidMiner/#_4","text":"\u70b9\u51fbConfigure,\u5728Global\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eGlobal\u6d4b\u8bd5\u6210\u529f \u5728Hadoop\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHadoop\u6d4b\u8bd5\u6210\u529f \u5728Spark\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eSpark\u6d4b\u8bd5\u6210\u529f \u5728Hive\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHive\u6d4b\u8bd5\u6210\u529f \u5728Manage Radoop Connections \u7a97\u53e3\uff0c\u9009\u4e2d\u6240\u521b\u5efa\u7684\u8fde\u63a5\uff0c\u70b9\u51fbFull test\u8fdb\u884c\u5b8c\u6574\u6d4b\u8bd5\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u5b8c\u6574\u6d4b\u8bd5\u901a\u8fc7","title":"\u6d4b\u8bd5\u8fde\u63a5"},{"location":"Data_Analysis/RapidMiner/#radoop","text":"\u5728RapidMiner Studio \u4e3b\u9875\u9762\uff0cHelp->Tutorials->User Hadoop->Rapidminer Radoop \u6839\u636eTutorials\u7684\u6307\u5bfc\u8fd0\u884c\u6837\u4f8b\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a","title":"Radoop\u6837\u4f8b\u8fd0\u884c"},{"location":"Data_Analysis/RapidMiner/#faq","text":"\u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0c\u63d0\u793aICMP port unreachable/Error retrieving Hive object list\u95ee\u9898 \u68c0\u67e5\u96c6\u7fa4\u4e2d\u7aef\u53e3\u7ed1\u5b9a\u7a0b\u5e8f\u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff0c\u7ed1\u5b9a\u7684\u7aef\u53e3\u662f\u5426\u6b63\u786e\u3002RapidMiner\u5728\u6d4b\u8bd5\u65f6\uff0c\u4f1a\u4e0e\u96c6\u7fa4\u768488\u7aef\u53e3\u8fde\u63a5\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight\u5e73\u53f0\u5bf9\u7aef\u53e3\u8fdb\u884c\u4e86\u89c4\u5212\uff0cKerberos\u8ba4\u8bc1\u4f7f\u7528\u7684\u7aef\u53e3\u662f21732\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u63d0\u793aGSS initiate failed \u68c0\u67e5\u672c\u5730host\u6587\u4ef6\u662f\u5426\u6dfb\u52a0\u4e86\u96c6\u7fa4IP\u4e0e\u4e3b\u673a\u540d\u7684\u5bf9\u5e94\u5173\u7cfb\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u5c06\u5404\u79cd\u7248\u672c\u90fd\u6d4b\u8bd5\u4e86\u4e00\u904d\uff0c\u6700\u540e\u63d0\u793aSpark test failed \u68c0\u67e5\u6dfb\u52a0\u7684\u4e24\u4e2aAdvanced Parameters\u662f\u5426\u586b\u5199\u6b63\u786e\uff0c\u5176value\u503c\u4e2d\u7684\u7edd\u5bf9\u8def\u5f84\u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5f53\u96c6\u7fa4\u91cd\u88c5\u540e\u9700\u8981\u4fee\u6539\u8be5\u503c\u3002","title":"FAQ"},{"location":"Data_Analysis/Splunk/","text":"Splunk\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Splunk7.2.4 \u2194 FusionInsight HD V100R002C80SPC200 Splunk7.2.4 \u2194 FusionInsight HD 6.5.0 \u5b89\u88c5\u4e0e\u542f\u52a8Splunk,\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6 \u00b6 \u5173\u95ed\u4e3b\u673a\u9632\u706b\u5899 systemctl stop firewalld \u5b89\u88c5Splunk7.2.4,\u5728\u7f51\u5740 https://www.splunk.com/en_us/download/splunk-enterprise.html \u4e0b\u8f7dLinux\u5e73\u53f0\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf splunk-7.2.4-8a94541dcfac-Linux-x86_64.tgz \u89e3\u538b\u51fasplunk\u76ee\u5f55\u3002 Splunk \u5bf9\u63a5Hadoop\u96c6\u7fa4\u9700\u8981\u4f7f\u7528Splunk Analytics for Hadoop \u7ec4\u4ef6\uff0c\u8be5\u7ec4\u4ef6\u4e0d\u652f\u6301Windows\u7248\u672c\u7684Splunk Enterprise\uff0c\u9700\u4e0b\u8f7dLinux\u7248\u672cSplunk \u542f\u52a8\u548c\u505c\u6b62splunk,\u8fdb\u5165splunk\u76ee\u5f55\u6267\u884c ./bin/splunk start ./bin/splunk stop \u7b2c\u4e00\u6b21\u542f\u52a8\u4f1a\u663e\u793aLicence Agreement\u9875\u9762\uff0c\u8f93\u5165 y ,\u7136\u540e\u8f93\u5165\u7528\u6237\u540d\u548c\u5bc6\u7801 \u542f\u52a8\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b \u5728\u6d4f\u89c8\u5668\u8f93\u5165 http://ip:8080 \uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801\u5373\u53ef\u8fdb\u5165splunk\u9875\u9762\u3002 - \u5728\u96c6\u7fa4\u670d\u52a1\u7aef\uff0c\u83b7\u53d6mapred\u7528\u6237\u7684keytab\u6587\u4ef6\u4ee5\u53ca\u96c6\u7fa4\u7684krb5.conf\u6587,\u4e0a\u4f20\u81f3splunk\u4e3b\u673a\u4e2d,\u4f8b\u5982 /opt/splunk/ \u76ee\u5f55\u4e0b \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f \u00b6 \u8fdb\u5165splunk\u4e3b\u754c\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u8bbe\u7f6e\uff0c\u9009\u62e9\u865a\u62df\u7d22\u5f15 \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u63d0\u4f9b\u7a0b\u5e8f\u5e8f\u5217\uff1ahadoop Java\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfJAVA_HOME\u7684\u503c Hadoop\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfHADOOP_HOME\u7684\u503c \u586b\u5199Hadoop\u96c6\u7fa4\u4fe1\u606f Hadoop\u7248\u672c\uff1aHadoop2.x(Yarn) \u6587\u4ef6\u7cfb\u7edf\uff1ahdfs://hacluster \u52fe\u9009\u542f\u7528\u901a\u8fc7\u8eab\u4efd\u9a8c\u8bc1 \u8d44\u6e90\u7ba1\u7406\u5668\u5730\u5740:resourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip\u6216\u4e3b\u673a\u540d,\u7aef\u53e3\u4e3a26004,\u5728\u96c6\u7fa4manager\u754c\u9762,\u9009\u62e9\u670d\u52a1\u7ba1\u7406->yarn->resourcemanager\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip,\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\uff0c\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u7aef\u53e3 \u8d44\u6e90\u8ba1\u5212\u7a0b\u5e8f\u5730\u5740:\u8282\u70b9\u540cresourcemanager,\u7aef\u53e3\u53ef\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\u67e5\u770b HDFS\u5de5\u4f5c\u76ee\u5f55\uff1a\u81ea\u884c\u5236\u5b9a \u586b\u5199\u5b89\u5168\u8bbe\u7f6e\u4fe1\u606f \u52fe\u9009\u6dfb\u52a0\u5b89\u5168\u96c6\u7fa4\uff0c\u5b89\u5168\u6a21\u5f0f\u9009\u62e9kerberos Kerberos\u670d\u52a1\u5668\u914d\u7f6e\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u8def\u5f84,\u586b\u5199\u8def\u5f84 kerberos\u4e3b\u4f53\u540d\u79f0:mapred/hadoop. hadoop.com@HADOOP.com kerberos\u5bc6\u94a5\u5373\u4e3akeytab\u6587\u4ef6 HDFS\u4e3b\u4f53:hdfa/hadoop. hadoop.com@HADOOP.com MapreReduce\u4e3b\u4f53\u4e3a:mapred/hadoop. hadoop.com@HADOOP.com \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u4e0e\u8282\u70b9\u7ba1\u7406\u5668\u4e3b\u4f53\u53ef\u4e0d\u586b \u5176\u4ed6\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb\u201c\u4fdd\u5b58\u201d\u3002 \u65b0\u5efa\u865a\u62df\u7d22\u5f15 \u00b6 \u5728\u65b0\u5efa\u7d22\u5f15\u754c\u9762\uff0c\u81ea\u5b9a\u4e49\u7d22\u5f15\u540d\u79f0\uff0c\u63d0\u4f9b\u7a0b\u5e8f\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\uff0cHDFS \u4e2d\u6570\u636e\u7684\u8def\u5f84\u6839\u636e\u9700\u8981\u641c\u7d22\u7684\u8def\u5f84\u8fdb\u884c\u586b\u5199\uff0c\u52fe\u9009\u9012\u5f52\u5904\u7406\u76ee\u5f55\uff0c\u70b9\u51fb\u4fdd\u5b58\u3002 \u4f7f\u7528\u641c\u7d22\u7a0b\u5e8f \u00b6 \u5728splunk\u4e3b\u9875\u9762\uff0c\u70b9\u51fb\u6d4f\u89c8\u6570\u636e \u9009\u62e9\u5df2\u521b\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\u548c\u865a\u62df\u7d22\u5f15 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u9009\u62e9\u8981\u641c\u7d22\u7684\u6587\u4ef6 \u5728\u6570\u636e\u9884\u89c8\u4e2d\uff0c\u9009\u62e9\u6570\u636e\u6765\u6e90\u7c7b\u578b\uff0c\u6839\u636e\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u9009\u62e9 \u5728\u4e0a\u4e0b\u6587\u914d\u7f6e\u4e2d\u9009\u62e9\u5e94\u7528\u7a0b\u5e8f\u7684\u4e0a\u4e0b\u6587\uff0c\u70b9\u51fb\u4e0b\u4e00\u6b65 \u70b9\u51fb\u5b8c\u6210 \u70b9\u51fb\u641c\u7d22\u53ef\u4ee5\u8fdb\u5165\u5bf9\u6b64\u6587\u4ef6\u7684\u641c\u7d22\u9875\u9762 \u53ef\u4ee5\u6839\u636e\u67e5\u8be2\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u53ef\u89c6\u5316\u5c55\u793a \u8bfb\u53d6Hive\u8868 \u00b6 \u901a\u8fc7Splunk\u8bfb\u53d6Hive\u8868\uff0c\u9700\u8981\u5728\u63d0\u4f9b\u7a0b\u5e8f\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u914d\u7f6e\uff1a vix.splunk.search.splitter = HiveSplitGenerator vix.splunk.search.splitter.hive.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe \u5728\u865a\u62df\u7d22\u5f15\u4e2d\u914d\u7f6e\u8981\u641c\u7d22\u7684\u8868\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u6570\u636e\u5e93\u540d\u79f0\uff0c\u8868\u540d\uff0c\u8868\u5934\uff0c\u5b57\u6bb5\u7c7b\u578b\uff0c\u6587\u4ef6\u7c7b\u578b\uff0c\u5206\u9694\u7b26\uff0c\u6362\u884c\u7b26 \u76ee\u524d\u4ec5\u80fd\u6b63\u786e\u8bfb\u53d6rcfile\u683c\u5f0f\u7684\u6587\u4ef6 \u7136\u540e\u5728\u865a\u62df\u7d22\u5f15\u5904\u70b9\u51fb \u641c\u7d22 \uff0c\u8fdb\u5165\u641c\u7d22\u9875\u9762\uff0c\u5e76\u5728\u641c\u7d22\u6846\u524d\u9009\u62e9 \u6240\u6709\u65f6\u95f4 \uff0c\u5373\u53ef\u770b\u5230\u8868\u4e2d\u6570\u636e","title":"\u5bf9\u63a5Splunk"},{"location":"Data_Analysis/Splunk/#splunkfusioninsight-hd","text":"","title":"Splunk\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Analysis/Splunk/#_1","text":"Splunk7.2.4 \u2194 FusionInsight HD V100R002C80SPC200 Splunk7.2.4 \u2194 FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/Splunk/#splunk","text":"\u5173\u95ed\u4e3b\u673a\u9632\u706b\u5899 systemctl stop firewalld \u5b89\u88c5Splunk7.2.4,\u5728\u7f51\u5740 https://www.splunk.com/en_us/download/splunk-enterprise.html \u4e0b\u8f7dLinux\u5e73\u53f0\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf splunk-7.2.4-8a94541dcfac-Linux-x86_64.tgz \u89e3\u538b\u51fasplunk\u76ee\u5f55\u3002 Splunk \u5bf9\u63a5Hadoop\u96c6\u7fa4\u9700\u8981\u4f7f\u7528Splunk Analytics for Hadoop \u7ec4\u4ef6\uff0c\u8be5\u7ec4\u4ef6\u4e0d\u652f\u6301Windows\u7248\u672c\u7684Splunk Enterprise\uff0c\u9700\u4e0b\u8f7dLinux\u7248\u672cSplunk \u542f\u52a8\u548c\u505c\u6b62splunk,\u8fdb\u5165splunk\u76ee\u5f55\u6267\u884c ./bin/splunk start ./bin/splunk stop \u7b2c\u4e00\u6b21\u542f\u52a8\u4f1a\u663e\u793aLicence Agreement\u9875\u9762\uff0c\u8f93\u5165 y ,\u7136\u540e\u8f93\u5165\u7528\u6237\u540d\u548c\u5bc6\u7801 \u542f\u52a8\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b \u5728\u6d4f\u89c8\u5668\u8f93\u5165 http://ip:8080 \uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801\u5373\u53ef\u8fdb\u5165splunk\u9875\u9762\u3002 - \u5728\u96c6\u7fa4\u670d\u52a1\u7aef\uff0c\u83b7\u53d6mapred\u7528\u6237\u7684keytab\u6587\u4ef6\u4ee5\u53ca\u96c6\u7fa4\u7684krb5.conf\u6587,\u4e0a\u4f20\u81f3splunk\u4e3b\u673a\u4e2d,\u4f8b\u5982 /opt/splunk/ \u76ee\u5f55\u4e0b","title":"\u5b89\u88c5\u4e0e\u542f\u52a8Splunk,\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Analysis/Splunk/#_2","text":"\u8fdb\u5165splunk\u4e3b\u754c\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u8bbe\u7f6e\uff0c\u9009\u62e9\u865a\u62df\u7d22\u5f15 \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u63d0\u4f9b\u7a0b\u5e8f\u5e8f\u5217\uff1ahadoop Java\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfJAVA_HOME\u7684\u503c Hadoop\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfHADOOP_HOME\u7684\u503c \u586b\u5199Hadoop\u96c6\u7fa4\u4fe1\u606f Hadoop\u7248\u672c\uff1aHadoop2.x(Yarn) \u6587\u4ef6\u7cfb\u7edf\uff1ahdfs://hacluster \u52fe\u9009\u542f\u7528\u901a\u8fc7\u8eab\u4efd\u9a8c\u8bc1 \u8d44\u6e90\u7ba1\u7406\u5668\u5730\u5740:resourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip\u6216\u4e3b\u673a\u540d,\u7aef\u53e3\u4e3a26004,\u5728\u96c6\u7fa4manager\u754c\u9762,\u9009\u62e9\u670d\u52a1\u7ba1\u7406->yarn->resourcemanager\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip,\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\uff0c\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u7aef\u53e3 \u8d44\u6e90\u8ba1\u5212\u7a0b\u5e8f\u5730\u5740:\u8282\u70b9\u540cresourcemanager,\u7aef\u53e3\u53ef\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\u67e5\u770b HDFS\u5de5\u4f5c\u76ee\u5f55\uff1a\u81ea\u884c\u5236\u5b9a \u586b\u5199\u5b89\u5168\u8bbe\u7f6e\u4fe1\u606f \u52fe\u9009\u6dfb\u52a0\u5b89\u5168\u96c6\u7fa4\uff0c\u5b89\u5168\u6a21\u5f0f\u9009\u62e9kerberos Kerberos\u670d\u52a1\u5668\u914d\u7f6e\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u8def\u5f84,\u586b\u5199\u8def\u5f84 kerberos\u4e3b\u4f53\u540d\u79f0:mapred/hadoop. hadoop.com@HADOOP.com kerberos\u5bc6\u94a5\u5373\u4e3akeytab\u6587\u4ef6 HDFS\u4e3b\u4f53:hdfa/hadoop. hadoop.com@HADOOP.com MapreReduce\u4e3b\u4f53\u4e3a:mapred/hadoop. hadoop.com@HADOOP.com \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u4e0e\u8282\u70b9\u7ba1\u7406\u5668\u4e3b\u4f53\u53ef\u4e0d\u586b \u5176\u4ed6\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb\u201c\u4fdd\u5b58\u201d\u3002","title":"\u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f"},{"location":"Data_Analysis/Splunk/#_3","text":"\u5728\u65b0\u5efa\u7d22\u5f15\u754c\u9762\uff0c\u81ea\u5b9a\u4e49\u7d22\u5f15\u540d\u79f0\uff0c\u63d0\u4f9b\u7a0b\u5e8f\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\uff0cHDFS \u4e2d\u6570\u636e\u7684\u8def\u5f84\u6839\u636e\u9700\u8981\u641c\u7d22\u7684\u8def\u5f84\u8fdb\u884c\u586b\u5199\uff0c\u52fe\u9009\u9012\u5f52\u5904\u7406\u76ee\u5f55\uff0c\u70b9\u51fb\u4fdd\u5b58\u3002","title":"\u65b0\u5efa\u865a\u62df\u7d22\u5f15"},{"location":"Data_Analysis/Splunk/#_4","text":"\u5728splunk\u4e3b\u9875\u9762\uff0c\u70b9\u51fb\u6d4f\u89c8\u6570\u636e \u9009\u62e9\u5df2\u521b\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\u548c\u865a\u62df\u7d22\u5f15 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u9009\u62e9\u8981\u641c\u7d22\u7684\u6587\u4ef6 \u5728\u6570\u636e\u9884\u89c8\u4e2d\uff0c\u9009\u62e9\u6570\u636e\u6765\u6e90\u7c7b\u578b\uff0c\u6839\u636e\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u9009\u62e9 \u5728\u4e0a\u4e0b\u6587\u914d\u7f6e\u4e2d\u9009\u62e9\u5e94\u7528\u7a0b\u5e8f\u7684\u4e0a\u4e0b\u6587\uff0c\u70b9\u51fb\u4e0b\u4e00\u6b65 \u70b9\u51fb\u5b8c\u6210 \u70b9\u51fb\u641c\u7d22\u53ef\u4ee5\u8fdb\u5165\u5bf9\u6b64\u6587\u4ef6\u7684\u641c\u7d22\u9875\u9762 \u53ef\u4ee5\u6839\u636e\u67e5\u8be2\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u53ef\u89c6\u5316\u5c55\u793a","title":"\u4f7f\u7528\u641c\u7d22\u7a0b\u5e8f"},{"location":"Data_Analysis/Splunk/#hive","text":"\u901a\u8fc7Splunk\u8bfb\u53d6Hive\u8868\uff0c\u9700\u8981\u5728\u63d0\u4f9b\u7a0b\u5e8f\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u914d\u7f6e\uff1a vix.splunk.search.splitter = HiveSplitGenerator vix.splunk.search.splitter.hive.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe \u5728\u865a\u62df\u7d22\u5f15\u4e2d\u914d\u7f6e\u8981\u641c\u7d22\u7684\u8868\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u6570\u636e\u5e93\u540d\u79f0\uff0c\u8868\u540d\uff0c\u8868\u5934\uff0c\u5b57\u6bb5\u7c7b\u578b\uff0c\u6587\u4ef6\u7c7b\u578b\uff0c\u5206\u9694\u7b26\uff0c\u6362\u884c\u7b26 \u76ee\u524d\u4ec5\u80fd\u6b63\u786e\u8bfb\u53d6rcfile\u683c\u5f0f\u7684\u6587\u4ef6 \u7136\u540e\u5728\u865a\u62df\u7d22\u5f15\u5904\u70b9\u51fb \u641c\u7d22 \uff0c\u8fdb\u5165\u641c\u7d22\u9875\u9762\uff0c\u5e76\u5728\u641c\u7d22\u6846\u524d\u9009\u62e9 \u6240\u6709\u65f6\u95f4 \uff0c\u5373\u53ef\u770b\u5230\u8868\u4e2d\u6570\u636e","title":"\u8bfb\u53d6Hive\u8868"},{"location":"Data_Integration/","text":"\u6570\u636e\u96c6\u6210 \u00b6 \u5bf9\u63a5IBM InfoSphere DataStage \u5bf9\u63a5Oracle GoldenGate \u5bf9\u63a5Informatica PowerCenter \u5bf9\u63a5Informatica PWX CDC \u5bf9\u63a5Apache NiFi \u5bf9\u63a5Kettle 6.1 \u5bf9\u63a5Talend \u5bf9\u63a5Knime \u5bf9\u63a5Denodo \u5bf9\u63a5Kafka Manager \u5bf9\u63a5Informatica PWX CDC \u5bf9\u63a5Informatica PowerCenter \u5bf9\u63a5TIBCO BusinessWorks","title":"Home"},{"location":"Data_Integration/#_1","text":"\u5bf9\u63a5IBM InfoSphere DataStage \u5bf9\u63a5Oracle GoldenGate \u5bf9\u63a5Informatica PowerCenter \u5bf9\u63a5Informatica PWX CDC \u5bf9\u63a5Apache NiFi \u5bf9\u63a5Kettle 6.1 \u5bf9\u63a5Talend \u5bf9\u63a5Knime \u5bf9\u63a5Denodo \u5bf9\u63a5Kafka Manager \u5bf9\u63a5Informatica PWX CDC \u5bf9\u63a5Informatica PowerCenter \u5bf9\u63a5TIBCO BusinessWorks","title":"\u6570\u636e\u96c6\u6210"},{"location":"Data_Integration/Apache_NiFi/","text":"Apache NiFi\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 \u5b89\u88c5Apache NiFi \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Apache NiFi 1.7.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.7.1-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.7.1\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /usr/nifi/nifi-1.7.1 \u6267\u884c vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Nifi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1) \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210 NiFi\u8fde\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e PutHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3a\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/nifitest/HDFS \u8def\u5f84\u4e0b \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c ListHDFS & FetchHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ListHDFS\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest \u5904\u7406\u5668RouteOnAttribute\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u56fe\u6807\u589e\u52a0\u4e00\u6761\u914d\u7f6e\uff0c Property \u914d\u7f6e\u4e3a requiredfilenames \uff0c Value \u914d\u7f6e\u4e3a ${filename:matches('sanguo.*')} \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} - \u5904\u7406\u5668RouteOnAttribute\u548c\u4e0a\u3001\u4e0b\u5904\u7406\u5668FetchHDFS\u7684\u8fde\u63a5\u914d\u7f6e\u5206\u522b\u5bf9\u5e94\u4e3a requiredfilenames \u548c unmatched \uff0c\u5982\u56fe\uff1a \u4e24\u4e2a\u5904\u7406\u5668FetchHDFS\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService \u4e0a\u3001\u4e0b\u5904\u7406\u5668PutFile\u914d\u7f6e\u5206\u522b\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff0c\u6267\u884c\u547d\u4ee4 hdfs dfs -ls /tmp/nifitest \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf /tmp/nifitest \u67e5\u770b\u6587\u4ef6 \u6d4b\u8bd5\u540e \u767b\u5f55FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS/matchedFiles \u548c /home/dataset/HDFS/unmatchedFiles \u5206\u522b\u67e5\u770b\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true - \u6267\u884c\u547d\u4ee4 cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868t2: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c: PutHiveQL \u5355\u884c\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt \u6570\u636e\u6587\u4ef6 iris_add.txt \u7684\u5185\u5bb9\u5982\u4e0b: \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" \u5904\u7406\u5668SplitText\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668ExtractText\u914d\u7f6e\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b: \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris_add.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a NiFi\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u66f4\u6362\u8def\u5f84 /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210 PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a GetHbase \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5 \u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210 \u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c NiFi\u8fde\u63a5Kafka \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e GetHTTP & PutKafka \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"\u5bf9\u63a5Apache NiFi"},{"location":"Data_Integration/Apache_NiFi/#apache-nififusioninsight","text":"","title":"Apache NiFi\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Apache_NiFi/#_1","text":"Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#apache-nifi","text":"","title":"\u5b89\u88c5Apache NiFi"},{"location":"Data_Integration/Apache_NiFi/#_2","text":"\u5b89\u88c5Apache NiFi 1.7.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_4","text":"\u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.7.1-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.7.1\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /usr/nifi/nifi-1.7.1 \u6267\u884c vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikerberos","text":"","title":"NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/Apache_NiFi/#_5","text":"NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Nifi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1)","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_7","text":"\u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihdfs","text":"","title":"NiFi\u8fde\u63a5HDFS"},{"location":"Data_Integration/Apache_NiFi/#_8","text":"NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_9","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#puthdfs","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3a\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv","title":"PutHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#gethdfs","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/nifitest/HDFS \u8def\u5f84\u4e0b \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c","title":"GetHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#listhdfs-fetchhdfs","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ListHDFS\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest \u5904\u7406\u5668RouteOnAttribute\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u56fe\u6807\u589e\u52a0\u4e00\u6761\u914d\u7f6e\uff0c Property \u914d\u7f6e\u4e3a requiredfilenames \uff0c Value \u914d\u7f6e\u4e3a ${filename:matches('sanguo.*')} \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} - \u5904\u7406\u5668RouteOnAttribute\u548c\u4e0a\u3001\u4e0b\u5904\u7406\u5668FetchHDFS\u7684\u8fde\u63a5\u914d\u7f6e\u5206\u522b\u5bf9\u5e94\u4e3a requiredfilenames \u548c unmatched \uff0c\u5982\u56fe\uff1a \u4e24\u4e2a\u5904\u7406\u5668FetchHDFS\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService \u4e0a\u3001\u4e0b\u5904\u7406\u5668PutFile\u914d\u7f6e\u5206\u522b\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff0c\u6267\u884c\u547d\u4ee4 hdfs dfs -ls /tmp/nifitest \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf /tmp/nifitest \u67e5\u770b\u6587\u4ef6 \u6d4b\u8bd5\u540e \u767b\u5f55FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS/matchedFiles \u548c /home/dataset/HDFS/unmatchedFiles \u5206\u522b\u67e5\u770b\u7ed3\u679c\uff1a","title":"ListHDFS &amp; FetchHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihive","text":"","title":"NiFi\u8fde\u63a5Hive"},{"location":"Data_Integration/Apache_NiFi/#_10","text":"NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_11","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#hiveconnectionpool","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true - \u6267\u884c\u547d\u4ee4 cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar","title":"HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#selecthiveql-hive","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868t2: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a","title":"SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthiveql","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c:","title":"PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthiveql_1","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt \u6570\u636e\u6587\u4ef6 iris_add.txt \u7684\u5185\u5bb9\u5982\u4e0b: \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" \u5904\u7406\u5668SplitText\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668ExtractText\u914d\u7f6e\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b: \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris_add.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"PutHiveQL \u5355\u884c\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihbase","text":"","title":"NiFi\u8fde\u63a5HBase"},{"location":"Data_Integration/Apache_NiFi/#_12","text":"NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_13","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#hbase_1_1_2_clientservice","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u66f4\u6362\u8def\u5f84 /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210","title":"HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthbasejson-hbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a","title":"PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868"},{"location":"Data_Integration/Apache_NiFi/#gethbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a","title":"GetHbase \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifispark","text":"","title":"NiFi\u8fde\u63a5Spark"},{"location":"Data_Integration/Apache_NiFi/#_14","text":"NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_15","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#livysessioncontroller","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210","title":"\u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#spark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#pyspark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#sparkr","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikafka","text":"","title":"NiFi\u8fde\u63a5Kafka"},{"location":"Data_Integration/Apache_NiFi/#_16","text":"NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_17","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#gethttp-putkafka","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning","title":"GetHTTP &amp; PutKafka \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_0_11","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Denodo/","text":"Denodo\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Denodo Platform 7.0 <-> FusionInsight HD V100R002C80SPC100 Denodo Platform 7.0 <-> FusionInsight HD 6.5.0 \u51c6\u5907\u5de5\u4f5c \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Denodo Platform 7.0 Denodo\u662f\u4e00\u4e2a\u6570\u636e\u865a\u62df\u5316\u7cfb\u7edf\uff0c\u5141\u8bb8\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u5e76\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7edf\u4e00\u7684\u8bbf\u95ee\u63a5\u53e3\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u6e90\u5b9e\u65f6\u5730\u8bbf\u95ee\u548c\u96c6\u6210\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u4ece\u6570\u636e\u6e90\u590d\u5236\u6216\u79fb\u52a8\u6570\u636e\u3002\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u5728\u865a\u62df\u5c42\u4e2d\u5b9a\u4e49\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u72ec\u7acb\u4e8e\u5b58\u50a8\u6570\u636e\u7684\u7269\u7406\u6e90\u3002 \u4ece https://community.denodo.com/express/download \u4e0b\u8f7dDenodo Platform 7.0\u7684**Denodo Express Installer**\u548c**Denodo Express License**\u3002\u4e0b\u8f7d\u9009\u62e9\u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672c\u662fWindows 64 bits\u3002 \u4e0b\u8f7d\u5b8c\u6210\u540e\u5b89\u88c5\u4e8e\u672c\u5730 C:\\Denodo\\ \u3002 FusionInsight HD\u76f8\u5173\u914d\u7f6e\uff08\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff09 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88Hive\u548cSpark2x\u7684\u6240\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06**user.keytab**\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c \u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini \uff0c\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55FusionInsight Manager \u4e3b\u673a->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u4e0b\u8f7dFusionInsight HD\u5ba2\u6237\u7aef\u5230\u672c\u5730\u3002 \u5bf9\u63a5Hive\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Hive\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive\\ \uff0c\u5982\u679chive\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5bf9\u63a5Spark2x\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u5982\u679cspark2x\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5982\u679c\u662f**FusionInsight HD 6.5.0**\u7248\u672c\uff0c\u8fd8\u9700\u8981\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \u3002\u5982\u679c\u662fFusionInsight HD V100R002C80SPC100\u7248\u672c\uff0c\u5219\u4e0d\u9700\u8981\u3002 \u51c6\u5907\u6570\u636e Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u521b\u5efa\u4e0estudent.class_id\u76f8\u5173\u7684\u6570\u636e\u5b58\u653e\u4e8eexcel\u8868\u4e2d\u3002\u4f8b\u5982\u521b\u5efa C:\\developuser\\Class.xlsx \uff0csheet\u547d\u540d\u4e3a**Class**\uff0c\u5305\u542b\u4e24\u5217\uff0c\u5206\u522b\u662f**id**\u548c**name**\uff0cid\u5217\u7684\u53d6\u503c\u5fc5\u987b\u5b58\u5728\u4e8estudent.class_id\u4e2d\u3002 JDBC\u8fde\u63a5\u9700\u8981\u67e5\u8be2Zookeeper\uff0cZookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8\u5e76\u914d\u7f6eDenodo \u00b6 \u70b9\u51fb \u5f00\u59cb->Denodo Platform->Denodo Platform 7.0 \u542f\u52a8Denodo Platform Control Center\u3002 \u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server\u3002 \u70b9\u51fb Virtual DataPort->Configure \u3002 \u70b9\u51fb JVM Options \u3002 Virtual DataPort Server\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \uff0c\u4e24\u4e2aOptions\u4e4b\u95f4\u7528\u7a7a\u683c\u9694\u5f00\u3002\u70b9\u51fb Ok \u3002 \u70b9\u51fb Virtual DataPort \u8fd4\u56de\u4e3b\u754c\u9762\uff0c\u70b9\u51fb Start \u542f\u52a8Virtual DataPort Server\u3002 \u542f\u52a8Virtual DataPort Administration Tool\u3002 Virtual DataPort Server\u542f\u52a8\u6210\u529f\u540e\u72b6\u6001\u663e\u793a\u4e3a**Running**\uff0c\u70b9\u51fb LAUNCH \u542f\u52a8Virtual DataPort Administration Tool\u3002 \u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Connect \u767b\u5f55\u3002 \u6210\u529f\u767b\u5f55Virtual DataPort Administration Tool\u3002 \u5bf9\u63a5Hive\u6216\u8005Spark2x \u00b6 \u521b\u5efaJDBC\u8fde\u63a5\u7684Data source \u00b6 \u53f3\u952e admin->Big Data \u9009\u62e9 New->Data source->JDBC \u3002 \u914d\u7f6e\u8fde\u63a5\u4fe1\u606f\uff1a Name\uff1a\u81ea\u547d\u540d\u7684\u65b0\u5efa\u7684Data Source\u540d\u79f0\u3002 Driver class path\uff1aHive\u6216\u8005Spark2x\u7684Jar\u5305\u6240\u5728\u7684\u4f4d\u7f6e\u3002\u5177\u4f53\u914d\u7f6e\u8def\u5f84\u53c2\u8003\u672c\u6587 \u51c6\u5907\u5de5\u4f5c->\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5 \u7ae0\u8282\u3002 Database URI\uff1aHive\u6216\u8005Spark2x\u8fde\u63a5\u7684URL\u3002 Authentication\uff1a\u9009\u62e9Kerberos\u8ba4\u8bc1\u3002 \u5bf9\u63a5Hive\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: hive_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u5bf9\u63a5Spark2x\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: spark2x_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u70b9\u51fb Test connection \uff0c\u8fd4\u56de JDBC connection tested successfully \u3002\u5982\u679c\u8fd4\u56de\u5931\u8d25\uff0c\u53ef\u5728 C:\\Denodo\\DenodoPlatform7.0\\logs\\vdp\\vdp.log \u67e5\u770b\u8be6\u7ec6\u7684\u5931\u8d25\u65e5\u5fd7\u3002\u70b9\u51fb Ok \u5173\u95ed\u6210\u529f\u63d0\u793a\u3002 \u70b9\u51fb Save \u4fdd\u5b58hive_ds\u3002 \u4fdd\u5b58\u6210\u529f\u540e\uff0c\u5de6\u8fb9\u663e\u793a\u7684 admin->Big Data->hive_ds \u5373\u4e3a\u65b0\u589e\u7684Data Source\u3002 \u521b\u5efaHive\u6570\u636e\u6e90 \u00b6 \u4e3a\u4e86\u66f4\u597d\u89c2\u5bdf\uff0c\u53f3\u952eBig Data\u6587\u4ef6\u5939 New->Folder \u65b0\u5efa\u4e09\u4e2a\u6587\u4ef6\u5939\u5206\u522b\u5b58\u5728Data source(01_data source)\u3001base views(02_base views)\u3001\u96c6\u6210\u6570\u636e(03_reports)\uff0c\u5e76\u628a\u5df2\u521b\u5efa\u7684data sources\u79fb\u5165\u6587\u4ef6\u593901_data source\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684Data Source hive_ds \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Create base view \u3002\u9009\u62e9employee\u8868 default->Tables->student \uff0c\u518d\u70b9\u51fb Create selected \u3002 View name\u547d\u540d\u4e3a student \uff0c\u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View student \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56de\u7684student\u8868\u7684\u6570\u636e\u3002 \u521b\u5efaExcel\u8868\u6570\u636e\u6e90 \u00b6 \u53f3\u952e\u6587\u4ef6\u593901_data source\uff0c\u9009\u62e9 New->Data source->Excel \u3002 \u9009\u62e9\u5bfc\u5165\u5df2\u51c6\u5907\u597d\u5b58\u653e\u4e8e C:\\developuser\\ \u7684**Class.xlsx**\u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u5177\u4f53\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Name: class Type of file: \u6839\u636e\u51c6\u5907\u7684Excel\u8868\u7684\u7248\u672c\u9009\u62e9 File location: \u4e0b\u62c9\u6846\u9009\u62e9Local\u540e\uff0c\u518d\u70b9\u51fbConfigure\u9009\u62e9C:\\developuser\\Class.xlsx Worksheets: \u8f93\u5165\u51c6\u5907\u6570\u636e\u5bf9\u5e94\u7684Sheet\u540d\u79f0Class Start cell: \u51c6\u5907\u6570\u636e\u5f00\u59cb\u7684\u5355\u5143\u683c End cell: \u51c6\u5907\u6570\u636e\u7ed3\u675f\u7684\u5355\u5143\u683c Has headers: \u52fe\u9009 Stream tuples: \u52fe\u9009 \u9009\u62e9Data source class \uff0c\u70b9\u51fb Create base view \u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View class \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deExcel\u7684Class\u7684\u6570\u636e\u3002 \u5c06View class \u548c student \u79fb\u5165\u6587\u4ef6\u593902_base views\u3002 \u7ec4\u5408Hive\u548cExcel\u7684\u6570\u636e \u00b6 \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New->Join \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06View name\u8bbe\u7f6e\u4e3a**student_class**\uff0c\u5c06student\u7684class_id\u548cclass\u7684id\u5220\u9664\u3002 \u91cd\u547d\u540dclass\u7684name\u4e3aclass_name\u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u9009\u62e9 student_class \uff0c\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deHive\u548cExcel\u7ec4\u5408\u540e\u7684\u6570\u636e\u3002 \u4f7f\u7528DbVisualizer\u67e5\u770bDenodo Views\u7684\u6570\u636e \u00b6 \u4ece https://www.dbvis.com/download/ \u4e0b\u8f7dDbVisualizer\u5e76\u5b89\u88c5\u4e8e\u672c\u5730\u3002 \u6253\u5f00DbVisualizer\uff0c\u9009\u62e9 Tools->Driver Manager \u3002 \u9009\u62e9 Drive->Create Driver \u3002 \u8f93\u5165\u4ee5\u4e0b\u914d\u7f6e\u4fe1\u606f\u540e\u5173\u95ed\u8be5\u754c\u9762\uff1a Name: Denodo 7.0 URL Format: jdbc:vdb://host:port/database Drive Class: \u70b9\u51fb\u6587\u4ef6\u5939\u5bfc\u5165Denodo\u81ea\u5e26\u7684JDBC jar\u5305\uff0c\u4f8b\u5982C:\\Denodo\\DenodoPlatform7.0\\tools\\client-drivers\\jdbc\\denodo-vdp-jdbcdriver.jar\uff0c\u518d\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9com.denodo.vdp.jdbc.Driver \u8fd4\u56de\u4e3b\u754c\u9762\u540e\uff0c\u9009\u62e9 Database->Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u9009\u62e9 Denodo 7.0 \uff0c\u70b9\u51fb Next \u3002 \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb Finish \u3002 \u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff1a Database URL: jdbc:vdb://localhost:9999/admin Database Userid: admin Database Password: admin \u8fde\u63a5Denodo\u9ed8\u8ba4\u7684admin\u6570\u636e\u5e93\u6210\u529f\u3002 \u53cc\u51fb VIEW->student_class \uff0c\u9009\u62e9 Open Object \u3002 \u70b9\u51fb Data \u67e5\u8be2\u8fd4\u56de\u6570\u636e\u6b63\u786e\u3002 \u767b\u5f55RESTful Web service\u67e5\u770bAssociations \u00b6 \u521b\u5efastudent\u548cclass\u7684Association \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New Association \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u5e76\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06Association name\u8bbe\u7f6e\u4e3a**student_class**\uff0cEnd point 'student'\u4e3a**Principal**\u4e14Role name\u4e3a**class**\uff0cEnd point 'class'\u4e3a**Dependent**\u4e14Role name\u4e3a**belongs_to_student**\u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u4fdd\u5b58\u540e\u53ef\u4ee5\u5728\u6587\u4ef6\u593903_reports\u4e0b\u9762\u770b\u5230Association student_class\u3002 \u767b\u5f55Denodo\u7684RESTful Web service\u67e5\u770bAssociation student_class \u767b\u5f55 http://localhost:9090/denodo-restfulws/admin/ \uff0c\u7528\u6237\u540d\u4e3a admin \uff0c\u5bc6\u7801\u4e3a admin \u3002 \u70b9\u51fb class \uff0c\u8fd4\u56de\u8be5view\u7684\u76f8\u5173\u4fe1\u606f\u3002 \u70b9\u51fb belongs_to_student \uff0c\u8fd4\u56de\u5c5e\u4e8e\u8be5class\u7684\u6240\u6709student\u3002 \u767b\u5f55Data Catalog\u67e5\u770bViews \u00b6 \u5728\u4e3b\u754c\u9762\u70b9\u51fb Denodo Platform Control Center->Virtual DataPort->Start \u542f\u52a8Data Catalog\u670d\u52a1\u3002 \u72b6\u6001\u663e\u793a\u4e3a**Running**\uff0cData Catalog\u670d\u52a1\u542f\u52a8\u6210\u529f\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:9090/denodo-data-catalog \uff0c\u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Sign In \u767b\u5f55\u3002 \u9009\u62e9 Browser->DB/Folders \u3002 \u9009\u62e9 admin->Big Data->02_base views->student->Query \u67e5\u8be2\u89c6\u56festudent\u7684\u6570\u636e\u3002 \u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u8f93\u51fa\u5217\u3002 \u8f93\u5165Name= id \uff0cExpression= id \uff0c\u70b9\u51fb Save \u3002 id\u5217\u6210\u529f\u4fdd\u5b58\u5728Output columns\u3002 \u91c7\u7528\u540c\u6837\u7684\u64cd\u4f5c\uff0c\u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u5176\u4ed6\u8f93\u51fa\u5217\uff0c\u4f8b\u5982**name**\u3002 \u70b9\u51fb Run \u67e5\u8be2\u6210\u529f\u8fd4\u56destudent\u8868\u7684id\u3001name\u3001\u4e24\u5217\u7684\u503c\u3002 FAQ \u00b6 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: KrbException: Cannot locate default realm \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06developuser\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u8bc1krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u540e\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spar2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.lang.SecurityException: class \"com.ctc.wstx.io.SystemId\"'s signer information does not match signer information of other classes in the same package \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u5c06**woodstox-core-5.0.3.jar**\u5305\u62f7\u8d1d\u81f3**Drive class path\u5bf9\u5e94\u7684\u76ee\u5f55**\uff0c\u4f8b\u5982\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spark2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.sql.SQLException: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 uri from ZooKeeper \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5219\u521b\u5efa\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u914d\u7f6eVirtual DataPort Server\u7684JVM Options\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \u3002\u5982\u679c\u6ca1\u6709\uff0c\u5219\u5148\u505c\u6b62Virtual DataPort Server\uff0c\u518d\u5728Virtual DataPort Server\u7684JVM Options\u4e2d\u65b0\u589ejaas.conf\u7684\u914d\u7f6e\u3002\u8be6\u7ec6\u64cd\u4f5c\u53ef\u53c2\u8003\u672c\u6587\u7684 \u542f\u52a8\u5e76\u914d\u7f6eDenodo->\u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server \u7ae0\u8282\u3002 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: Clock skew too great (37) - PREAUTH_FAILED \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u5ba2\u6237\u7aef\u673a\u5668\uff08\u672c\u5730\uff09\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u7684\u65f6\u95f4\u5dee\u662f\u5426\u5c0f\u4e8e5\u5206\u949f\u3002\u5982\u679c\u4e0d\u662f\uff0c\u5efa\u8bae\u4fee\u6539\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4fdd\u6301\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002","title":"\u5bf9\u63a5Denodo"},{"location":"Data_Integration/Denodo/#denodofusioninsight","text":"","title":"Denodo\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Denodo/#_1","text":"Denodo Platform 7.0 <-> FusionInsight HD V100R002C80SPC100 Denodo Platform 7.0 <-> FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Denodo/#_2","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Denodo Platform 7.0 Denodo\u662f\u4e00\u4e2a\u6570\u636e\u865a\u62df\u5316\u7cfb\u7edf\uff0c\u5141\u8bb8\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u5e76\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7edf\u4e00\u7684\u8bbf\u95ee\u63a5\u53e3\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u6e90\u5b9e\u65f6\u5730\u8bbf\u95ee\u548c\u96c6\u6210\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u4ece\u6570\u636e\u6e90\u590d\u5236\u6216\u79fb\u52a8\u6570\u636e\u3002\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u5728\u865a\u62df\u5c42\u4e2d\u5b9a\u4e49\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u72ec\u7acb\u4e8e\u5b58\u50a8\u6570\u636e\u7684\u7269\u7406\u6e90\u3002 \u4ece https://community.denodo.com/express/download \u4e0b\u8f7dDenodo Platform 7.0\u7684**Denodo Express Installer**\u548c**Denodo Express License**\u3002\u4e0b\u8f7d\u9009\u62e9\u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672c\u662fWindows 64 bits\u3002 \u4e0b\u8f7d\u5b8c\u6210\u540e\u5b89\u88c5\u4e8e\u672c\u5730 C:\\Denodo\\ \u3002 FusionInsight HD\u76f8\u5173\u914d\u7f6e\uff08\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff09 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88Hive\u548cSpark2x\u7684\u6240\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06**user.keytab**\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c \u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini \uff0c\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55FusionInsight Manager \u4e3b\u673a->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u4e0b\u8f7dFusionInsight HD\u5ba2\u6237\u7aef\u5230\u672c\u5730\u3002 \u5bf9\u63a5Hive\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Hive\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive\\ \uff0c\u5982\u679chive\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5bf9\u63a5Spark2x\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u5982\u679cspark2x\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5982\u679c\u662f**FusionInsight HD 6.5.0**\u7248\u672c\uff0c\u8fd8\u9700\u8981\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \u3002\u5982\u679c\u662fFusionInsight HD V100R002C80SPC100\u7248\u672c\uff0c\u5219\u4e0d\u9700\u8981\u3002 \u51c6\u5907\u6570\u636e Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u521b\u5efa\u4e0estudent.class_id\u76f8\u5173\u7684\u6570\u636e\u5b58\u653e\u4e8eexcel\u8868\u4e2d\u3002\u4f8b\u5982\u521b\u5efa C:\\developuser\\Class.xlsx \uff0csheet\u547d\u540d\u4e3a**Class**\uff0c\u5305\u542b\u4e24\u5217\uff0c\u5206\u522b\u662f**id**\u548c**name**\uff0cid\u5217\u7684\u53d6\u503c\u5fc5\u987b\u5b58\u5728\u4e8estudent.class_id\u4e2d\u3002 JDBC\u8fde\u63a5\u9700\u8981\u67e5\u8be2Zookeeper\uff0cZookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; };","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Denodo/#denodo","text":"\u70b9\u51fb \u5f00\u59cb->Denodo Platform->Denodo Platform 7.0 \u542f\u52a8Denodo Platform Control Center\u3002 \u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server\u3002 \u70b9\u51fb Virtual DataPort->Configure \u3002 \u70b9\u51fb JVM Options \u3002 Virtual DataPort Server\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \uff0c\u4e24\u4e2aOptions\u4e4b\u95f4\u7528\u7a7a\u683c\u9694\u5f00\u3002\u70b9\u51fb Ok \u3002 \u70b9\u51fb Virtual DataPort \u8fd4\u56de\u4e3b\u754c\u9762\uff0c\u70b9\u51fb Start \u542f\u52a8Virtual DataPort Server\u3002 \u542f\u52a8Virtual DataPort Administration Tool\u3002 Virtual DataPort Server\u542f\u52a8\u6210\u529f\u540e\u72b6\u6001\u663e\u793a\u4e3a**Running**\uff0c\u70b9\u51fb LAUNCH \u542f\u52a8Virtual DataPort Administration Tool\u3002 \u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Connect \u767b\u5f55\u3002 \u6210\u529f\u767b\u5f55Virtual DataPort Administration Tool\u3002","title":"\u542f\u52a8\u5e76\u914d\u7f6eDenodo"},{"location":"Data_Integration/Denodo/#hivespark2x","text":"","title":"\u5bf9\u63a5Hive\u6216\u8005Spark2x"},{"location":"Data_Integration/Denodo/#jdbcdata-source","text":"\u53f3\u952e admin->Big Data \u9009\u62e9 New->Data source->JDBC \u3002 \u914d\u7f6e\u8fde\u63a5\u4fe1\u606f\uff1a Name\uff1a\u81ea\u547d\u540d\u7684\u65b0\u5efa\u7684Data Source\u540d\u79f0\u3002 Driver class path\uff1aHive\u6216\u8005Spark2x\u7684Jar\u5305\u6240\u5728\u7684\u4f4d\u7f6e\u3002\u5177\u4f53\u914d\u7f6e\u8def\u5f84\u53c2\u8003\u672c\u6587 \u51c6\u5907\u5de5\u4f5c->\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5 \u7ae0\u8282\u3002 Database URI\uff1aHive\u6216\u8005Spark2x\u8fde\u63a5\u7684URL\u3002 Authentication\uff1a\u9009\u62e9Kerberos\u8ba4\u8bc1\u3002 \u5bf9\u63a5Hive\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: hive_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u5bf9\u63a5Spark2x\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: spark2x_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u70b9\u51fb Test connection \uff0c\u8fd4\u56de JDBC connection tested successfully \u3002\u5982\u679c\u8fd4\u56de\u5931\u8d25\uff0c\u53ef\u5728 C:\\Denodo\\DenodoPlatform7.0\\logs\\vdp\\vdp.log \u67e5\u770b\u8be6\u7ec6\u7684\u5931\u8d25\u65e5\u5fd7\u3002\u70b9\u51fb Ok \u5173\u95ed\u6210\u529f\u63d0\u793a\u3002 \u70b9\u51fb Save \u4fdd\u5b58hive_ds\u3002 \u4fdd\u5b58\u6210\u529f\u540e\uff0c\u5de6\u8fb9\u663e\u793a\u7684 admin->Big Data->hive_ds \u5373\u4e3a\u65b0\u589e\u7684Data Source\u3002","title":"\u521b\u5efaJDBC\u8fde\u63a5\u7684Data source"},{"location":"Data_Integration/Denodo/#hive","text":"\u4e3a\u4e86\u66f4\u597d\u89c2\u5bdf\uff0c\u53f3\u952eBig Data\u6587\u4ef6\u5939 New->Folder \u65b0\u5efa\u4e09\u4e2a\u6587\u4ef6\u5939\u5206\u522b\u5b58\u5728Data source(01_data source)\u3001base views(02_base views)\u3001\u96c6\u6210\u6570\u636e(03_reports)\uff0c\u5e76\u628a\u5df2\u521b\u5efa\u7684data sources\u79fb\u5165\u6587\u4ef6\u593901_data source\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684Data Source hive_ds \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Create base view \u3002\u9009\u62e9employee\u8868 default->Tables->student \uff0c\u518d\u70b9\u51fb Create selected \u3002 View name\u547d\u540d\u4e3a student \uff0c\u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View student \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56de\u7684student\u8868\u7684\u6570\u636e\u3002","title":"\u521b\u5efaHive\u6570\u636e\u6e90"},{"location":"Data_Integration/Denodo/#excel","text":"\u53f3\u952e\u6587\u4ef6\u593901_data source\uff0c\u9009\u62e9 New->Data source->Excel \u3002 \u9009\u62e9\u5bfc\u5165\u5df2\u51c6\u5907\u597d\u5b58\u653e\u4e8e C:\\developuser\\ \u7684**Class.xlsx**\u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u5177\u4f53\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Name: class Type of file: \u6839\u636e\u51c6\u5907\u7684Excel\u8868\u7684\u7248\u672c\u9009\u62e9 File location: \u4e0b\u62c9\u6846\u9009\u62e9Local\u540e\uff0c\u518d\u70b9\u51fbConfigure\u9009\u62e9C:\\developuser\\Class.xlsx Worksheets: \u8f93\u5165\u51c6\u5907\u6570\u636e\u5bf9\u5e94\u7684Sheet\u540d\u79f0Class Start cell: \u51c6\u5907\u6570\u636e\u5f00\u59cb\u7684\u5355\u5143\u683c End cell: \u51c6\u5907\u6570\u636e\u7ed3\u675f\u7684\u5355\u5143\u683c Has headers: \u52fe\u9009 Stream tuples: \u52fe\u9009 \u9009\u62e9Data source class \uff0c\u70b9\u51fb Create base view \u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View class \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deExcel\u7684Class\u7684\u6570\u636e\u3002 \u5c06View class \u548c student \u79fb\u5165\u6587\u4ef6\u593902_base views\u3002","title":"\u521b\u5efaExcel\u8868\u6570\u636e\u6e90"},{"location":"Data_Integration/Denodo/#hiveexcel","text":"\u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New->Join \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06View name\u8bbe\u7f6e\u4e3a**student_class**\uff0c\u5c06student\u7684class_id\u548cclass\u7684id\u5220\u9664\u3002 \u91cd\u547d\u540dclass\u7684name\u4e3aclass_name\u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u9009\u62e9 student_class \uff0c\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deHive\u548cExcel\u7ec4\u5408\u540e\u7684\u6570\u636e\u3002","title":"\u7ec4\u5408Hive\u548cExcel\u7684\u6570\u636e"},{"location":"Data_Integration/Denodo/#dbvisualizerdenodo-views","text":"\u4ece https://www.dbvis.com/download/ \u4e0b\u8f7dDbVisualizer\u5e76\u5b89\u88c5\u4e8e\u672c\u5730\u3002 \u6253\u5f00DbVisualizer\uff0c\u9009\u62e9 Tools->Driver Manager \u3002 \u9009\u62e9 Drive->Create Driver \u3002 \u8f93\u5165\u4ee5\u4e0b\u914d\u7f6e\u4fe1\u606f\u540e\u5173\u95ed\u8be5\u754c\u9762\uff1a Name: Denodo 7.0 URL Format: jdbc:vdb://host:port/database Drive Class: \u70b9\u51fb\u6587\u4ef6\u5939\u5bfc\u5165Denodo\u81ea\u5e26\u7684JDBC jar\u5305\uff0c\u4f8b\u5982C:\\Denodo\\DenodoPlatform7.0\\tools\\client-drivers\\jdbc\\denodo-vdp-jdbcdriver.jar\uff0c\u518d\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9com.denodo.vdp.jdbc.Driver \u8fd4\u56de\u4e3b\u754c\u9762\u540e\uff0c\u9009\u62e9 Database->Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u9009\u62e9 Denodo 7.0 \uff0c\u70b9\u51fb Next \u3002 \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb Finish \u3002 \u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff1a Database URL: jdbc:vdb://localhost:9999/admin Database Userid: admin Database Password: admin \u8fde\u63a5Denodo\u9ed8\u8ba4\u7684admin\u6570\u636e\u5e93\u6210\u529f\u3002 \u53cc\u51fb VIEW->student_class \uff0c\u9009\u62e9 Open Object \u3002 \u70b9\u51fb Data \u67e5\u8be2\u8fd4\u56de\u6570\u636e\u6b63\u786e\u3002","title":"\u4f7f\u7528DbVisualizer\u67e5\u770bDenodo Views\u7684\u6570\u636e"},{"location":"Data_Integration/Denodo/#restful-web-serviceassociations","text":"\u521b\u5efastudent\u548cclass\u7684Association \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New Association \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u5e76\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06Association name\u8bbe\u7f6e\u4e3a**student_class**\uff0cEnd point 'student'\u4e3a**Principal**\u4e14Role name\u4e3a**class**\uff0cEnd point 'class'\u4e3a**Dependent**\u4e14Role name\u4e3a**belongs_to_student**\u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u4fdd\u5b58\u540e\u53ef\u4ee5\u5728\u6587\u4ef6\u593903_reports\u4e0b\u9762\u770b\u5230Association student_class\u3002 \u767b\u5f55Denodo\u7684RESTful Web service\u67e5\u770bAssociation student_class \u767b\u5f55 http://localhost:9090/denodo-restfulws/admin/ \uff0c\u7528\u6237\u540d\u4e3a admin \uff0c\u5bc6\u7801\u4e3a admin \u3002 \u70b9\u51fb class \uff0c\u8fd4\u56de\u8be5view\u7684\u76f8\u5173\u4fe1\u606f\u3002 \u70b9\u51fb belongs_to_student \uff0c\u8fd4\u56de\u5c5e\u4e8e\u8be5class\u7684\u6240\u6709student\u3002","title":"\u767b\u5f55RESTful Web service\u67e5\u770bAssociations"},{"location":"Data_Integration/Denodo/#data-catalogviews","text":"\u5728\u4e3b\u754c\u9762\u70b9\u51fb Denodo Platform Control Center->Virtual DataPort->Start \u542f\u52a8Data Catalog\u670d\u52a1\u3002 \u72b6\u6001\u663e\u793a\u4e3a**Running**\uff0cData Catalog\u670d\u52a1\u542f\u52a8\u6210\u529f\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:9090/denodo-data-catalog \uff0c\u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Sign In \u767b\u5f55\u3002 \u9009\u62e9 Browser->DB/Folders \u3002 \u9009\u62e9 admin->Big Data->02_base views->student->Query \u67e5\u8be2\u89c6\u56festudent\u7684\u6570\u636e\u3002 \u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u8f93\u51fa\u5217\u3002 \u8f93\u5165Name= id \uff0cExpression= id \uff0c\u70b9\u51fb Save \u3002 id\u5217\u6210\u529f\u4fdd\u5b58\u5728Output columns\u3002 \u91c7\u7528\u540c\u6837\u7684\u64cd\u4f5c\uff0c\u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u5176\u4ed6\u8f93\u51fa\u5217\uff0c\u4f8b\u5982**name**\u3002 \u70b9\u51fb Run \u67e5\u8be2\u6210\u529f\u8fd4\u56destudent\u8868\u7684id\u3001name\u3001\u4e24\u5217\u7684\u503c\u3002","title":"\u767b\u5f55Data Catalog\u67e5\u770bViews"},{"location":"Data_Integration/Denodo/#faq","text":"\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: KrbException: Cannot locate default realm \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06developuser\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u8bc1krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u540e\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spar2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.lang.SecurityException: class \"com.ctc.wstx.io.SystemId\"'s signer information does not match signer information of other classes in the same package \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u5c06**woodstox-core-5.0.3.jar**\u5305\u62f7\u8d1d\u81f3**Drive class path\u5bf9\u5e94\u7684\u76ee\u5f55**\uff0c\u4f8b\u5982\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spark2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.sql.SQLException: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 uri from ZooKeeper \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5219\u521b\u5efa\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u914d\u7f6eVirtual DataPort Server\u7684JVM Options\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \u3002\u5982\u679c\u6ca1\u6709\uff0c\u5219\u5148\u505c\u6b62Virtual DataPort Server\uff0c\u518d\u5728Virtual DataPort Server\u7684JVM Options\u4e2d\u65b0\u589ejaas.conf\u7684\u914d\u7f6e\u3002\u8be6\u7ec6\u64cd\u4f5c\u53ef\u53c2\u8003\u672c\u6587\u7684 \u542f\u52a8\u5e76\u914d\u7f6eDenodo->\u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server \u7ae0\u8282\u3002 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: Clock skew too great (37) - PREAUTH_FAILED \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u5ba2\u6237\u7aef\u673a\u5668\uff08\u672c\u5730\uff09\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u7684\u65f6\u95f4\u5dee\u662f\u5426\u5c0f\u4e8e5\u5206\u949f\u3002\u5982\u679c\u4e0d\u662f\uff0c\u5efa\u8bae\u4fee\u6539\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4fdd\u6301\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002","title":"FAQ"},{"location":"Data_Integration/H2O.ai/","text":"H2O.ai \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 H2O.ai 3.24.0.2 \u2194 FusionInsight HD 6.5 \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7dH2O.ai\u5b89\u88c5\u5305 \u4e0b\u8f7d\u5730\u5740\u4e3a http://h2o-release.s3.amazonaws.com/h2o/rel-yates/2/h2o-3.24.0.2-cdh6.0.zip \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55h2o-3.24.0.2-cdh6.0 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient H2o\u4f7f\u7528 \u00b6 \u542f\u52a8H2O cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 > -nodes \u6307\u5b9aH2o\u96c6\u7fa4\u4e2d\u8282\u70b9\u6570\u91cf > -mapperXmx \u6307\u5b9aH2O\u96c6\u7fa4\u4f7f\u7528\u5185\u5b58\u5927\u5c0f > -network \u6307\u5b9aH2Oweb\u754c\u9762\u8bbf\u95ee\u7684IP\u5730\u5740\u8303\u56f4 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165http://172.16.4.21:54321\uff0c\u5373\u53ef\u8bbf\u95eeH2O \u8fde\u63a5HDFS \u00b6 \u5728H2O\u7684web\u754c\u9762\u4e0a\uff0c\u4f7f\u7528 Import Files \uff0c\u586b\u5165HDFS\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u70b9\u51fb import \u5373\u53ef \u5728\u4e0b\u9762\u53ef\u4ee5\u770b\u5230\u6267\u884c\u7ed3\u679c * \u53ef\u4ee5\u5bf9\u6587\u4ef6\u8fdb\u884c\u4e00\u4e9b\u8f6c\u6362\uff0c\u9884\u5904\u7406 \u8fde\u63a5GaussDB \u00b6 \u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u4e0a\u4f20\u81f3\u8282\u70b9\uff0c\u4f8b\u5982 /opt/h2o-3.24.0.2-cdh6.0 \u76ee\u5f55\u4e0b \u8fde\u63a5GaussDB \u9700\u8981\u52a0\u8f7dJDBC\u9a71\u52a8\u5305\uff0c\u9700\u5728\u542f\u52a8H2O\u96c6\u7fa4\u65f6\u6307\u5b9a,\u4f7f\u7528\u4ee5\u4e0b\u8bed\u53e5\u542f\u52a8H2O\u96c6\u7fa4 cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -libjars gsjdbc4.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 * \u5728H2O\u7684web\u754c\u9762\uff0c\u4f7f\u7528 import SQL Table \uff0c\u586b\u5165\u4ee5\u4e0b\u4fe1\u606f,\u70b9\u51fb import \u70b9\u51fb view Data \uff0c\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"\u5bf9\u63a5H2O.ai"},{"location":"Data_Integration/H2O.ai/#h2oai-fusioninsight","text":"","title":"H2O.ai \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/H2O.ai/#_1","text":"H2O.ai 3.24.0.2 \u2194 FusionInsight HD 6.5","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/H2O.ai/#_2","text":"\u4e0b\u8f7dH2O.ai\u5b89\u88c5\u5305 \u4e0b\u8f7d\u5730\u5740\u4e3a http://h2o-release.s3.amazonaws.com/h2o/rel-yates/2/h2o-3.24.0.2-cdh6.0.zip \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55h2o-3.24.0.2-cdh6.0 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/H2O.ai/#h2o","text":"\u542f\u52a8H2O cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 > -nodes \u6307\u5b9aH2o\u96c6\u7fa4\u4e2d\u8282\u70b9\u6570\u91cf > -mapperXmx \u6307\u5b9aH2O\u96c6\u7fa4\u4f7f\u7528\u5185\u5b58\u5927\u5c0f > -network \u6307\u5b9aH2Oweb\u754c\u9762\u8bbf\u95ee\u7684IP\u5730\u5740\u8303\u56f4 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165http://172.16.4.21:54321\uff0c\u5373\u53ef\u8bbf\u95eeH2O","title":"H2o\u4f7f\u7528"},{"location":"Data_Integration/H2O.ai/#hdfs","text":"\u5728H2O\u7684web\u754c\u9762\u4e0a\uff0c\u4f7f\u7528 Import Files \uff0c\u586b\u5165HDFS\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u70b9\u51fb import \u5373\u53ef \u5728\u4e0b\u9762\u53ef\u4ee5\u770b\u5230\u6267\u884c\u7ed3\u679c * \u53ef\u4ee5\u5bf9\u6587\u4ef6\u8fdb\u884c\u4e00\u4e9b\u8f6c\u6362\uff0c\u9884\u5904\u7406","title":"\u8fde\u63a5HDFS"},{"location":"Data_Integration/H2O.ai/#gaussdb","text":"\u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u4e0a\u4f20\u81f3\u8282\u70b9\uff0c\u4f8b\u5982 /opt/h2o-3.24.0.2-cdh6.0 \u76ee\u5f55\u4e0b \u8fde\u63a5GaussDB \u9700\u8981\u52a0\u8f7dJDBC\u9a71\u52a8\u5305\uff0c\u9700\u5728\u542f\u52a8H2O\u96c6\u7fa4\u65f6\u6307\u5b9a,\u4f7f\u7528\u4ee5\u4e0b\u8bed\u53e5\u542f\u52a8H2O\u96c6\u7fa4 cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -libjars gsjdbc4.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 * \u5728H2O\u7684web\u754c\u9762\uff0c\u4f7f\u7528 import SQL Table \uff0c\u586b\u5165\u4ee5\u4e0b\u4fe1\u606f,\u70b9\u51fb import \u70b9\u51fb view Data \uff0c\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"\u8fde\u63a5GaussDB"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/","text":"IBM InfoSphere DataStage\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210IBM InfoSphere DataStage 11.5.0.2\u7684\u5b89\u88c5\u90e8\u7f72\uff08\u672c\u6587\u90e8\u7f72\u5728Centos7.2\u4e0a\uff09 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD V100R002C60U20 \u51c6\u5907\u5de5\u4f5c \u00b6 \u914d\u7f6e\u57df\u540d\u89e3\u6790 \u00b6 \u4f7f\u7528 vi /etc/hosts \u547d\u4ee4\u4fee\u6539DataStage Server\u548cClient\u7684hosts\u6587\u4ef6\uff0c\u6dfb\u52a0FI\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f\uff0c\u5982\uff1a 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u5728FI\u7ba1\u7406\u754c\u9762\u521b\u5efaDataStage\u5bf9\u63a5\u7528\u6237\uff0c\u5e76\u8d4b\u4e88\u8be5\u7528\u6237\u6240\u9700\u6743\u9650\uff0c\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u4e0b\u8f7d\u7684tar\u6587\u4ef6\uff0c\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u7684keytab\u6587\u4ef6\u3002 \u4ee5root\u767b\u5f55DataStage Server\u8282\u70b9\uff0c\u5c06FI\u96c6\u7fa4\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u5230 /etc \u76ee\u5f55\u3002 \u5c06\u7528\u6237\u7684user.keytab\u6587\u4ef6\u4e0a\u4f20\u5230DataStage Server\u8282\u70b9\u7684\u4efb\u610f\u76ee\u5f55\uff0c\u5982 /home/dsadm \u3002 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u00b6 \u53c2\u8003FI\u4ea7\u54c1\u6587\u6863\uff0c\u5728FI\u670d\u52a1\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230DataStageServer\uff0c\u5b89\u88c5\u81f3\u81ea\u5b9a\u4e49\u76ee\u5f55\uff0c\u5982 /opt/ficlient \u3002 \u5bf9\u63a5HDFS \u00b6 \u5bfc\u5165FI\u96c6\u7fa4\u7684SSL\u8bc1\u4e66 \u00b6 \u6d4f\u89c8\u5668\u5bfc\u51faFI\u96c6\u7fa4\u7684\u6839\u8bc1\u4e66 \u6d4f\u89c8\u5668\u6253\u5f00FI\u7ba1\u7406\u754c\u9762\uff0c\u67e5\u770b\u8bc1\u4e66\uff0c\u70b9\u51fb\u201c\u8bc1\u4e66\u8def\u5f84\u201d\u9875\u7b7e\uff0c\u9009\u62e9\u6839\u8def\u5f84\uff0c\u67e5\u770b\u6839\u8bc1\u4e66\uff0c\u5728\u201c\u8be6\u7ec6\u4fe1\u606f\u201d\u9875\u7b7e\u4e0b\uff0c\u70b9\u51fb\u201c\u590d\u5236\u5230\u6587\u4ef6\u201d\uff0c\u5bfc\u51fa\u4e3acer\u683c\u5f0f \u8bc1\u4e66\u5bfc\u5165DataStage\u7684keystore\u6587\u4ef6 \u5c06\u5bfc\u51fa\u7684FI\u6839\u8bc1\u4e66fi-root-ca.cer\u4e0a\u4f20\u5230DataStage\u670d\u52a1\u7aef\uff0c\u5982 /home/dsadm \u8def\u5f84\u4e0b\uff0c\u5c06\u8bc1\u4e66\u5bfc\u5165\u5230keystore\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003\uff1a /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks \u751f\u6210\u5e76\u4fdd\u5b58\u52a0\u5bc6\u540e\u7684keystore\u5bc6\u7801 \u4f7f\u7528 vi /home/dsadm/authenticate.properties \u547d\u4ee4\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6\uff0c\u4fdd\u5b58\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5bc6\u6587\uff1a password={iisenc}SvtJ2f/uNTrvbuh26XDzag== \u6267\u884c chown dsadm:dstage /home/dsadm/ authenticate.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u7684\u5c5e\u4e3b \u5bfc\u51fatruststore\u73af\u5883\u53d8\u91cf \u4f7f\u7528 vi /opt/IBM/InformationServer/Server/DSEngine/dsenv \u7f16\u8f91DSEngine\u7684\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6700\u540e\u6dfb\u52a0 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties \u91cd\u542fDSEngine\uff0c\u53c2\u8003\u547d\u4ee4 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfs2sf \u6dfb\u52a0File_Connector\u7ec4\u4ef6\u548cSequential File\u7ec4\u4ef6\uff0c\u4ee5\u53caFile_Connector\u5230Sequential File\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u7f16\u8bd1\uff0c\u8fd0\u884c \u5728\u83dc\u5355 Tools -> Run Director \u4e2d\u6253\u5f00Director\u5ba2\u6237\u7aef\uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7 \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u5199\u5165HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfswrite \u6dfb\u52a0Row Generator\u7ec4\u4ef6\u548cFile Connector\u7ec4\u4ef6\uff0c\u4ee5\u53caRow Generator\u5230File Connector\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u5199\u5165\u6570\u636e \u5bf9\u63a5Hive \u00b6 \u4f7f\u7528Hive Connector \u00b6 \u8bf4\u660e\uff1aHive Connector\u5b98\u65b9\u8ba4\u8bc1\u8fc7\u7684Hive JDBC Driver\u53ea\u6709DataDirect Hive Driver(IShive.jar)\uff0c\u7528DataStage 11.5.0.2\u4e2d\u81ea\u5e26\u7684IShive.jar\u8fde\u63a5FusionInsight\u7684hive\u65f6\uff0c\u4f1a\u6709thrift protocol\u62a5\u9519\uff0c\u9700\u8981\u54a8\u8be2IBM\u6280\u672f\u652f\u6301\u63d0\u4f9b\u7684\u6700\u65b0\u7684IShive.jar \u8bbe\u7f6eJDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u5728$DSHOME\u8def\u5f84\u4e0b\u521b\u5efaisjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0DataDirect Hive Driver (IShive.jar)\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.ibm.isf.jdbc.hive.HiveDriver\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u5728isjdbc.config\u4e2d\u6dfb\u52a0\u5982\u4e0b\u4fe1\u606f: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver \u914d\u7f6eKerberos\u8ba4\u8bc1\u4fe1\u606f\uff1a \u5728IShive.jar\u6240\u5728\u76ee\u5f55\u4e0b\u521b\u5efaJDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\u5982\u4e0b\u8fdb\u884c\u914d\u7f6e\uff1a jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; \u5176\u4e2dJDBC_DRIVER_test_keytab\u4e3a\u4e0a\u4e00\u6b65\u6307\u5b9a\u7684\u9274\u6743\u4fe1\u606f \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u6570\u636e\u5199\u5165Hive\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff0c\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62\u201911\u201d \u67e5\u770bHive\u8868\u6570\u636e\uff1a Hive Connector\u5411Hive\u8868\u5199\u6570\u636e\u4f7f\u7528Insert\u8bed\u53e5\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u5c06\u6570\u636e\u76f4\u63a5\u5199\u5165HDFS\u6587\u4ef6\u3002 \u4f7f\u7528JDBC Connector \u00b6 \u5982\u679c\u8981\u4f7f\u7528FusionInsight\u7684Hive JDBC\u9a71\u52a8\uff0c \u7528isjdbc.config\u6587\u4ef6CLASSPATH\u4e2d\u6dfb\u52a0jdbc\u9a71\u52a8\u548c\u4f9d\u8d56\u5305\u7684\u65b9\u5f0f\uff0c\u5728\u8fd0\u884c\u4f5c\u4e1a\u65f6\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff0c\u6b64\u65f6\u9700\u8981\u7528\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u52a0\u8f7d \u800c\u4e14\u53ea\u80fd\u7528JDBC Connector\uff0c\u4e0d\u80fd\u7528Hive Connector\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 Hive jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eHive\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Hive/Beeline/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u8fd9\u4e9bjar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm vi $DSHOME/dsenv \u6587\u4ef6\u6700\u540e\u6dfb\u52a0\u76f8\u5173\u7684jar\u5305\uff08\u5177\u4f53\u8def\u5f84\u6839\u636e\u5b9e\u9645\u73af\u5883\u8c03\u6574\uff09 export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u5176\u4e2dURL\u4e3a\uff1a jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c \u6570\u636e\u5199\u5165Hive\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5199\u51655\u6761\u6570\u636e\uff0c\u7528\u65f61\u201949\u201d \u6570\u636e\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u5199\u5165\u6570\u636e hive\u8868\u6570\u636e\u589e\u91cf100 \u589e\u91cf\u6570\u636e\u5b9a\u671f\u81ea\u52a8\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6 \u00b6 \u589e\u91cf\u6570\u636e\u53ef\u4ee5\u65b0\u589eHDFS\u6587\u4ef6\u7684\u65b9\u5f0f\u5bfc\u5165hive\uff0c\u5982\u679c\u8981\u5b9a\u671f\u81ea\u52a8\u5316\u6267\u884c\uff0c\u5bfc\u5165\u7684\u6587\u4ef6\u540d\u4e2d\u9700\u8981\u5305\u542b\u53ef\u53d8\u53c2\u6570\u8fdb\u884c\u8bbe\u7f6e\u548c\u533a\u5206\uff0c\u7136\u540e\u4ee5\u547d\u4ee4\u6216\u811a\u672c\u65b9\u5f0f\u8fd0\u884c\u4f5c\u4e1a\uff0c\u7ed9\u8be5\u53c2\u6570\u8d4b\u503c\u3002 \u521b\u5efa\u4f5c\u4e1a \u8bbe\u7f6e\u4f5c\u4e1a\u53c2\u6570 \u70b9\u51fb\u201cjob properties\u201d\u6309\u94ae\uff0c\u8bbe\u7f6e\u53c2\u6570\u5982\u4e0b \u4fee\u6539\u914d\u7f6e File Connector\u914d\u7f6e\u5bfc\u51fa\u6587\u4ef6\u7684\u540d\u79f0\uff0c\u4ee5\u201c#\u201d\u5f15\u7528\u8bbe\u7f6e\u7684\u53c2\u6570 dsjob\u547d\u4ee4\u8fd0\u884c\u4f5c\u4e1a \u4fdd\u5b58\u7f16\u8bd1\u4f5c\u4e1a\uff0c\u5728DataStage Server\u4e0a\u6267\u884cdsjob -run\u547d\u4ee4\uff0c\u683c\u5f0f\u4e3a\uff1a dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME \u547d\u4ee4\u53c2\u8003: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append \u67e5\u770bHDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u6570\u636e\u589e\u91cf\u4e3a200\u6761 \u5bf9\u63a5SparkSQL \u00b6 \u4e0e\u4f7f\u7528FI Hive JDBC\u9a71\u52a8\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u7528SparkSQL JDBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u540c\u6837\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 SparkSQL jdbc\u4e0d\u652f\u6301insert into\u8bed\u53e5\uff0c\u53ea\u80fd\u7528\u6765\u8bfbhive\u6570\u636e\uff0c\u4e0d\u80fd\u63d2\u5165\u6570\u636e\u5230hive\u8868\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 SparkSQL jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eSpark\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Spark/spark/lib/ \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caspark\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08SparkSQL jdbc\u8fde\u63a5hive\u65f6\u9700\u8981\u8bfb\u53d6hive-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6Hive\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c \u5bf9\u63a5Phoenix \u00b6 \u4f7f\u7528Phoenix\u4ee5JDBC\u65b9\u5f0f\u8bbf\u95eeHBase\u8868\uff0c\u4e5f\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 Phoenix\u76f8\u5173\u7684jar\u5305\u4f4d\u4e8eHBase\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/HBase/hbase/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caHBase\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08phoenix\u8fde\u63a5\u65f6\u9700\u8981\u8bfb\u53d6hbase-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u521b\u5efajaas\u914d\u7f6e\u6587\u4ef6 \u00b6 Phoenix\u8fde\u63a5\u9700\u8981\u67e5\u8be2zookeeper \uff0czookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6 su - admin vi /home/dsadm/jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u8bfb\u53d6Phoenix\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab \u914d\u7f6eJVM options\u4e3a -Djava.security.auth.login.config=/home/dsadm/jaas.conf \u7f16\u8bd1\u8fd0\u884c \u5199\u5165Phoenix\u8868\u6570\u636e \u00b6 Phoenix\u63d2\u5165\u8bed\u53e5\u662fupsert into\uff0c\u4e0d\u652f\u6301Insert into \u8bed\u53e5\uff0c\u6240\u4ee5\u4e0d\u80fd\u7528JDBC Connector\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210SQL\u8bed\u53e5\uff0c\u9700\u8981\u81ea\u5df1\u586b\u5199\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff1a main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5bf9\u63a5Fiber \u00b6 \u5bf9\u63a5Fiber\u9700\u8981\u5148\u5b89\u88c5FI\u5ba2\u6237\u7aef \u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0Fiber jdbc driver\u53ca\u4f9d\u8d56\u5305\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver \u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\u5982\u4e0b\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver \u4fee\u6539Fiber\u914d\u7f6e\u6587\u4ef6 \u00b6 DataStage\u4f7f\u7528IBM jdk\uff0c\u9700\u8981\u65b0\u5efaFiber\u914d\u7f6e\u6587\u4ef6\u7ed9DataStage\u4f7f\u7528 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml \u4fee\u6539fiber_ibm.xml\u4e2dphoenix,hive,spark\u5404driver\u7684\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570\uff1a java.security.auth.login.config \u4fee\u6539\u4e3a /home/dsadm/jaas.conf zookeeper.kinit \u4fee\u6539\u4e3a /opt/IBM/InformationServer/jdk/jre/bin/kinit \u6587\u4ef6/home/dsadm/jaas.conf\u7684\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u5176\u5b83\u914d\u7f6e\u9879\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863Fiber\u5ba2\u6237\u7aef\u914d\u7f6e\u6307\u5bfc\u4fee\u6539\u3002 \u4f7f\u7528Hive Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Hive Driver\u5199\u5165\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Spark Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Phoenix Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u76ee\u524d\u672a\u80fd\u8bfb\u53d6\u5230\u6570\u636e\uff0c\u201dThe connector could not determine the value for the fetch size.\u201d\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d \u4f7f\u7528Phoenix Driver\u5199\u5165\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u5199\u5165\u6570\u636e0\u884c\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d \u5bf9\u63a5Kafka \u00b6 \u8bf4\u660e\uff1akafka Connector\u4e0d\u652f\u6301\u53d1\u9001\u6216\u8005\u6d88\u8d39integer, float, double, numeric, decimal\u7b49\u6570\u503c\u7c7b\u578b\u7684\u5b57\u6bb5\uff0c\u9700\u8981\u8f6c\u6362\u6210char, varchar, longvarchar\u7b49\u7c7b\u578b\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff1a main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL). \u5b89\u88c5kafka\u5ba2\u6237\u7aef \u00b6 kafka Connector\u9700\u8981\u914d\u7f6eKafka client Classpath\uff0c\u53ef\u4ee5\u5728DataStage\u8282\u70b9\u5b89\u88c5kafka\u5ba2\u6237\u7aef\u6765\u83b7\u53d6kafka-client jar\u5305\u3002\u5b89\u88c5\u6b65\u9aa4\u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u3002 Kafka Client Classpath \u9700\u8981\u63d0\u4f9bkafka-client, log4j, slf4j-api \u4e09\u4e2ajar\u5305\u7684\u8def\u5f84\uff0c\u5982\uff1a /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar \u53d1\u9001\u6d88\u606f\u5230kafka \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e RowGenerator \u751f\u6210\u6570\u636e transformer\u6570\u636e\u7c7b\u578b\u8f6c\u6362\uff1a Kafka\u914d\u7f6e\uff1a \u7f16\u8bd1\u8fd0\u884c \u8bfb\u53d6Kafka\u6d88\u606f \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u5bf9\u63a5MPPDB \u00b6 \u83b7\u53d6MPPDB JDBC Driver \u00b6 \u4eceMPPDB\u53d1\u5e03\u5305\u4e2d\u83b7\u53d6\uff0c\u5305\u540d\u4e3aGauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz \u89e3\u538b\u540e\u5f97\u5230gsjdbc4.jar\uff0c\u4e0a\u4f20\u5230DataStage Server \u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0MPPDB Driver \u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver; \u8bfb\u53d6MPPDB\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u6570\u636e\u5199\u5165MPPDB\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u67e5\u770bMPPDB\u8868\u6570\u636e\uff1a","title":"\u5bf9\u63a5IBM InfoSphere DataStage"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#ibm-infosphere-datastagefusioninsight","text":"","title":"IBM InfoSphere DataStage\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_1","text":"IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_2","text":"\u5df2\u5b8c\u6210IBM InfoSphere DataStage 11.5.0.2\u7684\u5b89\u88c5\u90e8\u7f72\uff08\u672c\u6587\u90e8\u7f72\u5728Centos7.2\u4e0a\uff09 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD V100R002C60U20","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_3","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_4","text":"\u4f7f\u7528 vi /etc/hosts \u547d\u4ee4\u4fee\u6539DataStage Server\u548cClient\u7684hosts\u6587\u4ef6\uff0c\u6dfb\u52a0FI\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f\uff0c\u5982\uff1a 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3","title":"\u914d\u7f6e\u57df\u540d\u89e3\u6790"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kerberos","text":"\u5728FI\u7ba1\u7406\u754c\u9762\u521b\u5efaDataStage\u5bf9\u63a5\u7528\u6237\uff0c\u5e76\u8d4b\u4e88\u8be5\u7528\u6237\u6240\u9700\u6743\u9650\uff0c\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u4e0b\u8f7d\u7684tar\u6587\u4ef6\uff0c\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u7684keytab\u6587\u4ef6\u3002 \u4ee5root\u767b\u5f55DataStage Server\u8282\u70b9\uff0c\u5c06FI\u96c6\u7fa4\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u5230 /etc \u76ee\u5f55\u3002 \u5c06\u7528\u6237\u7684user.keytab\u6587\u4ef6\u4e0a\u4f20\u5230DataStage Server\u8282\u70b9\u7684\u4efb\u610f\u76ee\u5f55\uff0c\u5982 /home/dsadm \u3002","title":"\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fusioninsight","text":"\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863\uff0c\u5728FI\u670d\u52a1\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230DataStageServer\uff0c\u5b89\u88c5\u81f3\u81ea\u5b9a\u4e49\u76ee\u5f55\uff0c\u5982 /opt/ficlient \u3002","title":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fissl","text":"\u6d4f\u89c8\u5668\u5bfc\u51faFI\u96c6\u7fa4\u7684\u6839\u8bc1\u4e66 \u6d4f\u89c8\u5668\u6253\u5f00FI\u7ba1\u7406\u754c\u9762\uff0c\u67e5\u770b\u8bc1\u4e66\uff0c\u70b9\u51fb\u201c\u8bc1\u4e66\u8def\u5f84\u201d\u9875\u7b7e\uff0c\u9009\u62e9\u6839\u8def\u5f84\uff0c\u67e5\u770b\u6839\u8bc1\u4e66\uff0c\u5728\u201c\u8be6\u7ec6\u4fe1\u606f\u201d\u9875\u7b7e\u4e0b\uff0c\u70b9\u51fb\u201c\u590d\u5236\u5230\u6587\u4ef6\u201d\uff0c\u5bfc\u51fa\u4e3acer\u683c\u5f0f \u8bc1\u4e66\u5bfc\u5165DataStage\u7684keystore\u6587\u4ef6 \u5c06\u5bfc\u51fa\u7684FI\u6839\u8bc1\u4e66fi-root-ca.cer\u4e0a\u4f20\u5230DataStage\u670d\u52a1\u7aef\uff0c\u5982 /home/dsadm \u8def\u5f84\u4e0b\uff0c\u5c06\u8bc1\u4e66\u5bfc\u5165\u5230keystore\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003\uff1a /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks \u751f\u6210\u5e76\u4fdd\u5b58\u52a0\u5bc6\u540e\u7684keystore\u5bc6\u7801 \u4f7f\u7528 vi /home/dsadm/authenticate.properties \u547d\u4ee4\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6\uff0c\u4fdd\u5b58\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5bc6\u6587\uff1a password={iisenc}SvtJ2f/uNTrvbuh26XDzag== \u6267\u884c chown dsadm:dstage /home/dsadm/ authenticate.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u7684\u5c5e\u4e3b \u5bfc\u51fatruststore\u73af\u5883\u53d8\u91cf \u4f7f\u7528 vi /opt/IBM/InformationServer/Server/DSEngine/dsenv \u7f16\u8f91DSEngine\u7684\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6700\u540e\u6dfb\u52a0 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties \u91cd\u542fDSEngine\uff0c\u53c2\u8003\u547d\u4ee4 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u5bfc\u5165FI\u96c6\u7fa4\u7684SSL\u8bc1\u4e66"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs_1","text":"\u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfs2sf \u6dfb\u52a0File_Connector\u7ec4\u4ef6\u548cSequential File\u7ec4\u4ef6\uff0c\u4ee5\u53caFile_Connector\u5230Sequential File\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u7f16\u8bd1\uff0c\u8fd0\u884c \u5728\u83dc\u5355 Tools -> Run Director \u4e2d\u6253\u5f00Director\u5ba2\u6237\u7aef\uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7 \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs_2","text":"\u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfswrite \u6dfb\u52a0Row Generator\u7ec4\u4ef6\u548cFile Connector\u7ec4\u4ef6\uff0c\u4ee5\u53caRow Generator\u5230File Connector\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u5199\u5165\u6570\u636e","title":"\u5199\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-connector","text":"\u8bf4\u660e\uff1aHive Connector\u5b98\u65b9\u8ba4\u8bc1\u8fc7\u7684Hive JDBC Driver\u53ea\u6709DataDirect Hive Driver(IShive.jar)\uff0c\u7528DataStage 11.5.0.2\u4e2d\u81ea\u5e26\u7684IShive.jar\u8fde\u63a5FusionInsight\u7684hive\u65f6\uff0c\u4f1a\u6709thrift protocol\u62a5\u9519\uff0c\u9700\u8981\u54a8\u8be2IBM\u6280\u672f\u652f\u6301\u63d0\u4f9b\u7684\u6700\u65b0\u7684IShive.jar","title":"\u4f7f\u7528Hive Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver","text":"\u5728$DSHOME\u8def\u5f84\u4e0b\u521b\u5efaisjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0DataDirect Hive Driver (IShive.jar)\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.ibm.isf.jdbc.hive.HiveDriver\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u5728isjdbc.config\u4e2d\u6dfb\u52a0\u5982\u4e0b\u4fe1\u606f: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver \u914d\u7f6eKerberos\u8ba4\u8bc1\u4fe1\u606f\uff1a \u5728IShive.jar\u6240\u5728\u76ee\u5f55\u4e0b\u521b\u5efaJDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"\u8bbe\u7f6eJDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\u5982\u4e0b\u8fdb\u884c\u914d\u7f6e\uff1a jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; \u5176\u4e2dJDBC_DRIVER_test_keytab\u4e3a\u4e0a\u4e00\u6b65\u6307\u5b9a\u7684\u9274\u6743\u4fe1\u606f \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff0c\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62\u201911\u201d \u67e5\u770bHive\u8868\u6570\u636e\uff1a Hive Connector\u5411Hive\u8868\u5199\u6570\u636e\u4f7f\u7528Insert\u8bed\u53e5\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u5c06\u6570\u636e\u76f4\u63a5\u5199\u5165HDFS\u6587\u4ef6\u3002","title":"\u6570\u636e\u5199\u5165Hive\u8868"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-connector","text":"\u5982\u679c\u8981\u4f7f\u7528FusionInsight\u7684Hive JDBC\u9a71\u52a8\uff0c \u7528isjdbc.config\u6587\u4ef6CLASSPATH\u4e2d\u6dfb\u52a0jdbc\u9a71\u52a8\u548c\u4f9d\u8d56\u5305\u7684\u65b9\u5f0f\uff0c\u5728\u8fd0\u884c\u4f5c\u4e1a\u65f6\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff0c\u6b64\u65f6\u9700\u8981\u7528\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u52a0\u8f7d \u800c\u4e14\u53ea\u80fd\u7528JDBC Connector\uff0c\u4e0d\u80fd\u7528Hive Connector\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519","title":"\u4f7f\u7528JDBC Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath","text":"Hive jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eHive\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Hive/Beeline/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u8fd9\u4e9bjar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm vi $DSHOME/dsenv \u6587\u4ef6\u6700\u540e\u6dfb\u52a0\u76f8\u5173\u7684jar\u5305\uff08\u5177\u4f53\u8def\u5f84\u6839\u636e\u5b9e\u9645\u73af\u5883\u8c03\u6574\uff09 export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_3","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u5176\u4e2dURL\u4e3a\uff1a jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_4","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5199\u51655\u6761\u6570\u636e\uff0c\u7528\u65f61\u201949\u201d","title":"\u6570\u636e\u5199\u5165Hive\u8868"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hivehdfs","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u5199\u5165\u6570\u636e hive\u8868\u6570\u636e\u589e\u91cf100","title":"\u6570\u636e\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hivehdfs_1","text":"\u589e\u91cf\u6570\u636e\u53ef\u4ee5\u65b0\u589eHDFS\u6587\u4ef6\u7684\u65b9\u5f0f\u5bfc\u5165hive\uff0c\u5982\u679c\u8981\u5b9a\u671f\u81ea\u52a8\u5316\u6267\u884c\uff0c\u5bfc\u5165\u7684\u6587\u4ef6\u540d\u4e2d\u9700\u8981\u5305\u542b\u53ef\u53d8\u53c2\u6570\u8fdb\u884c\u8bbe\u7f6e\u548c\u533a\u5206\uff0c\u7136\u540e\u4ee5\u547d\u4ee4\u6216\u811a\u672c\u65b9\u5f0f\u8fd0\u884c\u4f5c\u4e1a\uff0c\u7ed9\u8be5\u53c2\u6570\u8d4b\u503c\u3002 \u521b\u5efa\u4f5c\u4e1a \u8bbe\u7f6e\u4f5c\u4e1a\u53c2\u6570 \u70b9\u51fb\u201cjob properties\u201d\u6309\u94ae\uff0c\u8bbe\u7f6e\u53c2\u6570\u5982\u4e0b \u4fee\u6539\u914d\u7f6e File Connector\u914d\u7f6e\u5bfc\u51fa\u6587\u4ef6\u7684\u540d\u79f0\uff0c\u4ee5\u201c#\u201d\u5f15\u7528\u8bbe\u7f6e\u7684\u53c2\u6570 dsjob\u547d\u4ee4\u8fd0\u884c\u4f5c\u4e1a \u4fdd\u5b58\u7f16\u8bd1\u4f5c\u4e1a\uff0c\u5728DataStage Server\u4e0a\u6267\u884cdsjob -run\u547d\u4ee4\uff0c\u683c\u5f0f\u4e3a\uff1a dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME \u547d\u4ee4\u53c2\u8003: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append \u67e5\u770bHDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u6570\u636e\u589e\u91cf\u4e3a200\u6761","title":"\u589e\u91cf\u6570\u636e\u5b9a\u671f\u81ea\u52a8\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#sparksql","text":"\u4e0e\u4f7f\u7528FI Hive JDBC\u9a71\u52a8\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u7528SparkSQL JDBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u540c\u6837\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 SparkSQL jdbc\u4e0d\u652f\u6301insert into\u8bed\u53e5\uff0c\u53ea\u80fd\u7528\u6765\u8bfbhive\u6570\u636e\uff0c\u4e0d\u80fd\u63d2\u5165\u6570\u636e\u5230hive\u8868\u3002","title":"\u5bf9\u63a5SparkSQL"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath_1","text":"SparkSQL jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eSpark\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Spark/spark/lib/ \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caspark\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08SparkSQL jdbc\u8fde\u63a5hive\u65f6\u9700\u8981\u8bfb\u53d6hive-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_5","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Hive\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix","text":"\u4f7f\u7528Phoenix\u4ee5JDBC\u65b9\u5f0f\u8bbf\u95eeHBase\u8868\uff0c\u4e5f\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002","title":"\u5bf9\u63a5Phoenix"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath_2","text":"Phoenix\u76f8\u5173\u7684jar\u5305\u4f4d\u4e8eHBase\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/HBase/hbase/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caHBase\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08phoenix\u8fde\u63a5\u65f6\u9700\u8981\u8bfb\u53d6hbase-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jaas","text":"Phoenix\u8fde\u63a5\u9700\u8981\u67e5\u8be2zookeeper \uff0czookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6 su - admin vi /home/dsadm/jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"\u521b\u5efajaas\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab \u914d\u7f6eJVM options\u4e3a -Djava.security.auth.login.config=/home/dsadm/jaas.conf \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Phoenix\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix_2","text":"Phoenix\u63d2\u5165\u8bed\u53e5\u662fupsert into\uff0c\u4e0d\u652f\u6301Insert into \u8bed\u53e5\uff0c\u6240\u4ee5\u4e0d\u80fd\u7528JDBC Connector\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210SQL\u8bed\u53e5\uff0c\u9700\u8981\u81ea\u5df1\u586b\u5199\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff1a main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c","title":"\u5199\u5165Phoenix\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fiber","text":"\u5bf9\u63a5Fiber\u9700\u8981\u5148\u5b89\u88c5FI\u5ba2\u6237\u7aef","title":"\u5bf9\u63a5Fiber"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver_1","text":"\u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0Fiber jdbc driver\u53ca\u4f9d\u8d56\u5305\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver \u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\u5982\u4e0b\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver","title":"\u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fiber_1","text":"DataStage\u4f7f\u7528IBM jdk\uff0c\u9700\u8981\u65b0\u5efaFiber\u914d\u7f6e\u6587\u4ef6\u7ed9DataStage\u4f7f\u7528 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml \u4fee\u6539fiber_ibm.xml\u4e2dphoenix,hive,spark\u5404driver\u7684\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570\uff1a java.security.auth.login.config \u4fee\u6539\u4e3a /home/dsadm/jaas.conf zookeeper.kinit \u4fee\u6539\u4e3a /opt/IBM/InformationServer/jdk/jre/bin/kinit \u6587\u4ef6/home/dsadm/jaas.conf\u7684\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u5176\u5b83\u914d\u7f6e\u9879\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863Fiber\u5ba2\u6237\u7aef\u914d\u7f6e\u6307\u5bfc\u4fee\u6539\u3002","title":"\u4fee\u6539Fiber\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Hive Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-driver_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Hive Driver\u5199\u5165\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#spark-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Spark Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u76ee\u524d\u672a\u80fd\u8bfb\u53d6\u5230\u6570\u636e\uff0c\u201dThe connector could not determine the value for the fetch size.\u201d\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d","title":"\u4f7f\u7528Phoenix Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix-driver_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u5199\u5165\u6570\u636e0\u884c\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d","title":"\u4f7f\u7528Phoenix Driver\u5199\u5165\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka","text":"\u8bf4\u660e\uff1akafka Connector\u4e0d\u652f\u6301\u53d1\u9001\u6216\u8005\u6d88\u8d39integer, float, double, numeric, decimal\u7b49\u6570\u503c\u7c7b\u578b\u7684\u5b57\u6bb5\uff0c\u9700\u8981\u8f6c\u6362\u6210char, varchar, longvarchar\u7b49\u7c7b\u578b\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff1a main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL).","title":"\u5bf9\u63a5Kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_1","text":"kafka Connector\u9700\u8981\u914d\u7f6eKafka client Classpath\uff0c\u53ef\u4ee5\u5728DataStage\u8282\u70b9\u5b89\u88c5kafka\u5ba2\u6237\u7aef\u6765\u83b7\u53d6kafka-client jar\u5305\u3002\u5b89\u88c5\u6b65\u9aa4\u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u3002 Kafka Client Classpath \u9700\u8981\u63d0\u4f9bkafka-client, log4j, slf4j-api \u4e09\u4e2ajar\u5305\u7684\u8def\u5f84\uff0c\u5982\uff1a /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar","title":"\u5b89\u88c5kafka\u5ba2\u6237\u7aef"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e RowGenerator \u751f\u6210\u6570\u636e transformer\u6570\u636e\u7c7b\u578b\u8f6c\u6362\uff1a Kafka\u914d\u7f6e\uff1a \u7f16\u8bd1\u8fd0\u884c","title":"\u53d1\u9001\u6d88\u606f\u5230kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_3","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6Kafka\u6d88\u606f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb","text":"","title":"\u5bf9\u63a5MPPDB"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb-jdbc-driver","text":"\u4eceMPPDB\u53d1\u5e03\u5305\u4e2d\u83b7\u53d6\uff0c\u5305\u540d\u4e3aGauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz \u89e3\u538b\u540e\u5f97\u5230gsjdbc4.jar\uff0c\u4e0a\u4f20\u5230DataStage Server","title":"\u83b7\u53d6MPPDB JDBC Driver"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver_2","text":"\u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0MPPDB Driver \u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;","title":"\u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6MPPDB\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u67e5\u770bMPPDB\u8868\u6570\u636e\uff1a","title":"\u6570\u636e\u5199\u5165MPPDB\u8868"},{"location":"Data_Integration/Informatica_PWX_CDC/","text":"Informatica PowerExchange CDC\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD 6.5 \u73af\u5883\u4fe1\u606f \u00b6 Informatica PowerExchange CDC 10.2.0 Linux & Windows\u7248\u672c Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka\u5ba2\u6237\u7aef \u90e8\u7f72\u65b9\u6848 \u00b6 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72oracle\u6570\u636e\u5e93\uff0c\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u540c\u65f6\u90e8\u7f72Informatica PWX CDC\uff0c\u5e76\u542f\u7528listener\u548clogger\u8fdb\u884c\u65e5\u5fd7\u76d1\u542c\uff0c\u518d\u5b89\u88c5PWX Publisher,\u5c06\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e\u4f20\u9001\u5230kafka\u7684topic\u4e2d\u3002 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u5b89\u88c5FusionInsight HD Kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39PWX Publisher\u4f20\u9001\u8fc7\u6765\u7684\u6570\u636e (\u53ef\u9009)\u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5PWX CDC\uff0c\u542f\u7528listener\uff0c\u542f\u52a8navigator\u56fe\u5f62\u5316\u754c\u9762\uff0c\u67e5\u770bPWX\u6355\u83b7\u5230\u7684\u6570\u636e. \u6570\u636e\u5e93\u914d\u7f6e \u00b6 >\u6b64\u90e8\u5206\u914d\u7f6e\u8bf7\u53c2\u8003Informatica PowerExchange CDC\u6307\u5bfc\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html \u5207\u6362\u81f3oracle\u7528\u6237,\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >\u5efa\u8bae\u5728\u4e24\u6b21SHUTDOWN\u64cd\u4f5c\u4e4b\u524d\u5907\u4efd\u6570\u636e\u5e93. \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; \u62f7\u8d1dOracle Catalog \u81f3\u5f52\u6863\u65e5\u5fd7\u4e2d EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u521b\u5efa\u666e\u901a\u7528\u6237C##PWX,\u8d4b\u4e88\u521b\u5efa\u8868\u7684\u6743\u9650\uff0c\u8fde\u63a5\u81f3\u6570\u636e\u5e93 \u521b\u5efa\u6d4b\u8bd5\u8868,\u5411\u8868\u4e2d\u63d2\u5165\u4e00\u4e9b\u6570\u636e. Informatica PWX CDC & PWX Publisher \u5b89\u88c5\u914d\u7f6e \u00b6 \u5728Linux\u4e0a\u5b89\u88c5Informatica PWX CDC \u00b6 \u83b7\u53d6\u5b89\u88c5\u5305 pwx1020_linux_em64t.tar . \u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8bbe\u7f6e\u5b89\u88c5\u8def\u5f84\u5373\u53ef,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/PowerExchange/10.2.0 . \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u6253\u5f00\u914d\u7f6e\u6587\u4ef6 vi ~/.bash_profile \u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * \u6267\u884c source ~/.bash_profile ,source\u73af\u5883\u53d8\u91cf * \u6267\u884c dtlinfo ,\u68c0\u67e5\u5b89\u88c5\u4ee5\u53ca\u914d\u7f6e\u662f\u5426\u6210\u529f \u914d\u7f6edbmover.cfg\u4e0epwxccl.cfg\u6587\u4ef6 \u00b6 \u4fee\u6539PWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6\u5982\u4e0b nodeln \u4e3a\u81ea\u5b9a\u4e49\u7684\u76d1\u542c\u8282\u70b9\u540d ORACLEID\u4e2d\u7684\u7b2c\u4e8c\u4e2aORCL\uff0c\u4e3a\u88ab\u76d1\u542c\u7684\u6570\u636e\u5e93\u540d\u79f0\uff0c\u6b64\u5904\u4e3a\u9ed8\u8ba4\u7684ORCL CAPT_PATH\u6307\u5b9a\u4e86CDC\u7684\u63a7\u5236\u6587\u4ef6\u8def\u5f84\uff0c\u9700\u63d0\u524d\u521b\u5efa\u597d\u76f8\u5e94\u76ee\u5f55 \u6307\u5b9aSVCNODE\u548cCMDNODE\u540d\u79f0 \u4fee\u6539pwxccl.cfg\u6587\u4ef6\u5982\u4e0b CONDENSENAME\u9700\u8981\u548cdbmover.cfg\u6587\u4ef6\u4e2dSVCNODE\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 DBID \u4e3a\u6570\u636e\u5e93\u540d\u79f0 CAPTURE_NODE \u4e3a\u8fdb\u884c\u6355\u83b7\u8282\u70b9\u540d\u79f0 CAPTURE_NODE_UID \u4e3a\u767b\u5f55\u6570\u636e\u5e93\u7684\u7528\u6237\u540d CAPTURE_NODE_PWD \u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 \u542f\u52a8listener\u4ee5\u53calogger PWX CDC \u6355\u83b7ORACLE\u65e5\u5fd7\u6570\u636e \u00b6 ### \u5728Windows\u4e0a\u5b89\u88c5Informatica PWX CDC Windows\u4e0a\u5b89\u88c5Informatica PWX CDC\u4e3b\u8981\u662f\u53ef\u4ee5\u4f7f\u7528Navigator\u754c\u9762,\u67e5\u770b\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e. \u83b7\u53d6\u5b89\u88c5\u5305\u4e4b\u540e\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5,\u4fee\u6539\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH,\u6dfb\u52a0PWX\u5b89\u88c5\u76ee\u5f55. * \u6dfb\u52a0\u73af\u5883\u53d8\u91cfPWX_CONFIG,\u8bbe\u7f6e\u4e3aPWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6 * \u914d\u7f6edbmover.cfg\u6587\u4ef6 - \u914d\u7f6elistener\u540d\u79f0,\u6dfb\u52a0\u670d\u52a1\u7aeflistener\u914d\u7f6e\u4fe1\u606f - \u6307\u5b9a\u76d1\u542c\u6570\u636e\u5e93\u540d\u79f0 - \u8bbe\u7f6e\u63a7\u5236\u6587\u4ef6\u8def\u5f84 * \u542f\u52a8listener * \u4ece\u5f00\u59cb\u83dc\u5355\u680f\u542f\u52a8Navigator * \u5728\u83dc\u5355\u680f\u8d44\u6e90->\u6570\u636e\u6355\u83b7->\u6ce8\u518c\u7ec4\uff0c\u53f3\u952e\u65b0\u5efa\u6ce8\u518c\u7ec4\uff0c\u586b\u5199\u4fe1\u606f\u5982\u4e0b - \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 - \u4f4d\u7f6e\uff1aLinux\u670d\u52a1\u7aef\u76d1\u542c\u8282\u70b9\u540d\u79f0 - \u7c7b\u578b\uff1aORACLE - \u7528\u6237ID\u548c\u5bc6\u7801\uff1a\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 - \u96c6\u5408\u6807\u5fd7\u7b26\uff1a\u6570\u636e\u5e93\u540dORCL \u70b9\u51fb\u4e0b\u4e00\u6b65 \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u67b6\u6784\uff1aschema\u540d\u79f0\uff0c\u5373\u7528\u6237\u540d \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4f1a\u770b\u5230\u521a\u624d\u521b\u5efa\u7684test\u8868\uff0c\u53cc\u51fb\u8868\u540d\uff0c\u88ab\u9009\u5165\u53f3\u4fa7\uff0c\u9009\u62e9\u6240\u6709\u5217 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4fee\u6539\u72b6\u6001\u4e3a \u6d3b\u52a8 \uff0c\u52fe\u9009 \u7acb\u5373\u6267\u884cDDL ,\u70b9\u51fb\u5b8c\u6210 \u5728\u63d0\u53d6\u7ec4\uff0c\u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684orcl12,\u8fdb\u5165\u63d0\u53d6\u7ec4\u754c\u9762\uff0c\u53f3\u952e\uff0c\u6dfb\u52a0\u63d0\u53d6\u81ea\u5b9a\u4e49\uff0c\u586b\u5199\u6620\u5c04\u540d\u79f0\u4ee5\u53ca\u8868\u540d\u79f0 * \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u521b\u5efa\u7684\u6ce8\u518c \u70b9\u51fb\u6dfb\u52a0\uff0c\u5b8c\u6210 \u70b9\u51fb\u83dc\u5355\u680f\u56fe\u8868\uff0c\u6267\u884c\u884c\u6d4b\u8bd5,\u53ef\u770b\u5230\u6355\u83b7\u5230\u7684\u6570\u636e\u5e93\u65e5\u5fd7\u8bb0\u5f55 \u4f7f\u7528PWX CDC publisher\u5bf9\u63a5Kafka \u00b6 ### \u4fee\u6539kafka\u914d\u7f6e\u6587\u4ef6 * \u4fee\u6539producer.properties\u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * \u4fee\u6539jaas.conf\u6587\u4ef6\u5982\u4e0b ![](assets/Informatica_PWX_CDC/14cae.png) \u521b\u5efa\u4e00\u4e2akafka topic, pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### \u5b89\u88c5\u914d\u7f6eInformatica PWX Publisher * \u83b7\u53d6\u5b89\u88c5\u5305 pwxcdcpub120_linux_x64.tar.gz ,\u4ee5root\u7528\u6237\u8eab\u4efd\u89e3\u538b\u81f3\u5b89\u88c5\u76ee\u5f55\u5373\u53ef \u4ee5root\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u5728~/.bash_profile\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0csource\u73af\u5883\u53d8\u91cf,\u8fdb\u884ckerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser \u5c06\u5b89\u88c5\u76ee\u5f55samples\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\u590d\u5236\u5230instanceA/config\u76ee\u5f55\u4e0b\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u5185\u5bb9 > \u914d\u7f6ePWX Publisher\u53ef\u53c2\u8003Informatica \u5b98\u65b9\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html cdcPublisherAvro.cfg\u6587\u4ef6\u914d\u7f6e\u5982\u4e0b - cdcPublisherCommon.cfg\u6587\u4ef6\u4e2d\u6307\u5b9a\u7aef\u53e3 - cdcPublisherKafka.cfg\u6587\u4ef6\u4e2d\u6307\u5b9akafka topic\u540d\u79f0\u4ee5\u53caproperties\u6587\u4ef6\u8def\u5f84 - cdcPowerExchange.cfg\u6587\u4ef6\u4e2d\u914d\u7f6e\u5982\u4e0b * Extract.pwxCapiConnectionName\u4e3a\u5728dbmover.cfg\u4e2dCAPI_CONNECTION\u914d\u7f6e\u7684name * Extract.pwxExtractionMapSchemaName \u4e3apwx \u6355\u83b7\u6620\u5c04\u4e2d\u7684schema\u540d\u79f0\uff0c\u901a\u5e38\u683c\u5f0f\u4e3a unninstance \u6216\u8005 dnninstance \uff0c\u8fd9\u91cc\u4e3a u8orcl * Extract.pwxNodeLocation \u914d\u7f6e\u4e3apwx\u8282\u70b9\u540d\u79f0 * Extract.pwxNodeUserId\uff0cExtract.pwxNodePwd\u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 * Extract.pwxXmapUserId\u4e3a\u8bbf\u95eepwx\u63d0\u53d6\u6620\u5c04\u7684\u7528\u6237\u540d\u5bc6\u7801 * \u4fee\u6539\u5b89\u88c5\u8def\u5f84bin\u76ee\u5f55\u4e0b\u7684PwxCDCPublisher.sh\u542f\u52a8\u811a\u672c\u6587\u4ef6,\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u52a0\u5165\u4e00\u884c RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" * \u542f\u52a8pwx CDC Publisher,\u5728bin\u76ee\u5f55\u4e0b\u6267\u884c sh PwxCDCPublisher.sh \u542f\u52a8kafka consumer\uff0c\u67e5\u770b\u6d88\u8d39\u5230\u7684\u6570\u636e \u00b6 \u5728FusionInsight HD Kafka \u5ba2\u6237\u7aef,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cinsert\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cupdate\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cdelete\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b Q&A \u00b6 1.\u82e5\u542f\u52a8pwxccl\u62a5\u9519\u5982\u4e0b A:\u68c0\u67e5\u5728oracle\u4e2d\u662f\u5426\u6267\u884c\u8fc7 exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u82e5\u6267\u884c\u6210\u529f\uff0c\u4ecd\u7136\u62a5\u9519\uff0c\u7ed9C##PWX\u7528\u6237\u8d4b\u4e88sysdba\u6743\u9650 grant sysdba to C##PWX","title":"\u5bf9\u63a5Informatica PWX CDC"},{"location":"Data_Integration/Informatica_PWX_CDC/#informatica-powerexchange-cdcfusioninsight","text":"","title":"Informatica PowerExchange CDC\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Informatica_PWX_CDC/#_1","text":"Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD 6.5","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PWX_CDC/#_2","text":"Informatica PowerExchange CDC 10.2.0 Linux & Windows\u7248\u672c Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka\u5ba2\u6237\u7aef","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Informatica_PWX_CDC/#_3","text":"\u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72oracle\u6570\u636e\u5e93\uff0c\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u540c\u65f6\u90e8\u7f72Informatica PWX CDC\uff0c\u5e76\u542f\u7528listener\u548clogger\u8fdb\u884c\u65e5\u5fd7\u76d1\u542c\uff0c\u518d\u5b89\u88c5PWX Publisher,\u5c06\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e\u4f20\u9001\u5230kafka\u7684topic\u4e2d\u3002 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u5b89\u88c5FusionInsight HD Kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39PWX Publisher\u4f20\u9001\u8fc7\u6765\u7684\u6570\u636e (\u53ef\u9009)\u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5PWX CDC\uff0c\u542f\u7528listener\uff0c\u542f\u52a8navigator\u56fe\u5f62\u5316\u754c\u9762\uff0c\u67e5\u770bPWX\u6355\u83b7\u5230\u7684\u6570\u636e.","title":"\u90e8\u7f72\u65b9\u6848"},{"location":"Data_Integration/Informatica_PWX_CDC/#_4","text":">\u6b64\u90e8\u5206\u914d\u7f6e\u8bf7\u53c2\u8003Informatica PowerExchange CDC\u6307\u5bfc\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html \u5207\u6362\u81f3oracle\u7528\u6237,\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >\u5efa\u8bae\u5728\u4e24\u6b21SHUTDOWN\u64cd\u4f5c\u4e4b\u524d\u5907\u4efd\u6570\u636e\u5e93. \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; \u62f7\u8d1dOracle Catalog \u81f3\u5f52\u6863\u65e5\u5fd7\u4e2d EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u521b\u5efa\u666e\u901a\u7528\u6237C##PWX,\u8d4b\u4e88\u521b\u5efa\u8868\u7684\u6743\u9650\uff0c\u8fde\u63a5\u81f3\u6570\u636e\u5e93 \u521b\u5efa\u6d4b\u8bd5\u8868,\u5411\u8868\u4e2d\u63d2\u5165\u4e00\u4e9b\u6570\u636e.","title":"\u6570\u636e\u5e93\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PWX_CDC/#informatica-pwx-cdc-pwx-publisher","text":"","title":"Informatica PWX CDC &amp; PWX Publisher \u5b89\u88c5\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PWX_CDC/#linuxinformatica-pwx-cdc","text":"\u83b7\u53d6\u5b89\u88c5\u5305 pwx1020_linux_em64t.tar . \u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8bbe\u7f6e\u5b89\u88c5\u8def\u5f84\u5373\u53ef,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/PowerExchange/10.2.0 .","title":"\u5728Linux\u4e0a\u5b89\u88c5Informatica PWX CDC"},{"location":"Data_Integration/Informatica_PWX_CDC/#_5","text":"\u6253\u5f00\u914d\u7f6e\u6587\u4ef6 vi ~/.bash_profile \u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * \u6267\u884c source ~/.bash_profile ,source\u73af\u5883\u53d8\u91cf * \u6267\u884c dtlinfo ,\u68c0\u67e5\u5b89\u88c5\u4ee5\u53ca\u914d\u7f6e\u662f\u5426\u6210\u529f","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/Informatica_PWX_CDC/#dbmovercfgpwxcclcfg","text":"\u4fee\u6539PWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6\u5982\u4e0b nodeln \u4e3a\u81ea\u5b9a\u4e49\u7684\u76d1\u542c\u8282\u70b9\u540d ORACLEID\u4e2d\u7684\u7b2c\u4e8c\u4e2aORCL\uff0c\u4e3a\u88ab\u76d1\u542c\u7684\u6570\u636e\u5e93\u540d\u79f0\uff0c\u6b64\u5904\u4e3a\u9ed8\u8ba4\u7684ORCL CAPT_PATH\u6307\u5b9a\u4e86CDC\u7684\u63a7\u5236\u6587\u4ef6\u8def\u5f84\uff0c\u9700\u63d0\u524d\u521b\u5efa\u597d\u76f8\u5e94\u76ee\u5f55 \u6307\u5b9aSVCNODE\u548cCMDNODE\u540d\u79f0 \u4fee\u6539pwxccl.cfg\u6587\u4ef6\u5982\u4e0b CONDENSENAME\u9700\u8981\u548cdbmover.cfg\u6587\u4ef6\u4e2dSVCNODE\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 DBID \u4e3a\u6570\u636e\u5e93\u540d\u79f0 CAPTURE_NODE \u4e3a\u8fdb\u884c\u6355\u83b7\u8282\u70b9\u540d\u79f0 CAPTURE_NODE_UID \u4e3a\u767b\u5f55\u6570\u636e\u5e93\u7684\u7528\u6237\u540d CAPTURE_NODE_PWD \u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 \u542f\u52a8listener\u4ee5\u53calogger","title":"\u914d\u7f6edbmover.cfg\u4e0epwxccl.cfg\u6587\u4ef6"},{"location":"Data_Integration/Informatica_PWX_CDC/#pwx-cdc-oracle","text":"### \u5728Windows\u4e0a\u5b89\u88c5Informatica PWX CDC Windows\u4e0a\u5b89\u88c5Informatica PWX CDC\u4e3b\u8981\u662f\u53ef\u4ee5\u4f7f\u7528Navigator\u754c\u9762,\u67e5\u770b\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e. \u83b7\u53d6\u5b89\u88c5\u5305\u4e4b\u540e\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5,\u4fee\u6539\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH,\u6dfb\u52a0PWX\u5b89\u88c5\u76ee\u5f55. * \u6dfb\u52a0\u73af\u5883\u53d8\u91cfPWX_CONFIG,\u8bbe\u7f6e\u4e3aPWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6 * \u914d\u7f6edbmover.cfg\u6587\u4ef6 - \u914d\u7f6elistener\u540d\u79f0,\u6dfb\u52a0\u670d\u52a1\u7aeflistener\u914d\u7f6e\u4fe1\u606f - \u6307\u5b9a\u76d1\u542c\u6570\u636e\u5e93\u540d\u79f0 - \u8bbe\u7f6e\u63a7\u5236\u6587\u4ef6\u8def\u5f84 * \u542f\u52a8listener * \u4ece\u5f00\u59cb\u83dc\u5355\u680f\u542f\u52a8Navigator * \u5728\u83dc\u5355\u680f\u8d44\u6e90->\u6570\u636e\u6355\u83b7->\u6ce8\u518c\u7ec4\uff0c\u53f3\u952e\u65b0\u5efa\u6ce8\u518c\u7ec4\uff0c\u586b\u5199\u4fe1\u606f\u5982\u4e0b - \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 - \u4f4d\u7f6e\uff1aLinux\u670d\u52a1\u7aef\u76d1\u542c\u8282\u70b9\u540d\u79f0 - \u7c7b\u578b\uff1aORACLE - \u7528\u6237ID\u548c\u5bc6\u7801\uff1a\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 - \u96c6\u5408\u6807\u5fd7\u7b26\uff1a\u6570\u636e\u5e93\u540dORCL \u70b9\u51fb\u4e0b\u4e00\u6b65 \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u67b6\u6784\uff1aschema\u540d\u79f0\uff0c\u5373\u7528\u6237\u540d \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4f1a\u770b\u5230\u521a\u624d\u521b\u5efa\u7684test\u8868\uff0c\u53cc\u51fb\u8868\u540d\uff0c\u88ab\u9009\u5165\u53f3\u4fa7\uff0c\u9009\u62e9\u6240\u6709\u5217 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4fee\u6539\u72b6\u6001\u4e3a \u6d3b\u52a8 \uff0c\u52fe\u9009 \u7acb\u5373\u6267\u884cDDL ,\u70b9\u51fb\u5b8c\u6210 \u5728\u63d0\u53d6\u7ec4\uff0c\u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684orcl12,\u8fdb\u5165\u63d0\u53d6\u7ec4\u754c\u9762\uff0c\u53f3\u952e\uff0c\u6dfb\u52a0\u63d0\u53d6\u81ea\u5b9a\u4e49\uff0c\u586b\u5199\u6620\u5c04\u540d\u79f0\u4ee5\u53ca\u8868\u540d\u79f0 * \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u521b\u5efa\u7684\u6ce8\u518c \u70b9\u51fb\u6dfb\u52a0\uff0c\u5b8c\u6210 \u70b9\u51fb\u83dc\u5355\u680f\u56fe\u8868\uff0c\u6267\u884c\u884c\u6d4b\u8bd5,\u53ef\u770b\u5230\u6355\u83b7\u5230\u7684\u6570\u636e\u5e93\u65e5\u5fd7\u8bb0\u5f55","title":"PWX CDC \u6355\u83b7ORACLE\u65e5\u5fd7\u6570\u636e"},{"location":"Data_Integration/Informatica_PWX_CDC/#pwx-cdc-publisherkafka","text":"### \u4fee\u6539kafka\u914d\u7f6e\u6587\u4ef6 * \u4fee\u6539producer.properties\u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * \u4fee\u6539jaas.conf\u6587\u4ef6\u5982\u4e0b ![](assets/Informatica_PWX_CDC/14cae.png) \u521b\u5efa\u4e00\u4e2akafka topic, pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### \u5b89\u88c5\u914d\u7f6eInformatica PWX Publisher * \u83b7\u53d6\u5b89\u88c5\u5305 pwxcdcpub120_linux_x64.tar.gz ,\u4ee5root\u7528\u6237\u8eab\u4efd\u89e3\u538b\u81f3\u5b89\u88c5\u76ee\u5f55\u5373\u53ef \u4ee5root\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u5728~/.bash_profile\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0csource\u73af\u5883\u53d8\u91cf,\u8fdb\u884ckerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser \u5c06\u5b89\u88c5\u76ee\u5f55samples\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\u590d\u5236\u5230instanceA/config\u76ee\u5f55\u4e0b\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u5185\u5bb9 > \u914d\u7f6ePWX Publisher\u53ef\u53c2\u8003Informatica \u5b98\u65b9\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html cdcPublisherAvro.cfg\u6587\u4ef6\u914d\u7f6e\u5982\u4e0b - cdcPublisherCommon.cfg\u6587\u4ef6\u4e2d\u6307\u5b9a\u7aef\u53e3 - cdcPublisherKafka.cfg\u6587\u4ef6\u4e2d\u6307\u5b9akafka topic\u540d\u79f0\u4ee5\u53caproperties\u6587\u4ef6\u8def\u5f84 - cdcPowerExchange.cfg\u6587\u4ef6\u4e2d\u914d\u7f6e\u5982\u4e0b * Extract.pwxCapiConnectionName\u4e3a\u5728dbmover.cfg\u4e2dCAPI_CONNECTION\u914d\u7f6e\u7684name * Extract.pwxExtractionMapSchemaName \u4e3apwx \u6355\u83b7\u6620\u5c04\u4e2d\u7684schema\u540d\u79f0\uff0c\u901a\u5e38\u683c\u5f0f\u4e3a unninstance \u6216\u8005 dnninstance \uff0c\u8fd9\u91cc\u4e3a u8orcl * Extract.pwxNodeLocation \u914d\u7f6e\u4e3apwx\u8282\u70b9\u540d\u79f0 * Extract.pwxNodeUserId\uff0cExtract.pwxNodePwd\u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 * Extract.pwxXmapUserId\u4e3a\u8bbf\u95eepwx\u63d0\u53d6\u6620\u5c04\u7684\u7528\u6237\u540d\u5bc6\u7801 * \u4fee\u6539\u5b89\u88c5\u8def\u5f84bin\u76ee\u5f55\u4e0b\u7684PwxCDCPublisher.sh\u542f\u52a8\u811a\u672c\u6587\u4ef6,\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u52a0\u5165\u4e00\u884c RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" * \u542f\u52a8pwx CDC Publisher,\u5728bin\u76ee\u5f55\u4e0b\u6267\u884c sh PwxCDCPublisher.sh","title":"\u4f7f\u7528PWX CDC publisher\u5bf9\u63a5Kafka"},{"location":"Data_Integration/Informatica_PWX_CDC/#kafka-consumer","text":"\u5728FusionInsight HD Kafka \u5ba2\u6237\u7aef,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cinsert\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cupdate\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cdelete\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b","title":"\u542f\u52a8kafka consumer\uff0c\u67e5\u770b\u6d88\u8d39\u5230\u7684\u6570\u636e"},{"location":"Data_Integration/Informatica_PWX_CDC/#qa","text":"1.\u82e5\u542f\u52a8pwxccl\u62a5\u9519\u5982\u4e0b A:\u68c0\u67e5\u5728oracle\u4e2d\u662f\u5426\u6267\u884c\u8fc7 exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u82e5\u6267\u884c\u6210\u529f\uff0c\u4ecd\u7136\u62a5\u9519\uff0c\u7ed9C##PWX\u7528\u6237\u8d4b\u4e88sysdba\u6743\u9650 grant sysdba to C##PWX","title":"Q&amp;A"},{"location":"Data_Integration/Informatica_PowerCenter/","text":"Informatica PowerCenter \u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 \u73af\u5883\u4fe1\u606f \u00b6 Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD \u5ba2\u6237\u7aef \u90e8\u7f72\u65b9\u6848 \u00b6 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72Informatica Server\uff0c\u5e76\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5Informatica PowerCenter Client \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u00b6 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88HDFS,Hive\u6240\u6709\u6743\u9650\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.confh\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/ \u76ee\u5f55\u4e0b \u5728Linux\u4e0a\u5b89\u88c5Oracle database \u4ee5\u53ca Informatica Server \u00b6 \u521b\u5efaoracle \u7528\u6237\uff0c\u5b89\u88c5oracle \u6570\u636e\u5e93 \u521b\u5efainfa\u7528\u6237\uff0c\u4f7f\u7528 sqlplus / as sysdba \u767b\u5f55\u81f3oracle\u6570\u636e\u5e93\u4e2d\uff0c\u6267\u884c\u4ee5\u4e0bsql\u8bed\u53e5 create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; \u83b7\u53d6Informatica Server\u5b89\u88c5\u5305\u5e76\u4e0a\u4f20\u81f3\u8282\u70b9,\u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u5b89\u88c5,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /home/infa/Informatica/10.2.0 . \u5b89\u88c5\u5b8c\u6210\u540e,Informatica Server\u4f1a\u81ea\u884c\u542f\u52a8\uff0c\u5728\u6d4f\u89c8\u5668\u8f93\u5165ip:6008\u7aef\u53e3\uff0c\u6253\u5f00Administrator \u7ba1\u7406\u754c\u9762\uff0c\u8f93\u5165\u5b89\u88c5\u65f6\u8bbe\u7f6e\u7684\u7528\u6237\u540d\u5bc6\u7801\u8fdb\u884c\u767b\u5f55\u3002 Informatica Server\u914d\u7f6e \u00b6 \u521b\u5efaPowerCenter \u5b58\u50a8\u5e93 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter \u5b58\u50a8\u5e93 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u6570\u636e\u5e93\u4fe1\u606f\uff0c\u5b8c\u6210 - \u70b9\u51fb\u53f3\u4e0a\u89d2\u6309\u94ae\u542f\u7528\u5b58\u50a8\u5e93\uff0c\u5e76\u4e3a\u5b58\u50a8\u5e93\u521b\u5efa\u5185\u5bb9 \u5728\u5b58\u50a8\u5e93\u5c5e\u6027\u4e2d\uff0c\u4fee\u6539\u64cd\u4f5c\u7c7b\u578b\u4e3a\u666e\u901a\uff0c\u5e76\u91cd\u542f\u670d\u52a1 \u521b\u5efaPowerCenter \u6570\u636e\u96c6\u6210\u670d\u52a1 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter\u96c6\u6210\u670d\u52a1 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u5b58\u50a8\u5e93\u4fe1\u606f\uff0c\u70b9\u51fb\u5b8c\u6210\uff0c\u5e76\u542f\u7528\u670d\u52a1 \u5728infa server\u521b\u5efadevelopuser \u5728\u5b89\u5168\u9875\u7b7e\u4e0b\uff0c\u521b\u5efa\u4e00\u4e2a\u7528\u6237\uff0c\u540d\u4e3adevelopuser\uff0c\u4e0eHadoop\u96c6\u7fa4\u7528\u6237\u4fdd\u6301\u4e00\u81f4 \u4fee\u6539\u7528\u6237\u7684\u4f18\u5148\u7ea7\u4ee5\u53ca\u7528\u6237\u7ec4 \u5728infa Server \u8fdb\u884cHadoop\u914d\u7f6e \u5c06 /opt \u76ee\u5f55\u4e0b\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u81f3 /etc \u76ee\u5f55\u4e0b\u4ee5\u53cainformatica\u5b89\u88c5\u76ee\u5f55 ${INFA_HOME}java/jre/lib/security/ \u4e0b\uff0c\u5e76\u8d4b\u4e88infa\u7528\u6237\u6539\u6587\u4ef6\u7684\u8bfb\u53d6\u6743\u9650. \u4ee5infa\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\uff0c\u4f8b\u5982 /opt/pwx-hadoop/conf \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u4ee5\u4e0b\u914d\u7f6e\u6587\u4ef6,\u653e\u81f3 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650\u81f3775 - \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884cKerberos\u8ba4\u8bc1,\u5e76\u6307\u5b9acache\u6587\u4ef6,infa\u7528\u6237\u9700\u8981\u5bf9\u6307\u5b9a\u7684\u8def\u5f84\u6709\u8bfb\u5199\u6743\u9650 source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - \u4fee\u6539 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\u7684 core-site.xml \u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - \u5728Administrator \u7ba1\u7406\u754c\u9762\uff0c\u4e3a\u96c6\u6210\u670d\u52a1\u521b\u5efa\u81ea\u5b9a\u4e49\u53c2\u6570\uff0c\u5e76\u91cd\u542f\u96c6\u6210\u670d\u52a1 \u5220\u9664 /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ \u76ee\u5f55\u4e0bhive\u76f8\u5173\u7684jar\u5305\uff0c\u5e76\u5c06 /opt/hadoopclient/Hive/Beeline/lib \u4e0bhive\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u81f3\u8be5\u76ee\u5f55\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650 rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* PowerCenter Client\u914d\u7f6e \u00b6 PowerCenter Repository Manager\u914d\u7f6e \u00b6 \u83b7\u53d6PowerCenter Client\u5b89\u88c5\u5305\uff0c\u5b89\u88c5\u65f6\u9009\u53d6PowerCenter Client,\u542f\u52a8PowerCenter Repository Manager\uff0c\u9009\u62e9\u83dc\u5355\u680f\u4ed3\u5e93->\u914d\u7f6e\u57df\uff0c\u914d\u7f6e\u5b8c\u6210\u53ef\u4ee5\u770b\u5230\u4e4b\u524d\u521b\u5efa\u7684\u5b58\u50a8\u5e93 \u53cc\u51fb\u5b58\u50a8\u5e93\uff0c\u8f93\u5165\u5bc6\u7801\uff0c\u8fde\u63a5 \u9009\u62e9\u83dc\u5355\u680f\u6587\u4ef6\u5939,\u521b\u5efa\u6587\u4ef6\u5939 PowerCenter Designer\u914d\u7f6e \u00b6 \u6253\u5f00PowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762 - \u70b9\u51fb\u83dc\u5355\u680fSources->import from databases\uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u521b\u5efasitDSN\uff0c\u586b\u5199\u6570\u636e\u5e93\u76f8\u5173\u4fe1\u606f\uff0c\u6d4b\u8bd5\u8fde\u63a5 - \u9009\u62e9\u521a\u624d\u521b\u5efa\u7684\u6570\u636e\u6e90\uff0c\u586b\u5165\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801\uff0c\u8fde\u63a5\uff0c\u53ef\u4ee5\u770b\u5230\u6570\u636e\u5e93\u4e2d\u7684\u8868 - \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868 - \u53cc\u51fb\u8868\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File \u5728mapping\u8bbe\u7f6e\u9875\u9762\uff0c\u521b\u5efa\u65b0\u7684mapping\uff0c\u62d6\u5165source\u548ctarget\u8868\uff0c\u5e76\u8fde\u7ebf \u6253\u5f00PowerCenter Workflow Manager \u00b6 \u5728\u83dc\u5355\u680f\u9009\u62e9task,\u65b0\u5efa\u4e00\u4e2atask,\u547d\u540d\u5e76\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684map \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165\u521a\u624d\u65b0\u5efa\u7684task\uff0c\u5e76\u8fde\u7ebf \u5728\u83dc\u5355\u680fconnection\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2aapplication connection,\u9009\u62e9Hadoop HDFS Connection \u5177\u4f53\u4fe1\u606f\u586b\u5199\u5982\u4e0b HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser \u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684task\uff0c\u5728mapping\u9009\u9879\u5361\uff0c\u70b9\u51fbtarget\uff0c\u8bbe\u7f6e\u5199\u5165\u7c7b\u578b\u4e3a HDFS Flat Write \uff0c\u5e76\u9009\u62e9\u8fde\u63a5\u4e3a\u521a\u624d\u521b\u5efa\u7684connection\uff0c\u5e76\u5728properties\u4e2d\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u4fdd\u5b58\u5f53\u524dworkflow\uff0c\u53f3\u952e\uff0c\u542f\u52a8workflow \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u60c5\u51b5 \u5728HDFS\u4e2d\u53ef\u4ee5\u770b\u5230\u5bfc\u5165\u7684\u6570\u636e \u5728task\u914d\u7f6e\u4e2d\u52fe\u9009\u5199\u5165Hive\u8868\uff0c\u586b\u5165\u4e4b\u524d\u521b\u5efa\u7684\u8868\u540d\uff0c\u8fd0\u884cworkflow \u5728Hive\u4e2d\u53ef\u4ee5\u770b\u5230\u8868\u4e2d\u7684\u6570\u636e","title":"\u5bf9\u63a5Informatica PowerCenter"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-powercenter-fusioninsight-hd","text":"","title":"Informatica PowerCenter \u5bf9\u63a5FusionInsight HD"},{"location":"Data_Integration/Informatica_PowerCenter/#_1","text":"Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PowerCenter/#_2","text":"Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD \u5ba2\u6237\u7aef","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Informatica_PowerCenter/#_3","text":"\u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72Informatica Server\uff0c\u5e76\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5Informatica PowerCenter Client","title":"\u90e8\u7f72\u65b9\u6848"},{"location":"Data_Integration/Informatica_PowerCenter/#_4","text":"","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Informatica_PowerCenter/#fusioninsight-hd","text":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88HDFS,Hive\u6240\u6709\u6743\u9650\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.confh\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/ \u76ee\u5f55\u4e0b","title":"\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef"},{"location":"Data_Integration/Informatica_PowerCenter/#linuxoracle-database-informatica-server","text":"\u521b\u5efaoracle \u7528\u6237\uff0c\u5b89\u88c5oracle \u6570\u636e\u5e93 \u521b\u5efainfa\u7528\u6237\uff0c\u4f7f\u7528 sqlplus / as sysdba \u767b\u5f55\u81f3oracle\u6570\u636e\u5e93\u4e2d\uff0c\u6267\u884c\u4ee5\u4e0bsql\u8bed\u53e5 create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; \u83b7\u53d6Informatica Server\u5b89\u88c5\u5305\u5e76\u4e0a\u4f20\u81f3\u8282\u70b9,\u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u5b89\u88c5,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /home/infa/Informatica/10.2.0 . \u5b89\u88c5\u5b8c\u6210\u540e,Informatica Server\u4f1a\u81ea\u884c\u542f\u52a8\uff0c\u5728\u6d4f\u89c8\u5668\u8f93\u5165ip:6008\u7aef\u53e3\uff0c\u6253\u5f00Administrator \u7ba1\u7406\u754c\u9762\uff0c\u8f93\u5165\u5b89\u88c5\u65f6\u8bbe\u7f6e\u7684\u7528\u6237\u540d\u5bc6\u7801\u8fdb\u884c\u767b\u5f55\u3002","title":"\u5728Linux\u4e0a\u5b89\u88c5Oracle database \u4ee5\u53ca Informatica Server"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-server","text":"\u521b\u5efaPowerCenter \u5b58\u50a8\u5e93 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter \u5b58\u50a8\u5e93 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u6570\u636e\u5e93\u4fe1\u606f\uff0c\u5b8c\u6210 - \u70b9\u51fb\u53f3\u4e0a\u89d2\u6309\u94ae\u542f\u7528\u5b58\u50a8\u5e93\uff0c\u5e76\u4e3a\u5b58\u50a8\u5e93\u521b\u5efa\u5185\u5bb9 \u5728\u5b58\u50a8\u5e93\u5c5e\u6027\u4e2d\uff0c\u4fee\u6539\u64cd\u4f5c\u7c7b\u578b\u4e3a\u666e\u901a\uff0c\u5e76\u91cd\u542f\u670d\u52a1 \u521b\u5efaPowerCenter \u6570\u636e\u96c6\u6210\u670d\u52a1 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter\u96c6\u6210\u670d\u52a1 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u5b58\u50a8\u5e93\u4fe1\u606f\uff0c\u70b9\u51fb\u5b8c\u6210\uff0c\u5e76\u542f\u7528\u670d\u52a1 \u5728infa server\u521b\u5efadevelopuser \u5728\u5b89\u5168\u9875\u7b7e\u4e0b\uff0c\u521b\u5efa\u4e00\u4e2a\u7528\u6237\uff0c\u540d\u4e3adevelopuser\uff0c\u4e0eHadoop\u96c6\u7fa4\u7528\u6237\u4fdd\u6301\u4e00\u81f4 \u4fee\u6539\u7528\u6237\u7684\u4f18\u5148\u7ea7\u4ee5\u53ca\u7528\u6237\u7ec4 \u5728infa Server \u8fdb\u884cHadoop\u914d\u7f6e \u5c06 /opt \u76ee\u5f55\u4e0b\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u81f3 /etc \u76ee\u5f55\u4e0b\u4ee5\u53cainformatica\u5b89\u88c5\u76ee\u5f55 ${INFA_HOME}java/jre/lib/security/ \u4e0b\uff0c\u5e76\u8d4b\u4e88infa\u7528\u6237\u6539\u6587\u4ef6\u7684\u8bfb\u53d6\u6743\u9650. \u4ee5infa\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\uff0c\u4f8b\u5982 /opt/pwx-hadoop/conf \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u4ee5\u4e0b\u914d\u7f6e\u6587\u4ef6,\u653e\u81f3 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650\u81f3775 - \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884cKerberos\u8ba4\u8bc1,\u5e76\u6307\u5b9acache\u6587\u4ef6,infa\u7528\u6237\u9700\u8981\u5bf9\u6307\u5b9a\u7684\u8def\u5f84\u6709\u8bfb\u5199\u6743\u9650 source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - \u4fee\u6539 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\u7684 core-site.xml \u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - \u5728Administrator \u7ba1\u7406\u754c\u9762\uff0c\u4e3a\u96c6\u6210\u670d\u52a1\u521b\u5efa\u81ea\u5b9a\u4e49\u53c2\u6570\uff0c\u5e76\u91cd\u542f\u96c6\u6210\u670d\u52a1 \u5220\u9664 /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ \u76ee\u5f55\u4e0bhive\u76f8\u5173\u7684jar\u5305\uff0c\u5e76\u5c06 /opt/hadoopclient/Hive/Beeline/lib \u4e0bhive\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u81f3\u8be5\u76ee\u5f55\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650 rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive*","title":"Informatica Server\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-client","text":"","title":"PowerCenter Client\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-repository-manager","text":"\u83b7\u53d6PowerCenter Client\u5b89\u88c5\u5305\uff0c\u5b89\u88c5\u65f6\u9009\u53d6PowerCenter Client,\u542f\u52a8PowerCenter Repository Manager\uff0c\u9009\u62e9\u83dc\u5355\u680f\u4ed3\u5e93->\u914d\u7f6e\u57df\uff0c\u914d\u7f6e\u5b8c\u6210\u53ef\u4ee5\u770b\u5230\u4e4b\u524d\u521b\u5efa\u7684\u5b58\u50a8\u5e93 \u53cc\u51fb\u5b58\u50a8\u5e93\uff0c\u8f93\u5165\u5bc6\u7801\uff0c\u8fde\u63a5 \u9009\u62e9\u83dc\u5355\u680f\u6587\u4ef6\u5939,\u521b\u5efa\u6587\u4ef6\u5939","title":"PowerCenter Repository Manager\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-designer","text":"\u6253\u5f00PowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762 - \u70b9\u51fb\u83dc\u5355\u680fSources->import from databases\uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u521b\u5efasitDSN\uff0c\u586b\u5199\u6570\u636e\u5e93\u76f8\u5173\u4fe1\u606f\uff0c\u6d4b\u8bd5\u8fde\u63a5 - \u9009\u62e9\u521a\u624d\u521b\u5efa\u7684\u6570\u636e\u6e90\uff0c\u586b\u5165\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801\uff0c\u8fde\u63a5\uff0c\u53ef\u4ee5\u770b\u5230\u6570\u636e\u5e93\u4e2d\u7684\u8868 - \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868 - \u53cc\u51fb\u8868\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File \u5728mapping\u8bbe\u7f6e\u9875\u9762\uff0c\u521b\u5efa\u65b0\u7684mapping\uff0c\u62d6\u5165source\u548ctarget\u8868\uff0c\u5e76\u8fde\u7ebf","title":"PowerCenter Designer\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-workflow-manager","text":"\u5728\u83dc\u5355\u680f\u9009\u62e9task,\u65b0\u5efa\u4e00\u4e2atask,\u547d\u540d\u5e76\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684map \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165\u521a\u624d\u65b0\u5efa\u7684task\uff0c\u5e76\u8fde\u7ebf \u5728\u83dc\u5355\u680fconnection\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2aapplication connection,\u9009\u62e9Hadoop HDFS Connection \u5177\u4f53\u4fe1\u606f\u586b\u5199\u5982\u4e0b HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser \u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684task\uff0c\u5728mapping\u9009\u9879\u5361\uff0c\u70b9\u51fbtarget\uff0c\u8bbe\u7f6e\u5199\u5165\u7c7b\u578b\u4e3a HDFS Flat Write \uff0c\u5e76\u9009\u62e9\u8fde\u63a5\u4e3a\u521a\u624d\u521b\u5efa\u7684connection\uff0c\u5e76\u5728properties\u4e2d\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u4fdd\u5b58\u5f53\u524dworkflow\uff0c\u53f3\u952e\uff0c\u542f\u52a8workflow \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u60c5\u51b5 \u5728HDFS\u4e2d\u53ef\u4ee5\u770b\u5230\u5bfc\u5165\u7684\u6570\u636e \u5728task\u914d\u7f6e\u4e2d\u52fe\u9009\u5199\u5165Hive\u8868\uff0c\u586b\u5165\u4e4b\u524d\u521b\u5efa\u7684\u8868\u540d\uff0c\u8fd0\u884cworkflow \u5728Hive\u4e2d\u53ef\u4ee5\u770b\u5230\u8868\u4e2d\u7684\u6570\u636e","title":"\u6253\u5f00PowerCenter Workflow Manager"},{"location":"Data_Integration/Kafka_Manager/","text":"Kafka Manager \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 /kafka-manager-1.3.3.21 \u2194 FusionInsight HD V100R002C80SPC200 \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5JDK1.8\u53ca\u4ee5\u4e0a\u7248\u672c \u4e0b\u8f7dkafka-manager\u6e90\u7801 \u4e0b\u8f7d\u5730\u5740\u4e3a https://github.com/yahoo/kafka-manager \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55kafka-manager yum\u5b89\u88c5sbt yum install sbt \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u83b7\u53d6kafka\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6,\u767b\u5f55FusionInsight\u96c6\u7fa4\u8282\u70b9,\u5c06/opt/huawei/Bigdata/om-server_V100R002C80SPC200/apache-tomcat-8.5.28/conf/security/kafka.keytab\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u5e76\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef\u8282\u70b9/opt/hadoopclient/\u76ee\u5f55\u4e0b \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237kafkauser\uff0c\u5e76\u9009\u62e9kafka\u548ckafkaadmin\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.conf\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/hadoopclient/ \u76ee\u5f55\u4e0b kafka-manager\u7f16\u8bd1\u53ca\u914d\u7f6e \u00b6 \u4fee\u6539\u6e90\u7801 \u8fdb\u5165\u5b89\u88c5\u76ee\u5f55/kafka-manager/app/kafka/manager/jmx\uff0c\u4fee\u6539KafkaJMX.scala\u6587\u4ef6\u4e2d\u5199\u6b7b\u7684jmx\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u5c06'jmxrmi'\u4fee\u6539\u4e3a'kafka',\u5982\u4e0b\u56fe\uff1a \u7f16\u8bd1kafka-manager,\u83b7\u53d6\u538b\u7f29\u5305 cd /opt/kafka-manager ./sbt clean dist cp /opt/kafka-manager/target/universal/kafka-manager-1.3.3.21.zip /opt cd /opt uzip kafka-manager-1.3.3.21.zip cd /opt/kafka-manager-1.3.3.21 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6conf/application.conf,'kafka-manager.zkhosts'\u4fee\u6539\u4e3azookeeper\u96c6\u7fa4\u8282\u70b9IP:\u7aef\u53e3,FI\u96c6\u7fa4\u9ed8\u8ba4\u7aef\u53e3\u4e3a24002 kafka-manager.zkhosts=\"172.21.3.115:24002\" \u65b0\u5efaconf/jaas.conf\u6587\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; \u5c06kafka-manager\u7684lib\u5e93\u4e2dzookeeper\u7684jar\u5305\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e2dzookeeper\u7684jar\u5305,\u5e76\u91cd\u547d\u540d cp /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar /opt/kafka-manager-1.3.3.21/lib cd /opt/kafka-manager-1.3.3.21/lib rm org.apache.zookeeper.zookeeper-3.4.10.jar mv zookeeper-3.5.1.jar org.apache.zookeeper.zookeeper-3.4.10.jar kafka-manager\u4f7f\u7528 \u00b6 \u542f\u52a8kafka-manager cd /opt/kafka-manager-1.3.3.21 nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Djava.security.auth.login.config=conf/jaas.conf -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com & > \u53ef\u901a\u8fc7-Dhttp.port=port\u6307\u5b9a\u8bbf\u95ee\u7aef\u53e3,\u9ed8\u8ba4\u4e3a9000 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165172.21.3.48:9000\uff0c\u5373\u53ef\u8bbf\u95eekafka-manager \u70b9\u51fbcluster->Add Cluster\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199 Cluster Name:\u81ea\u5b9a\u4e49 Cluster Zookeeper Hosts:ZooKeeper\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f,\u53ef\u5199\u591a\u4e2a\u6216\u8005\u4e00\u4e2a\u8282\u70b9,\u4e00\u5b9a\u8981\u52a0\u4e0akafka\u540e\u7f00 Kafka Version:\u5f53\u524dFI\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684\u662f0.11.0.1\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u76840.11.0.2\u5373\u53ef \u52fe\u9009Enable JMX\u590d\u9009\u6846 \u5c06\u4ee5\u4e0b\u51e0\u4e2asize\u5927\u5c0f\u8bbe\u7f6e\u4e3a\u5927\u4e8e\u7b49\u4e8e2 \u5176\u4ed6\u8bbe\u7f6e\u53ef\u4ee5\u4fdd\u6301\u9ed8\u8ba4\u6216\u8005\u6839\u636e\u9700\u8981\u4fee\u6539,\u70b9\u51fbSave\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u521b\u5efa\u6210\u529f \u70b9\u51fbGo to cluster view\uff0c\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f \u5728Brokers\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u7684brokers\u60c5\u51b5 \u5728Topic->Create\u83dc\u5355\u680f\u53ef\u4ee5\u521b\u5efa\u65b0\u7684topic \u5728Topic->List\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u6240\u6709\u7684topic","title":"\u5bf9\u63a5Kafka-manager"},{"location":"Data_Integration/Kafka_Manager/#kafka-manager-fusioninsight","text":"","title":"Kafka Manager \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kafka_Manager/#_1","text":"/kafka-manager-1.3.3.21 \u2194 FusionInsight HD V100R002C80SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kafka_Manager/#_2","text":"\u5b89\u88c5JDK1.8\u53ca\u4ee5\u4e0a\u7248\u672c \u4e0b\u8f7dkafka-manager\u6e90\u7801 \u4e0b\u8f7d\u5730\u5740\u4e3a https://github.com/yahoo/kafka-manager \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55kafka-manager yum\u5b89\u88c5sbt yum install sbt \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u83b7\u53d6kafka\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6,\u767b\u5f55FusionInsight\u96c6\u7fa4\u8282\u70b9,\u5c06/opt/huawei/Bigdata/om-server_V100R002C80SPC200/apache-tomcat-8.5.28/conf/security/kafka.keytab\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u5e76\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef\u8282\u70b9/opt/hadoopclient/\u76ee\u5f55\u4e0b \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237kafkauser\uff0c\u5e76\u9009\u62e9kafka\u548ckafkaadmin\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.conf\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/hadoopclient/ \u76ee\u5f55\u4e0b","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kafka_Manager/#kafka-manager","text":"\u4fee\u6539\u6e90\u7801 \u8fdb\u5165\u5b89\u88c5\u76ee\u5f55/kafka-manager/app/kafka/manager/jmx\uff0c\u4fee\u6539KafkaJMX.scala\u6587\u4ef6\u4e2d\u5199\u6b7b\u7684jmx\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u5c06'jmxrmi'\u4fee\u6539\u4e3a'kafka',\u5982\u4e0b\u56fe\uff1a \u7f16\u8bd1kafka-manager,\u83b7\u53d6\u538b\u7f29\u5305 cd /opt/kafka-manager ./sbt clean dist cp /opt/kafka-manager/target/universal/kafka-manager-1.3.3.21.zip /opt cd /opt uzip kafka-manager-1.3.3.21.zip cd /opt/kafka-manager-1.3.3.21 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6conf/application.conf,'kafka-manager.zkhosts'\u4fee\u6539\u4e3azookeeper\u96c6\u7fa4\u8282\u70b9IP:\u7aef\u53e3,FI\u96c6\u7fa4\u9ed8\u8ba4\u7aef\u53e3\u4e3a24002 kafka-manager.zkhosts=\"172.21.3.115:24002\" \u65b0\u5efaconf/jaas.conf\u6587\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; \u5c06kafka-manager\u7684lib\u5e93\u4e2dzookeeper\u7684jar\u5305\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e2dzookeeper\u7684jar\u5305,\u5e76\u91cd\u547d\u540d cp /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar /opt/kafka-manager-1.3.3.21/lib cd /opt/kafka-manager-1.3.3.21/lib rm org.apache.zookeeper.zookeeper-3.4.10.jar mv zookeeper-3.5.1.jar org.apache.zookeeper.zookeeper-3.4.10.jar","title":"kafka-manager\u7f16\u8bd1\u53ca\u914d\u7f6e"},{"location":"Data_Integration/Kafka_Manager/#kafka-manager_1","text":"\u542f\u52a8kafka-manager cd /opt/kafka-manager-1.3.3.21 nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Djava.security.auth.login.config=conf/jaas.conf -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com & > \u53ef\u901a\u8fc7-Dhttp.port=port\u6307\u5b9a\u8bbf\u95ee\u7aef\u53e3,\u9ed8\u8ba4\u4e3a9000 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165172.21.3.48:9000\uff0c\u5373\u53ef\u8bbf\u95eekafka-manager \u70b9\u51fbcluster->Add Cluster\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199 Cluster Name:\u81ea\u5b9a\u4e49 Cluster Zookeeper Hosts:ZooKeeper\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f,\u53ef\u5199\u591a\u4e2a\u6216\u8005\u4e00\u4e2a\u8282\u70b9,\u4e00\u5b9a\u8981\u52a0\u4e0akafka\u540e\u7f00 Kafka Version:\u5f53\u524dFI\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684\u662f0.11.0.1\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u76840.11.0.2\u5373\u53ef \u52fe\u9009Enable JMX\u590d\u9009\u6846 \u5c06\u4ee5\u4e0b\u51e0\u4e2asize\u5927\u5c0f\u8bbe\u7f6e\u4e3a\u5927\u4e8e\u7b49\u4e8e2 \u5176\u4ed6\u8bbe\u7f6e\u53ef\u4ee5\u4fdd\u6301\u9ed8\u8ba4\u6216\u8005\u6839\u636e\u9700\u8981\u4fee\u6539,\u70b9\u51fbSave\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u521b\u5efa\u6210\u529f \u70b9\u51fbGo to cluster view\uff0c\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f \u5728Brokers\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u7684brokers\u60c5\u51b5 \u5728Topic->Create\u83dc\u5355\u680f\u53ef\u4ee5\u521b\u5efa\u65b0\u7684topic \u5728Topic->List\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u6240\u6709\u7684topic","title":"kafka-manager\u4f7f\u7528"},{"location":"Data_Integration/Kettle_6.1/","text":"Kettle\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 \u73af\u5883\u51c6\u5907 \u00b6 Linux\u5e73\u53f0 \u00b6 \u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5CentOS6.5 Desktop \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kettle \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Windows\u5e73\u53f0 \u00b6 \u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u590d\u5236 C:\\Windows \u76ee\u5f55\u4e0b\uff0c\u91cd\u547d\u540d\u4e3akrb5.ini \u6dfb\u52a0\u7cfb\u7edf\u73af\u5883\u53d8\u91cfKRB5_CONFIG\uff08\u53ef\u9009\u6b65\u9aa4\uff09 KRB5_CONFIG=C:\\Windows \u914d\u7f6e\u5e76\u542f\u52a8Kettle \u00b6 \u4ece\u4ee5\u4e0b\u5730\u5740 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ \u4e0b\u8f7dKettle6.1\u7248\u672c \u89e3\u538b\u5f97\u5230data-integration\u76ee\u5f55 \u66ff\u6362pentaho-big-data-plugin\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u7528\u89e3\u538b\u76ee\u5f55\u4e0b Hive/jdbc-examples/conf/core-site.xml \u6587\u4ef6 \u66ff\u6362 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23 \u76ee\u5f55\u4e0b\u7684core-site.xml\u6587\u4ef6 \u66ff\u6362Hive\u76f8\u5173jar\u5305 \u5c06 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib \u4e0b\u7684hive\u76f8\u5173\u7684jar\u5305 \u66ff\u6362\u6210Hive\u5ba2\u6237\u7aef\u4e0bjdbc-examples/lib\u76ee\u5f55\u4e0b\u7684\u4ee5\u4e0bjar\u5305 \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 Kerberos\u8ba4\u8bc1\uff08\u53ef\u9009\u6b65\u9aa4\uff09 \u5728\u5bf9\u63a5Hive\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\uff0c\u6216\u8005\u5728jdbc URL\u4e2d\u6307\u5b9aprincipal\u548ckeytab\u6587\u4ef6\u8fdb\u884c\u8ba4\u8bc1\uff08\u5bf9\u63a5HDFS\u65f6\uff0c\u53ea\u80fd\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff09 \u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff0c\u9700\u8981\u5728\u542f\u52a8kettle\u524d\u5148\u5b8c\u6210\u8ba4\u8bc1\u3002 \u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1akettle\u53ea\u5728\u542f\u52a8\u65f6\u8bfb\u53d6\u4e00\u6b21\u7968\u636e\uff0c\u800c\u4e0d\u662f\u8fde\u63a5\u65f6\u5b9e\u65f6\u8bfb\u53d6\u5f53\u524d\u7968\u636e\u4fe1\u606f\uff0c\u6240\u4ee5\u5f53kettle\u542f\u52a8\u65f6\u83b7\u53d6\u7684\u7968\u636e\u8fc7\u671f\u4ee5\u540e\uff0c\u8fde\u63a5Hive\u4f1a\u5931\u8d25\uff0c\u5fc5\u987b\u91cd\u542fkettle\u3002 \u542f\u52a8kettle Linux\u5e73\u53f0 VNC\u767b\u5f55CentOS\u684c\u9762\uff0c\u6253\u5f00Terminal cd /opt/data-integration/ ./spoon.sh Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat \u5bf9\u63a5Hive \u00b6 \u521b\u5efaHive\u8fde\u63a5 \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u5982\u679c\u8981\u5728\u8fde\u63a5Hive\u65f6\u4f7f\u7528keytab\u6587\u4ef6\u8ba4\u8bc1\uff0c\u589e\u52a0user.principal\u548cuser.keytab\u4e24\u4e2a\u53c2\u6570\uff1a \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0cHadoop\u7248\u672c\u9009\u7528HDP2.3 \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5 \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u4ee5hive -> postgresql\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3ahive2postgres.ktr \u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c \u8f93\u51fa -> \u8868\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u4fee\u6539postgresql\u8868\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u51fa \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5\u4e2d \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6\u76ee\u6807\u8868\u914d\u7f6e \u5982\u4e0b\uff08\u9700\u8981\u5148\u5728postgresql\u6570\u636e\u5e93\u521b\u5efa\u76ee\u6807\u8868\uff09 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a postgresql\u6570\u636e\u5e93\u67e5\u770b\uff1a \u5199\u5165Hive\u6570\u636e \u00b6 \u4ee5oracle -> hive\u4e3a\u4f8b \u6dfb\u52a0Oracle JDBC Driver \u4ece http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html \u4e0b\u8f7d\u5bf9\u5e94\u7248\u672c\u7684jdbc Driver\uff0c\u653e\u5230 data-integration/lib \u76ee\u5f55\u4e0b\uff0c\u91cd\u542fkettle \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3aoracle2hive.ktr \u521b\u5efaOracle\u8fde\u63a5 \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS kettle_export ( id int , name string ); \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u4fee\u6539\u6b65\u9aa4\u914d\u7f6e Oracle\u8868\u8f93\u5165\u914d\u7f6e Hive\u8868\u8f93\u51fa\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516513\u6761\u6570\u636e\uff0c\u7528\u65f64min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6 \u5bf9\u63a5HDFS \u00b6 \u521b\u5efaHadoop Cluster \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199NameNode\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f25000\uff0c\u5982\u679cNaneNode\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f26004\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP\u3002 \u70b9\u51fb \u6d4b\u8bd5 kettle6.1\u4e0d\u652f\u6301HDFS NameNode\u548cYarn ResourceManager\u7684HA\u914d\u7f6e \u5bfc\u5165HDFS\u6587\u4ef6 \u00b6 \u4ee5postgresql -> HDFS\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3apostgres2hdfs.ktr \u53c2\u8003\u524d\u9762\u7ae0\u8282\u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string , description string , total_emp int , salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"[,]\" ) STORED AS TEXTFILE ; \u5982\u679c\u6570\u636e\u4e2d\u542b\u6709\u201d,\u201d\uff0c\u5217\u5206\u9694\u7b26\u4e0d\u53ef\u4ee5\u4f7f\u7528\u9ed8\u8ba4\u7684\u201d,\u201d\uff0c\u672c\u6837\u4f8b\u4f7f\u7528\u591a\u5b57\u8282\u5206\u9694\u7b26\u201d[,]\u201d \u4fee\u6539postgresql\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u8868 \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9\u5230hive\u8868\u5bf9\u5e94\u7684hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u5206\u9694\u7b26\u8bbe\u7f6e\u4e0e\u524d\u9762\u521b\u5efa\u7684Hive\u8868\u76f8\u540c\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u5bfc\u5165\u7684HDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 kettle\u4f1a\u81ea\u52a8\u626b\u63cf\u6587\u4ef6\u4e2d\u7684\u5b57\u6bb5\u7c7b\u578b\u548c\u957f\u5ea6 \u53ef\u4ee5\u624b\u52a8\u4fee\u6539\u5b57\u6bb5\u540d\u79f0\u548c\u957f\u5ea6 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"\u5bf9\u63a5Kettle"},{"location":"Data_Integration/Kettle_6.1/#kettlefusioninsight","text":"","title":"Kettle\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kettle_6.1/#_1","text":"Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kettle_6.1/#_2","text":"","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kettle_6.1/#linux","text":"\u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5CentOS6.5 Desktop \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kettle \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/","title":"Linux\u5e73\u53f0"},{"location":"Data_Integration/Kettle_6.1/#windows","text":"\u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u590d\u5236 C:\\Windows \u76ee\u5f55\u4e0b\uff0c\u91cd\u547d\u540d\u4e3akrb5.ini \u6dfb\u52a0\u7cfb\u7edf\u73af\u5883\u53d8\u91cfKRB5_CONFIG\uff08\u53ef\u9009\u6b65\u9aa4\uff09 KRB5_CONFIG=C:\\Windows","title":"Windows\u5e73\u53f0"},{"location":"Data_Integration/Kettle_6.1/#kettle","text":"\u4ece\u4ee5\u4e0b\u5730\u5740 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ \u4e0b\u8f7dKettle6.1\u7248\u672c \u89e3\u538b\u5f97\u5230data-integration\u76ee\u5f55 \u66ff\u6362pentaho-big-data-plugin\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u7528\u89e3\u538b\u76ee\u5f55\u4e0b Hive/jdbc-examples/conf/core-site.xml \u6587\u4ef6 \u66ff\u6362 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23 \u76ee\u5f55\u4e0b\u7684core-site.xml\u6587\u4ef6 \u66ff\u6362Hive\u76f8\u5173jar\u5305 \u5c06 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib \u4e0b\u7684hive\u76f8\u5173\u7684jar\u5305 \u66ff\u6362\u6210Hive\u5ba2\u6237\u7aef\u4e0bjdbc-examples/lib\u76ee\u5f55\u4e0b\u7684\u4ee5\u4e0bjar\u5305 \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 Kerberos\u8ba4\u8bc1\uff08\u53ef\u9009\u6b65\u9aa4\uff09 \u5728\u5bf9\u63a5Hive\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\uff0c\u6216\u8005\u5728jdbc URL\u4e2d\u6307\u5b9aprincipal\u548ckeytab\u6587\u4ef6\u8fdb\u884c\u8ba4\u8bc1\uff08\u5bf9\u63a5HDFS\u65f6\uff0c\u53ea\u80fd\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff09 \u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff0c\u9700\u8981\u5728\u542f\u52a8kettle\u524d\u5148\u5b8c\u6210\u8ba4\u8bc1\u3002 \u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1akettle\u53ea\u5728\u542f\u52a8\u65f6\u8bfb\u53d6\u4e00\u6b21\u7968\u636e\uff0c\u800c\u4e0d\u662f\u8fde\u63a5\u65f6\u5b9e\u65f6\u8bfb\u53d6\u5f53\u524d\u7968\u636e\u4fe1\u606f\uff0c\u6240\u4ee5\u5f53kettle\u542f\u52a8\u65f6\u83b7\u53d6\u7684\u7968\u636e\u8fc7\u671f\u4ee5\u540e\uff0c\u8fde\u63a5Hive\u4f1a\u5931\u8d25\uff0c\u5fc5\u987b\u91cd\u542fkettle\u3002 \u542f\u52a8kettle Linux\u5e73\u53f0 VNC\u767b\u5f55CentOS\u684c\u9762\uff0c\u6253\u5f00Terminal cd /opt/data-integration/ ./spoon.sh Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat","title":"\u914d\u7f6e\u5e76\u542f\u52a8Kettle"},{"location":"Data_Integration/Kettle_6.1/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/Kettle_6.1/#hive_1","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u5982\u679c\u8981\u5728\u8fde\u63a5Hive\u65f6\u4f7f\u7528keytab\u6587\u4ef6\u8ba4\u8bc1\uff0c\u589e\u52a0user.principal\u548cuser.keytab\u4e24\u4e2a\u53c2\u6570\uff1a \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0cHadoop\u7248\u672c\u9009\u7528HDP2.3 \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5","title":"\u521b\u5efaHive\u8fde\u63a5"},{"location":"Data_Integration/Kettle_6.1/#hive_2","text":"\u4ee5hive -> postgresql\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3ahive2postgres.ktr \u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c \u8f93\u51fa -> \u8868\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u4fee\u6539postgresql\u8868\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u51fa \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5\u4e2d \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6\u76ee\u6807\u8868\u914d\u7f6e \u5982\u4e0b\uff08\u9700\u8981\u5148\u5728postgresql\u6570\u636e\u5e93\u521b\u5efa\u76ee\u6807\u8868\uff09 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a postgresql\u6570\u636e\u5e93\u67e5\u770b\uff1a","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_6.1/#hive_3","text":"\u4ee5oracle -> hive\u4e3a\u4f8b \u6dfb\u52a0Oracle JDBC Driver \u4ece http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html \u4e0b\u8f7d\u5bf9\u5e94\u7248\u672c\u7684jdbc Driver\uff0c\u653e\u5230 data-integration/lib \u76ee\u5f55\u4e0b\uff0c\u91cd\u542fkettle \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3aoracle2hive.ktr \u521b\u5efaOracle\u8fde\u63a5 \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS kettle_export ( id int , name string ); \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u4fee\u6539\u6b65\u9aa4\u914d\u7f6e Oracle\u8868\u8f93\u5165\u914d\u7f6e Hive\u8868\u8f93\u51fa\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516513\u6761\u6570\u636e\uff0c\u7528\u65f64min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6","title":"\u5199\u5165Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_6.1/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/Kettle_6.1/#hadoop-cluster","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199NameNode\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f25000\uff0c\u5982\u679cNaneNode\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f26004\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP\u3002 \u70b9\u51fb \u6d4b\u8bd5 kettle6.1\u4e0d\u652f\u6301HDFS NameNode\u548cYarn ResourceManager\u7684HA\u914d\u7f6e","title":"\u521b\u5efaHadoop Cluster"},{"location":"Data_Integration/Kettle_6.1/#hdfs_1","text":"\u4ee5postgresql -> HDFS\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3apostgres2hdfs.ktr \u53c2\u8003\u524d\u9762\u7ae0\u8282\u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string , description string , total_emp int , salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"[,]\" ) STORED AS TEXTFILE ; \u5982\u679c\u6570\u636e\u4e2d\u542b\u6709\u201d,\u201d\uff0c\u5217\u5206\u9694\u7b26\u4e0d\u53ef\u4ee5\u4f7f\u7528\u9ed8\u8ba4\u7684\u201d,\u201d\uff0c\u672c\u6837\u4f8b\u4f7f\u7528\u591a\u5b57\u8282\u5206\u9694\u7b26\u201d[,]\u201d \u4fee\u6539postgresql\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u8868 \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9\u5230hive\u8868\u5bf9\u5e94\u7684hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u5206\u9694\u7b26\u8bbe\u7f6e\u4e0e\u524d\u9762\u521b\u5efa\u7684Hive\u8868\u76f8\u540c\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u5bfc\u5165\u7684HDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u8868\u6570\u636e\uff1a","title":"\u5bfc\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_6.1/#hdfs_2","text":"\u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 kettle\u4f1a\u81ea\u52a8\u626b\u63cf\u6587\u4ef6\u4e2d\u7684\u5b57\u6bb5\u7c7b\u578b\u548c\u957f\u5ea6 \u53ef\u4ee5\u624b\u52a8\u4fee\u6539\u5b57\u6bb5\u540d\u79f0\u548c\u957f\u5ea6 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Knime/","text":"Knime \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Knime 3.6.1 \u2194 FusionInsight HD V100R002C80SPC200 Knime 3.6.1 \u2194 FusionInsight HD 6.5.0 \u73af\u5883\u51c6\u5907\u4ee5\u53caKnime\u4e0b\u8f7d \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u4e0b\u8f7dKnime \u00b6 \u5728Knime\u5b98\u7f51 https://www.knime.com/downloads/download-knime \u9009\u62e9\u5408\u9002\u7684\u5b89\u88c5\u5305\u8fdb\u884c\u4e0b\u8f7d. \u4e0b\u8f7dKnime extension \u00b6 \u5728\u83dc\u5355\u680f File->Install Knime extensions \u641c\u7d22 big data ,\u5728\u7ed3\u679c\u4e2d\u9009\u62e9 KNIME Big data Extensions ,\u7136\u540e next accept licence ,\u70b9\u51fb finish \u5f00\u59cb\u5b89\u88c5. \u5728\u53f3\u4e0b\u89d2\u53ef\u4ee5\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6 \u5b89\u88c5\u5b8c\u6210\u540e\u91cd\u542fKnime \u914d\u7f6eKnime \u00b6 \u83b7\u53d6\u96c6\u7fa4\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6\uff0c\u4fdd\u5b58\u5728\u672c\u5730. \u5728Knime\u7684\u5b89\u88c5\u76ee\u5f55\u4e2d\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u201cknime.ini\u201d,\u5728\u672b\u5c3e\u6dfb\u52a0\u4e00\u884c `Djava.security.krb5.conf=path\\to\\krb5.conf` \u53cc\u51fb Knime.exe \uff0c\u542f\u52a8Knime \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Hadoop \uff0c\u5728 Hadoop Configuration \u4e2d\u586b\u5165\u672c\u5730\u4fdd\u5b58\u7684HDFS\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Kerberos \uff0c\u586b\u5165kerberos\u8ba4\u8bc1\u7528\u6237\u540d\u548c\u672c\u5730keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5e76\u9009\u62e9 Enable Kerberos Logging ,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 Knime\u8fde\u63a5HDFS \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5efa\u7acbHDFS\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 * \u5728Node Repository\u4e2d\u641c\u7d22 HDFS \u5c06 HDFS Connection \u8282\u70b9\u62d6\u5165\u5de5\u4f5c\u533a \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Host: NameNode\u4e3b\u8282\u70b9 Port: 25000 Authentication: Kerberos \u70b9\u51fb Test connection ,\u663e\u793a\u5982\u4e0b\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f - \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 Download \u8282\u70b9\uff0c\u5c06\u5176\u4e0e HDFS Connection \u76f8\u8fde \u53cc\u51fb Download \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4eceHDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u7684\u6587\u4ef6\u4ee5\u53ca\u6587\u4ef6\u7684\u672c\u5730\u4fdd\u5b58\u8def\u5f84 \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u67e5\u770b\u672c\u5730\u6587\u4ef6 \u4e0a\u4f20\u6587\u4ef6\u81f3HDFS \u00b6 \u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u653e\u5728\u672c\u5730\u7684\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u4f8b\u5982 C:\\KnimeData \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 List Files , String to URI \u4ee5\u53ca Upload \u8282\u70b9\uff0c\u5c06\u5176\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u53cc\u51fb List Files \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4e0a\u4f20\u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53cc\u51fb Upload \u8282\u70b9\uff0c\u9009\u62e9\u5728HDFS\u4e2d\u6587\u4ef6\u4fdd\u5b58\u7684\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770bHDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u6240\u4e0a\u4f20\u7684\u6587\u4ef6 Knime\u8fde\u63a5Hive \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5efa\u7acbHive\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Hive Connector \u8282\u70b9 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Hostname: HIve\u4e3b\u8282\u70b9 Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u5199\u5165Hive\u8868 \u00b6 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u8282\u70b9\uff0c\u5e76\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u5176\u4e2d HDFS Connection \u8282\u70b9\u914d\u7f6e\u53c2\u8003\u4e0a\u8282\u4e2d\u5efa\u7acbHDFS\u8fde\u63a5\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e File Reader \u8282\u70b9\u4e2d\u9009\u62e9\u672c\u5730\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e Hive Loader \u8282\u70b9\u4e2d\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u7684\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868 Knime\u8fde\u63a5Spark \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5b89\u88c5spark-job-server \u00b6 \u6b64\u90e8\u5206\u53ef\u53c2\u8003KNIME\u5b98\u65b9\u6587\u6863 https://download.knime.org/store/3.6/knime_extension_for_apache_spark_2.3.0.pdf \u6253\u5f00 https://www.knime.com/knime-extension-for-apache-spark \uff0c\u6839\u636e\u96c6\u7fa4\u4ee5\u53ca\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u83b7\u53d6\u5bf9\u5e94\u7684 spark-job-server \u5b89\u88c5\u5305\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u4f8b\u5982 /opt \u76ee\u5f55\u4e0b\u3002 \u5bf9\u4e8eFusionInsight\u96c6\u7fa4\uff0cspark\u7248\u672c\u4e3a1.5\u548c2.1\uff0c\u6839\u636e\u8981\u4f7f\u7528\u7684spark\u7248\u672c\u9009\u62e9\u5bf9\u5e94\u7684spark-job-server\u8fdb\u884c\u5b89\u88c5\uff0c\u8fd9\u91cc\u4ee5spark2.1\u4e3a\u4f8b \u5bf9\u4e8e\u4f7f\u7528spark2x\u7684\u96c6\u7fa4\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5\u914d\u7f6e LINKNAME=spark2-job-server useradd -d /opt/${LINKNAME}/ -M -r -s /bin/false spark-job-server su -l -c \"hdfs dfs -mkdir -p /user/spark-job-server ; hdfs dfs -chown -R spark-job-server /user/spark-job-server\" hdfs cp /path/to/spark-job-server-xxx.tar.gz /opt cd /opt tar xzf spark-job-server-xxx.tar.gz ln -s spark-job-server-xxx ${LINKNAME} chown -R spark-job-server:spark-job-server ${LINKNAME} spark-job-server-xxx/ \u5bf9\u4e8e RHEL 6.x/CentOS 6.x \u64cd\u4f5c\u7cfb\u7edf\uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} chkconfig --levels 2345 ${LINKNAME} on \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a RHEL 7.x/CentOS 7.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} systemctl daemon-reload systemctl enable ${LINKNAME} \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 14.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d-ubuntu-sysv /etc/init.d/${LINKNAME} update-rc.d ${LINKNAME} start 20 2 3 4 5 . stop 20 0 1 6 . \u4fee\u6539 environment.conf \u6587\u4ef6,\u8bbe\u7f6e master = yarn-client \uff0c\u4ee5yarn-client\u6a21\u5f0f\u8fd0\u884cspark. \u4fee\u6539 settings.sh \u6587\u4ef6\uff0c\u8bbe\u7f6e SPARK_HOME=/opt/hadoopclient/Spark2x/spark \u914d\u7f6eKerberos\u5b89\u5168\u8ba4\u8bc1 \u00b6 \u5c06Kerberos\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u5e76\u6267\u884c\u4e00\u4e0b\u547d\u4ee4 chown spark-job-server:spark-job-server /path/to/keytab chmod go= /path/to/keytab \u5728 environment.conf \u6587\u4ef6\u4e2d\uff0c\u8fdb\u884c\u5982\u4e0b\u8bbe\u7f6e spark { jobserver { context-per-jvm = true } } shiro { authentication = on config.path = \"shiro.ini\" use-as-proxy-user = on } \u5728 setting.sh \u6587\u4ef6\u4e2d\uff0c\u7f16\u8f91\u4ee5\u4e0b\u51e0\u884c\uff0c\u914d\u7f6e\u5bf9\u5e94\u7684keytab\u6587\u4ef6\u8def\u5f84\u4ee5\u53ca\u7528\u6237principal export JOBSERVER_KEYTAB=/path/to/keytab export JOBSERVER_PRINCIPAL=user/host@REALM \u5728FusionInsight\u7684manager\u7ba1\u7406\u9875\u9762\uff0c\u4fee\u6539HDFS\u7684core-site.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4e3b\u9875\u9762\u9009\u62e9 \u670d\u52a1\u7ba1\u7406->HDFS->\u670d\u52a1\u914d\u7f6e ,\u53c2\u6570\u914d\u7f6e\u9009\u62e9 \u5168\u90e8\u670d\u52a1 \uff0c\u5728\u5de6\u4fa7\u9009\u62e9 HDFS->\u81ea\u5b9a\u4e49 ,\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570 hadoop.proxyuser.spark-job-server.hosts = * hadoop.proxyuser.spark-job-server.groups = * \u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1\u3002 \u542f\u52a8\u548c\u505c\u6b62Spark-job-server \u00b6 \u542f\u52a8Spark-job-server cd /etc/init.d ./spark2-job-server start \u542f\u52a8\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165http://ip:8090,\u53ef\u4ee5\u770b\u5230\u4ee5\u4e0b\u754c\u9762 \u505c\u6b62Spark-job-server cd /etc/init.d ./spark2-job-server stop \u5efa\u7acbSpark\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58,\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Create Spark Context \u8282\u70b9\uff0c\u53cc\u51fb\u540e\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u5728Context Settings\u9875\u9762 Spark version:\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c Context name\uff1a\u5efa\u7acb\u7684Spark Context \u540d\u5b57 \u5728Connection Settings\u9875\u9762\uff0cIP\u4e3aspark job server \u6240\u5728\u8282\u70b9IP Jobserver URL:http://ip:8090/ Authentication: None \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53ef\u70b9\u51fb\u83dc\u5355\u680f \u6309\u94ae\uff0c\u6d4b\u8bd5\u8fde\u63a5\u662f\u5426\u6709\u9519\uff0c\u82e5\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u8282\u70b9\u914d\u7f6e\u65e0\u8bef\u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00Jobserver URL\u4e2d\u914d\u7f6e\u7684\u5730\u5740\uff0c\u53ef\u4ee5\u8fdb\u5165Spark Job Server UI\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u5efa\u7acb\u7684Spark Context\uff0c\u663e\u793a\u5982\u4e0b\uff1a Spark\u5e94\u7528\u5b9e\u4f8b \u00b6 Spark\u5e94\u7528\u5b9e\u4f8b\u4e0b\u8f7d\u5730\u5740 https://www.knime.com/nodeguide/big-data/spark-executor Hive to Spark to Hive \u00b6 \u4e0b\u8f7d\u5b8c\u6210\u6253\u5f00\u5e94\u7528\u5b9e\u4f8b\uff0c\u914d\u7f6e HDFS Connection \uff0c File Reader \uff0c Hive Connector \uff0c Hive Loader \uff0c Create Spark Context \u548c Spark to Hive \u8282\u70b9\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u767b\u5f55\u8282\u70b9\uff0c\u6267\u884c beeline \u8fdb\u5165Hive\u754c\u9762\uff0c\u6267\u884c show tables; \u67e5\u770b\u5bfc\u5165\u7684\u8868. \u53ef\u4ee5\u770b\u5230\uff0c\u901a\u8fc7 Hive Loader \u8282\u70b9\u5bfc\u5165\u7684\u8868 contactdata \u4ee5\u53ca Spark to Hive \u8282\u70b9\u5bfc\u5165\u7684\u8868 sparktohivetable \u5747\u5df2\u5bfc\u5165Hive\u3002","title":"\u5bf9\u63a5Knime"},{"location":"Data_Integration/Knime/#knime-fusioninsight","text":"","title":"Knime \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Knime/#_1","text":"Knime 3.6.1 \u2194 FusionInsight HD V100R002C80SPC200 Knime 3.6.1 \u2194 FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Knime/#knime","text":"","title":"\u73af\u5883\u51c6\u5907\u4ee5\u53caKnime\u4e0b\u8f7d"},{"location":"Data_Integration/Knime/#_2","text":"\u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin;","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Knime/#knime_1","text":"\u5728Knime\u5b98\u7f51 https://www.knime.com/downloads/download-knime \u9009\u62e9\u5408\u9002\u7684\u5b89\u88c5\u5305\u8fdb\u884c\u4e0b\u8f7d.","title":"\u4e0b\u8f7dKnime"},{"location":"Data_Integration/Knime/#knime-extension","text":"\u5728\u83dc\u5355\u680f File->Install Knime extensions \u641c\u7d22 big data ,\u5728\u7ed3\u679c\u4e2d\u9009\u62e9 KNIME Big data Extensions ,\u7136\u540e next accept licence ,\u70b9\u51fb finish \u5f00\u59cb\u5b89\u88c5. \u5728\u53f3\u4e0b\u89d2\u53ef\u4ee5\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6 \u5b89\u88c5\u5b8c\u6210\u540e\u91cd\u542fKnime","title":"\u4e0b\u8f7dKnime extension"},{"location":"Data_Integration/Knime/#knime_2","text":"\u83b7\u53d6\u96c6\u7fa4\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6\uff0c\u4fdd\u5b58\u5728\u672c\u5730. \u5728Knime\u7684\u5b89\u88c5\u76ee\u5f55\u4e2d\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u201cknime.ini\u201d,\u5728\u672b\u5c3e\u6dfb\u52a0\u4e00\u884c `Djava.security.krb5.conf=path\\to\\krb5.conf` \u53cc\u51fb Knime.exe \uff0c\u542f\u52a8Knime \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Hadoop \uff0c\u5728 Hadoop Configuration \u4e2d\u586b\u5165\u672c\u5730\u4fdd\u5b58\u7684HDFS\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Kerberos \uff0c\u586b\u5165kerberos\u8ba4\u8bc1\u7528\u6237\u540d\u548c\u672c\u5730keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5e76\u9009\u62e9 Enable Kerberos Logging ,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002","title":"\u914d\u7f6eKnime"},{"location":"Data_Integration/Knime/#knimehdfs","text":"","title":"Knime\u8fde\u63a5HDFS"},{"location":"Data_Integration/Knime/#_3","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#hdfs","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 * \u5728Node Repository\u4e2d\u641c\u7d22 HDFS \u5c06 HDFS Connection \u8282\u70b9\u62d6\u5165\u5de5\u4f5c\u533a \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Host: NameNode\u4e3b\u8282\u70b9 Port: 25000 Authentication: Kerberos \u70b9\u51fb Test connection ,\u663e\u793a\u5982\u4e0b\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f - \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e","title":"\u5efa\u7acbHDFS\u8fde\u63a5"},{"location":"Data_Integration/Knime/#hdfs_1","text":"\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 Download \u8282\u70b9\uff0c\u5c06\u5176\u4e0e HDFS Connection \u76f8\u8fde \u53cc\u51fb Download \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4eceHDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u7684\u6587\u4ef6\u4ee5\u53ca\u6587\u4ef6\u7684\u672c\u5730\u4fdd\u5b58\u8def\u5f84 \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u67e5\u770b\u672c\u5730\u6587\u4ef6","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Knime/#hdfs_2","text":"\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u653e\u5728\u672c\u5730\u7684\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u4f8b\u5982 C:\\KnimeData \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 List Files , String to URI \u4ee5\u53ca Upload \u8282\u70b9\uff0c\u5c06\u5176\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u53cc\u51fb List Files \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4e0a\u4f20\u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53cc\u51fb Upload \u8282\u70b9\uff0c\u9009\u62e9\u5728HDFS\u4e2d\u6587\u4ef6\u4fdd\u5b58\u7684\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770bHDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u6240\u4e0a\u4f20\u7684\u6587\u4ef6","title":"\u4e0a\u4f20\u6587\u4ef6\u81f3HDFS"},{"location":"Data_Integration/Knime/#knimehive","text":"","title":"Knime\u8fde\u63a5Hive"},{"location":"Data_Integration/Knime/#_4","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#hive","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Hive Connector \u8282\u70b9 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Hostname: HIve\u4e3b\u8282\u70b9 Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e","title":"\u5efa\u7acbHive\u8fde\u63a5"},{"location":"Data_Integration/Knime/#hive_1","text":"\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u8282\u70b9\uff0c\u5e76\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u5176\u4e2d HDFS Connection \u8282\u70b9\u914d\u7f6e\u53c2\u8003\u4e0a\u8282\u4e2d\u5efa\u7acbHDFS\u8fde\u63a5\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e File Reader \u8282\u70b9\u4e2d\u9009\u62e9\u672c\u5730\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e Hive Loader \u8282\u70b9\u4e2d\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u7684\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868","title":"\u5199\u5165Hive\u8868"},{"location":"Data_Integration/Knime/#knimespark","text":"","title":"Knime\u8fde\u63a5Spark"},{"location":"Data_Integration/Knime/#_5","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#spark-job-server","text":"\u6b64\u90e8\u5206\u53ef\u53c2\u8003KNIME\u5b98\u65b9\u6587\u6863 https://download.knime.org/store/3.6/knime_extension_for_apache_spark_2.3.0.pdf \u6253\u5f00 https://www.knime.com/knime-extension-for-apache-spark \uff0c\u6839\u636e\u96c6\u7fa4\u4ee5\u53ca\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u83b7\u53d6\u5bf9\u5e94\u7684 spark-job-server \u5b89\u88c5\u5305\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u4f8b\u5982 /opt \u76ee\u5f55\u4e0b\u3002 \u5bf9\u4e8eFusionInsight\u96c6\u7fa4\uff0cspark\u7248\u672c\u4e3a1.5\u548c2.1\uff0c\u6839\u636e\u8981\u4f7f\u7528\u7684spark\u7248\u672c\u9009\u62e9\u5bf9\u5e94\u7684spark-job-server\u8fdb\u884c\u5b89\u88c5\uff0c\u8fd9\u91cc\u4ee5spark2.1\u4e3a\u4f8b \u5bf9\u4e8e\u4f7f\u7528spark2x\u7684\u96c6\u7fa4\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5\u914d\u7f6e LINKNAME=spark2-job-server useradd -d /opt/${LINKNAME}/ -M -r -s /bin/false spark-job-server su -l -c \"hdfs dfs -mkdir -p /user/spark-job-server ; hdfs dfs -chown -R spark-job-server /user/spark-job-server\" hdfs cp /path/to/spark-job-server-xxx.tar.gz /opt cd /opt tar xzf spark-job-server-xxx.tar.gz ln -s spark-job-server-xxx ${LINKNAME} chown -R spark-job-server:spark-job-server ${LINKNAME} spark-job-server-xxx/ \u5bf9\u4e8e RHEL 6.x/CentOS 6.x \u64cd\u4f5c\u7cfb\u7edf\uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} chkconfig --levels 2345 ${LINKNAME} on \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a RHEL 7.x/CentOS 7.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} systemctl daemon-reload systemctl enable ${LINKNAME} \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 14.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d-ubuntu-sysv /etc/init.d/${LINKNAME} update-rc.d ${LINKNAME} start 20 2 3 4 5 . stop 20 0 1 6 . \u4fee\u6539 environment.conf \u6587\u4ef6,\u8bbe\u7f6e master = yarn-client \uff0c\u4ee5yarn-client\u6a21\u5f0f\u8fd0\u884cspark. \u4fee\u6539 settings.sh \u6587\u4ef6\uff0c\u8bbe\u7f6e SPARK_HOME=/opt/hadoopclient/Spark2x/spark","title":"\u5b89\u88c5spark-job-server"},{"location":"Data_Integration/Knime/#kerberos","text":"\u5c06Kerberos\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u5e76\u6267\u884c\u4e00\u4e0b\u547d\u4ee4 chown spark-job-server:spark-job-server /path/to/keytab chmod go= /path/to/keytab \u5728 environment.conf \u6587\u4ef6\u4e2d\uff0c\u8fdb\u884c\u5982\u4e0b\u8bbe\u7f6e spark { jobserver { context-per-jvm = true } } shiro { authentication = on config.path = \"shiro.ini\" use-as-proxy-user = on } \u5728 setting.sh \u6587\u4ef6\u4e2d\uff0c\u7f16\u8f91\u4ee5\u4e0b\u51e0\u884c\uff0c\u914d\u7f6e\u5bf9\u5e94\u7684keytab\u6587\u4ef6\u8def\u5f84\u4ee5\u53ca\u7528\u6237principal export JOBSERVER_KEYTAB=/path/to/keytab export JOBSERVER_PRINCIPAL=user/host@REALM \u5728FusionInsight\u7684manager\u7ba1\u7406\u9875\u9762\uff0c\u4fee\u6539HDFS\u7684core-site.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4e3b\u9875\u9762\u9009\u62e9 \u670d\u52a1\u7ba1\u7406->HDFS->\u670d\u52a1\u914d\u7f6e ,\u53c2\u6570\u914d\u7f6e\u9009\u62e9 \u5168\u90e8\u670d\u52a1 \uff0c\u5728\u5de6\u4fa7\u9009\u62e9 HDFS->\u81ea\u5b9a\u4e49 ,\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570 hadoop.proxyuser.spark-job-server.hosts = * hadoop.proxyuser.spark-job-server.groups = * \u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1\u3002","title":"\u914d\u7f6eKerberos\u5b89\u5168\u8ba4\u8bc1"},{"location":"Data_Integration/Knime/#spark-job-server_1","text":"\u542f\u52a8Spark-job-server cd /etc/init.d ./spark2-job-server start \u542f\u52a8\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165http://ip:8090,\u53ef\u4ee5\u770b\u5230\u4ee5\u4e0b\u754c\u9762 \u505c\u6b62Spark-job-server cd /etc/init.d ./spark2-job-server stop","title":"\u542f\u52a8\u548c\u505c\u6b62Spark-job-server"},{"location":"Data_Integration/Knime/#spark","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58,\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Create Spark Context \u8282\u70b9\uff0c\u53cc\u51fb\u540e\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u5728Context Settings\u9875\u9762 Spark version:\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c Context name\uff1a\u5efa\u7acb\u7684Spark Context \u540d\u5b57 \u5728Connection Settings\u9875\u9762\uff0cIP\u4e3aspark job server \u6240\u5728\u8282\u70b9IP Jobserver URL:http://ip:8090/ Authentication: None \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53ef\u70b9\u51fb\u83dc\u5355\u680f \u6309\u94ae\uff0c\u6d4b\u8bd5\u8fde\u63a5\u662f\u5426\u6709\u9519\uff0c\u82e5\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u8282\u70b9\u914d\u7f6e\u65e0\u8bef\u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00Jobserver URL\u4e2d\u914d\u7f6e\u7684\u5730\u5740\uff0c\u53ef\u4ee5\u8fdb\u5165Spark Job Server UI\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u5efa\u7acb\u7684Spark Context\uff0c\u663e\u793a\u5982\u4e0b\uff1a","title":"\u5efa\u7acbSpark\u8fde\u63a5"},{"location":"Data_Integration/Knime/#spark_1","text":"Spark\u5e94\u7528\u5b9e\u4f8b\u4e0b\u8f7d\u5730\u5740 https://www.knime.com/nodeguide/big-data/spark-executor","title":"Spark\u5e94\u7528\u5b9e\u4f8b"},{"location":"Data_Integration/Knime/#hive-to-spark-to-hive","text":"\u4e0b\u8f7d\u5b8c\u6210\u6253\u5f00\u5e94\u7528\u5b9e\u4f8b\uff0c\u914d\u7f6e HDFS Connection \uff0c File Reader \uff0c Hive Connector \uff0c Hive Loader \uff0c Create Spark Context \u548c Spark to Hive \u8282\u70b9\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u767b\u5f55\u8282\u70b9\uff0c\u6267\u884c beeline \u8fdb\u5165Hive\u754c\u9762\uff0c\u6267\u884c show tables; \u67e5\u770b\u5bfc\u5165\u7684\u8868. \u53ef\u4ee5\u770b\u5230\uff0c\u901a\u8fc7 Hive Loader \u8282\u70b9\u5bfc\u5165\u7684\u8868 contactdata \u4ee5\u53ca Spark to Hive \u8282\u70b9\u5bfc\u5165\u7684\u8868 sparktohivetable \u5747\u5df2\u5bfc\u5165Hive\u3002","title":"Hive to Spark to Hive"},{"location":"Data_Integration/Oracle_GoldenGate/","text":"Oracle GoldenGate\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Oracle GoldenGate 12.2 \u2194 FusionInsight HD V100R002C60U20 Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C70SPC200 Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C80SPC100 Oracle GoldenGate 12.2 \u2194 FusionInsight HD 6.5.0 \u73af\u5883\u4fe1\u606f \u00b6 \u8f6f\u4ef6\u4fe1\u606f \u00b6 Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20 \u786c\u4ef6\u4fe1\u606f \u00b6 \u6e90\u7aefOGG VM: 162.1.115.68 Redhat6.5 \uff08\u5305\u542bOracle DB12c\u7684\u6570\u636e\u5e93\uff09 \u76ee\u6807\u7aefOGG VM: 162.1.115.69 Redhat6.5\uff08\u5305\u542bHadoop\u7684\u5ba2\u6237\u7aef\uff09 \u62d3\u6734\u7ed3\u6784 \u00b6 \u6d4b\u8bd5\u62d3\u6734\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6d4b\u8bd5\u8868 \u00b6 \u6e90\u7aef\u6d4b\u8bd5\u8868\uff1a \u5728\u6e90\u7aefOracle\u7684PDBORCL\u6570\u636e\u5e93\u7684test\u7528\u6237\u4e0b\u521b\u5efatest1\u8868\uff0c\u5176\u4e2dID\u4e3a\u4e3b\u952e OGG for Oracle\u5b89\u88c5 \u00b6 \u524d\u7f6e\u6761\u4ef6\uff1a\u5b8c\u6210oracle12c\u6570\u636e\u5e93\u7684\u5b89\u88c5\uff08IP\uff1a162.1.115.68\uff09 \u8f6f\u4ef6\u7248\u672c\uff1alinuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Oracle \u00b6 \u5c06fbo_ggs_Linux_x64_shiphome.zip\u4e0a\u4f20\u81f3oracle\u5ba2\u6237\u7aef\uff08ip\uff1a162.1.115.68\uff09 /home/oracle \u76ee\u5f55\u4e0b\uff0c\u5207\u6362\u81f3oracle\u7528\u6237\uff0c\u89e3\u538b\u751f\u6210bo_ggs_Linux_x64_shiphome\u76ee\u5f55\u3002 \u5728 /home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1 \u76ee\u5f55\u4e0b\uff0c\u8fd0\u884c ./runInstaller \u5b89\u88c5\u6210\u529f\uff0c/home/orcle/OGG/\u662fOGG for Oracle\u7684\u5b89\u88c5\u76ee\u5f55\u3002 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u5207\u6362\u5230oracle\u7528\u6237 su - oracle vi .bash_profile \u6587\u4ef6.bash_profile\u5185\u5bb9\u5982\u4e0b\uff1a ```shell # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH= PATH: PATH: HOME/bin export PATH PATH= PATH: PATH: HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib ``` \u8fd0\u884cOGG \u6253\u5f00\u6570\u636e\u5e93\u5f52\u6863\u53ca\u5f00\u542f\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7 \u00b6 \u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: shutdown immediate ; startup mount ; alter database archivelog ; alter database open ; archive log list ; \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication ; alter system set enable_goldengate_replication = true scope = both ; \u914d\u7f6eDB12c PDB\u7684tnsname\u4fe1\u606f vi $ORACLE_HOME/network/admin/tnsnames.ora \uff1a \u5728\u6570\u636e\u5e93\u4e2d\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 \u00b6 \u4f7f\u7528 sqlplus / as sysdba \u767b\u9646\u6570\u636e\u5e93\u540e\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 create user c ## ogg identified by welcome1 ; grant dba to c ## ogg container = all ; grant create session , connect , resource to c ## ogg container = all ; grant alter any table to c ## ogg container = all ; grant alter system to c ## ogg container = all ; exec dbms_goldengate_auth . grant_admin_privilege ( 'c##ogg' , container => 'all' ); \u914d\u7f6eGoldenGate \u767b\u9646\u6570\u636e\u5e93\u7684\u522b\u540d \u00b6 \u5728GoldenGate\u4e2d\u521b\u5efa\u7528\u6237\u522b\u540d\uff0c\u7528\u4e8e\u767b\u5f55Oracle\u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u5e93\u65e5\u5fd7\uff1a add credentialstore ALTER CREDENTIALSTORE ADD USER c ## ogg PASSWORD welcome1 ALIAS ogg_src \u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u522b\u540dogg_src\u767b\u9646\u6570\u636e\u5e93\u4e86\uff1a dblogin useridalias ogg_src C##ogg\u662fOracle DB12c\u7684\u666e\u901a\u7528\u6237\uff0c\u53ef\u4ee5\u8bbf\u95ee\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\u3002 \u521b\u5efatest\u7528\u6237\u548ctest1\u8868 \u00b6 test\u7528\u6237\u662f\u57fa\u4e8epdborcl\u6570\u636e\u5e93\u5b9e\u4f8b\u7684\uff1a \u767b\u9646\u6570\u636e\u5e93 Sqlplus / as sysdba \u521b\u5efa\u7528\u6237 alter session set container = pdborcl ; alter database open ; create user test identified by welcome1 ; grant resource , connect to test ; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500 M UNIFORM SIZE 128 k ; alter user test quota unlimited on test ; alter user test quota unlimited on users ; \u521b\u5efa\u6d4b\u8bd5\u8868 conn test / welcome1 @ pdborcl ; create table test1 ( id number primary key , name varchar2 ( 50 )); \u914d\u7f6eGoldenGate\u6355\u83b7\u8fdb\u7a0b \u00b6 \u7f16\u8f91eora.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cedit param eora\u547d\u4ee4\uff1a GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume \u7f16\u8f91 diroby/eora.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/eora.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) \u4f7f\u7528oracle\u7528\u6237\u521b\u5efadiroby\u76ee\u5f55\uff1a cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby \u6ce8\u610f\u8fdb\u7a0b\u540deora\u548c\u6570\u636e\u6587\u4ef6dirdat/eo\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cobey diroby/eora.oby\u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0beora\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/eora.oby \u628a\u6355\u83b7\u8fdb\u7a0beora\u6ce8\u518c\u5230pdborcl\u6570\u636e\u5e93\u4e2d\uff1a GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) \u4e3apdborcl.test\u4e0b\u7684\u6240\u6709\u8868\u6dfb\u52a0\u8868\u7ea7\u9644\u52a0\u65e5\u5fd7\uff1a GGSCI> add schematrandata pdborcl.test allcols \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0beora: GGSCI> start eora \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HDFS\u5904\u7406\u3002 \u7f16\u8f91phdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phdfs \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phdfs.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phdfs.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phdfs.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phdfs**\u548c\u6570\u636e\u6587\u4ef6dirdat/rs\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phdfs.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphdfs\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phdfs.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphdfs: GGSCI> start phdfs \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HBASE\u5904\u7406\u3002 \u7f16\u8f91phbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phbase \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phbase.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phbase.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phbase.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phbase**\u548c\u6570\u636e\u6587\u4ef6dirdat/se\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phbase.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphbase\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phbase.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphbase: GGSCI> start phbase \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate FLUME\u5904\u7406\u3002 \u7f16\u8f91pflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pflume \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pflume.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pflume.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pflume.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pflume**\u548c\u6570\u636e\u6587\u4ef6dirdat/rf\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pflume.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpflume\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/pflume.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpflume: GGSCI> start pflume \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate Kafka\u5904\u7406\u3002 \u7f16\u8f91pkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pkafka \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pkafka.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pkafka.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pkafka.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pkafka**\u548c\u6570\u636e\u6587\u4ef6dirdat/rk\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pkafka.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpkafka\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/ pkafka.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpkafka: GGSCI> start pkafka \u67e5\u770bGoldenGate\u8fdb\u7a0b\u8fd0\u884c\u72b6\u6001 \u00b6 \u67e5\u770bGoldenGate\u8fdb\u7a0b\u72b6\u6001\uff1a(EORCL\u662f\u4e0eELK\u5bf9\u63a5\u7684\u8fdb\u7a0b) GGSCI> info all \u67e5\u770b\u67d0\u4e2a\u8fdb\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a GGSCI> info eora detail \u67e5\u770bGoldenGate\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a GGSCI> stats eora, latest \u67e5\u770bGoldenGate\u8fdb\u7a0b\u62a5\u544a\uff0c\u7528\u4e8e\u5b9a\u4f4d\u95ee\u9898\uff1a GGSCI> view report eora OGG for Bigdata\u5b89\u88c5 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728Bigdata\u5ba2\u6237\u7aef\u673a\u5668\u4e0a\uff08ip\uff1a162.1.115.69\uff09\u6309\u7167FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u3002\u5c06\u5ba2\u6237\u7aefJDK\u66ff\u6362\u62101.7\u7248\u672c\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5oracle JDK1.7 \u5c06krb5.conf\u653e\u5728/etc/\u76ee\u5f55\u4e0b \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Bigdata \u5c06122011_ggs_Adapters_Linux_x64.zip\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef/opt\u76ee\u5f55\u4e0b\uff1a unzip 122011_ggs_Adapters_Linux_x64.zip \u5c06\u89e3\u538b\u540e\u7684ggs_Adapters_Linux_x64.tar\u89e3\u538b\u5230/opt/OGG_HADOOP\u76ee\u5f55\u4e0b\uff1a \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u66f4\u6539\u73af\u5883\u53d8\u91cf\uff0c\u7f16\u8f91\u6839\u76ee\u5f55\u4e0b vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME = /usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH = $CLASSPATH : $JAVA_HOME /lib: $JAVA_HOME /jre/lib PATH = $JAVA_HOME /bin: $PATH : $HOME /bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib: $LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/local/lib: $LD_LIBRARY_PATH Source\u73af\u5883\u53d8\u91cf\uff0c source .bash_profile . \u5c06 /opt/OGG_HADOOP/AdapterExamples/big-data \u4e0b\u7684\u56db\u4e2a\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/OGG_HADOOP/dirprm \u76ee\u5f55\u4e0b\u3002 \u914d\u7f6eGoldenGate\u7ba1\u7406\u8fdb\u7a0b \u00b6 \u7f16\u8f91mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all \u914d\u7f6eGoldenGate HDFS \u590d\u5236\u8fdb\u7a0b \u00b6 \u7f16\u8f91rhdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhdfs \u547d\u4ee4\uff1a GGSCI> edit param rhdfs \u7f16\u8f91hdfs.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hdfs.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hdfs.props \u9700\u8981\u5728HDFS\u4e2d\u521b\u5efa/ogg1\u76ee\u5f55\u3002 \u5c06hdfs.keytab\u6587\u4ef6\u62f7\u8d1d\u5230/opt/OGG_HADOOP/dirprm\u76ee\u5f55\u4e2d\uff1a \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhdfs\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all \u914d\u7f6eGoldenGate HBase \u590d\u5236\u8fdb\u7a0b \u00b6 \u7f16\u8f91rhbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhbase \u547d\u4ee4\uff1a GGSCI> edit param rhbase \u7f16\u8f91hbase.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hbase.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hbase.props \u62f7\u8d1dhbase.keytab\u548cjaas.conf\u5230 /opt/OGG_HADOOP/dirprm/ \u4e0b\uff1a jaas.conf \u6587\u4ef6 \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhbase\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all \u914d\u7f6eGoldenGate Kafka \u590d\u5236\u8fdb\u7a0b \u00b6 \u521b\u5efakafka\u6d88\u606f\uff0c\u8fdb\u5165FusionInsight\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin Kafka\u521b\u5efa\u6d88\u606f\uff1a ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka\u67e5\u770b\u6d88\u606f\uff1a ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka\u7ed9\u6d88\u606f\u6388\u6743\uff1a ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test \u7f16\u8f91rkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rkafka \u547d\u4ee4\uff1a GGSCI> edit param rkafka \u7f16\u8f91kafka.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/kafka.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/kafka.props \u5176\u4e2d gg.handler.kafkahandler.BlockingSend \u5c5e\u6027\u63a7\u5236\u540c\u6b65\u548c\u5f02\u6b65\uff0c\u9ed8\u8ba4false\uff0c\u5f02\u6b65\u3002 GGSCI> shell vi dirprm/custom_kafka_producer.properties \u4fee\u6539Kafka\u91cc\u7684\u914d\u7f6e\uff0c\u5c06\u5982\u4e0b\u9009\u9879\u4fee\u6539\u4e3aTrue \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brkafka\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all \u914d\u7f6eGoldenGate Flume \u590d\u5236\u8fdb\u7a0b \u00b6 \u5b89\u88c5Flume\u5ba2\u6237\u7aef\uff0c\u914d\u7f6e\u975e\u52a0\u5bc6\u4f20\u8f93 \u914d\u7f6eServer\u7684\u914d\u7f6e\u6587\u4ef6properties.properties \u5bfc\u51fa\u7684properties.properties\u6587\u4ef6\uff0c\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a \u53ef\u4ee5\u5728HDFS\u4e2d\u589e\u52a0/ogg/flume\u76ee\u5f55 \u5c06\u6b64properties.properties\u6587\u4ef6\u4e0a\u4f20\u81f3FusionInsight\u3002 \u7f16\u8f91rflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rflume \u547d\u4ee4\uff1a GGSCI> edit param rflume \u7f16\u8f91flume.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/flume.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false \u63a7\u5236DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false \u76f8\u540cSCHAME\u6253\u5305 GGSCI> shell vi dirprm/custom-flume-rpc.properties \u62f7\u8d1dflume.keytab\u6587\u4ef6\u5230 /opt/OGG_HADOOP/dirprm/ \u76ee\u5f55\u4e0b \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brflume\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all \u6d4b\u8bd5\u7ed3\u679c \u00b6 Oracle\u7aef\u542f\u52a8\u6240\u6709\u7684\u4f20\u8f93\u8fdb\u7a0b \u00b6 \u786e\u4fdd\u6240\u6709\u4f20\u8f93\u8fdb\u7a0b\u5747\u5df2\u7ecf\u6b63\u5e38\u542f\u52a8 \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aInsert\u64cd\u4f5c \u00b6 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aUpdate\u64cd\u4f5c \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aDelete\u64cd\u4f5c \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0chadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5bf9\u63a5Oracle GoldenGate"},{"location":"Data_Integration/Oracle_GoldenGate/#oracle-goldengatefusioninsight","text":"","title":"Oracle GoldenGate\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Oracle_GoldenGate/#_1","text":"Oracle GoldenGate 12.2 \u2194 FusionInsight HD V100R002C60U20 Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C70SPC200 Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C80SPC100 Oracle GoldenGate 12.2 \u2194 FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Oracle_GoldenGate/#_2","text":"","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_3","text":"Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20","title":"\u8f6f\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_4","text":"\u6e90\u7aefOGG VM: 162.1.115.68 Redhat6.5 \uff08\u5305\u542bOracle DB12c\u7684\u6570\u636e\u5e93\uff09 \u76ee\u6807\u7aefOGG VM: 162.1.115.69 Redhat6.5\uff08\u5305\u542bHadoop\u7684\u5ba2\u6237\u7aef\uff09","title":"\u786c\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_5","text":"\u6d4b\u8bd5\u62d3\u6734\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"\u62d3\u6734\u7ed3\u6784"},{"location":"Data_Integration/Oracle_GoldenGate/#_6","text":"\u6e90\u7aef\u6d4b\u8bd5\u8868\uff1a \u5728\u6e90\u7aefOracle\u7684PDBORCL\u6570\u636e\u5e93\u7684test\u7528\u6237\u4e0b\u521b\u5efatest1\u8868\uff0c\u5176\u4e2dID\u4e3a\u4e3b\u952e","title":"\u6d4b\u8bd5\u8868"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-oracle","text":"\u524d\u7f6e\u6761\u4ef6\uff1a\u5b8c\u6210oracle12c\u6570\u636e\u5e93\u7684\u5b89\u88c5\uff08IP\uff1a162.1.115.68\uff09 \u8f6f\u4ef6\u7248\u672c\uff1alinuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip","title":"OGG for Oracle\u5b89\u88c5"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-oracle_1","text":"\u5c06fbo_ggs_Linux_x64_shiphome.zip\u4e0a\u4f20\u81f3oracle\u5ba2\u6237\u7aef\uff08ip\uff1a162.1.115.68\uff09 /home/oracle \u76ee\u5f55\u4e0b\uff0c\u5207\u6362\u81f3oracle\u7528\u6237\uff0c\u89e3\u538b\u751f\u6210bo_ggs_Linux_x64_shiphome\u76ee\u5f55\u3002 \u5728 /home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1 \u76ee\u5f55\u4e0b\uff0c\u8fd0\u884c ./runInstaller \u5b89\u88c5\u6210\u529f\uff0c/home/orcle/OGG/\u662fOGG for Oracle\u7684\u5b89\u88c5\u76ee\u5f55\u3002","title":"\u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Oracle"},{"location":"Data_Integration/Oracle_GoldenGate/#_7","text":"\u5207\u6362\u5230oracle\u7528\u6237 su - oracle vi .bash_profile \u6587\u4ef6.bash_profile\u5185\u5bb9\u5982\u4e0b\uff1a ```shell # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH= PATH: PATH: HOME/bin export PATH PATH= PATH: PATH: HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib ``` \u8fd0\u884cOGG","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/Oracle_GoldenGate/#_8","text":"\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: shutdown immediate ; startup mount ; alter database archivelog ; alter database open ; archive log list ; \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication ; alter system set enable_goldengate_replication = true scope = both ; \u914d\u7f6eDB12c PDB\u7684tnsname\u4fe1\u606f vi $ORACLE_HOME/network/admin/tnsnames.ora \uff1a","title":"\u6253\u5f00\u6570\u636e\u5e93\u5f52\u6863\u53ca\u5f00\u542f\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg","text":"\u4f7f\u7528 sqlplus / as sysdba \u767b\u9646\u6570\u636e\u5e93\u540e\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 create user c ## ogg identified by welcome1 ; grant dba to c ## ogg container = all ; grant create session , connect , resource to c ## ogg container = all ; grant alter any table to c ## ogg container = all ; grant alter system to c ## ogg container = all ; exec dbms_goldengate_auth . grant_admin_privilege ( 'c##ogg' , container => 'all' );","title":"\u5728\u6570\u636e\u5e93\u4e2d\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate","text":"\u5728GoldenGate\u4e2d\u521b\u5efa\u7528\u6237\u522b\u540d\uff0c\u7528\u4e8e\u767b\u5f55Oracle\u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u5e93\u65e5\u5fd7\uff1a add credentialstore ALTER CREDENTIALSTORE ADD USER c ## ogg PASSWORD welcome1 ALIAS ogg_src \u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u522b\u540dogg_src\u767b\u9646\u6570\u636e\u5e93\u4e86\uff1a dblogin useridalias ogg_src C##ogg\u662fOracle DB12c\u7684\u666e\u901a\u7528\u6237\uff0c\u53ef\u4ee5\u8bbf\u95ee\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\u3002","title":"\u914d\u7f6eGoldenGate \u767b\u9646\u6570\u636e\u5e93\u7684\u522b\u540d"},{"location":"Data_Integration/Oracle_GoldenGate/#testtest1","text":"test\u7528\u6237\u662f\u57fa\u4e8epdborcl\u6570\u636e\u5e93\u5b9e\u4f8b\u7684\uff1a \u767b\u9646\u6570\u636e\u5e93 Sqlplus / as sysdba \u521b\u5efa\u7528\u6237 alter session set container = pdborcl ; alter database open ; create user test identified by welcome1 ; grant resource , connect to test ; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500 M UNIFORM SIZE 128 k ; alter user test quota unlimited on test ; alter user test quota unlimited on users ; \u521b\u5efa\u6d4b\u8bd5\u8868 conn test / welcome1 @ pdborcl ; create table test1 ( id number primary key , name varchar2 ( 50 ));","title":"\u521b\u5efatest\u7528\u6237\u548ctest1\u8868"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_1","text":"\u7f16\u8f91eora.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cedit param eora\u547d\u4ee4\uff1a GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume \u7f16\u8f91 diroby/eora.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/eora.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) \u4f7f\u7528oracle\u7528\u6237\u521b\u5efadiroby\u76ee\u5f55\uff1a cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby \u6ce8\u610f\u8fdb\u7a0b\u540deora\u548c\u6570\u636e\u6587\u4ef6dirdat/eo\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cobey diroby/eora.oby\u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0beora\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/eora.oby \u628a\u6355\u83b7\u8fdb\u7a0beora\u6ce8\u518c\u5230pdborcl\u6570\u636e\u5e93\u4e2d\uff1a GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) \u4e3apdborcl.test\u4e0b\u7684\u6240\u6709\u8868\u6dfb\u52a0\u8868\u7ea7\u9644\u52a0\u65e5\u5fd7\uff1a GGSCI> add schematrandata pdborcl.test allcols \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0beora: GGSCI> start eora","title":"\u914d\u7f6eGoldenGate\u6355\u83b7\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatephdfs","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HDFS\u5904\u7406\u3002 \u7f16\u8f91phdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phdfs \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phdfs.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phdfs.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phdfs.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phdfs**\u548c\u6570\u636e\u6587\u4ef6dirdat/rs\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phdfs.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphdfs\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phdfs.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphdfs: GGSCI> start phdfs","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatephbase","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HBASE\u5904\u7406\u3002 \u7f16\u8f91phbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phbase \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phbase.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phbase.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phbase.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phbase**\u548c\u6570\u636e\u6587\u4ef6dirdat/se\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phbase.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphbase\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phbase.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphbase: GGSCI> start phbase","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatepflume","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate FLUME\u5904\u7406\u3002 \u7f16\u8f91pflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pflume \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pflume.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pflume.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pflume.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pflume**\u548c\u6570\u636e\u6587\u4ef6dirdat/rf\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pflume.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpflume\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/pflume.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpflume: GGSCI> start pflume","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatepkafka","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate Kafka\u5904\u7406\u3002 \u7f16\u8f91pkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pkafka \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pkafka.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pkafka.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pkafka.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pkafka**\u548c\u6570\u636e\u6587\u4ef6dirdat/rk\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pkafka.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpkafka\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/ pkafka.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpkafka: GGSCI> start pkafka","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_2","text":"\u67e5\u770bGoldenGate\u8fdb\u7a0b\u72b6\u6001\uff1a(EORCL\u662f\u4e0eELK\u5bf9\u63a5\u7684\u8fdb\u7a0b) GGSCI> info all \u67e5\u770b\u67d0\u4e2a\u8fdb\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a GGSCI> info eora detail \u67e5\u770bGoldenGate\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a GGSCI> stats eora, latest \u67e5\u770bGoldenGate\u8fdb\u7a0b\u62a5\u544a\uff0c\u7528\u4e8e\u5b9a\u4f4d\u95ee\u9898\uff1a GGSCI> view report eora","title":"\u67e5\u770bGoldenGate\u8fdb\u7a0b\u8fd0\u884c\u72b6\u6001"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-bigdata","text":"","title":"OGG for Bigdata\u5b89\u88c5"},{"location":"Data_Integration/Oracle_GoldenGate/#_9","text":"\u4e0b\u8f7d\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728Bigdata\u5ba2\u6237\u7aef\u673a\u5668\u4e0a\uff08ip\uff1a162.1.115.69\uff09\u6309\u7167FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u3002\u5c06\u5ba2\u6237\u7aefJDK\u66ff\u6362\u62101.7\u7248\u672c\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5oracle JDK1.7 \u5c06krb5.conf\u653e\u5728/etc/\u76ee\u5f55\u4e0b \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Bigdata \u5c06122011_ggs_Adapters_Linux_x64.zip\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef/opt\u76ee\u5f55\u4e0b\uff1a unzip 122011_ggs_Adapters_Linux_x64.zip \u5c06\u89e3\u538b\u540e\u7684ggs_Adapters_Linux_x64.tar\u89e3\u538b\u5230/opt/OGG_HADOOP\u76ee\u5f55\u4e0b\uff1a \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u66f4\u6539\u73af\u5883\u53d8\u91cf\uff0c\u7f16\u8f91\u6839\u76ee\u5f55\u4e0b vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME = /usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH = $CLASSPATH : $JAVA_HOME /lib: $JAVA_HOME /jre/lib PATH = $JAVA_HOME /bin: $PATH : $HOME /bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib: $LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/local/lib: $LD_LIBRARY_PATH Source\u73af\u5883\u53d8\u91cf\uff0c source .bash_profile . \u5c06 /opt/OGG_HADOOP/AdapterExamples/big-data \u4e0b\u7684\u56db\u4e2a\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/OGG_HADOOP/dirprm \u76ee\u5f55\u4e0b\u3002","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_3","text":"\u7f16\u8f91mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all","title":"\u914d\u7f6eGoldenGate\u7ba1\u7406\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-hdfs","text":"\u7f16\u8f91rhdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhdfs \u547d\u4ee4\uff1a GGSCI> edit param rhdfs \u7f16\u8f91hdfs.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hdfs.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hdfs.props \u9700\u8981\u5728HDFS\u4e2d\u521b\u5efa/ogg1\u76ee\u5f55\u3002 \u5c06hdfs.keytab\u6587\u4ef6\u62f7\u8d1d\u5230/opt/OGG_HADOOP/dirprm\u76ee\u5f55\u4e2d\uff1a \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhdfs\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all","title":"\u914d\u7f6eGoldenGate HDFS \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-hbase","text":"\u7f16\u8f91rhbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhbase \u547d\u4ee4\uff1a GGSCI> edit param rhbase \u7f16\u8f91hbase.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hbase.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hbase.props \u62f7\u8d1dhbase.keytab\u548cjaas.conf\u5230 /opt/OGG_HADOOP/dirprm/ \u4e0b\uff1a jaas.conf \u6587\u4ef6 \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhbase\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all","title":"\u914d\u7f6eGoldenGate HBase \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-kafka","text":"\u521b\u5efakafka\u6d88\u606f\uff0c\u8fdb\u5165FusionInsight\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin Kafka\u521b\u5efa\u6d88\u606f\uff1a ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka\u67e5\u770b\u6d88\u606f\uff1a ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka\u7ed9\u6d88\u606f\u6388\u6743\uff1a ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test \u7f16\u8f91rkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rkafka \u547d\u4ee4\uff1a GGSCI> edit param rkafka \u7f16\u8f91kafka.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/kafka.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/kafka.props \u5176\u4e2d gg.handler.kafkahandler.BlockingSend \u5c5e\u6027\u63a7\u5236\u540c\u6b65\u548c\u5f02\u6b65\uff0c\u9ed8\u8ba4false\uff0c\u5f02\u6b65\u3002 GGSCI> shell vi dirprm/custom_kafka_producer.properties \u4fee\u6539Kafka\u91cc\u7684\u914d\u7f6e\uff0c\u5c06\u5982\u4e0b\u9009\u9879\u4fee\u6539\u4e3aTrue \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brkafka\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all","title":"\u914d\u7f6eGoldenGate Kafka \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-flume","text":"\u5b89\u88c5Flume\u5ba2\u6237\u7aef\uff0c\u914d\u7f6e\u975e\u52a0\u5bc6\u4f20\u8f93 \u914d\u7f6eServer\u7684\u914d\u7f6e\u6587\u4ef6properties.properties \u5bfc\u51fa\u7684properties.properties\u6587\u4ef6\uff0c\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a \u53ef\u4ee5\u5728HDFS\u4e2d\u589e\u52a0/ogg/flume\u76ee\u5f55 \u5c06\u6b64properties.properties\u6587\u4ef6\u4e0a\u4f20\u81f3FusionInsight\u3002 \u7f16\u8f91rflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rflume \u547d\u4ee4\uff1a GGSCI> edit param rflume \u7f16\u8f91flume.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/flume.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false \u63a7\u5236DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false \u76f8\u540cSCHAME\u6253\u5305 GGSCI> shell vi dirprm/custom-flume-rpc.properties \u62f7\u8d1dflume.keytab\u6587\u4ef6\u5230 /opt/OGG_HADOOP/dirprm/ \u76ee\u5f55\u4e0b \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brflume\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all","title":"\u914d\u7f6eGoldenGate Flume \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#_10","text":"","title":"\u6d4b\u8bd5\u7ed3\u679c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracle","text":"\u786e\u4fdd\u6240\u6709\u4f20\u8f93\u8fdb\u7a0b\u5747\u5df2\u7ecf\u6b63\u5e38\u542f\u52a8","title":"Oracle\u7aef\u542f\u52a8\u6240\u6709\u7684\u4f20\u8f93\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#oracleinsert","text":"su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aInsert\u64cd\u4f5c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracleupdate","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aUpdate\u64cd\u4f5c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracledelete","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0chadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aDelete\u64cd\u4f5c"},{"location":"Data_Integration/TIBCO_BusinessWorks/","text":"TIBCO Business Works(BW) 5.13 \u5bf9\u63a5GaussDB200 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Tibco BW5.13 \u2194 GaussDB200 \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Tibco BW5.13\uff0c\u53c2\u8003Tibco\u5b98\u65b9\u6587\u6863 https://docs.tibco.com/pub/activematrix_businessworks/5.13.0/doc/pdf/TIB_BW_5.13.0_installation.pdf?id=0 \u5b89\u88c5\u5b8c\u6210\u540e\u542f\u52a8TIBCO Designer\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7a7a\u767d\u5de5\u7a0b \u8fde\u63a5GaussDB \u00b6 \u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u653e\u5728TIBCO\u5b89\u88c5\u76ee\u5f55 tpcl\\version\\jdbc \u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:\\tibco\\tpcl\\5.10\\jdbc \u76ee\u5f55\u4e0b \u5728TIBCO Designer\u4e2d\uff0c\u6dfb\u52a0 \u4e00\u4e2a\u65b0\u7684process\uff0c\u8fd9\u91cc\u53d6\u540d\u4e3a getTable \u5728\u5de5\u7a0b\u7684 Shared Resources \u4e2d\u6dfb\u52a0\u4e00\u4e2a JDBC Connection \uff0c\u547d\u540d\u4e3a Gauss Connection \uff0c\u5e76\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e JDBC Driver \u4e3apostgresql\u7684\u9a71\u52a8\u7c7b\u540d Database URL\u4e3apostgres\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u4e3ajdbc:postgresql://ip:port/postgresql,\u5bf9\u4e8eGaussDB\uff0c\u9ed8\u8ba4\u7aef\u53e3\u4e3a25308 UserName\u548cPassword\u4e3a\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801 \u70b9\u51fb Test Connection \uff0c\u5f39\u51fa\u8fde\u63a5\u6210\u529f\u7a97\u53e3 \u53cc\u51fb\u8fdb\u5165 getTable \uff0c\u4ece\u5de6\u4fa7\u7684activity\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u6d41\u7a0b \u5728 JDBC Query \u4e2d\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e,\u70b9\u51fb\u53f3\u4fa7\u641c\u7d22\u6807\u5fd7\uff0c\u5728\u5f39\u51fa\u7a97\u53e3\u4e2d\u9009\u62e9\u521a\u624d\u521b\u5efa\u7684 GAUSS COnnection ,\u8f93\u5165\u60f3\u8981\u6267\u884c\u7684SQL\u8bed\u53e5,\u8fd9\u91cc\u662f\u67e5\u8be2test\u8868\u4e2d\u7684\u6240\u6709\u5185\u5bb9 \u70b9\u51fb\u5de6\u4e0b\u89d2 fetch \uff0c\u4f1a\u5728Output\u9875\u7b7e\u4e2d\u770b\u5230\u8981\u67e5\u8be2\u7684\u8868\u7684\u5b57\u6bb5\u4fe1\u606f \u5728\u5de6\u4fa7 tester \u4e2d\u70b9\u51fb\u6267\u884c\u6309\u94ae\uff0c\u9009\u62e9 getTable \uff0c Load & Start Current \uff0c\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 * \u5728 JDBC Query \u7684output\u4e2d\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c ![](assets/TIBCO_BusinessWorks/1289b.png) \u5bf9\u4e8eJDBC\u7684\u5176\u4ed6\u64cd\u4f5c\uff0c\u4f8b\u5982 JDBC Update\uff0cCall Procedure \u914d\u7f6e\u7c7b\u4f3c\uff0c\u53ef\u53c2\u8003TIBCO\u7684\u5b98\u65b9\u6587\u6863 http://tutorialspedia.com/jdbc-call-procedure-tutorial/","title":"\u5bf9\u63a5TIBCO BW"},{"location":"Data_Integration/TIBCO_BusinessWorks/#tibco-business-worksbw-513-gaussdb200","text":"","title":"TIBCO Business Works(BW) 5.13 \u5bf9\u63a5GaussDB200"},{"location":"Data_Integration/TIBCO_BusinessWorks/#_1","text":"Tibco BW5.13 \u2194 GaussDB200","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/TIBCO_BusinessWorks/#_2","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Tibco BW5.13\uff0c\u53c2\u8003Tibco\u5b98\u65b9\u6587\u6863 https://docs.tibco.com/pub/activematrix_businessworks/5.13.0/doc/pdf/TIB_BW_5.13.0_installation.pdf?id=0 \u5b89\u88c5\u5b8c\u6210\u540e\u542f\u52a8TIBCO Designer\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7a7a\u767d\u5de5\u7a0b","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/TIBCO_BusinessWorks/#gaussdb","text":"\u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u653e\u5728TIBCO\u5b89\u88c5\u76ee\u5f55 tpcl\\version\\jdbc \u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:\\tibco\\tpcl\\5.10\\jdbc \u76ee\u5f55\u4e0b \u5728TIBCO Designer\u4e2d\uff0c\u6dfb\u52a0 \u4e00\u4e2a\u65b0\u7684process\uff0c\u8fd9\u91cc\u53d6\u540d\u4e3a getTable \u5728\u5de5\u7a0b\u7684 Shared Resources \u4e2d\u6dfb\u52a0\u4e00\u4e2a JDBC Connection \uff0c\u547d\u540d\u4e3a Gauss Connection \uff0c\u5e76\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e JDBC Driver \u4e3apostgresql\u7684\u9a71\u52a8\u7c7b\u540d Database URL\u4e3apostgres\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u4e3ajdbc:postgresql://ip:port/postgresql,\u5bf9\u4e8eGaussDB\uff0c\u9ed8\u8ba4\u7aef\u53e3\u4e3a25308 UserName\u548cPassword\u4e3a\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801 \u70b9\u51fb Test Connection \uff0c\u5f39\u51fa\u8fde\u63a5\u6210\u529f\u7a97\u53e3 \u53cc\u51fb\u8fdb\u5165 getTable \uff0c\u4ece\u5de6\u4fa7\u7684activity\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u6d41\u7a0b \u5728 JDBC Query \u4e2d\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e,\u70b9\u51fb\u53f3\u4fa7\u641c\u7d22\u6807\u5fd7\uff0c\u5728\u5f39\u51fa\u7a97\u53e3\u4e2d\u9009\u62e9\u521a\u624d\u521b\u5efa\u7684 GAUSS COnnection ,\u8f93\u5165\u60f3\u8981\u6267\u884c\u7684SQL\u8bed\u53e5,\u8fd9\u91cc\u662f\u67e5\u8be2test\u8868\u4e2d\u7684\u6240\u6709\u5185\u5bb9 \u70b9\u51fb\u5de6\u4e0b\u89d2 fetch \uff0c\u4f1a\u5728Output\u9875\u7b7e\u4e2d\u770b\u5230\u8981\u67e5\u8be2\u7684\u8868\u7684\u5b57\u6bb5\u4fe1\u606f \u5728\u5de6\u4fa7 tester \u4e2d\u70b9\u51fb\u6267\u884c\u6309\u94ae\uff0c\u9009\u62e9 getTable \uff0c Load & Start Current \uff0c\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 * \u5728 JDBC Query \u7684output\u4e2d\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c ![](assets/TIBCO_BusinessWorks/1289b.png) \u5bf9\u4e8eJDBC\u7684\u5176\u4ed6\u64cd\u4f5c\uff0c\u4f8b\u5982 JDBC Update\uff0cCall Procedure \u914d\u7f6e\u7c7b\u4f3c\uff0c\u53ef\u53c2\u8003TIBCO\u7684\u5b98\u65b9\u6587\u6863 http://tutorialspedia.com/jdbc-call-procedure-tutorial/","title":"\u8fde\u63a5GaussDB"},{"location":"Data_Integration/Talend/","text":"Talend\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200(HDFS,HBase\u7ec4\u4ef6) Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200(hive\u7ec4\u4ef6) \u6ce8\uff1a\u56e0\u4e3aTalend 7.0.1\u7248\u672cbug\uff0cHive\u7ec4\u4ef6\u65e0\u6cd5\u5728\u7248\u672c7.0.1\u4e2d\u901a\u8fc7\uff0c\u5bf9\u63a5hive\u7ec4\u4ef6\u4f7f\u7528Talend 6.4.1\u7248\u672c \u5b89\u88c5Talend \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Talend 7.0.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5(\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6) \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cfJAVA_HOME,Path \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5411FusionInsight HD\u96c6\u7fa4\u7ba1\u7406\u5458\u83b7\u53d6\u96c6\u7fa4Kerberos\u7684krb5.conf\u6587\u4ef6,\u628a\u76f8\u5e94\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini,\u5e76\u653e\u5230 C:\\ProgramData\\Kerberos \u76ee\u5f55\u4e2d\uff0c\u540c\u65f6\u5c06krb5.ini\u6587\u4ef6\u653e\u5230 C:\\Windows \u76ee\u5f55\u4e0b\uff08Talend\u9ed8\u8ba4\u4ece\u6b64\u76ee\u5f55\u4e0b\u67e5\u627e\uff09 \u4e0b\u8f7dTOS\u5e76\u4fee\u6539TOS\u542f\u52a8\u53c2\u6570 \u5728 https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dTOS\uff0c\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8TOS_BD\uff0c\u8fd0\u884cTOS_BD-win-x86_64.exe \u5b89\u88c5\u5fc5\u9700\u7684\u7b2c\u4e09\u65b9\u5e93 Talend\u8fde\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HDFS\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 HDFS Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6dfb\u52a0tHDFSConnection\u7ec4\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\uff1a 1: \u9009\u62e9Cloudera\uff0c\u7248\u672c\u4e3aCloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - \u6d4b\u8bd5\u7ed3\u679c\uff1a HDFS Get \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSGet\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a /tmp/talend_test \u8def\u5f84\u4e0b\u5df2\u7ecf\u4f20\u5165\u6587\u4ef6 out.csv \uff0c C:/SOFT \u4e3a\u672c\u5730\u8f93\u51fa\u6587\u4ef6\u8def\u5f84 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5230\u672c\u5730\u8def\u5f84 C:/SOFT \u4e0b\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c HDFS Put \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSPut\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u672c\u5730\u76ee\u5f55 C:/SOFT \u4e0b\u521b\u5efa\u6587\u4ef6 HDFSPut.txt , \u5185\u5bb9\u5982\u4e0b\uff1a It is create on local PC. \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767b\u5f55\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a Talend\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 6.4.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 Hive Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bf9\u63a5Hive\u7ec4\u4ef6Talend\u7248\u672c\u9700\u89816.4.1 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fbDistritution\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u5bfc\u5165FusionInsight HD\u5ba2\u6237\u7aefHive\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 \u6d4b\u8bd5\u7ed3\u679c\uff1a Hive Create Table & Load \u64cd\u4f5c\u6b65\u9aa4 \u00b6 tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveCreateTable\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6e\u9700\u8981\u5bfc\u5165hive\u8868\u7684\u7ed3\u6784 tHiveLoad\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u63d0\u524d\u9700\u8981\u5411hdfs\u6587\u4ef6\u5b58\u50a8\u7cfb\u7edf /tmp/talend_test/ \u8def\u5f84\u4e0b\u4f20\u5165\u6587\u4ef6 out.csv out.csv \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b: \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5728\u96c6\u7fa4\u4e0a\u68c0\u67e5\u4f20\u5165\u7684\u8868 createdTableTalend Hive Input \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6d4b\u8bd5\u7ed3\u679c\uff1a Hive Row \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveRow\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u8fde\u63a5\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c Talend\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 HBase Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: \u7528eclipse\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\uff08\u6837\u4f8b\u4ee3\u7801\u8def\u5f84\u5982 C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example \uff09 \u5728Talend\u91cc\u63d2\u5165tHbaseConnection\u7ec4\u4ef6\uff0c\u70b9\u51fb\u7ec4\u4ef6\u8fdb\u884c\u8bbe\u7f6e \u9996\u5148\u70b9\u51fbtHBaseConnection\u56fe\u6807\u4e0b\u9762\u7684\u7ec4\u4ef6\u6309\u94ae\uff0c\u9009\u62e9\u7248\u672c\u4e3a Custom - Unsupported \u548c Hadoop 2 \uff0c\u518d\u70b9\u51fb\u7248\u672c\u65c1\u8fb9\u7684\u6309\u94ae\u5bfc\u5165jar\u5305\uff0c\u9700\u8981\u5bfc\u5165\u7684\u662f\u4e0a\u4e00\u6b65\u5bfc\u51fa\u7684hbase_loginUtil.jar\u4ee5\u53caFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801 hbase-example \u4e2d\u5f15\u7528\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 hbase-example \u6837\u4f8b\u4ee3\u7801\u4e2dlib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u5982\u4e0b\uff1a \u4f7f\u7528tLibraryLoad\u7ec4\u4ef6\u5bfc\u5165hbase_loginUtil.jar \u70b9\u51fb Advanced settings \u5728Import\u4e2d\u589e\u52a0 import com.huawei.hadoop.security.LoginUtil; tHBaseConnection\u914d\u7f6e\u5982\u4e0b: \u5f15\u5165tJava\u7ec4\u4ef6\u7528\u5b9a\u5236\u4ee3\u7801\u66ff\u4ee3Connection\u7ec4\u4ef6 \u4ee3\u7801\u5185\u5bb9\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - \u6d4b\u8bd5\u7ed3\u679c HBase Input Output \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tLibraryLoad\uff0ctHBaseConnection\uff0ctJava\u914d\u7f6e\u4e0d\u53d8 \u52a0\u5165tFileInputDelimited\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\uff0c\u6839\u636e\u9700\u8981\u5b58\u5165\u6587\u4ef6(out.csv)\u7684\u683c\u5f0f\u5b9a\u4e49\u5217\u548c\u7c7b\u578b out.csv \u6d4b\u8bd5\u6570\u636e\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u52a0\u5165tHBaseOutput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u7f16\u8f91\u8868\u7684\u67b6\u6784\uff1a tHBaseInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u540c\u6837\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u914d\u7f6e\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e \u6d4b\u8bd5\u7ed3\u679c \u68c0\u67e5\u96c6\u7fa4\u521b\u5efa\u7684HBase\u8868 hbaseInputOutputTest \u5728\u96c6\u7fa4\u4e0a\u4f7f\u7528\u4ee3\u7801 hbase shell scan 'hbaseInputOutputTest'","title":"\u5bf9\u63a5Talend"},{"location":"Data_Integration/Talend/#talendfusioninsight","text":"","title":"Talend\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Talend/#_1","text":"Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200(HDFS,HBase\u7ec4\u4ef6) Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200(hive\u7ec4\u4ef6) \u6ce8\uff1a\u56e0\u4e3aTalend 7.0.1\u7248\u672cbug\uff0cHive\u7ec4\u4ef6\u65e0\u6cd5\u5728\u7248\u672c7.0.1\u4e2d\u901a\u8fc7\uff0c\u5bf9\u63a5hive\u7ec4\u4ef6\u4f7f\u7528Talend 6.4.1\u7248\u672c","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Talend/#talend","text":"","title":"\u5b89\u88c5Talend"},{"location":"Data_Integration/Talend/#_2","text":"\u5b89\u88c5Talend 7.0.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5(\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6)","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cfJAVA_HOME,Path \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5411FusionInsight HD\u96c6\u7fa4\u7ba1\u7406\u5458\u83b7\u53d6\u96c6\u7fa4Kerberos\u7684krb5.conf\u6587\u4ef6,\u628a\u76f8\u5e94\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini,\u5e76\u653e\u5230 C:\\ProgramData\\Kerberos \u76ee\u5f55\u4e2d\uff0c\u540c\u65f6\u5c06krb5.ini\u6587\u4ef6\u653e\u5230 C:\\Windows \u76ee\u5f55\u4e0b\uff08Talend\u9ed8\u8ba4\u4ece\u6b64\u76ee\u5f55\u4e0b\u67e5\u627e\uff09 \u4e0b\u8f7dTOS\u5e76\u4fee\u6539TOS\u542f\u52a8\u53c2\u6570 \u5728 https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dTOS\uff0c\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8TOS_BD\uff0c\u8fd0\u884cTOS_BD-win-x86_64.exe \u5b89\u88c5\u5fc5\u9700\u7684\u7b2c\u4e09\u65b9\u5e93","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#talendhdfs","text":"","title":"Talend\u8fde\u63a5HDFS"},{"location":"Data_Integration/Talend/#_5","text":"Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HDFS\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend/#hdfs-connection","text":"\u6dfb\u52a0tHDFSConnection\u7ec4\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\uff1a 1: \u9009\u62e9Cloudera\uff0c\u7248\u672c\u4e3aCloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"HDFS Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#hdfs-get","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSGet\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a /tmp/talend_test \u8def\u5f84\u4e0b\u5df2\u7ecf\u4f20\u5165\u6587\u4ef6 out.csv \uff0c C:/SOFT \u4e3a\u672c\u5730\u8f93\u51fa\u6587\u4ef6\u8def\u5f84 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5230\u672c\u5730\u8def\u5f84 C:/SOFT \u4e0b\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"HDFS Get \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#hdfs-put","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSPut\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u672c\u5730\u76ee\u5f55 C:/SOFT \u4e0b\u521b\u5efa\u6587\u4ef6 HDFSPut.txt , \u5185\u5bb9\u5982\u4e0b\uff1a It is create on local PC. \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767b\u5f55\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"HDFS Put \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#talendhive","text":"","title":"Talend\u8fde\u63a5Hive"},{"location":"Data_Integration/Talend/#_7","text":"Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend/#_8","text":"\u5df2\u7ecf\u5b8c\u6210Talend 6.4.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend/#hive-connection","text":"\u5bf9\u63a5Hive\u7ec4\u4ef6Talend\u7248\u672c\u9700\u89816.4.1 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fbDistritution\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u5bfc\u5165FusionInsight HD\u5ba2\u6237\u7aefHive\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"Hive Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#hive-create-table-load","text":"tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveCreateTable\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6e\u9700\u8981\u5bfc\u5165hive\u8868\u7684\u7ed3\u6784 tHiveLoad\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u63d0\u524d\u9700\u8981\u5411hdfs\u6587\u4ef6\u5b58\u50a8\u7cfb\u7edf /tmp/talend_test/ \u8def\u5f84\u4e0b\u4f20\u5165\u6587\u4ef6 out.csv out.csv \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b: \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5728\u96c6\u7fa4\u4e0a\u68c0\u67e5\u4f20\u5165\u7684\u8868 createdTableTalend","title":"Hive Create Table &amp; Load \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#hive-input","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"Hive Input \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#hive-row","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveRow\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u8fde\u63a5\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"Hive Row \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#talendhbase","text":"","title":"Talend\u8fde\u63a5HBase"},{"location":"Data_Integration/Talend/#_9","text":"Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend/#_10","text":"\u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend/#hbase-connection","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: \u7528eclipse\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\uff08\u6837\u4f8b\u4ee3\u7801\u8def\u5f84\u5982 C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example \uff09 \u5728Talend\u91cc\u63d2\u5165tHbaseConnection\u7ec4\u4ef6\uff0c\u70b9\u51fb\u7ec4\u4ef6\u8fdb\u884c\u8bbe\u7f6e \u9996\u5148\u70b9\u51fbtHBaseConnection\u56fe\u6807\u4e0b\u9762\u7684\u7ec4\u4ef6\u6309\u94ae\uff0c\u9009\u62e9\u7248\u672c\u4e3a Custom - Unsupported \u548c Hadoop 2 \uff0c\u518d\u70b9\u51fb\u7248\u672c\u65c1\u8fb9\u7684\u6309\u94ae\u5bfc\u5165jar\u5305\uff0c\u9700\u8981\u5bfc\u5165\u7684\u662f\u4e0a\u4e00\u6b65\u5bfc\u51fa\u7684hbase_loginUtil.jar\u4ee5\u53caFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801 hbase-example \u4e2d\u5f15\u7528\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 hbase-example \u6837\u4f8b\u4ee3\u7801\u4e2dlib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u5982\u4e0b\uff1a \u4f7f\u7528tLibraryLoad\u7ec4\u4ef6\u5bfc\u5165hbase_loginUtil.jar \u70b9\u51fb Advanced settings \u5728Import\u4e2d\u589e\u52a0 import com.huawei.hadoop.security.LoginUtil; tHBaseConnection\u914d\u7f6e\u5982\u4e0b: \u5f15\u5165tJava\u7ec4\u4ef6\u7528\u5b9a\u5236\u4ee3\u7801\u66ff\u4ee3Connection\u7ec4\u4ef6 \u4ee3\u7801\u5185\u5bb9\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - \u6d4b\u8bd5\u7ed3\u679c","title":"HBase Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend/#hbase-input-output","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tLibraryLoad\uff0ctHBaseConnection\uff0ctJava\u914d\u7f6e\u4e0d\u53d8 \u52a0\u5165tFileInputDelimited\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\uff0c\u6839\u636e\u9700\u8981\u5b58\u5165\u6587\u4ef6(out.csv)\u7684\u683c\u5f0f\u5b9a\u4e49\u5217\u548c\u7c7b\u578b out.csv \u6d4b\u8bd5\u6570\u636e\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u52a0\u5165tHBaseOutput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u7f16\u8f91\u8868\u7684\u67b6\u6784\uff1a tHBaseInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u540c\u6837\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u914d\u7f6e\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e \u6d4b\u8bd5\u7ed3\u679c \u68c0\u67e5\u96c6\u7fa4\u521b\u5efa\u7684HBase\u8868 hbaseInputOutputTest \u5728\u96c6\u7fa4\u4e0a\u4f7f\u7528\u4ee3\u7801 hbase shell scan 'hbaseInputOutputTest'","title":"HBase Input Output \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Database/","text":"\u6570\u636e\u5e93 \u00b6","title":"Home"},{"location":"Database/#_1","text":"","title":"\u6570\u636e\u5e93"},{"location":"Development/","text":"\u96c6\u6210\u5f00\u53d1\u73af\u5883 \u00b6 R\u8bed\u8a00 \u5bf9\u63a5RStudio Notebook\u7c7b\u5f00\u53d1IDE \u5bf9\u63a5Apache Zeppelin Zeppelin0.7.2 <-> FusionInsight HD V100R002C60U20 Zeppelin0.7.3 <-> FusionInsight HD V100R002C70SPC100 Zeppelin0.8.0 <-> FusionInsight HD V100R002C80SPC200 \u5bf9\u63a5Jupyter Notebook SQL\u7c7b\u5f00\u53d1IDE \u5bf9\u63a5DBeaver \u5bf9\u63a5DbVisualizer \u5bf9\u63a5Squirrel","title":"Home"},{"location":"Development/#_1","text":"R\u8bed\u8a00 \u5bf9\u63a5RStudio Notebook\u7c7b\u5f00\u53d1IDE \u5bf9\u63a5Apache Zeppelin Zeppelin0.7.2 <-> FusionInsight HD V100R002C60U20 Zeppelin0.7.3 <-> FusionInsight HD V100R002C70SPC100 Zeppelin0.8.0 <-> FusionInsight HD V100R002C80SPC200 \u5bf9\u63a5Jupyter Notebook SQL\u7c7b\u5f00\u53d1IDE \u5bf9\u63a5DBeaver \u5bf9\u63a5DbVisualizer \u5bf9\u63a5Squirrel","title":"\u96c6\u6210\u5f00\u53d1\u73af\u5883"},{"location":"Development/DBeaver/","text":"DBeaver\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DBeaver 4.0.8 \u2194 FusionInsight HD V100R002C60U20 DBeaver 4.2.1 \u2194 FusionInsight HD V100R002C70SPC200 \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DBeaver\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 Linux\u4e0bDBeaver\u8fde\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b89\u88c5\u597dLinux\uff08Redhat Linux Enterprise 6.5 64bit\uff09Desktop\u64cd\u4f5c\u7cfb\u7edf\uff1b \u5df2\u7ecf\u5b89\u88c5\u597d\u7684Linux\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5jdk1.8\uff0cDBeaver4.0.8\u9700\u8981jdk1.8\u4ee5\u4e0a\u7248\u672c tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf/etc/profile\uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH \u4e0b\u8f7d\u5730\u5740\uff1a http://dbeaver.jkiss.org/download/ , \u8f6f\u4ef6 dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \uff0c\u5b89\u88c5DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u4ea7\u54c1\u6587\u6863\u300b\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \uff0c\u5176\u4e2dFiber\u5ba2\u6237\u7aef\u76ee\u5f55 /opt/hadoopclient/Fiber/ \u3002 \u4fee\u6539Fiber\u7684\u914d\u7f6e\u6587\u4ef6 /opt/hadoopclient/Fiber/conf/fiber.xml \uff0c\u5c06\u5176\u4e2dhive\u3001spark\u3001phoenix\u7684\u8ba4\u8bc1\u65b9\u5f0f\u6539\u4e3a\u5b89\u5168\u6a21\u5f0fkeytab\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u6cd5\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 Hive JDBC\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>{HIVE_CLIENT_ZK_PRINCIPAL}</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Spark\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Phoenix\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> jaas.conf\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u6253\u5f00DBeaver\uff0c\u8fdb\u5165DBeaver\u7684\u5b89\u88c5\u76ee\u5f55\u6267\u884c ./dbeaver \uff0c\u542f\u52a8dbeaver \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef /opt/hadoopclient/Fiber/lib/ \u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027\uff1a \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection , \u7c7b\u578b\u9009\u62e9Fiber User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u914d\u7f6eDriver properties\u91cc\u9762\u7684defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\uff0c\u70b9\u51fbnext Network\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u8f93\u5165\u81ea\u5b9a\u4e49Connection name\u540e\uff0c\u70b9\u51fb finish , \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u94fe\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u94fe\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e Windows\u4e0bDBeaver\u8fde\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix \u524d\u63d0\u6761\u4ef6 \u00b6 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5b8c\u6210windows\u4e0a\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u64cd\u4f5c\u6b65\u9aa4 \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u53ef\u4ee5\u7528kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\uff0c\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002kinit\u8ba4\u8bc1\u7684\u6709\u6548\u671f\u662f24\u5c0f\u65f6\uff0ckeytab\u8ba4\u8bc1\u65b9\u5f0f\u957f\u671f\u6709\u6548 - \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e - \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\uff0c\u5e76\u5b89\u88c5 http://web.mit.edu/kerberos/dist/#kfw-4.0 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u89d2\u8272\u548c\u4eba\u673a\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u7cfb\u7edf\u8bbe\u7f6e -> \u6743\u9650\u7ba1\u7406 -> \u7528\u6237\u7ba1\u7406 -> \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\uff0c\u521b\u5efa\u7528\u6237\u201ctest\u201d \u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6 user.keytab \u4ee5\u53ca krb5.conf \u6587\u4ef6\uff0c\u628a krb5.conf \u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d(\u5982\uff1a test@HADOOP.COM )\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; \u4fee\u6539 fiber.xml \u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5\u524d\u786e\u8ba4kerberos\u8ba4\u8bc1\u6709\u6548 \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab\u548chbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5Fiber \u00b6 \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef\uff08 /opt/hadoopclient/Fiber/lib/ \uff09\u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027 \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u786e\u8ba4defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\u3002 Network\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u81ea\u5b9a\u4e49Connection name\uff0c\u70b9\u51fbfinish \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u8fde\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u8fde\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e DBeaver\u5bf9\u63a5Fiber\u529f\u80fd\u9a8c\u8bc1 \u00b6 Hive\u589e\u52a0\u67e5\u770b\u6570\u636e \u00b6 \u5c06JDBC\u7684defaultDrive\u5207\u6362\u81f3Hive Hive\u67e5\u8be2\u6570\u636e\uff1a\u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 \u67e5\u770b\u66f4\u65b0\u540e\u6570\u636e\uff1a Spark\u589e\u52a0\u67e5\u770b\u6570\u636e \u00b6 \u5c06JDBC \u7684defaultDriver\u5207\u6362\u81f3Spark Spark\u67e5\u8be2\u6570\u636e\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Spark\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3Spark\u7684JDBCServer(\u4e3b)\u5b9e\u4f8b\u6240\u5728\u7684\u8282\u70b9\u7684/opt/\u76ee\u5f55\u4e0b \u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info \u67e5\u770b\u7ed3\u679c\uff1a Phoenix\u589e\u5220\u6539\u67e5\u6570\u636e \u00b6 \u5c06JDBC \u7684defaultDrive\u5207\u6362\u81f3Phoenix Phoenix\u589e\u52a0\u6570\u636e \u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2 \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') \u67e5\u770b\u589e\u52a0\u7684\u6570\u636e\uff1a Phoenix\u5220\u9664\u6570\u636e \u9875\u9762\u4e0a\u5220\u9664\uff1a\u9009\u62e9\u5f85\u5220\u9664\u7684\u5217\uff0c\u7136\u540e\u70b9\u51fb\u4e0b\u65b9 \u5220\u9664 \u6309\u94ae\uff0c\u7136\u540e\u70b9\u51fb save \u6309\u94ae\uff1a \u811a\u672c\u5220\u9664\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae delete from TB_PHOENIX where ID=104; \u67e5\u770b\u8f93\u51fa\u540e\u7684\u6570\u636e Phoenix\u66f4\u65b0\u6570\u636e, \u7f16\u8f91\u66f4\u65b0\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') \u67e5\u770b\u66f4\u65b0\u540e\u7684\u6570\u636e\uff1a \u67e5\u770b\u6570\u636e\uff1a\u7f16\u8f91\u67e5\u8be2\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae\u3002 SELECT * FROM TB_PHOENIX","title":"\u5bf9\u63a5DBeaver"},{"location":"Development/DBeaver/#dbeaverfusioninsight","text":"","title":"DBeaver\u5bf9\u63a5FusionInsight"},{"location":"Development/DBeaver/#_1","text":"DBeaver 4.0.8 \u2194 FusionInsight HD V100R002C60U20 DBeaver 4.2.1 \u2194 FusionInsight HD V100R002C70SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DBeaver/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DBeaver\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/DBeaver/#linuxdbeaverfiber","text":"","title":"Linux\u4e0bDBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver/#_3","text":"\u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver/#_4","text":"\u5df2\u7ecf\u5b89\u88c5\u597dLinux\uff08Redhat Linux Enterprise 6.5 64bit\uff09Desktop\u64cd\u4f5c\u7cfb\u7edf\uff1b \u5df2\u7ecf\u5b89\u88c5\u597d\u7684Linux\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver/#_5","text":"\u5b89\u88c5jdk1.8\uff0cDBeaver4.0.8\u9700\u8981jdk1.8\u4ee5\u4e0a\u7248\u672c tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf/etc/profile\uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH \u4e0b\u8f7d\u5730\u5740\uff1a http://dbeaver.jkiss.org/download/ , \u8f6f\u4ef6 dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \uff0c\u5b89\u88c5DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u4ea7\u54c1\u6587\u6863\u300b\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \uff0c\u5176\u4e2dFiber\u5ba2\u6237\u7aef\u76ee\u5f55 /opt/hadoopclient/Fiber/ \u3002 \u4fee\u6539Fiber\u7684\u914d\u7f6e\u6587\u4ef6 /opt/hadoopclient/Fiber/conf/fiber.xml \uff0c\u5c06\u5176\u4e2dhive\u3001spark\u3001phoenix\u7684\u8ba4\u8bc1\u65b9\u5f0f\u6539\u4e3a\u5b89\u5168\u6a21\u5f0fkeytab\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u6cd5\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 Hive JDBC\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>{HIVE_CLIENT_ZK_PRINCIPAL}</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Spark\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Phoenix\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> jaas.conf\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u6253\u5f00DBeaver\uff0c\u8fdb\u5165DBeaver\u7684\u5b89\u88c5\u76ee\u5f55\u6267\u884c ./dbeaver \uff0c\u542f\u52a8dbeaver \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef /opt/hadoopclient/Fiber/lib/ \u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027\uff1a \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection , \u7c7b\u578b\u9009\u62e9Fiber User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u914d\u7f6eDriver properties\u91cc\u9762\u7684defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\uff0c\u70b9\u51fbnext Network\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u8f93\u5165\u81ea\u5b9a\u4e49Connection name\u540e\uff0c\u70b9\u51fb finish , \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u94fe\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u94fe\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver/#windowsdbeaverfiber","text":"","title":"Windows\u4e0bDBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver/#_6","text":"\u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver/#_7","text":"Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5b8c\u6210windows\u4e0a\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver/#_8","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u53ef\u4ee5\u7528kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\uff0c\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002kinit\u8ba4\u8bc1\u7684\u6709\u6548\u671f\u662f24\u5c0f\u65f6\uff0ckeytab\u8ba4\u8bc1\u65b9\u5f0f\u957f\u671f\u6709\u6548 - \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e - \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver/#kinit","text":"\u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\uff0c\u5e76\u5b89\u88c5 http://web.mit.edu/kerberos/dist/#kfw-4.0 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u89d2\u8272\u548c\u4eba\u673a\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u7cfb\u7edf\u8bbe\u7f6e -> \u6743\u9650\u7ba1\u7406 -> \u7528\u6237\u7ba1\u7406 -> \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\uff0c\u521b\u5efa\u7528\u6237\u201ctest\u201d \u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6 user.keytab \u4ee5\u53ca krb5.conf \u6587\u4ef6\uff0c\u628a krb5.conf \u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d(\u5982\uff1a test@HADOOP.COM )\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; \u4fee\u6539 fiber.xml \u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5\u524d\u786e\u8ba4kerberos\u8ba4\u8bc1\u6709\u6548","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver/#keytab","text":"\u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab\u548chbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver/#dbeaverfiber","text":"\u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef\uff08 /opt/hadoopclient/Fiber/lib/ \uff09\u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027 \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u786e\u8ba4defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\u3002 Network\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u81ea\u5b9a\u4e49Connection name\uff0c\u70b9\u51fbfinish \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u8fde\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u8fde\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e","title":"DBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver/#dbeaverfiber_1","text":"","title":"DBeaver\u5bf9\u63a5Fiber\u529f\u80fd\u9a8c\u8bc1"},{"location":"Development/DBeaver/#hive","text":"\u5c06JDBC\u7684defaultDrive\u5207\u6362\u81f3Hive Hive\u67e5\u8be2\u6570\u636e\uff1a\u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 \u67e5\u770b\u66f4\u65b0\u540e\u6570\u636e\uff1a","title":"Hive\u589e\u52a0\u67e5\u770b\u6570\u636e"},{"location":"Development/DBeaver/#spark","text":"\u5c06JDBC \u7684defaultDriver\u5207\u6362\u81f3Spark Spark\u67e5\u8be2\u6570\u636e\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Spark\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3Spark\u7684JDBCServer(\u4e3b)\u5b9e\u4f8b\u6240\u5728\u7684\u8282\u70b9\u7684/opt/\u76ee\u5f55\u4e0b \u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"Spark\u589e\u52a0\u67e5\u770b\u6570\u636e"},{"location":"Development/DBeaver/#phoenix","text":"\u5c06JDBC \u7684defaultDrive\u5207\u6362\u81f3Phoenix Phoenix\u589e\u52a0\u6570\u636e \u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2 \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') \u67e5\u770b\u589e\u52a0\u7684\u6570\u636e\uff1a Phoenix\u5220\u9664\u6570\u636e \u9875\u9762\u4e0a\u5220\u9664\uff1a\u9009\u62e9\u5f85\u5220\u9664\u7684\u5217\uff0c\u7136\u540e\u70b9\u51fb\u4e0b\u65b9 \u5220\u9664 \u6309\u94ae\uff0c\u7136\u540e\u70b9\u51fb save \u6309\u94ae\uff1a \u811a\u672c\u5220\u9664\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae delete from TB_PHOENIX where ID=104; \u67e5\u770b\u8f93\u51fa\u540e\u7684\u6570\u636e Phoenix\u66f4\u65b0\u6570\u636e, \u7f16\u8f91\u66f4\u65b0\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') \u67e5\u770b\u66f4\u65b0\u540e\u7684\u6570\u636e\uff1a \u67e5\u770b\u6570\u636e\uff1a\u7f16\u8f91\u67e5\u8be2\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae\u3002 SELECT * FROM TB_PHOENIX","title":"Phoenix\u589e\u5220\u6539\u67e5\u6570\u636e"},{"location":"Development/DbVisualizer/","text":"DbVisualizer\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DbVisualizer 9.5.7 <-> FusionInsight HD V100R002C60U20 DbVisualizer 10.0.1 <-> FusionInsight HD V100R002C70SPC200 \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DbVisualizer\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 DbVisualizer\u5b89\u88c5 \u00b6 DbVisualizer9.5.7\u9700\u8981jdk1.8\uff0c\u4e0b\u8f7d\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u4fee\u6539C:\\Windows\\System32\\drivers\\etc\\hosts\u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982C:\\Fiber\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dDbVisualizer\uff0c\u5730\u5740\uff1a http://www.dbvis.com/download/ \uff0c\u4e0b\u8f7d\u8f6f\u4ef6dbvis_windows-x64_9_5_7_jre.exe \u53cc\u51fbdbvis_windows-x64_9_5_7_jre.exe\u5b89\u88c5 DbVisualizer\u8fde\u63a5Fiber \u00b6 \u914d\u7f6eDbVisualizer\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001Spark\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00DbVisualizer9.5.7\uff0c\u70b9\u51fb Cancel \u83dc\u5355\u680f\u9009\u62e9 ToolsDriver Manager \u65b0\u5efadriver Name\uff1aFiber(\u81ea\u5b9a\u4e49) URL Format\uff1ajdbc:fiber:// User Specified\uff1a\u5c06C:\\Fiber\\lib\\\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Driver Class\uff1a\u52a0\u5165jar\u5305\u540e\u9009\u62e9com.huawei.fiber.FiberDriver \u83dc\u5355\u680f Database -> Create Database Connection \u9009\u62e9 Use Wizard {width=\"4.2in\" height=\"1.4in\"} \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982Fiber \u9009\u62e9Driver Fiber \u586b\u5199URL\uff1ajdbc:fiber:// \u70b9\u51fb Finish \u67e5\u8be2Hive\u8868\u6570\u636e \u00b6 \u6253\u5f00 Properties \u9762\u677f\uff0c\u586b\u5199defaultDriver\u548cfiberconfig\u5c5e\u6027\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\uff0c\u53ef\u4ee5\u5728\u5de6\u4fa7\u770b\u5230hive\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 \u67e5\u8be2SparkSQL\u4e2d\u7684\u6570\u636e \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aspark\uff1a\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aspark\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00Connection\u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230SparkSQL\u4e2d\u7684\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 \u67e5\u8be2Phoenix\u4e2d\u7684\u6570\u636e \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aphoenix\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230phoenix\u6570\u636e\u8868 \u67e5\u770bphoenix\u8868TB_PHOENIX\u4e2d\u7684\u6570\u636e\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e \u00b6 Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e\uff0c\u9700\u8981\u5728Fiber\u4e2dhbase\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u4e2d\u52a0\u5165\u5982\u4e0b\u53c2\u6570\uff0c\u5426\u5219\u4e0d\u4f1a\u81ea\u52a8Commit \u4fee\u6539Hbase-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \uff0c\u7136\u540e\u91cd\u542fDbVisualizer\u3002 <property> <name>phoenix.connection.autoCommit</name> <value>true</value> </property> Phoenix\u8868\u589e\u52a0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix\u8868\u5220\u9664\u6570\u636e delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix\u8868\u66f4\u65b0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix;","title":"\u5bf9\u63a5DbVisualizer"},{"location":"Development/DbVisualizer/#dbvisualizerfusioninsight","text":"","title":"DbVisualizer\u5bf9\u63a5FusionInsight"},{"location":"Development/DbVisualizer/#_1","text":"DbVisualizer 9.5.7 <-> FusionInsight HD V100R002C60U20 DbVisualizer 10.0.1 <-> FusionInsight HD V100R002C70SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DbVisualizer/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DbVisualizer\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/DbVisualizer/#dbvisualizer","text":"DbVisualizer9.5.7\u9700\u8981jdk1.8\uff0c\u4e0b\u8f7d\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u4fee\u6539C:\\Windows\\System32\\drivers\\etc\\hosts\u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982C:\\Fiber\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dDbVisualizer\uff0c\u5730\u5740\uff1a http://www.dbvis.com/download/ \uff0c\u4e0b\u8f7d\u8f6f\u4ef6dbvis_windows-x64_9_5_7_jre.exe \u53cc\u51fbdbvis_windows-x64_9_5_7_jre.exe\u5b89\u88c5","title":"DbVisualizer\u5b89\u88c5"},{"location":"Development/DbVisualizer/#dbvisualizerfiber","text":"\u914d\u7f6eDbVisualizer\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001Spark\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00DbVisualizer9.5.7\uff0c\u70b9\u51fb Cancel \u83dc\u5355\u680f\u9009\u62e9 ToolsDriver Manager \u65b0\u5efadriver Name\uff1aFiber(\u81ea\u5b9a\u4e49) URL Format\uff1ajdbc:fiber:// User Specified\uff1a\u5c06C:\\Fiber\\lib\\\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Driver Class\uff1a\u52a0\u5165jar\u5305\u540e\u9009\u62e9com.huawei.fiber.FiberDriver \u83dc\u5355\u680f Database -> Create Database Connection \u9009\u62e9 Use Wizard {width=\"4.2in\" height=\"1.4in\"} \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982Fiber \u9009\u62e9Driver Fiber \u586b\u5199URL\uff1ajdbc:fiber:// \u70b9\u51fb Finish","title":"DbVisualizer\u8fde\u63a5Fiber"},{"location":"Development/DbVisualizer/#hive","text":"\u6253\u5f00 Properties \u9762\u677f\uff0c\u586b\u5199defaultDriver\u548cfiberconfig\u5c5e\u6027\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\uff0c\u53ef\u4ee5\u5728\u5de6\u4fa7\u770b\u5230hive\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2Hive\u8868\u6570\u636e"},{"location":"Development/DbVisualizer/#sparksql","text":"\u5c06defaultDriver\u5207\u6362\u4e3aspark\uff1a\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aspark\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00Connection\u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230SparkSQL\u4e2d\u7684\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2SparkSQL\u4e2d\u7684\u6570\u636e"},{"location":"Development/DbVisualizer/#phoenix","text":"\u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aphoenix\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230phoenix\u6570\u636e\u8868 \u67e5\u770bphoenix\u8868TB_PHOENIX\u4e2d\u7684\u6570\u636e\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2Phoenix\u4e2d\u7684\u6570\u636e"},{"location":"Development/DbVisualizer/#phoenix_1","text":"Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e\uff0c\u9700\u8981\u5728Fiber\u4e2dhbase\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u4e2d\u52a0\u5165\u5982\u4e0b\u53c2\u6570\uff0c\u5426\u5219\u4e0d\u4f1a\u81ea\u52a8Commit \u4fee\u6539Hbase-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \uff0c\u7136\u540e\u91cd\u542fDbVisualizer\u3002 <property> <name>phoenix.connection.autoCommit</name> <value>true</value> </property> Phoenix\u8868\u589e\u52a0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix\u8868\u5220\u9664\u6570\u636e delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix\u8868\u66f4\u65b0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix;","title":"Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e"},{"location":"Development/Jupyter_Notebook/","text":"Jupyter_Notebook\u5bf9\u63a5FusionInsight \u00b6 \u5b89\u88c5Jupyter notebook \u00b6 Jupyter notebook\u7684\u5b89\u88c5\u4f9d\u8d56\u4e8ePython\uff0c\u4e14\u6d89\u53ca\u5230\u8bb8\u591a\u5de5\u5177\u7684\u4f9d\u8d56\u5305\uff0c\u76f8\u4e92\u4e4b\u95f4\u8fd8\u5b58\u5728\u7248\u672c\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u8f83\u9ebb\u70e6\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u5b89\u88c5Anaconda\u5305\uff0c\u91cc\u9762\u5305\u542b\u4e86Python\u3001Jupyter Notebook\uff0c\u4ee5\u53ca\u4f17\u591a\u7684\u79d1\u5b66\u5de5\u5177\u5305\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u5b89\u88c5Anaconda \u4eceAnaconda\u5b98\u7f51\u4e0b\u8f7d\u5e76\u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh \u751f\u6210Jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 jupyter notebook --generate-config --allow-root \u4fee\u6539Jupyter notebook\u7684\u914d\u7f6eIPc.NotebookApp.ip\u4e3a\u672c\u673aIP\u5730\u5740 vi /root/.jupyter/jupyter_notebook_config.py \u542f\u52a8Jupyter notebook:: jupyter notebook --allow-root \u51fa\u73b0\u5982\u4e0b\u63d0\u793a\u8868\u793aJupyter notebook\u542f\u52a8\u6210\u529f [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 \u4f7f\u7528 Ctrl+C \u53ef\u4ee5\u9000\u51faJupyter notebook \u5b89\u88c5FusionInsight Client \u00b6 \u53c2\u8003FusionInsight\u7684\u4ea7\u54c1\u6587\u6863\u5b8c\u6210Linux\u4e0b\u7684FusionInsight\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5b89\u88c5\u5230 /opt/hadoopclient \u76ee\u5f55 \u5b8c\u6210Kerberos\u8ba4\u8bc1 \u00b6 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /opt/hadoopclient/ source bigdata_env kinit sparkuser \u5bfc\u5165ipython\u76f8\u5173\u73af\u5883\u53d8\u91cf \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165\u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u4e24\u884c\u6dfb\u52a0\u5230 /opt/hadoopclient/bigdata_env\u6587\u4ef6 \uff0c\u540e\u7eedsource bigdata_env\u65f6\u53ef\u4ee5\u81ea\u52a8\u5c06\u73af\u5883\u53d8\u91cf\u5bfc\u5165 export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" Jupyter notebook\u4e2d\u4f7f\u7528pyspark\u8fdb\u884c\u5206\u6790 \u00b6 \u6267\u884cpyspark\u4f1a\u81ea\u52a8\u542f\u52a8Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 \u6253\u5f00\u4e0a\u8ff0\u94fe\u63a5\uff0c\u53ef\u4ee5\u8fdb\u884c\u6570\u636e\u5206\u6790 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10-1.2.0\") sqlContext <- sparkRSQL.init(sc) flightsDF <- read.df(sqlContext, \"/user/sparkuser/flights.csv\", source = \"com.databricks.spark.csv\", header = \"true\") destDF <- select(flightsDF, \"dest\", \"cancelled\") groupBy(flightsDF, flightsDF$date) %>% summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10) Jupyter notebook\u4e2d\u4f7f\u7528R\u8bed\u8a00\u8fdb\u884c\u5206\u6790 \u00b6 TBD","title":"\u5bf9\u63a5Jypyter Notebook"},{"location":"Development/Jupyter_Notebook/#jupyter_notebookfusioninsight","text":"","title":"Jupyter_Notebook\u5bf9\u63a5FusionInsight"},{"location":"Development/Jupyter_Notebook/#jupyter-notebook","text":"Jupyter notebook\u7684\u5b89\u88c5\u4f9d\u8d56\u4e8ePython\uff0c\u4e14\u6d89\u53ca\u5230\u8bb8\u591a\u5de5\u5177\u7684\u4f9d\u8d56\u5305\uff0c\u76f8\u4e92\u4e4b\u95f4\u8fd8\u5b58\u5728\u7248\u672c\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u8f83\u9ebb\u70e6\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u5b89\u88c5Anaconda\u5305\uff0c\u91cc\u9762\u5305\u542b\u4e86Python\u3001Jupyter Notebook\uff0c\u4ee5\u53ca\u4f17\u591a\u7684\u79d1\u5b66\u5de5\u5177\u5305\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u5b89\u88c5Anaconda \u4eceAnaconda\u5b98\u7f51\u4e0b\u8f7d\u5e76\u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh \u751f\u6210Jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 jupyter notebook --generate-config --allow-root \u4fee\u6539Jupyter notebook\u7684\u914d\u7f6eIPc.NotebookApp.ip\u4e3a\u672c\u673aIP\u5730\u5740 vi /root/.jupyter/jupyter_notebook_config.py \u542f\u52a8Jupyter notebook:: jupyter notebook --allow-root \u51fa\u73b0\u5982\u4e0b\u63d0\u793a\u8868\u793aJupyter notebook\u542f\u52a8\u6210\u529f [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 \u4f7f\u7528 Ctrl+C \u53ef\u4ee5\u9000\u51faJupyter notebook","title":"\u5b89\u88c5Jupyter notebook"},{"location":"Development/Jupyter_Notebook/#fusioninsight-client","text":"\u53c2\u8003FusionInsight\u7684\u4ea7\u54c1\u6587\u6863\u5b8c\u6210Linux\u4e0b\u7684FusionInsight\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5b89\u88c5\u5230 /opt/hadoopclient \u76ee\u5f55","title":"\u5b89\u88c5FusionInsight Client"},{"location":"Development/Jupyter_Notebook/#kerberos","text":"\u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /opt/hadoopclient/ source bigdata_env kinit sparkuser","title":"\u5b8c\u6210Kerberos\u8ba4\u8bc1"},{"location":"Development/Jupyter_Notebook/#ipython","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165\u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u4e24\u884c\u6dfb\u52a0\u5230 /opt/hadoopclient/bigdata_env\u6587\u4ef6 \uff0c\u540e\u7eedsource bigdata_env\u65f6\u53ef\u4ee5\u81ea\u52a8\u5c06\u73af\u5883\u53d8\u91cf\u5bfc\u5165 export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\"","title":"\u5bfc\u5165ipython\u76f8\u5173\u73af\u5883\u53d8\u91cf"},{"location":"Development/Jupyter_Notebook/#jupyter-notebookpyspark","text":"\u6267\u884cpyspark\u4f1a\u81ea\u52a8\u542f\u52a8Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 \u6253\u5f00\u4e0a\u8ff0\u94fe\u63a5\uff0c\u53ef\u4ee5\u8fdb\u884c\u6570\u636e\u5206\u6790 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10-1.2.0\") sqlContext <- sparkRSQL.init(sc) flightsDF <- read.df(sqlContext, \"/user/sparkuser/flights.csv\", source = \"com.databricks.spark.csv\", header = \"true\") destDF <- select(flightsDF, \"dest\", \"cancelled\") groupBy(flightsDF, flightsDF$date) %>% summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10)","title":"Jupyter notebook\u4e2d\u4f7f\u7528pyspark\u8fdb\u884c\u5206\u6790"},{"location":"Development/Jupyter_Notebook/#jupyter-notebookr","text":"TBD","title":"Jupyter notebook\u4e2d\u4f7f\u7528R\u8bed\u8a00\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/","text":"RSutdio\u5bf9\u63a5FusionInsight Spark \u00b6 \u9002\u7528\u573a\u666f \u00b6 R-3.4.1 \u2194 FusionInsight HD V100R002C60U10 R-3.4.1 \u2194 FusionInsight HD V100R002C70SPC100 \u5bf9\u63a5\u65b9\u5f0f \u00b6 RStudio\u4e0eSpark\u96c6\u6210\u6709\u4e24\u79cd\u65b9\u5f0f\uff1a \u901a\u8fc7RStudio\u5b98\u65b9\u53d1\u5e03\u7684sparklyr\u4e0eSpark\u8fdb\u884c\u96c6\u6210 \u901a\u8fc7Apache Spark\u793e\u533a\u53d1\u5e03\u7684SparkR\u8fdb\u884c\u96c6\u6210 \u672c\u6587\u6863\u5305\u542b\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5\u7684\u6b65\u9aa4, \u76f8\u5173\u5bf9\u63a5\u6b65\u9aa4\u5982\u4e0b\uff1a \u5b89\u88c5R \u5b89\u88c5RStudio Server \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x) \u5b89\u88c5R \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728RStudio\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u672c\u6587\u4f7f\u7528\u7684RStudio\u8282\u70b9\u4e3aRedhat7.1\uff0cFusionInsight\u96c6\u7fa4\u8282\u70b9\u4e3aRedhat6.6 \u914d\u7f6eredhat\u7684yum\u6e90\uff0c\u56fd\u5185\u53ef\u4ee5\u914d\u7f6e aliyun\u7684\u6e90 \u6216\u8005 163\u7684\u6e90 \u914d\u7f6eEPEL\u7684\u6e90 \u5b89\u88c5R-3.4.1 \u914d\u7f6ealiyun\u7684\u6e90 \u00b6 \u914d\u7f6e\u597dRedhat7.1\u7684yum\u6e90 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache \u914d\u7f6e163\u7684\u6e90 \u00b6 \u914d\u7f6e\u597dRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 \u00b6 \u5b89\u88c5EPEL\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u5b89\u88c5R-3.4.1 \u00b6 \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u5b89\u88c5RStudio Server \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm \u4f7f\u7528 vi /etc/rstudio/rserver.conf \u4fee\u6539RStudio\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u6307\u5b9aRStudio Server\u4f7f\u7528\u7684R\u7684\u8def\u5f84 rsession-which-r=/usr/bin/R \u91cd\u542frstudio-server\u540e\uff0c\u67e5\u770b\u670d\u52a1\u662f\u5426\u6b63\u5e38 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server \u670d\u52a1\u6b63\u5e38\u542f\u52a8\u5982\u4e0b \u7531\u4e8eRStudio Server\u4e0d\u5141\u8bb8\u4f7f\u7528root\u7528\u6237\u767b\u9646\uff0c\u9700\u8981\u65b0\u5efa\u4e00\u4e2a\u666e\u901a\u7528\u6237\u7528\u4e8eWeb\u754c\u9762\u7684\u767b\u9646 useradd -d /home/test -m test passwd test \u7528\u6237\u65b0\u5efa\u5b8c\u6210\u540e\uff0c\u5173\u95ed\u9632\u706b\u5899\uff0c\u7136\u540e\u4f7f\u7528\u672c\u673aip:8787\u7aef\u53e3\u8bbf\u95eeRStudio Server\uff0c\u4f7f\u7528\u65b0\u5efa\u7684test\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 sudo systemctl stop firewalld \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u00b6 \u767b\u5f55FusionInsight Manager\u7cfb\u7edf\uff0c\u5355\u51fb \u670d\u52a1\u7ba1\u7406 \uff0c\u5728\u83dc\u5355\u680f\u4e2d\u5355\u51fb \u4e0b\u8f7d\u5ba2\u6237\u7aef , \u5ba2\u6237\u7aef\u7c7b\u578b\u52fe\u9009 \u5b8c\u6574\u5ba2\u6237\u7aef , \u662f\u5426\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e2d\u751f\u6210\u5ba2\u6237\u7aef\u6587\u4ef6\u9009\u62e9 \u5426 \u4f7f\u7528WinSCP\u5de5\u5177\u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684\u8f6f\u4ef6\u5305\u4e0a\u4f20\u5230Linux\u670d\u52a1\u5668\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u5207\u6362\u5230\u65b0\u5efa\u7684test\u7528\u6237 su test \u89e3\u538b\u8f6f\u4ef6\u5305\u3002\u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u3002\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u89e3\u538b\u5b89\u88c5\u5305\u5230\u672c\u5730\u76ee\u5f55 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar \u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u5ba2\u6237\u7aef\u5230\u6307\u5b9a\u76ee\u5f55\uff08\u7edd\u5bf9\u8def\u5f84\uff09\uff0c\u4f8b\u5982\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient \u5ba2\u6237\u7aef\u5c06\u88ab\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55\u4e2d \u68c0\u67e5\u5ba2\u6237\u7aef\u8282\u70b9\u4e0eFusionInsight\u96c6\u7fa4\u65f6\u95f4\u540c\u6b65\uff08\u5dee\u8ddd\u4e0d\u80fd\u8d85\u8fc75\u5206\u949f\uff09 \u68c0\u67e5SparkR\u662f\u5426\u53ef\u7528 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u6267\u884c sparkR \u542f\u52a8SparkR, \u6b63\u5e38\u542f\u52a8\u51fa\u73b0\u4ee5\u4e0b\u754c\u9762 \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u00b6 \u4f7f\u7528\u65b0\u5efa\u7684\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u754c\u9762\u4e2d\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u521d\u59cb\u5316SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10:1.2.0\") sqlContext <- sparkRSQL.init(sc) \u521d\u59cb\u5316\u6210\u529f\u540e\u5982\u4e0b\u56fe \u5728Yarn\u7684ResourceManager\u754c\u9762\u53ef\u4ee5\u770b\u5230sparkuser\u5728\u96c6\u7fa4\u542f\u52a8\u4e86\u4e00\u4e2aSparkR\u7684\u5e94\u7528 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u00b6 R DataFrame \u8f6c\u5316\u4e3aSparkR DataFrame df <- createDataFrame(sqlContext, faithful) head(df) \u901a\u8fc7JSON\u6587\u4ef6\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790\u5904\u7406 \u5c06\u6d4b\u8bd5\u6570\u636eput\u5230HDFS\u4e2d wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ \u6267\u884c\u6587\u4ef6\u52a0\u8f7d\u5206\u6790 people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") head(people) printSchema(people) \u4eceHive\u8868\u4e2d\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790 hiveContext <- sparkRHive.init(sc) results <- sql(hiveContext, \"SELECT * FROM employees\") head(results) DataFrame Operations Selecting rows, columns df <- createDataFrame(sqlContext, faithful) df head(select(df, df$eruptions)) head(select(df, \"eruptions\")) head(filter(df, df$waiting < 50)) Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting)) head(arrange(waiting_counts, desc(waiting_counts$count))) Operating on Columns df$waiting_secs <- df$waiting * 60 head(df) Running SQL Queries from SparkR people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") registerTempTable(people, \"people\") teenagers <- sql(sqlContext, \"SELECT name FROM people WHERE age >= 13 AND age <= 19\") head(teenagers) Machine Learning df <- createDataFrame(sqlContext, iris) model <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\") summary(model) predictions <- predict(model, newData = df) head(select(predictions, \"Sepal_Length\", \"prediction\")) \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u00b6 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u4e2d\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5b89\u88c5\u6240\u9700\u7684library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") \u901a\u8fc7spark_connect\u8fde\u63a5spark\u96c6\u7fa4 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") \u8fd9\u91cc\u5982\u679cSPARK_HOME\u6307\u5411/home/test/hadoopclient/Spark/spark\uff0c\u540c\u65f6\u8bbe\u7f6eversion\u4e3a1.6.1\uff0c\u5219\u4f1a\u5bf9\u63a5\u4e0a1.5.1\u7684Spark sparklyr\u5b98\u65b9\u652f\u6301\u662f1.6.1\u4ee5\u4e0a\u7684Spark\uff0c\u8fd9\u91cc\u5f3a\u5236\u6307\u5b9aversion\u4e3a1.6.1\uff0c\u4e3b\u8981\u529f\u80fd\u5747\u6b63\u5e38\uff0c\u90e8\u5206Spark1.6.1\u652f\u6301\u800c1.5.1\u4e0d\u652f\u6301\u7684\u7279\u6027\u6267\u884c\u4f1a\u5931\u8d25 \u542f\u52a8\u6210\u529f\u540e\uff0c\u5728FusionInsgiht\u7684Yarn\u7684ResourceManager\u9875\u9762\u53ef\u4ee5\u770b\u5230sparklyr\u7684\u4efb\u52a1\u5df2\u7ecf\u542f\u52a8 \u5728RStudio\u7684Spark\u9762\u677f\u5237\u65b0\u4e00\u4e0b\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u6709hive\u7684\u8868 \u9009\u62e9hive\u8868\u53f3\u8fb9\u7684\u6570\u636e\u56fe\u8868\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u00b6 Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc <- spark_connect(master = \"yarn-client\", version = \"1.6.1\", spark_home = \"/home/test/hadoopclient/Spark/spark\") Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl <- copy_to(sc, babynames, \"babynames\") applicants_tbl <- copy_to(sc, applicants, \"applicants\") birthsYearly <- applicants_tbl %>% mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl <- filteredNames_tbl %>% group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl <- yearlyNames_tbl %>% filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly <- yearlyNames_tbl %>% inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl <- yearlyNames_tbl %>% filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly <- yearlyNames_tbl %>% inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName <- babynames_tbl %>% mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE) \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x) \u00b6 Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) ** Assess model performance** Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { sdf_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- sdf_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights. FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u00b6 \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167 \u914d\u7f6eEPEL\u7684\u6e90\u5b89\u88c5R \u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u5b89\u88c5sparklyr\u62a5\u9519configuration failed for package \u2018openssl\u2019 \u00b6 \u64cd\u4f5c\u7cfb\u7edf\u9700\u8981\u6267\u884c yum install openssl-devel \u5b89\u88c5openssl-devel \u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/sparkuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports ;","title":"\u5bf9\u63a5RStudio"},{"location":"Development/RStudio/#rsutdiofusioninsight-spark","text":"","title":"RSutdio\u5bf9\u63a5FusionInsight Spark"},{"location":"Development/RStudio/#_1","text":"R-3.4.1 \u2194 FusionInsight HD V100R002C60U10 R-3.4.1 \u2194 FusionInsight HD V100R002C70SPC100","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/RStudio/#_2","text":"RStudio\u4e0eSpark\u96c6\u6210\u6709\u4e24\u79cd\u65b9\u5f0f\uff1a \u901a\u8fc7RStudio\u5b98\u65b9\u53d1\u5e03\u7684sparklyr\u4e0eSpark\u8fdb\u884c\u96c6\u6210 \u901a\u8fc7Apache Spark\u793e\u533a\u53d1\u5e03\u7684SparkR\u8fdb\u884c\u96c6\u6210 \u672c\u6587\u6863\u5305\u542b\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5\u7684\u6b65\u9aa4, \u76f8\u5173\u5bf9\u63a5\u6b65\u9aa4\u5982\u4e0b\uff1a \u5b89\u88c5R \u5b89\u88c5RStudio Server \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x)","title":"\u5bf9\u63a5\u65b9\u5f0f"},{"location":"Development/RStudio/#r","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728RStudio\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u672c\u6587\u4f7f\u7528\u7684RStudio\u8282\u70b9\u4e3aRedhat7.1\uff0cFusionInsight\u96c6\u7fa4\u8282\u70b9\u4e3aRedhat6.6 \u914d\u7f6eredhat\u7684yum\u6e90\uff0c\u56fd\u5185\u53ef\u4ee5\u914d\u7f6e aliyun\u7684\u6e90 \u6216\u8005 163\u7684\u6e90 \u914d\u7f6eEPEL\u7684\u6e90 \u5b89\u88c5R-3.4.1","title":"\u5b89\u88c5R"},{"location":"Development/RStudio/#aliyun","text":"\u914d\u7f6e\u597dRedhat7.1\u7684yum\u6e90 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache","title":"\u914d\u7f6ealiyun\u7684\u6e90"},{"location":"Development/RStudio/#163","text":"\u914d\u7f6e\u597dRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache","title":"\u914d\u7f6e163\u7684\u6e90"},{"location":"Development/RStudio/#epel","text":"\u5b89\u88c5EPEL\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache","title":"\u914d\u7f6eEPEL\u7684\u6e90"},{"location":"Development/RStudio/#r-341","text":"\u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a","title":"\u5b89\u88c5R-3.4.1"},{"location":"Development/RStudio/#rstudio-server","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm \u4f7f\u7528 vi /etc/rstudio/rserver.conf \u4fee\u6539RStudio\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u6307\u5b9aRStudio Server\u4f7f\u7528\u7684R\u7684\u8def\u5f84 rsession-which-r=/usr/bin/R \u91cd\u542frstudio-server\u540e\uff0c\u67e5\u770b\u670d\u52a1\u662f\u5426\u6b63\u5e38 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server \u670d\u52a1\u6b63\u5e38\u542f\u52a8\u5982\u4e0b \u7531\u4e8eRStudio Server\u4e0d\u5141\u8bb8\u4f7f\u7528root\u7528\u6237\u767b\u9646\uff0c\u9700\u8981\u65b0\u5efa\u4e00\u4e2a\u666e\u901a\u7528\u6237\u7528\u4e8eWeb\u754c\u9762\u7684\u767b\u9646 useradd -d /home/test -m test passwd test \u7528\u6237\u65b0\u5efa\u5b8c\u6210\u540e\uff0c\u5173\u95ed\u9632\u706b\u5899\uff0c\u7136\u540e\u4f7f\u7528\u672c\u673aip:8787\u7aef\u53e3\u8bbf\u95eeRStudio Server\uff0c\u4f7f\u7528\u65b0\u5efa\u7684test\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 sudo systemctl stop firewalld","title":"\u5b89\u88c5RStudio Server"},{"location":"Development/RStudio/#fusioninsight","text":"\u767b\u5f55FusionInsight Manager\u7cfb\u7edf\uff0c\u5355\u51fb \u670d\u52a1\u7ba1\u7406 \uff0c\u5728\u83dc\u5355\u680f\u4e2d\u5355\u51fb \u4e0b\u8f7d\u5ba2\u6237\u7aef , \u5ba2\u6237\u7aef\u7c7b\u578b\u52fe\u9009 \u5b8c\u6574\u5ba2\u6237\u7aef , \u662f\u5426\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e2d\u751f\u6210\u5ba2\u6237\u7aef\u6587\u4ef6\u9009\u62e9 \u5426 \u4f7f\u7528WinSCP\u5de5\u5177\u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684\u8f6f\u4ef6\u5305\u4e0a\u4f20\u5230Linux\u670d\u52a1\u5668\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u5207\u6362\u5230\u65b0\u5efa\u7684test\u7528\u6237 su test \u89e3\u538b\u8f6f\u4ef6\u5305\u3002\u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u3002\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u89e3\u538b\u5b89\u88c5\u5305\u5230\u672c\u5730\u76ee\u5f55 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar \u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u5ba2\u6237\u7aef\u5230\u6307\u5b9a\u76ee\u5f55\uff08\u7edd\u5bf9\u8def\u5f84\uff09\uff0c\u4f8b\u5982\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient \u5ba2\u6237\u7aef\u5c06\u88ab\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55\u4e2d \u68c0\u67e5\u5ba2\u6237\u7aef\u8282\u70b9\u4e0eFusionInsight\u96c6\u7fa4\u65f6\u95f4\u540c\u6b65\uff08\u5dee\u8ddd\u4e0d\u80fd\u8d85\u8fc75\u5206\u949f\uff09 \u68c0\u67e5SparkR\u662f\u5426\u53ef\u7528 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u6267\u884c sparkR \u542f\u52a8SparkR, \u6b63\u5e38\u542f\u52a8\u51fa\u73b0\u4ee5\u4e0b\u754c\u9762","title":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef"},{"location":"Development/RStudio/#sparkrrstudio","text":"\u4f7f\u7528\u65b0\u5efa\u7684\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u754c\u9762\u4e2d\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u521d\u59cb\u5316SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10:1.2.0\") sqlContext <- sparkRSQL.init(sc) \u521d\u59cb\u5316\u6210\u529f\u540e\u5982\u4e0b\u56fe \u5728Yarn\u7684ResourceManager\u754c\u9762\u53ef\u4ee5\u770b\u5230sparkuser\u5728\u96c6\u7fa4\u542f\u52a8\u4e86\u4e00\u4e2aSparkR\u7684\u5e94\u7528","title":"\u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/#rstudiosparkr","text":"R DataFrame \u8f6c\u5316\u4e3aSparkR DataFrame df <- createDataFrame(sqlContext, faithful) head(df) \u901a\u8fc7JSON\u6587\u4ef6\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790\u5904\u7406 \u5c06\u6d4b\u8bd5\u6570\u636eput\u5230HDFS\u4e2d wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ \u6267\u884c\u6587\u4ef6\u52a0\u8f7d\u5206\u6790 people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") head(people) printSchema(people) \u4eceHive\u8868\u4e2d\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790 hiveContext <- sparkRHive.init(sc) results <- sql(hiveContext, \"SELECT * FROM employees\") head(results) DataFrame Operations Selecting rows, columns df <- createDataFrame(sqlContext, faithful) df head(select(df, df$eruptions)) head(select(df, \"eruptions\")) head(filter(df, df$waiting < 50)) Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting)) head(arrange(waiting_counts, desc(waiting_counts$count))) Operating on Columns df$waiting_secs <- df$waiting * 60 head(df) Running SQL Queries from SparkR people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") registerTempTable(people, \"people\") teenagers <- sql(sqlContext, \"SELECT name FROM people WHERE age >= 13 AND age <= 19\") head(teenagers) Machine Learning df <- createDataFrame(sqlContext, iris) model <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\") summary(model) predictions <- predict(model, newData = df) head(select(predictions, \"Sepal_Length\", \"prediction\"))","title":"\u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790"},{"location":"Development/RStudio/#rstudio-sparklyrspark","text":"\u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u4e2d\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5b89\u88c5\u6240\u9700\u7684library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") \u901a\u8fc7spark_connect\u8fde\u63a5spark\u96c6\u7fa4 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") \u8fd9\u91cc\u5982\u679cSPARK_HOME\u6307\u5411/home/test/hadoopclient/Spark/spark\uff0c\u540c\u65f6\u8bbe\u7f6eversion\u4e3a1.6.1\uff0c\u5219\u4f1a\u5bf9\u63a5\u4e0a1.5.1\u7684Spark sparklyr\u5b98\u65b9\u652f\u6301\u662f1.6.1\u4ee5\u4e0a\u7684Spark\uff0c\u8fd9\u91cc\u5f3a\u5236\u6307\u5b9aversion\u4e3a1.6.1\uff0c\u4e3b\u8981\u529f\u80fd\u5747\u6b63\u5e38\uff0c\u90e8\u5206Spark1.6.1\u652f\u6301\u800c1.5.1\u4e0d\u652f\u6301\u7684\u7279\u6027\u6267\u884c\u4f1a\u5931\u8d25 \u542f\u52a8\u6210\u529f\u540e\uff0c\u5728FusionInsgiht\u7684Yarn\u7684ResourceManager\u9875\u9762\u53ef\u4ee5\u770b\u5230sparklyr\u7684\u4efb\u52a1\u5df2\u7ecf\u542f\u52a8 \u5728RStudio\u7684Spark\u9762\u677f\u5237\u65b0\u4e00\u4e0b\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u6709hive\u7684\u8868 \u9009\u62e9hive\u8868\u53f3\u8fb9\u7684\u6570\u636e\u56fe\u8868\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"\u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/#sparklyrsparkbabynames","text":"Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc <- spark_connect(master = \"yarn-client\", version = \"1.6.1\", spark_home = \"/home/test/hadoopclient/Spark/spark\") Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl <- copy_to(sc, babynames, \"babynames\") applicants_tbl <- copy_to(sc, applicants, \"applicants\") birthsYearly <- applicants_tbl %>% mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl <- filteredNames_tbl %>% group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl <- yearlyNames_tbl %>% filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly <- yearlyNames_tbl %>% inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl <- yearlyNames_tbl %>% filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly <- yearlyNames_tbl %>% inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName <- babynames_tbl %>% mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE)","title":"\u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6"},{"location":"Development/RStudio/#sparklyrsparkspark2x","text":"Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) ** Assess model performance** Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { sdf_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- sdf_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights.","title":"\u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x)"},{"location":"Development/RStudio/#faq","text":"","title":"FAQ"},{"location":"Development/RStudio/#fusioninsightr","text":"\u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167 \u914d\u7f6eEPEL\u7684\u6e90\u5b89\u88c5R \u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5","title":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R"},{"location":"Development/RStudio/#sparklyrconfiguration-failed-for-package-openssl","text":"\u64cd\u4f5c\u7cfb\u7edf\u9700\u8981\u6267\u884c yum install openssl-devel \u5b89\u88c5openssl-devel","title":"\u5b89\u88c5sparklyr\u62a5\u9519configuration failed for package \u2018openssl\u2019"},{"location":"Development/RStudio/#sparklyr","text":"\u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/sparkuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports ;","title":"\u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e"},{"location":"Development/Squirrel/","text":"Squirrel\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Squirrel 3.7.1 <-> FusionInsight HD V100R002C60U20 Squirrel 3.8.0 <-> FusionInsight HD V100R002C70SPC200 \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86Squirrel\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 Squirrel\u5b89\u88c5 \u00b6 \u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u3002 \u4fee\u6539 C:\\Windows\\System32\\drivers\\etc\\hosts \u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f\u3002 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dSquirrel\uff0c\u5730\u5740\uff1a http://www.squirrelsql.org/#installation \uff0c\u9009\u62e9Install jar of SQuirreL 3.7.1 for Windows/Linux/others\uff0c\u4e0b\u8f7d\u8f6f\u4ef6squirrel-sql-3.7.1-standard.jar \u53cc\u51fbsquirrel-sql-3.7.1-standard.jar\u5b89\u88c5 \u5728\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u8981\u5b89\u88c5\u54ea\u4e9b\u73af\u5883\uff0c\u4f7f\u7528\u7684\u6570\u636e\u5e93\u63d2\u4ef6\uff0c\u8bed\u8a00\u5305\u3002 Squirrel\u8fde\u63a5Fiber \u00b6 \u4f7f\u7528SQuirreL SQL Client\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9Drivers\uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path\uff1a\u5c06Fiber/lib\u4e0b\u7684jar\u5305\u90fd\u6dfb\u52a0\u8fdb\u6765 ClassName\uff1acom.huawei.fiber.FiberDriver \u53ef\u4ee5\u770b\u5230\u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002 \u5bf9\u63a5Hive \u00b6 \u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name\uff1atest Password\uff1a\u5bc6\u7801 \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u70b9\u51fb Connect \u67e5\u770bhive\u4e2d\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a \u5bf9\u63a5SparkSQL \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aspark\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fbFiber\uff0c\u70b9\u51fb Connet \uff0c\u5c06driver\u5207\u6362\u4e3aspark \u53ef\u4ee5\u770b\u5230\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Spark\u589e\u52a0\u6570\u636e \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a \u5bf9\u63a5Phoenix \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \uff0c\u5c06driver\u5207\u6362\u4e3aphoenix \u53ef\u4ee5\u770b\u5230\u6570\u636ephoenix\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5411phoenix\u8868\u4e2d\u589e\u52a0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') \u67e5\u8be2\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5220\u9664\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 delete from TB_PHOENIX where ID=109; \u67e5\u770b\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u66f4\u65b0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') \u67e5\u770b\u7ed3\u679c","title":"\u5bf9\u63a5Squirrel"},{"location":"Development/Squirrel/#squirrelfusioninsight","text":"","title":"Squirrel\u5bf9\u63a5FusionInsight"},{"location":"Development/Squirrel/#_1","text":"Squirrel 3.7.1 <-> FusionInsight HD V100R002C60U20 Squirrel 3.8.0 <-> FusionInsight HD V100R002C70SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Squirrel/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86Squirrel\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/Squirrel/#squirrel","text":"\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u3002 \u4fee\u6539 C:\\Windows\\System32\\drivers\\etc\\hosts \u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f\u3002 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dSquirrel\uff0c\u5730\u5740\uff1a http://www.squirrelsql.org/#installation \uff0c\u9009\u62e9Install jar of SQuirreL 3.7.1 for Windows/Linux/others\uff0c\u4e0b\u8f7d\u8f6f\u4ef6squirrel-sql-3.7.1-standard.jar \u53cc\u51fbsquirrel-sql-3.7.1-standard.jar\u5b89\u88c5 \u5728\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u8981\u5b89\u88c5\u54ea\u4e9b\u73af\u5883\uff0c\u4f7f\u7528\u7684\u6570\u636e\u5e93\u63d2\u4ef6\uff0c\u8bed\u8a00\u5305\u3002","title":"Squirrel\u5b89\u88c5"},{"location":"Development/Squirrel/#squirrelfiber","text":"\u4f7f\u7528SQuirreL SQL Client\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9Drivers\uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path\uff1a\u5c06Fiber/lib\u4e0b\u7684jar\u5305\u90fd\u6dfb\u52a0\u8fdb\u6765 ClassName\uff1acom.huawei.fiber.FiberDriver \u53ef\u4ee5\u770b\u5230\u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002","title":"Squirrel\u8fde\u63a5Fiber"},{"location":"Development/Squirrel/#hive","text":"\u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name\uff1atest Password\uff1a\u5bc6\u7801 \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u70b9\u51fb Connect \u67e5\u770bhive\u4e2d\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"\u5bf9\u63a5Hive"},{"location":"Development/Squirrel/#sparksql","text":"\u5c06defaultDriver\u5207\u6362\u4e3aspark\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fbFiber\uff0c\u70b9\u51fb Connet \uff0c\u5c06driver\u5207\u6362\u4e3aspark \u53ef\u4ee5\u770b\u5230\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Spark\u589e\u52a0\u6570\u636e \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"\u5bf9\u63a5SparkSQL"},{"location":"Development/Squirrel/#phoenix","text":"\u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \uff0c\u5c06driver\u5207\u6362\u4e3aphoenix \u53ef\u4ee5\u770b\u5230\u6570\u636ephoenix\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5411phoenix\u8868\u4e2d\u589e\u52a0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') \u67e5\u8be2\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5220\u9664\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 delete from TB_PHOENIX where ID=109; \u67e5\u770b\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u66f4\u65b0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') \u67e5\u770b\u7ed3\u679c","title":"\u5bf9\u63a5Phoenix"},{"location":"Development/Zeppelin_0.7.2/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.7.2 \u2194 FusionInsight HD V100R002C60U20 \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.7.2 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06\u8f6f\u4ef6\u5305zeppelin-0.7.2-bin-all.tgz\u4e0a\u4f20\u81f3/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.2-bin-all\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.7.2-bin-all.tgz \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/jdk1.7.0_51/ \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\uff0coverwrite\u9009\u62e9n \u5728/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u65b0\u5efa\u76ee\u5f55zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar \u5c06/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u4e0eFusionInsight\u51b2\u7a81\u768438\u4e2ajar\u5305\u79fb\u52a8\u5230zeppelin_hbase_jar\u76ee\u5f55\u4e2d commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar \u6700\u7ec8/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u6709152\u4e2ajar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b\u7684\u539f\u6709\u7684\u76f8\u5173\u7684jar\u5305\u5220\u9664 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684\u4ee5\u4e0bjar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u7684jackson\u7684\u76f8\u5173jar\u5305\u5220\u9664 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684jackson\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar \u5c06\u6b65\u9aa41\u548c\u6b65\u9aa42\u6240\u6709\u4ecespark\u5ba2\u6237\u7aef\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.2-bin-all/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"Zeppelin0.7.2 <--> C60"},{"location":"Development/Zeppelin_0.7.2/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.7.2/#_1","text":"Zeppelin 0.7.2 \u2194 FusionInsight HD V100R002C60U20","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#zeppelin","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.7.2/#_2","text":"\u5b89\u88c5Zeppelin0.7.2","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_4","text":"\u5c06\u8f6f\u4ef6\u5305zeppelin-0.7.2-bin-all.tgz\u4e0a\u4f20\u81f3/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.2-bin-all\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.7.2-bin-all.tgz \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/jdk1.7.0_51/ \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.7.2/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.7.2/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_10","text":"\u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\uff0coverwrite\u9009\u62e9n \u5728/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u65b0\u5efa\u76ee\u5f55zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar \u5c06/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u4e0eFusionInsight\u51b2\u7a81\u768438\u4e2ajar\u5305\u79fb\u52a8\u5230zeppelin_hbase_jar\u76ee\u5f55\u4e2d commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar \u6700\u7ec8/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u6709152\u4e2ajar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.7.2/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_12","text":"\u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_13","text":"\u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b\u7684\u539f\u6709\u7684\u76f8\u5173\u7684jar\u5305\u5220\u9664 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684\u4ee5\u4e0bjar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u7684jackson\u7684\u76f8\u5173jar\u5305\u5220\u9664 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684jackson\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar \u5c06\u6b65\u9aa41\u548c\u6b65\u9aa42\u6240\u6709\u4ecespark\u5ba2\u6237\u7aef\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.2-bin-all/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.7.2/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_15","text":"\u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#faq","text":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"FAQ"},{"location":"Development/Zeppelin_0.7.3/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C70SPC100 (Spark2.x) Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2.x) \u7f16\u8bd1Zeppelin \u00b6 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u5b89\u88c5bower npm install -g bower \u914d\u7f6ebower\u5141\u8bb8root\u7528\u6237\u6267\u884c echo '{ \"allow_root\": true }' > /root/.bowerrc \u6267\u884c bower -v \u83b7\u53d6Zeppelin0.7.3\u7684\u7248\u672c git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 \u4fee\u6539scala\u7248\u672c\uff0c\u9002\u914dFusionInsight_HD_V100R002C70SPC100\u7684Hadoop\u7248\u672c \u5728zeppelin\u4ee3\u7801\u6839\u76ee\u5f55\u6267\u884c vi ./dev/change_scala_version.sh \uff0c\u4fee\u6539\u4e0b\u56fe\u7684SCALA_LIB_VERSION\u4e3a2.11.8 \u6267\u884c\u547d\u4ee4\u5b8c\u6210scala\u7248\u672c\u7684\u4fee\u6539 ./dev/change_scala_version.sh 2.11 \u6267\u884c vi pom.xml \u6587\u4ef6\u7684\u4fee\u6539 \u4e3a0.9.3 \u6267\u884c vi hbase/pom.xml \u4fee\u6539hbase\u7248\u672c\u548chadoop\u7248\u672c \u7f16\u8bd1Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests \u7f16\u8bd1\u5b8c\u6210\u540e\u5728 zeppelin-distribution/target \u76ee\u5f55\u4e0b\u751f\u6210 zeppelin-0.7.3.tar.gz \u6587\u4ef6 \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.7.3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06\u7f16\u8bd1\u597d\u7684zeppelin-0.7.3.tar.gz\u4e0a\u4f20\u653e\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.3\u76ee\u5f55\u3002 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.3/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.3/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684sparkSQL\u8bed\u53e5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.3/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"Zeppelin0.7.3 <--> C70"},{"location":"Development/Zeppelin_0.7.3/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.7.3/#_1","text":"Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C70SPC100 (Spark2.x) Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2.x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#zeppelin","text":"\u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u5b89\u88c5bower npm install -g bower \u914d\u7f6ebower\u5141\u8bb8root\u7528\u6237\u6267\u884c echo '{ \"allow_root\": true }' > /root/.bowerrc \u6267\u884c bower -v \u83b7\u53d6Zeppelin0.7.3\u7684\u7248\u672c git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 \u4fee\u6539scala\u7248\u672c\uff0c\u9002\u914dFusionInsight_HD_V100R002C70SPC100\u7684Hadoop\u7248\u672c \u5728zeppelin\u4ee3\u7801\u6839\u76ee\u5f55\u6267\u884c vi ./dev/change_scala_version.sh \uff0c\u4fee\u6539\u4e0b\u56fe\u7684SCALA_LIB_VERSION\u4e3a2.11.8 \u6267\u884c\u547d\u4ee4\u5b8c\u6210scala\u7248\u672c\u7684\u4fee\u6539 ./dev/change_scala_version.sh 2.11 \u6267\u884c vi pom.xml \u6587\u4ef6\u7684\u4fee\u6539 \u4e3a0.9.3 \u6267\u884c vi hbase/pom.xml \u4fee\u6539hbase\u7248\u672c\u548chadoop\u7248\u672c \u7f16\u8bd1Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests \u7f16\u8bd1\u5b8c\u6210\u540e\u5728 zeppelin-distribution/target \u76ee\u5f55\u4e0b\u751f\u6210 zeppelin-0.7.3.tar.gz \u6587\u4ef6","title":"\u7f16\u8bd1Zeppelin"},{"location":"Development/Zeppelin_0.7.3/#zeppelin_1","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.7.3/#_2","text":"\u5b89\u88c5Zeppelin0.7.3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_4","text":"\u5c06\u7f16\u8bd1\u597d\u7684zeppelin-0.7.3.tar.gz\u4e0a\u4f20\u653e\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.3\u76ee\u5f55\u3002 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.7.3/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.3/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.7.3/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_10","text":"\u5c06 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.7.3/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_12","text":"\u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_13","text":"\u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.3/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684sparkSQL\u8bed\u53e5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.7.3/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_15","text":"\u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#faq","text":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.3/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"FAQ"},{"location":"Development/Zeppelin_0.8.0/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.8.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2.x) \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.8.0 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5Zeppelin 0.8.0,\u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf zeppelin-0.8.0-bin-all.tgz \u5b89\u88c5\u751f\u6210zeppelin-0.8.0-bin-all\u76ee\u5f55\u3002 \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 - \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all \u76ee\u5f55\u4e0b \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin \u91cd\u542fzeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u4f7f\u7528\u8d26\u6237developuser\u767b\u9646zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 502:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser/ \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b libfb303-0.9.3.jar \u548c libthrift-0.9.3.jar \u4e24\u4e2ajar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep \u8def\u5f84\u4e0b \u786e\u4fdd /usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter \u8def\u5f84\u4e0b\u6709\u4e14\u4ec5\u6709 libthrift-0.9.3.jar \u8fd9\u4e2a\u7248\u672c\u7684jar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u5982\u679c\u9047\u5230\u6e90yum-plugin-fastestmirror\u65e0\u6cd5\u4e0b\u8f7d\u65f6\uff0c\u53ef\u5728\u7f51\u5740 https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror \u4e0b\u9009\u62e9\u76f8\u5e94\u7684\u7248\u672c\u4ee3\u66ff\u4e0b\u8f7d\u5b89\u88c5 \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b Zeppelin\u8fde\u63a5Apache Livy \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u8fde\u63a5Livy \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8Livy\u670d\u52a1 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9livy\uff0c\u70b9\u51fb edit \u7f16\u8f91zeppelin.livy.url\u7684\u503c\u4e3a http://172.21.3.43:8998 \uff08\u53ef\u4ee5\u4e0d\u66f4\u6539\uff09\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982livy_connection_test \u5728Zeppelin\u4e2d\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cPySpark\u6837\u4f8b\u4ee3\u7801 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cSparkR\u6837\u4f8b\u4ee3\u7801 %livy.sparkr hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") Zeppelin\u8fde\u63a5FusionInsight Elk \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5FusionInsight Elk \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bElk\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55FusionInsight Elk, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93\uff0c \u6d4b\u8bd5\u6570\u636e\u8868 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25108 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**joe**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER joe WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO joe; \u521b\u5efaHDFS\u8868\u7a7a\u95f4\u3002 CREATE TABLESPACE hdfs_tablespace LOCATION '/srv/BigData/hadoop/hdfs_tablespace' WITH (filesystem = 'HDFS', cfgpath = '/opt/huawei/Bigdata/mppdb/conf', storepath = '/user/elk/tablespace/ hdfs_tablespace'); \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE TABLESPACE \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE DATABASE \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25108 -U joe -W Bigdata@123 \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\u3002 CREATE TABLE hdfs_001(id int,name varchar2(20) ) WITH (orientation=orc,version=0.12,compression=no) TABLESPACE hdfs_tablespace; \u4f7f\u7528INSERT\u547d\u4ee4\u63d2\u5165\u6570\u636e\u3002 \u63d2\u5165\u4e00\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'); \u63d2\u5165\u591a\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'),(2, 'Marketing'), (2, 'Purchasing'); \u68c0\u67e5\u7ed3\u679c Select * from hdfs_001 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6e\u96c6\u7fa4Elk\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 joe \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.52.190 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all joe 172.16.52.190/32 sha256\" \u4f7f\u7528\u201cjoe\u201d\u7528\u6237\u524d\uff0c\u9700\u5148\u672c\u5730\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u5e76\u5728\u6570\u636e\u5e93\u4e2d\u4f7f\u7528\u5982\u4e0b\u8bed\u53e5\u5efa\u7acb\u201cjoe\u201d\u7528\u6237\u3002 -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 joe \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.52.190/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u6267\u884c\u5982\u4e0b\u547d\u4ee4\u91cd\u542f\u96c6\u7fa4\u3002 gs_om -t stop && gs_om -t start \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 FusionInsight elk \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230Elk\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1aGauss200-OLAP-V100R007C10-REDHAT-64bit-Jdbc.tar.gz \u9a71\u52a8\u7c7b\uff1aorg.postgresql.Driver \u5177\u4f53\u4f4d\u7f6e\u4e3a\uff1aC:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\Elk \u9a71\u52a8jar\u5305\u7684\u540d\u5b57\u53eb gsjdbc4.jar \u5c06\u627e\u5230\u7684\u8fd9\u4e2a gsjdbc4.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/zeppelin-0.8.0-bin-all/interpreter/jdbc \u8def\u5f84\u4e0b\uff0c \u5e76\u4e14\u4f7f\u7528 \u4e0b\u9762\u547d\u4ee4\u66f4\u6539\u9a71\u52a8\u6743\u9650\u3002 chown 502:wheel gsjdbc4.jar chmod 755 gsjdbc4.jar \u542f\u52a8Zeppelin, \u914d\u7f6e JDBC interpreter\u5982\u4e0b: 1: default.driver = org.postgresql.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = joe \u68c0\u67e5\u7ed3\u679c\uff1a","title":"Zeppelin0.8.0 <--> C80"},{"location":"Development/Zeppelin_0.8.0/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.8.0/#_1","text":"Zeppelin 0.8.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2.x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#zeppelin","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.8.0/#_2","text":"\u5b89\u88c5Zeppelin0.8.0","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_4","text":"\u5b89\u88c5Zeppelin 0.8.0,\u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf zeppelin-0.8.0-bin-all.tgz \u5b89\u88c5\u751f\u6210zeppelin-0.8.0-bin-all\u76ee\u5f55\u3002 \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 - \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all \u76ee\u5f55\u4e0b \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin \u91cd\u542fzeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u4f7f\u7528\u8d26\u6237developuser\u767b\u9646zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.8.0/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 502:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser/ \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.8.0/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_10","text":"\u5c06 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.8.0/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_12","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_13","text":"\u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b libfb303-0.9.3.jar \u548c libthrift-0.9.3.jar \u4e24\u4e2ajar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep \u8def\u5f84\u4e0b \u786e\u4fdd /usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter \u8def\u5f84\u4e0b\u6709\u4e14\u4ec5\u6709 libthrift-0.9.3.jar \u8fd9\u4e2a\u7248\u672c\u7684jar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark)","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.8.0/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_15","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u5982\u679c\u9047\u5230\u6e90yum-plugin-fastestmirror\u65e0\u6cd5\u4e0b\u8f7d\u65f6\uff0c\u53ef\u5728\u7f51\u5740 https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror \u4e0b\u9009\u62e9\u76f8\u5e94\u7684\u7248\u672c\u4ee3\u66ff\u4e0b\u8f7d\u5b89\u88c5 \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinapache-livy","text":"","title":"Zeppelin\u8fde\u63a5Apache Livy"},{"location":"Development/Zeppelin_0.8.0/#_17","text":"Zeppelin\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u8fde\u63a5Livy","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_18","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_19","text":"\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8Livy\u670d\u52a1 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9livy\uff0c\u70b9\u51fb edit \u7f16\u8f91zeppelin.livy.url\u7684\u503c\u4e3a http://172.21.3.43:8998 \uff08\u53ef\u4ee5\u4e0d\u66f4\u6539\uff09\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982livy_connection_test \u5728Zeppelin\u4e2d\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cPySpark\u6837\u4f8b\u4ee3\u7801 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cSparkR\u6837\u4f8b\u4ee3\u7801 %livy.sparkr hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\")","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinfusioninsight-elk","text":"","title":"Zeppelin\u8fde\u63a5FusionInsight Elk"},{"location":"Development/Zeppelin_0.8.0/#_20","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5FusionInsight Elk","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_21","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bElk\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_22","text":"\u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55FusionInsight Elk, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93\uff0c \u6d4b\u8bd5\u6570\u636e\u8868 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25108 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**joe**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER joe WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO joe; \u521b\u5efaHDFS\u8868\u7a7a\u95f4\u3002 CREATE TABLESPACE hdfs_tablespace LOCATION '/srv/BigData/hadoop/hdfs_tablespace' WITH (filesystem = 'HDFS', cfgpath = '/opt/huawei/Bigdata/mppdb/conf', storepath = '/user/elk/tablespace/ hdfs_tablespace'); \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE TABLESPACE \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE DATABASE \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25108 -U joe -W Bigdata@123 \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\u3002 CREATE TABLE hdfs_001(id int,name varchar2(20) ) WITH (orientation=orc,version=0.12,compression=no) TABLESPACE hdfs_tablespace; \u4f7f\u7528INSERT\u547d\u4ee4\u63d2\u5165\u6570\u636e\u3002 \u63d2\u5165\u4e00\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'); \u63d2\u5165\u591a\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'),(2, 'Marketing'), (2, 'Purchasing'); \u68c0\u67e5\u7ed3\u679c Select * from hdfs_001 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6e\u96c6\u7fa4Elk\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 joe \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.52.190 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all joe 172.16.52.190/32 sha256\" \u4f7f\u7528\u201cjoe\u201d\u7528\u6237\u524d\uff0c\u9700\u5148\u672c\u5730\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u5e76\u5728\u6570\u636e\u5e93\u4e2d\u4f7f\u7528\u5982\u4e0b\u8bed\u53e5\u5efa\u7acb\u201cjoe\u201d\u7528\u6237\u3002 -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 joe \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.52.190/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u6267\u884c\u5982\u4e0b\u547d\u4ee4\u91cd\u542f\u96c6\u7fa4\u3002 gs_om -t stop && gs_om -t start \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 FusionInsight elk \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230Elk\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1aGauss200-OLAP-V100R007C10-REDHAT-64bit-Jdbc.tar.gz \u9a71\u52a8\u7c7b\uff1aorg.postgresql.Driver \u5177\u4f53\u4f4d\u7f6e\u4e3a\uff1aC:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\Elk \u9a71\u52a8jar\u5305\u7684\u540d\u5b57\u53eb gsjdbc4.jar \u5c06\u627e\u5230\u7684\u8fd9\u4e2a gsjdbc4.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/zeppelin-0.8.0-bin-all/interpreter/jdbc \u8def\u5f84\u4e0b\uff0c \u5e76\u4e14\u4f7f\u7528 \u4e0b\u9762\u547d\u4ee4\u66f4\u6539\u9a71\u52a8\u6743\u9650\u3002 chown 502:wheel gsjdbc4.jar chmod 755 gsjdbc4.jar \u542f\u52a8Zeppelin, \u914d\u7f6e JDBC interpreter\u5982\u4e0b: 1: default.driver = org.postgresql.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = joe \u68c0\u67e5\u7ed3\u679c\uff1a","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/","text":"\u5176\u4ed6 \u00b6 \u5bf9\u63a5FUSE \u5bf9\u63a5Gis-Tools-For-Hadoop \u5bf9\u63a5Apache Livy \u5bf9\u63a5Logstash \u5bf9\u63a5Kibana \u5bf9\u63a5ElasticSearch-head \u5bf9\u63a5beats","title":"Home"},{"location":"Other/#_1","text":"\u5bf9\u63a5FUSE \u5bf9\u63a5Gis-Tools-For-Hadoop \u5bf9\u63a5Apache Livy \u5bf9\u63a5Logstash \u5bf9\u63a5Kibana \u5bf9\u63a5ElasticSearch-head \u5bf9\u63a5beats","title":"\u5176\u4ed6"},{"location":"Other/Apache_Livy/","text":"Apache Livy\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Livy 0.5.0-incubating \u2194 FusionInsight HD V100R002C80SPC200 (Spark2.x) \u5b89\u88c5Livy \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5 Apache Livy 0.5.0 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5Apache Livy 0.5.0-incubating\uff0c\u5728\u7f51\u5740 https://livy.incubator.apache.org/download/ \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 unzip livy-0.5.0-incubating-bin.zip \u89e3\u538b\u751f\u6210livy-0.5.0-incubating-bin\u76ee\u5f55 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser \u76ee\u5f55\u4e0b \u5728 /usr/livy/livy-0.5.0-incubating-bin/conf \u8def\u5f84\u4e0b\u65b0\u5efalivy\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u914d\u7f6eLivy\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH \u7f16\u8f91livy.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true #livy.server.auth.type=kerberos livy.server.auth.kerberos.keytab=/opt/developuser/user.keytab livy.server.auth.kerberos.principal=developuser@HADOOP.COM livy.server.launch.kerberos.keytab=/opt/developuser/user.keytab livy.server.launch.kerberos.principal=developuser@HADOOP.COM \u7f16\u8f91livy-client.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf \u52a0\u5165\u5982\u672c\u673aip\u5730\u5740\uff1a livy.rsc.rpc.server.address =172.16.52.190 - \u7f16\u8f91livy-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 \u7f16\u8f91spark-blacklist.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf \u6ce8\u9500\u6389\u5982\u4e0b\u5185\u5bb9\uff1a spark.master spark.submit.deployMode \u542f\u52a8\u548c\u505c\u6b62Livy\uff0c\u5728\u8def\u5f84 /usr/livy/livy-0.5.0-incubating-bin \u4e0b bin/livy-server start \u542f\u52a8\u6210\u529f\u540e\u53ef\u4ee5\u5728http://172.16.52.190:8998\u8bbf\u95ee\u5230Livy\u670d\u52a1\u5668\uff1a \u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801\uff0c\u5305\u62ecSpark Shell\uff0cPySpark\uff0cSparkR \u6837\u4f8b\u4ee3\u7801\u53c2\u8003\u7f51\u5740 https://livy.incubator.apache.org/examples/ \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u5df2\u5b8c\u6210Anaconda\u548cR\u5728\u5ba2\u6237\u7aef\u4e3b\u673a\u4e0a\u7684\u5b89\u88c5\u3002 \u82e5\u6ca1\u6709\u5b89\u88c5Anaconda\u548cR\uff0c\u8bf7\u53c2\u8003Zeppelin0.8.0\u5bf9\u63a5FusionInsight HD V100R002C80SPC200 (Spark2.x)\u6307\u5bfc\u6587\u6863\u4e2d\u8fde\u63a5Spark\u548cSparkR\u90e8\u5206\u76f8\u5173\u5185\u5bb9 \u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u8f93\u5165\u547d\u4ee4 python \u542f\u52a8Anaconda \u8f93\u5165\u5982\u4e0bpython\u4ee3\u7801\u542f\u52a8\u4e00\u4e2aLivy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) \u5f53\u4e00\u4e2asession\u5b8c\u6210\u542f\u52a8\u540e\uff0c \u5b83\u5c06\u4f1a\u53d8\u4e3a\u95f2\u7f6e\u72b6\u6001 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() \u4e0b\u9762\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u7b80\u5355JSON\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u6765\u6267\u884cScala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u4e5f\u53ef\u4ee5\u5728\u7ec8\u7aef\u770b\u5230\u4ee5JSON\u683c\u5f0f\u8fd4\u56de\u7684\u7ed3\u679c \u66f4\u65b0Scala\u518d\u6b21\u8fd0\u884c data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES) \"\"\") } r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers) \u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3apyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session1 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cPython\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/1/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session1\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers) \u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3asparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session2 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cR\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/2/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session2\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers)","title":"\u5bf9\u63a5Apache Livy"},{"location":"Other/Apache_Livy/#apache-livyfusioninsight","text":"","title":"Apache Livy\u5bf9\u63a5FusionInsight"},{"location":"Other/Apache_Livy/#_1","text":"Apache Livy 0.5.0-incubating \u2194 FusionInsight HD V100R002C80SPC200 (Spark2.x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Apache_Livy/#livy","text":"","title":"\u5b89\u88c5Livy"},{"location":"Other/Apache_Livy/#_2","text":"\u5b89\u88c5 Apache Livy 0.5.0","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Other/Apache_Livy/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Apache_Livy/#_4","text":"\u5b89\u88c5Apache Livy 0.5.0-incubating\uff0c\u5728\u7f51\u5740 https://livy.incubator.apache.org/download/ \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 unzip livy-0.5.0-incubating-bin.zip \u89e3\u538b\u751f\u6210livy-0.5.0-incubating-bin\u76ee\u5f55 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser \u76ee\u5f55\u4e0b \u5728 /usr/livy/livy-0.5.0-incubating-bin/conf \u8def\u5f84\u4e0b\u65b0\u5efalivy\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u914d\u7f6eLivy\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH \u7f16\u8f91livy.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true #livy.server.auth.type=kerberos livy.server.auth.kerberos.keytab=/opt/developuser/user.keytab livy.server.auth.kerberos.principal=developuser@HADOOP.COM livy.server.launch.kerberos.keytab=/opt/developuser/user.keytab livy.server.launch.kerberos.principal=developuser@HADOOP.COM \u7f16\u8f91livy-client.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf \u52a0\u5165\u5982\u672c\u673aip\u5730\u5740\uff1a livy.rsc.rpc.server.address =172.16.52.190 - \u7f16\u8f91livy-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 \u7f16\u8f91spark-blacklist.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf \u6ce8\u9500\u6389\u5982\u4e0b\u5185\u5bb9\uff1a spark.master spark.submit.deployMode \u542f\u52a8\u548c\u505c\u6b62Livy\uff0c\u5728\u8def\u5f84 /usr/livy/livy-0.5.0-incubating-bin \u4e0b bin/livy-server start \u542f\u52a8\u6210\u529f\u540e\u53ef\u4ee5\u5728http://172.16.52.190:8998\u8bbf\u95ee\u5230Livy\u670d\u52a1\u5668\uff1a","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy/#livy_1","text":"","title":"\u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801"},{"location":"Other/Apache_Livy/#_5","text":"\u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801\uff0c\u5305\u62ecSpark Shell\uff0cPySpark\uff0cSparkR \u6837\u4f8b\u4ee3\u7801\u53c2\u8003\u7f51\u5740 https://livy.incubator.apache.org/examples/","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Other/Apache_Livy/#_6","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u5df2\u5b8c\u6210Anaconda\u548cR\u5728\u5ba2\u6237\u7aef\u4e3b\u673a\u4e0a\u7684\u5b89\u88c5\u3002 \u82e5\u6ca1\u6709\u5b89\u88c5Anaconda\u548cR\uff0c\u8bf7\u53c2\u8003Zeppelin0.8.0\u5bf9\u63a5FusionInsight HD V100R002C80SPC200 (Spark2.x)\u6307\u5bfc\u6587\u6863\u4e2d\u8fde\u63a5Spark\u548cSparkR\u90e8\u5206\u76f8\u5173\u5185\u5bb9","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Apache_Livy/#spark","text":"\u8f93\u5165\u547d\u4ee4 python \u542f\u52a8Anaconda \u8f93\u5165\u5982\u4e0bpython\u4ee3\u7801\u542f\u52a8\u4e00\u4e2aLivy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) \u5f53\u4e00\u4e2asession\u5b8c\u6210\u542f\u52a8\u540e\uff0c \u5b83\u5c06\u4f1a\u53d8\u4e3a\u95f2\u7f6e\u72b6\u6001 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() \u4e0b\u9762\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u7b80\u5355JSON\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u6765\u6267\u884cScala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u4e5f\u53ef\u4ee5\u5728\u7ec8\u7aef\u770b\u5230\u4ee5JSON\u683c\u5f0f\u8fd4\u56de\u7684\u7ed3\u679c \u66f4\u65b0Scala\u518d\u6b21\u8fd0\u884c data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES) \"\"\") } r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy/#pyspark","text":"\u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3apyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session1 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cPython\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/1/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session1\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy/#sparkr","text":"\u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3asparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session2 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cR\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/2/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session2\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/","text":"FusionInsight HD ES\u7ec4\u4ef6\u4e0e\u5468\u8fb9\u751f\u6001\u5bf9\u63a5 \u00b6 \u751f\u6001\u7b80\u4ecb \u00b6 Kibana: \u53ef\u6269\u5c55\u7684\u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u7ba1\u7406\u6574\u4e2a\u751f\u6001\u7ec4\u4ef6\uff08elasticsearch, logstash, beats\uff09\u4ee5\u53ca\u6570\u636e Elasticsearch: \u517c\u6709\u641c\u7d22\u5f15\u64ce\u548cNoSQL\u6570\u636e\u5e93\u529f\u80fd\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u57fa\u4e8eJAVA/Lucene\u6784\u5efa\uff0c\u5f00\u6e90\u3001\u5206\u5e03\u5f0f\u3001\u652f\u6301RESTful\u8bf7\u6c42 Logstash: \u5f00\u6e90\u7684\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u591a\u4e2a\u6e90\u5934\u6536\u96c6\u6570\u636e\uff0c\u4f20\u5230Elasticsearch\uff0c\u80fd\u591f\u548cElasticsearch\u4ea7\u751f\u534f\u540c\u6548\u5e94 beats: \u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u642c\u8fd0\u5de5\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u670d\u52a1\u5668\u4e0a\u5c06\u6570\u636e\u4f20\u8f93\u5230Logstash\u6216\u8005Elasticsearch elasticsearch-head: \u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u67e5\u8be2Elasticsearch\u4e2d\u7684\u6570\u636e \u6ce8\uff1a FusionInsight HD\u7684Elasticsearch\u7ec4\u4ef6\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u4f46\u662f\u76f8\u5173\u7684\u5468\u8fb9\u751f\u6001Kibana\uff0cLogstash\uff0cbeats\uff0c elasticseach-head\u4e3a\u5f00\u6e90\uff0c\u6682\u65f6\u65e0\u6cd5\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u6545\u91c7\u7528\u5b89\u5168FI HD\u96c6\u7fa4\u7684\u975e\u5b89\u5168ES\u7ec4\u4ef6\u8fdb\u884c\u5bf9\u63a5 Logstash\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Logstash 6.4.2 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch\u7ec4\u4ef6\u975e\u5b89\u5168\u6a21\u5f0f) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dlogstash 6.4.2, \u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**logstash-6.4.2.zip**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt/logstash \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 unzip logstash-6.4.2.zip \u89e3\u538b\u5b89\u88c5\u5305 \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2/config \u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u914d\u7f6e\u6587\u4ef6 logstash-Simple.conf ,\u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ stdin{ } } output { stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"hellow_world\" #user => \"elastic\" #password => \"changeme\" } } \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2 \u4e0b\uff0c\u6267\u884c\u547d\u4ee4 bin/logstash -f config/logstash-Simple.conf \u6839\u636e\u4e4b\u524d\u7684 logstash-Simple.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash, \u7136\u540e\u5728\u7ec8\u7aef\u624b\u52a8\u8f93\u5165\u6570\u5b571\u52306\uff1a \u767b\u5f55elasticsearch-head\u670d\u52a1\u5668\u67e5\u770b\u7ed3\u679c\uff08\u5bf9\u63a5\u6b65\u9aa4\u53c2\u89c1\u540e\u6587\uff09 Kibana\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kibana 6.1.3 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch\u7ec4\u4ef6\u975e\u5b89\u5168\u6a21\u5f0f) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dKibana 6.1.3, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Kibana 6.1.3**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf kibana-6.1.3-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 vi /opt/kibana-6.1.3-linux-x86_64/config/kibana.yml \u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\u9009\u9879: server.port: 5601 server.host: \"172.16.52.190\" server.name: \"LinuxTest\" elasticsearch.url: \"http://172.21.3.101:24100\" \u4f7f\u7528 bin/kibana \u542f\u52a8kibana \u8bbf\u95eekibana\u767b\u5f55\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u683c\u5f0f\u4e3ahttp://Kibana\u670d\u52a1IP\u5730\u5740:5601 \u9009\u62e9**Dev Tools** \u4f7f\u7528 GET _cat/indices \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4ES\u4e2d\u7684indices \u4f7f\u7528 GET hellow_world/_search \u547d\u4ee4\u67e5\u770b\u7ed3\u679c elasticsearch-head\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 elasticsearch-head \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch\u7ec4\u4ef6\u975e\u5b89\u5168\u6a21\u5f0f) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u5728FusionInsight Manager\uff0c\u9009\u62e9\u670d\u52a1\u7ba1\u7406->Elasticsearch\u670d\u52a1\u914d\u7f6e->\u670d\u52a1\u914d\u7f6e(\u9009\u62e9\u5168\u90e8\u914d\u7f6e)->\u81ea\u5b9a\u4e49\uff0c\u5728elasticsearch.yml\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4e0b\u9762\u4e24\u4e2a\u914d\u7f6e\u9879\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1 http.cors.enabled = true http.cors.allow-origin = \"*\" \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u4e14\u5b89\u88c5**elasticsearch-head**\u5230\u4e3b\u673a\u4e0a,\u5e76\u4e14\u542f\u52a8**elasticsearch-head**\u670d\u52a1 git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm install npm run start \u8bbf\u95eeelasticsearch-head\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u4e3ahttp://elasticsearch-head\u670d\u52a1IP\u5730\u5740:9100, \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u4e3a http://172.21.3.101:24100/ ,\u70b9\u51fb\u8fde\u63a5: \u67e5\u770b\u7ed3\u679c \u5728FI HD\u96c6\u7fa4\u4e0a\u90e8\u7f72beats \u00b6 \u9002\u7528\u573a\u666f \u00b6 filebeat-6.5.1 \u2194 FusionInsight HD V100R002C80SPC200 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4e0b\u8f7dFilebeat 6.5.1, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Filebeat 6.5.1**\u4f7f\u7528WinSCP\u5bfc\u5165FI HD\u96c6\u7fa4\u8282\u70b9\uff08172.21.3.103\uff09\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf filebeat-6.5.1-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 cd /opt/filebeat-6.5.1-linux-x86_64 \u8fdb\u5165filebeat\u5b89\u88c5\u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 filebeat_new.yml ,\u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-host3.log path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5046\"] \u6ce8\uff1a\u7aef\u53e35046\u53ef\u4ee5\u81ea\u5df1\u6307\u5b9a\uff0c\u4e0d\u51b2\u7a81\u5373\u53ef \u4f7f\u7528\u547d\u4ee4 ./filebeat -e -c filebeat_new.yml \u542f\u52a8filebeat \u5e94\u7528\u573a\u666f\u4e3e\u4f8b\u8bf4\u660e \u00b6 \u573a\u666f\u7b80\u4ecb \u00b6 \u5206\u522b\u5728FI HD\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72filebeat\u5b9e\u65f6\u83b7\u53d6\u4e24\u53f0\u670d\u52a1\u5668\u7684\u7684\u65e5\u5fd7\u6587\u4ef6\uff08/var/log/ipmitool.fi.log\uff09\uff0c\u901a\u8fc7logstash\u7ba1\u9053\u83b7\u53d6\u5e76\u8fc7\u6ee4\u5143\u65e5\u5fd7\u6587\u4ef6\u4e3a\u591a\u4e2a\u5b57\u6bb5\uff0c\u5e76\u4f20\u5230FI HD Elasticsearch\u7ec4\u4ef6\u4e0a\uff0c\u6700\u540e\u901a\u8fc7Kibana\u6765\u67e5\u770b\u83b7\u53d6\u7684\u65e5\u5fd7\u6587\u4ef6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210Kibana\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210logstash\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210filebeat\u7684\u5b89\u88c5 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9996\u5148\u767b\u9646FusionInsight HD\u96c6\u7fa4\u8282\u70b9172.21.3.102\u548c172.21.3.103\u4e0a \u53c2\u8003\u4e4b\u524d\u7684\u6b65\u9aa4\u5206\u522b\u90e8\u7f72filebeat\u5230172.21.3.102\u548c172.21.3.103\u4e0a\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u521b\u5efafilebeat\u914d\u7f6e\u6587\u4ef6 filebeat_new_host2.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5045\"] \u914d\u7f6e\u6587\u4ef6 filebeat_new_host3.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5047\"] \u767b\u9646\u5b89\u88c5logstash\u7684\u4e3b\u673a\uff0c\u914d\u7f6e\u4e00\u4e2a\u65b0\u7684\u542f\u52a8\u6587\u4ef6 logstash-beats-ipmitool.conf \u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ beats{ port => \"5045\" } beats{ port => \"5047\" } } filter{ grok{ match =>{\"message\" => \"(?<object>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<object2>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<status>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<number>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|%{GREEDYDATA:additional_info}\"} } } output{ stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"ipmitool_log\" } } \u542f\u52a8kibana \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-beats-ipmitool.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash \u5206\u522b\u542f\u52a8172.21.3.102\u548c172.21.3.103\u4e0a\u7684filebeat\u6765\u83b7\u53d6\u65e5\u5fd7\u6587\u4ef6 \u767b\u9646kibana\u7f51\u9875\u754c\u9762\uff0c\u9009\u62e9**Management**\u4e0b\u9762\u7684**Index Patterns** \u5728**Create index pattern**\u4e0b\u9009\u62e9\u521a\u521a\u7531logstash\u4f20\u8f93\u521b\u5efa\u7684index ipmitool_log \uff0c \u70b9\u51fb Next step \u5728step 2\u4e2d\u9009\u62e9**@timestamp**, \u70b9\u51fb**Create index pattern** \u5728 Discover \u90e8\u5206\u9009\u62e9\u521a\u521a\u751f\u6210\u7684index pattern ipmitool_log \u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u65e5\u5fd7\u7684\u60c5\u51b5 \u53ef\u6839\u636e\u4e0d\u540c\u7684\u5b57\u6bb5\u60c5\u51b5\u6765\u6574\u4f53\u4e86\u89e3\u65e5\u5fd7\u60c5\u51b5 \u5b8c\u6210","title":"\u5bf9\u63a5beats"},{"location":"Other/Elasticsearch_Related/#fusioninsight-hd-es","text":"","title":"FusionInsight HD ES\u7ec4\u4ef6\u4e0e\u5468\u8fb9\u751f\u6001\u5bf9\u63a5"},{"location":"Other/Elasticsearch_Related/#_1","text":"Kibana: \u53ef\u6269\u5c55\u7684\u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u7ba1\u7406\u6574\u4e2a\u751f\u6001\u7ec4\u4ef6\uff08elasticsearch, logstash, beats\uff09\u4ee5\u53ca\u6570\u636e Elasticsearch: \u517c\u6709\u641c\u7d22\u5f15\u64ce\u548cNoSQL\u6570\u636e\u5e93\u529f\u80fd\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u57fa\u4e8eJAVA/Lucene\u6784\u5efa\uff0c\u5f00\u6e90\u3001\u5206\u5e03\u5f0f\u3001\u652f\u6301RESTful\u8bf7\u6c42 Logstash: \u5f00\u6e90\u7684\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u591a\u4e2a\u6e90\u5934\u6536\u96c6\u6570\u636e\uff0c\u4f20\u5230Elasticsearch\uff0c\u80fd\u591f\u548cElasticsearch\u4ea7\u751f\u534f\u540c\u6548\u5e94 beats: \u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u642c\u8fd0\u5de5\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u670d\u52a1\u5668\u4e0a\u5c06\u6570\u636e\u4f20\u8f93\u5230Logstash\u6216\u8005Elasticsearch elasticsearch-head: \u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u67e5\u8be2Elasticsearch\u4e2d\u7684\u6570\u636e \u6ce8\uff1a FusionInsight HD\u7684Elasticsearch\u7ec4\u4ef6\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u4f46\u662f\u76f8\u5173\u7684\u5468\u8fb9\u751f\u6001Kibana\uff0cLogstash\uff0cbeats\uff0c elasticseach-head\u4e3a\u5f00\u6e90\uff0c\u6682\u65f6\u65e0\u6cd5\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u6545\u91c7\u7528\u5b89\u5168FI HD\u96c6\u7fa4\u7684\u975e\u5b89\u5168ES\u7ec4\u4ef6\u8fdb\u884c\u5bf9\u63a5","title":"\u751f\u6001\u7b80\u4ecb"},{"location":"Other/Elasticsearch_Related/#logstashfusioninsight-hd-es","text":"","title":"Logstash\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_2","text":"Logstash 6.4.2 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch\u7ec4\u4ef6\u975e\u5b89\u5168\u6a21\u5f0f)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_4","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dlogstash 6.4.2, \u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**logstash-6.4.2.zip**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt/logstash \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 unzip logstash-6.4.2.zip \u89e3\u538b\u5b89\u88c5\u5305 \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2/config \u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u914d\u7f6e\u6587\u4ef6 logstash-Simple.conf ,\u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ stdin{ } } output { stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"hellow_world\" #user => \"elastic\" #password => \"changeme\" } } \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2 \u4e0b\uff0c\u6267\u884c\u547d\u4ee4 bin/logstash -f config/logstash-Simple.conf \u6839\u636e\u4e4b\u524d\u7684 logstash-Simple.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash, \u7136\u540e\u5728\u7ec8\u7aef\u624b\u52a8\u8f93\u5165\u6570\u5b571\u52306\uff1a \u767b\u5f55elasticsearch-head\u670d\u52a1\u5668\u67e5\u770b\u7ed3\u679c\uff08\u5bf9\u63a5\u6b65\u9aa4\u53c2\u89c1\u540e\u6587\uff09","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#kibanafusioninsight-hd-es","text":"","title":"Kibana\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_5","text":"Kibana 6.1.3 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch\u7ec4\u4ef6\u975e\u5b89\u5168\u6a21\u5f0f)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_6","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_7","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dKibana 6.1.3, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Kibana 6.1.3**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf kibana-6.1.3-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 vi /opt/kibana-6.1.3-linux-x86_64/config/kibana.yml \u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\u9009\u9879: server.port: 5601 server.host: \"172.16.52.190\" server.name: \"LinuxTest\" elasticsearch.url: \"http://172.21.3.101:24100\" \u4f7f\u7528 bin/kibana \u542f\u52a8kibana \u8bbf\u95eekibana\u767b\u5f55\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u683c\u5f0f\u4e3ahttp://Kibana\u670d\u52a1IP\u5730\u5740:5601 \u9009\u62e9**Dev Tools** \u4f7f\u7528 GET _cat/indices \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4ES\u4e2d\u7684indices \u4f7f\u7528 GET hellow_world/_search \u547d\u4ee4\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#elasticsearch-headfusioninsight-hd-es","text":"","title":"elasticsearch-head\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_8","text":"elasticsearch-head \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch\u7ec4\u4ef6\u975e\u5b89\u5168\u6a21\u5f0f)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_9","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_10","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u5728FusionInsight Manager\uff0c\u9009\u62e9\u670d\u52a1\u7ba1\u7406->Elasticsearch\u670d\u52a1\u914d\u7f6e->\u670d\u52a1\u914d\u7f6e(\u9009\u62e9\u5168\u90e8\u914d\u7f6e)->\u81ea\u5b9a\u4e49\uff0c\u5728elasticsearch.yml\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4e0b\u9762\u4e24\u4e2a\u914d\u7f6e\u9879\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1 http.cors.enabled = true http.cors.allow-origin = \"*\" \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u4e14\u5b89\u88c5**elasticsearch-head**\u5230\u4e3b\u673a\u4e0a,\u5e76\u4e14\u542f\u52a8**elasticsearch-head**\u670d\u52a1 git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm install npm run start \u8bbf\u95eeelasticsearch-head\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u4e3ahttp://elasticsearch-head\u670d\u52a1IP\u5730\u5740:9100, \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u4e3a http://172.21.3.101:24100/ ,\u70b9\u51fb\u8fde\u63a5: \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#fi-hdbeats","text":"","title":"\u5728FI HD\u96c6\u7fa4\u4e0a\u90e8\u7f72beats"},{"location":"Other/Elasticsearch_Related/#_11","text":"filebeat-6.5.1 \u2194 FusionInsight HD V100R002C80SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_12","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_13","text":"\u4e0b\u8f7dFilebeat 6.5.1, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Filebeat 6.5.1**\u4f7f\u7528WinSCP\u5bfc\u5165FI HD\u96c6\u7fa4\u8282\u70b9\uff08172.21.3.103\uff09\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf filebeat-6.5.1-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 cd /opt/filebeat-6.5.1-linux-x86_64 \u8fdb\u5165filebeat\u5b89\u88c5\u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 filebeat_new.yml ,\u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-host3.log path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5046\"] \u6ce8\uff1a\u7aef\u53e35046\u53ef\u4ee5\u81ea\u5df1\u6307\u5b9a\uff0c\u4e0d\u51b2\u7a81\u5373\u53ef \u4f7f\u7528\u547d\u4ee4 ./filebeat -e -c filebeat_new.yml \u542f\u52a8filebeat","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#_14","text":"","title":"\u5e94\u7528\u573a\u666f\u4e3e\u4f8b\u8bf4\u660e"},{"location":"Other/Elasticsearch_Related/#_15","text":"\u5206\u522b\u5728FI HD\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72filebeat\u5b9e\u65f6\u83b7\u53d6\u4e24\u53f0\u670d\u52a1\u5668\u7684\u7684\u65e5\u5fd7\u6587\u4ef6\uff08/var/log/ipmitool.fi.log\uff09\uff0c\u901a\u8fc7logstash\u7ba1\u9053\u83b7\u53d6\u5e76\u8fc7\u6ee4\u5143\u65e5\u5fd7\u6587\u4ef6\u4e3a\u591a\u4e2a\u5b57\u6bb5\uff0c\u5e76\u4f20\u5230FI HD Elasticsearch\u7ec4\u4ef6\u4e0a\uff0c\u6700\u540e\u901a\u8fc7Kibana\u6765\u67e5\u770b\u83b7\u53d6\u7684\u65e5\u5fd7\u6587\u4ef6","title":"\u573a\u666f\u7b80\u4ecb"},{"location":"Other/Elasticsearch_Related/#_16","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210Kibana\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210logstash\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210filebeat\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_17","text":"\u9996\u5148\u767b\u9646FusionInsight HD\u96c6\u7fa4\u8282\u70b9172.21.3.102\u548c172.21.3.103\u4e0a \u53c2\u8003\u4e4b\u524d\u7684\u6b65\u9aa4\u5206\u522b\u90e8\u7f72filebeat\u5230172.21.3.102\u548c172.21.3.103\u4e0a\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u521b\u5efafilebeat\u914d\u7f6e\u6587\u4ef6 filebeat_new_host2.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5045\"] \u914d\u7f6e\u6587\u4ef6 filebeat_new_host3.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5047\"] \u767b\u9646\u5b89\u88c5logstash\u7684\u4e3b\u673a\uff0c\u914d\u7f6e\u4e00\u4e2a\u65b0\u7684\u542f\u52a8\u6587\u4ef6 logstash-beats-ipmitool.conf \u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ beats{ port => \"5045\" } beats{ port => \"5047\" } } filter{ grok{ match =>{\"message\" => \"(?<object>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<object2>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<status>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<number>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|%{GREEDYDATA:additional_info}\"} } } output{ stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"ipmitool_log\" } } \u542f\u52a8kibana \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-beats-ipmitool.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash \u5206\u522b\u542f\u52a8172.21.3.102\u548c172.21.3.103\u4e0a\u7684filebeat\u6765\u83b7\u53d6\u65e5\u5fd7\u6587\u4ef6 \u767b\u9646kibana\u7f51\u9875\u754c\u9762\uff0c\u9009\u62e9**Management**\u4e0b\u9762\u7684**Index Patterns** \u5728**Create index pattern**\u4e0b\u9009\u62e9\u521a\u521a\u7531logstash\u4f20\u8f93\u521b\u5efa\u7684index ipmitool_log \uff0c \u70b9\u51fb Next step \u5728step 2\u4e2d\u9009\u62e9**@timestamp**, \u70b9\u51fb**Create index pattern** \u5728 Discover \u90e8\u5206\u9009\u62e9\u521a\u521a\u751f\u6210\u7684index pattern ipmitool_log \u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u65e5\u5fd7\u7684\u60c5\u51b5 \u53ef\u6839\u636e\u4e0d\u540c\u7684\u5b57\u6bb5\u60c5\u51b5\u6765\u6574\u4f53\u4e86\u89e3\u65e5\u5fd7\u60c5\u51b5 \u5b8c\u6210","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/FUSE/","text":"FUSE\u5bf9\u63a5FusionInsight HDFS \u00b6 \u9002\u7528\u573a\u666f \u00b6 fuse 2.8.3 <-> FusionInsight HD V100R002C60U20\uff08\u975e\u5b89\u5168\u6a21\u5f0f\uff09 \u8bf4\u660e \u00b6 \u901a\u8fc7\u4f7f\u7528FUSE\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c06\u8fdc\u7aef\u7684HDFS\u6587\u4ef6\u7cfb\u7edfmount\u5230\u672c\u7aef\u7684Linux\u7cfb\u7edf\u4e2d\u4f7f\u7528 \u914d\u7f6e\u5bf9\u63a5 \u00b6 \u5b89\u88c5jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf /etc/profile \uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf export JAVA_HOME = /opt/jdk1.8.0_112 export CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar export PATH = $JAVA_HOME /bin: $PATH source /etc/profile \u5b89\u88c5rpm\u5305 yum install fuse fuse-devel fuse-libs \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u4ea7\u54c1\u6587\u6863\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u4f8b\u5982\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \u4e0b\u8f7dHadoop-2.7.2\u6e90\u7801hadoop-2.7.2-src.tar.gz\uff0c\u7f16\u8bd1fuse_dfs\uff0c\u5c06\u7f16\u8bd1\u597d\u7684fuse_dfs\u62f7\u8d1d\u5230/opt\u76ee\u5f55\u4e0b \u5c06\u6e90\u7801\u4e0b\u7684fuse_dfs_wrapper.sh\u811a\u672c\u62f7\u8d1d\u81f3 /opt \u76ee\u5f55\u4e0b\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u4f5c\u5982\u4e0b\u4fee\u6539(\u4fee\u6539HADOOP_PREFIX\u548cJAVA_HOME\u7684\u914d\u7f6e)\uff1a export HADOOP_PREFIX = /opt/hadoopclient/HDFS/hadoop if [ \" $OS_ARCH \" = \"\" ] ; then export OS_ARCH = amd64 fi if [ \" $JAVA_HOME \" = \"\" ] ; then export JAVA_HOME = /opt/jdk1.8.0_112 fi if [ \" $LD_LIBRARY_PATH \" = \"\" ] ; then export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX /share ] ; then export HADOOP_PREFIX = $HADOOP_PREFIX for f in ${ HADOOP_PREFIX } /share/hadoop/hdfs/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in $HADOOP_PREFIX /share/hadoop/hdfs/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/*.jar ; do export CLASSPATH = $CLASSPATH : $f done export PATH = /opt: $PATH export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib: $LD_LIBRARY_PATH fi fuse_dfs $@ \u66f4\u6539\u6587\u4ef6\u6743\u9650 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u672c\u5730\u521b\u5efamount\u76ee\u5f55 mkdir \u2013p /mnt/hdfs \u6267\u884c\u6302\u8f7d\u811a\u672c\uff0c\u5176\u4e2d162-1-95-196\u662fHDFS\u7684NameNode(hacluster,\u4e3b)\u7684\u4e3b\u673a\u540d ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ \u67e5\u770b/mnt/hdfs\u76ee\u5f55 \u5982\u679c\u9700\u8981\u5378\u8f7d\u6302\u8f7d\u70b9\uff0c\u6267\u884cumount /mnt/hdfs\u5373\u53ef \u9a8c\u8bc1\u5bf9\u63a5 \u00b6 hdfsclient\u5199 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient\u8bfb hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse\u5199 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse\u8bfb dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null","title":"\u5bf9\u63a5FUSE"},{"location":"Other/FUSE/#fusefusioninsight-hdfs","text":"","title":"FUSE\u5bf9\u63a5FusionInsight HDFS"},{"location":"Other/FUSE/#_1","text":"fuse 2.8.3 <-> FusionInsight HD V100R002C60U20\uff08\u975e\u5b89\u5168\u6a21\u5f0f\uff09","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/FUSE/#_2","text":"\u901a\u8fc7\u4f7f\u7528FUSE\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c06\u8fdc\u7aef\u7684HDFS\u6587\u4ef6\u7cfb\u7edfmount\u5230\u672c\u7aef\u7684Linux\u7cfb\u7edf\u4e2d\u4f7f\u7528","title":"\u8bf4\u660e"},{"location":"Other/FUSE/#_3","text":"\u5b89\u88c5jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf /etc/profile \uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf export JAVA_HOME = /opt/jdk1.8.0_112 export CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar export PATH = $JAVA_HOME /bin: $PATH source /etc/profile \u5b89\u88c5rpm\u5305 yum install fuse fuse-devel fuse-libs \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u4ea7\u54c1\u6587\u6863\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u4f8b\u5982\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \u4e0b\u8f7dHadoop-2.7.2\u6e90\u7801hadoop-2.7.2-src.tar.gz\uff0c\u7f16\u8bd1fuse_dfs\uff0c\u5c06\u7f16\u8bd1\u597d\u7684fuse_dfs\u62f7\u8d1d\u5230/opt\u76ee\u5f55\u4e0b \u5c06\u6e90\u7801\u4e0b\u7684fuse_dfs_wrapper.sh\u811a\u672c\u62f7\u8d1d\u81f3 /opt \u76ee\u5f55\u4e0b\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u4f5c\u5982\u4e0b\u4fee\u6539(\u4fee\u6539HADOOP_PREFIX\u548cJAVA_HOME\u7684\u914d\u7f6e)\uff1a export HADOOP_PREFIX = /opt/hadoopclient/HDFS/hadoop if [ \" $OS_ARCH \" = \"\" ] ; then export OS_ARCH = amd64 fi if [ \" $JAVA_HOME \" = \"\" ] ; then export JAVA_HOME = /opt/jdk1.8.0_112 fi if [ \" $LD_LIBRARY_PATH \" = \"\" ] ; then export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX /share ] ; then export HADOOP_PREFIX = $HADOOP_PREFIX for f in ${ HADOOP_PREFIX } /share/hadoop/hdfs/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in $HADOOP_PREFIX /share/hadoop/hdfs/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/*.jar ; do export CLASSPATH = $CLASSPATH : $f done export PATH = /opt: $PATH export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib: $LD_LIBRARY_PATH fi fuse_dfs $@ \u66f4\u6539\u6587\u4ef6\u6743\u9650 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u672c\u5730\u521b\u5efamount\u76ee\u5f55 mkdir \u2013p /mnt/hdfs \u6267\u884c\u6302\u8f7d\u811a\u672c\uff0c\u5176\u4e2d162-1-95-196\u662fHDFS\u7684NameNode(hacluster,\u4e3b)\u7684\u4e3b\u673a\u540d ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ \u67e5\u770b/mnt/hdfs\u76ee\u5f55 \u5982\u679c\u9700\u8981\u5378\u8f7d\u6302\u8f7d\u70b9\uff0c\u6267\u884cumount /mnt/hdfs\u5373\u53ef","title":"\u914d\u7f6e\u5bf9\u63a5"},{"location":"Other/FUSE/#_4","text":"hdfsclient\u5199 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient\u8bfb hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse\u5199 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse\u8bfb dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null","title":"\u9a8c\u8bc1\u5bf9\u63a5"},{"location":"Other/GIS_Tools/","text":"GIS Tools for Hadoop\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 GIS Tools for Hadoop <-> FusionInsight HD V100R002C60U20 aggregation-hive \u00b6 \u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive \u4e2d\u5173\u4e8e\u96c6\u6210Hive\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2aHiveAdmin\u89d2\u8272\uff0c\u5177\u4f53\u8bf7\u53c2\u52a0\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efaHive\u89d2\u8272 \u7ae0\u8282\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237 testuser \u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7FusionInsight HD\u7684\u5ba2\u6237\u7aef\u4e0a\u4f20\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u5c06\u76ee\u5f55gis-tools-for-hadoop-master\u76f4\u63a5\u653e\u5230HDFS\u7684\u6839\u76ee\u5f55\u4e0b\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master \u4fee\u6539\u6267\u884chive\u793a\u4f8b\u7684sql\u6587\u4ef6\uff0c\u4fee\u6539\u540e\u7684\u6587\u4ef6\u5982\u4e0b set role admin ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / esri - geometry - api . jar ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / spatial - sdk - hadoop . jar ; reload function ; DROP TABLE earthquakes ; DROP TABLE counties ; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point' ; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains' ; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING , latitude DOUBLE , longitude DOUBLE , depth DOUBLE , magnitude DOUBLE , magtype string , mbstations string , gap string , distance string , rms string , source string , eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data' ; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string , Perimeter string , State string , County string , Name string , BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data' ; SELECT counties . name , count ( * ) cnt FROM counties JOIN earthquakes WHERE ST_Contains ( counties . boundaryshape , ST_Point ( earthquakes . longitude , earthquakes . latitude )) GROUP BY counties . name ORDER BY cnt desc ; \u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u6267\u884c\u4fee\u6539\u540e\u7684sql\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql \u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff0c\u4e0eGIS\u5f00\u6e90\u7f51\u7ad9\u63cf\u8ff0\u4e00\u81f4 aggregation-mr \u00b6 \u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr \u4e2d\u5173\u4e8e\u96c6\u6210MR\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctestuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh \u5982\u4e0b\uff0c\u5176\u4e2d26004\u4e3ayarn\u914d\u7f6e\u7684yarn.resourcemanager.port\u7aef\u53e3 #!/bin/bash NAME_NODE_URL = hdfs://hacluster JOB_TRACKER_URL = 162 .1.93.103:26004 SAMPLE_DIR = /tmp/gistest JOB_DIR = $SAMPLE_DIR /job LIB_DIR = $SAMPLE_DIR /lib DATA_DIR = $SAMPLE_DIR /data OUTPUT_DIR = $SAMPLE_DIR /output \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh \u7684\u6267\u884c\u6743\u9650\uff0c\u5e76\u6267\u884c source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh \u6267\u884c\u5b8c\u6bd5\u5f97\u5230\u5982\u4e0b\u7ed3\u679c\u6587\u4ef6result.txt","title":"\u5bf9\u63a5Gis-Tools-For-Hadoop"},{"location":"Other/GIS_Tools/#gis-tools-for-hadoopfusioninsight","text":"","title":"GIS Tools for Hadoop\u5bf9\u63a5FusionInsight"},{"location":"Other/GIS_Tools/#_1","text":"GIS Tools for Hadoop <-> FusionInsight HD V100R002C60U20","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/GIS_Tools/#aggregation-hive","text":"\u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive \u4e2d\u5173\u4e8e\u96c6\u6210Hive\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2aHiveAdmin\u89d2\u8272\uff0c\u5177\u4f53\u8bf7\u53c2\u52a0\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efaHive\u89d2\u8272 \u7ae0\u8282\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237 testuser \u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7FusionInsight HD\u7684\u5ba2\u6237\u7aef\u4e0a\u4f20\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u5c06\u76ee\u5f55gis-tools-for-hadoop-master\u76f4\u63a5\u653e\u5230HDFS\u7684\u6839\u76ee\u5f55\u4e0b\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master \u4fee\u6539\u6267\u884chive\u793a\u4f8b\u7684sql\u6587\u4ef6\uff0c\u4fee\u6539\u540e\u7684\u6587\u4ef6\u5982\u4e0b set role admin ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / esri - geometry - api . jar ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / spatial - sdk - hadoop . jar ; reload function ; DROP TABLE earthquakes ; DROP TABLE counties ; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point' ; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains' ; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING , latitude DOUBLE , longitude DOUBLE , depth DOUBLE , magnitude DOUBLE , magtype string , mbstations string , gap string , distance string , rms string , source string , eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data' ; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string , Perimeter string , State string , County string , Name string , BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data' ; SELECT counties . name , count ( * ) cnt FROM counties JOIN earthquakes WHERE ST_Contains ( counties . boundaryshape , ST_Point ( earthquakes . longitude , earthquakes . latitude )) GROUP BY counties . name ORDER BY cnt desc ; \u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u6267\u884c\u4fee\u6539\u540e\u7684sql\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql \u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff0c\u4e0eGIS\u5f00\u6e90\u7f51\u7ad9\u63cf\u8ff0\u4e00\u81f4","title":"aggregation-hive"},{"location":"Other/GIS_Tools/#aggregation-mr","text":"\u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr \u4e2d\u5173\u4e8e\u96c6\u6210MR\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctestuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh \u5982\u4e0b\uff0c\u5176\u4e2d26004\u4e3ayarn\u914d\u7f6e\u7684yarn.resourcemanager.port\u7aef\u53e3 #!/bin/bash NAME_NODE_URL = hdfs://hacluster JOB_TRACKER_URL = 162 .1.93.103:26004 SAMPLE_DIR = /tmp/gistest JOB_DIR = $SAMPLE_DIR /job LIB_DIR = $SAMPLE_DIR /lib DATA_DIR = $SAMPLE_DIR /data OUTPUT_DIR = $SAMPLE_DIR /output \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh \u7684\u6267\u884c\u6743\u9650\uff0c\u5e76\u6267\u884c source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh \u6267\u884c\u5b8c\u6bd5\u5f97\u5230\u5982\u4e0b\u7ed3\u679c\u6587\u4ef6result.txt","title":"aggregation-mr"},{"location":"SQL_Analytics/","text":"SQL\u5206\u6790\u5f15\u64ce \u00b6 \u5bf9\u63a5Apache Kylin Apache Kylin 1.6.0 <-> FusionInsight HD V100R002C60U20 Apache Kylin 2.1.0 <-> FusionInsight HD V100R002C70SPC100 Apache Kylin 2.3.1 <-> FusionInsight HD V100R002C80SPC100 Apache Kylin 2.6.1 <-> FusionInsight HD 6.5 \u5bf9\u63a5Kyligence \u5bf9\u63a5Presto Presto0.155 <-> FusionInsight HD V100R002C60U20 Presto0.184 <-> FusionInsight HD V100R002C70SPC100 Presto0.210 <-> FusionInsight HD V100R002C80SPC200","title":"SQL\u5206\u6790\u5f15\u64ce"},{"location":"SQL_Analytics/#sql","text":"\u5bf9\u63a5Apache Kylin Apache Kylin 1.6.0 <-> FusionInsight HD V100R002C60U20 Apache Kylin 2.1.0 <-> FusionInsight HD V100R002C70SPC100 Apache Kylin 2.3.1 <-> FusionInsight HD V100R002C80SPC100 Apache Kylin 2.6.1 <-> FusionInsight HD 6.5 \u5bf9\u63a5Kyligence \u5bf9\u63a5Presto Presto0.155 <-> FusionInsight HD V100R002C60U20 Presto0.184 <-> FusionInsight HD V100R002C70SPC100 Presto0.210 <-> FusionInsight HD V100R002C80SPC200","title":"SQL\u5206\u6790\u5f15\u64ce"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/","text":"Apache Kylin\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 1.6.0 (\u57fa\u4e8eHBase1.0.2\u7684\u5206\u652f\u7248\u672c\uff1ayang21-hbase102) <-> FusionInsight HD V100R002C60U20 \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 162.2.200.200 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm \u7f16\u8bd1Kylin \u00b6 \u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-1.6.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u914d\u59571.0.2\u7684HBase\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-1.6.0\u57fa\u4e8eHBase1.0.2\u7248\u672c\u7684\u6e90\u7801 https://github.com/apache/kylin/tree/yang21-hbase102 \u5f97\u5230kylin-yang21-hbase102.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u7f16\u8bd1\u6253\u5305 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305 \u542f\u52a8Kylin \u00b6 \u89e3\u538b\u4e8c\u8fdb\u5236\u5305 \u00b6 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test_cn Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u7f16\u8f91find-hive-dependency.sh\uff1a vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'` \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u6784\u5efaCube \u00b6 \u9009\u62e9learn_kylin\u5de5\u7a0b\uff0c\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"Kylin 1.6.0 <--> C60"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#apache-kylinfusioninsight","text":"","title":"Apache Kylin\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_1","text":"Apache Kylin 1.6.0 (\u57fa\u4e8eHBase1.0.2\u7684\u5206\u652f\u7248\u672c\uff1ayang21-hbase102) <-> FusionInsight HD V100R002C60U20","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 162.2.200.200 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin","text":"\u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-1.6.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u914d\u59571.0.2\u7684HBase\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-1.6.0\u57fa\u4e8eHBase1.0.2\u7248\u672c\u7684\u6e90\u7801 https://github.com/apache/kylin/tree/yang21-hbase102 \u5f97\u5230kylin-yang21-hbase102.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u7f16\u8bd1\u6253\u5305 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305","title":"\u7f16\u8bd1Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_1","text":"","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_4","text":"\u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt","title":"\u89e3\u538b\u4e8c\u8fdb\u5236\u5305"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_5","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test_cn Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_2","text":"\u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u7f16\u8f91find-hive-dependency.sh\uff1a vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'`","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_3","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#cube","text":"\u9009\u62e9learn_kylin\u5de5\u7a0b\uff0c\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_6","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/","text":"Apache Kylin2.1.0\u5bf9\u63a5FusionInsight_HD_C70 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.1.0 <-> FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 <-> FusionInsight HD V100R002C70SPC200 \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm \u7f16\u8bd1Kylin \u00b6 \u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-2.1.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-2.1.0\u57fa\u4e8eHBase1.1.1\u7248\u672c\u7684\u6e90\u7801\u7801 https://github.com/apache/kylin/tree/2.1.x \u5f97\u5230kylin-2.1.x.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u4fee\u6539kylin\u6e90\u7801 \u4fee\u6539HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java \u4fee\u6539pom.xml vi /opt/kylin-2.1.x/pom.xml \u5c06HBase\u3001Hive\u3001Hadoop\u7248\u672c\u6539\u6210\u4e0eFusionInsight HD\u5bf9\u5e94\u7684\u7248\u672c \u7f16\u8bd1\u6253\u5305 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305 \u542f\u52a8Kylin \u00b6 \u89e3\u538b\u4e8c\u8fdb\u5236\u5305 \u00b6 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u9700\u8981\u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"Kylin 2.1.0 <--> C70"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#apache-kylin210fusioninsight_hd_c70","text":"","title":"Apache Kylin2.1.0\u5bf9\u63a5FusionInsight_HD_C70"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_1","text":"Apache Kylin 2.1.0 <-> FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 <-> FusionInsight HD V100R002C70SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin","text":"\u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-2.1.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-2.1.0\u57fa\u4e8eHBase1.1.1\u7248\u672c\u7684\u6e90\u7801\u7801 https://github.com/apache/kylin/tree/2.1.x \u5f97\u5230kylin-2.1.x.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u4fee\u6539kylin\u6e90\u7801 \u4fee\u6539HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java \u4fee\u6539pom.xml vi /opt/kylin-2.1.x/pom.xml \u5c06HBase\u3001Hive\u3001Hadoop\u7248\u672c\u6539\u6210\u4e0eFusionInsight HD\u5bf9\u5e94\u7684\u7248\u672c \u7f16\u8bd1\u6253\u5305 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305","title":"\u7f16\u8bd1Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_1","text":"","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_4","text":"\u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt","title":"\u89e3\u538b\u4e8c\u8fdb\u5236\u5305"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_5","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_2","text":"\u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u9700\u8981\u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_3","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_6","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/","text":"Apache Kylin2.3.1\u5bf9\u63a5FusionInsight_HD_C80 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.3.1 <-> FusionInsight HD V100R002C80SPC100 \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient \u5b89\u88c5JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm \u4e0b\u8f7dKylin \u00b6 Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.3.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin \u4e0b\u8f7d\u89e3\u538bKylin \u00b6 \u4e0b\u8f7dKylin-2.3.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz \u4e0a\u4f20apache-kylin-2.3.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt \u914d\u7f6eKylin \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh \u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879 \u00b6 \u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..* \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"Kylin 2.3.1 <--> C80"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#apache-kylin231fusioninsight_hd_c80","text":"","title":"Apache Kylin2.3.1\u5bf9\u63a5FusionInsight_HD_C80"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_1","text":"Apache Kylin 2.3.1 <-> FusionInsight HD V100R002C80SPC100","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient \u5b89\u88c5JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin","text":"Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.3.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin","title":"\u4e0b\u8f7dKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_1","text":"\u4e0b\u8f7dKylin-2.3.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz \u4e0a\u4f20apache-kylin-2.3.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt","title":"\u4e0b\u8f7d\u89e3\u538bKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_2","text":"","title":"\u914d\u7f6eKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#fusioninsighthive","text":"\u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..*","title":"\u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_3","text":"\u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_4","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_5","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/","text":"Apache Kylin2.6.1\u5bf9\u63a5FusionInsight_HD_C80 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.6.1 <-> FusionInsight HD 6.5.0 \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient \u4e0b\u8f7dKylin \u00b6 Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.6.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin \u4e0b\u8f7d\u89e3\u538bKylin \u00b6 \u4e0b\u8f7dKylin-2.6.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c [ https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-2.6.1/apache-kylin-2.6.1-bin-hbase1x.tar.gz ] \u4e0a\u4f20\u5b89\u88c5\u5305\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.6.1-hbase1x-bin.tar.gz -C /opt \u914d\u7f6eKylin \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.6.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.6.1-bin-hbase1x/bin ./check-env.sh \u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879 \u00b6 \u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..* \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2 Q&A \u4fee\u6539pom\u6587\u4ef6 *","title":"Kylin 2.6.1 <--> 6.5"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#apache-kylin261fusioninsight_hd_c80","text":"","title":"Apache Kylin2.6.1\u5bf9\u63a5FusionInsight_HD_C80"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_1","text":"Apache Kylin 2.6.1 <-> FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_3","text":"\u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin","text":"Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.6.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin","title":"\u4e0b\u8f7dKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_1","text":"\u4e0b\u8f7dKylin-2.6.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c [ https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-2.6.1/apache-kylin-2.6.1-bin-hbase1x.tar.gz ] \u4e0a\u4f20\u5b89\u88c5\u5305\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.6.1-hbase1x-bin.tar.gz -C /opt","title":"\u4e0b\u8f7d\u89e3\u538bKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_2","text":"","title":"\u914d\u7f6eKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.6.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.6.1-bin-hbase1x/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#fusioninsighthive","text":"\u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..*","title":"\u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_3","text":"\u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_4","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_5","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2 Q&A \u4fee\u6539pom\u6587\u4ef6 *","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Kyligence/","text":"Kyligence Analytics Platform\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kyligence Analytics Platform v2.2 <-> FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.3 <-> FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.4 <-> FusionInsight HD V100R002C70SPC200 Kyligence Analytics Platform v2.5 <-> FusionInsight HD V100R002C70SPC200 \u8bf4\u660e \u00b6 \u53c2\u8003 \u5b98\u65b9\u4ea7\u54c1\u6587\u6863 \u7684 \u5feb\u901f\u5b89\u88c5 \u7ae0\u8282","title":"\u5bf9\u63a5Kyligence"},{"location":"SQL_Analytics/Kyligence/#kyligence-analytics-platformfusioninsight","text":"","title":"Kyligence Analytics Platform\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Kyligence/#_1","text":"Kyligence Analytics Platform v2.2 <-> FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.3 <-> FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.4 <-> FusionInsight HD V100R002C70SPC200 Kyligence Analytics Platform v2.5 <-> FusionInsight HD V100R002C70SPC200","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Kyligence/#_2","text":"\u53c2\u8003 \u5b98\u65b9\u4ea7\u54c1\u6587\u6863 \u7684 \u5feb\u901f\u5b89\u88c5 \u7ae0\u8282","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.155/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto0.155 <-> FusionInsight HD V100R002C60U20 \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eHive Connector \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.155 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.155\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.155\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.155 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.155.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log \u68c0\u67e5FusionInsight Manager\u4e2dHDFS\u670d\u52a1\u914d\u7f6e\u4e2dhadoop.rpc.protection\u7684\u914d\u7f6e\uff0c\u5fc5\u987b\u8bbe\u7f6e\u4e3aauthentacation\u3002 \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.155-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.155/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar \u53c2\u8003 https://prestodb.io/docs/0.155/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"Presto0.146 <--> C60"},{"location":"SQL_Analytics/Presto_0.155/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.155/#_1","text":"Presto0.155 <-> FusionInsight HD V100R002C60U20","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.155/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.155/#hive-connector","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.155 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.155\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.155\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.155 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.155.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log \u68c0\u67e5FusionInsight Manager\u4e2dHDFS\u670d\u52a1\u914d\u7f6e\u4e2dhadoop.rpc.protection\u7684\u914d\u7f6e\uff0c\u5fc5\u987b\u8bbe\u7f6e\u4e3aauthentacation\u3002","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.155/#presto-clihive","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.155-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.155/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.155/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar \u53c2\u8003 https://prestodb.io/docs/0.155/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.184/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto0.184 <-> FusionInsight HD V100R002C70SPC100 Presto0.196 <-> FusionInsight HD V100R002C80SPC100 \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eHive Connector \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.184 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C70SPC100\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.184\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.184/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ \u6309\u7167\u4e0b\u56fe\u4fee\u6539hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.184\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.184 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.184.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.184-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.184/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar \u53c2\u8003 https://prestodb.io/docs/0.184/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"Presto0.184 <--> C70"},{"location":"SQL_Analytics/Presto_0.184/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.184/#_1","text":"Presto0.184 <-> FusionInsight HD V100R002C70SPC100 Presto0.196 <-> FusionInsight HD V100R002C80SPC100","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.184/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.184/#hive-connector","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.184 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C70SPC100\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.184\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.184/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ \u6309\u7167\u4e0b\u56fe\u4fee\u6539hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.184\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.184 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.184.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.184/#presto-clihive","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.184-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.184/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.184/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar \u53c2\u8003 https://prestodb.io/docs/0.184/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto0.210 <-> FusionInsight HD V100R002C80SPC200 Presto0.210 <-> FusionInsight HD 6.5.0 \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u8fdb\u884c\u5bf9\u63a5,\u5728Presto0.210\u7248\u672c\u4e2d\u652f\u6301\u4e0eFusionInsight\u7684ES\u8fdb\u884c\u5bf9\u63a5\u3002 \u83b7\u53d6\u5e76\u914d\u7f6epresto server \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.210/presto-server-0.210.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.210 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C80SPC200\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto-0.210\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool -genkeypair -alias testuser -keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d,\u5e76\u4e14\u8981\u5ffd\u7565\u5927\u5c0f\u5199\uff0c\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u9009\u62e9\u7528\u6237\u7ec4(hadoop\u548chive\u7ec4)\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.210/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.210/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.210/etc/catalog/ \u5c06hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4fee\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.210\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.210 \u83b7\u53d6Presto CLI\u542f\u52a8\u5305 \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.210/presto-cli-0.210-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.210-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u914d\u7f6eHive Connector \u00b6 \u8fdb\u5165\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://172.21.3.115:21088,thrift://172.21.3.116:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.210/etc/catalog/core-site.xml,/opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP\"\u7684\u503c\u4fee\u6539\u4e3a\u56fa\u5b9a\u7684\"auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.210.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.210/plugin/hive-hadoop2/presto-hive-0.210.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.210/bin/launcher start tailf /var/presto/data/var/log/server.log \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982/opt\uff1b \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.210/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.210/presto-jdbc-0.210.jar \u53c2\u8003 https://prestodb.io/docs/0.210/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://172.21.3.48:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from adult limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u914d\u7f6eElasticSearch Connector \u00b6 presto\u548cES\u5b98\u65b9\u90fd\u6ca1\u6709\u7ed9\u51fa\u9002\u914d\u7684\u6587\u6863\u4ecb\u7ecd\uff0c\u8fd9\u91cc\u6211\u4eec\u91c7\u7528\u5f00\u6e90\u7684\u9002\u914d\u5305\u8fdb\u884c\u9002\u914d\u3002\u5728https://github.com/harbby/presto-connectors \u4e0b\u8f7d\u9002\u914d\u5305\u6e90\u7801\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\uff0c\u89e3\u538b\u3002 \u4fee\u6539presto-elasticsearch-connectors\u6e90\u7801\u4ee5\u53ca\u914d\u7f6e \u8fdb\u5165/opt/presto-connectors-master/presto-elasticsearch6/src/main/java/com/facebook/presto/elasticsearch6/functions\u76ee\u5f55\uff0c\u53c2\u8003\u4e0b\u56fe\uff0c\u4fee\u6539MatchQueryFunction.java\u6587\u4ef6\uff0c\u6dfb\u52a0\u6784\u9020\u51fd\u6570\uff0c\u5c06\u51fd\u6570\u58f0\u660e\u4e3apublic \u8fdb\u5165presto-connectors-master\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-connectors-master vi pom.xml \u5728\u6700\u540e\u7684' '\u4e4b\u524d\uff0c\u6dfb\u52a0\u4ee5\u4e0bplugin\u4f9d\u8d56 <build> <pluginManagement> <plugins> <plugin> <groupId>pl.project13.maven</groupId> <artifactId>git-commit-id-plugin</artifactId> <configuration> <skip>true</skip> </configuration> </plugin> </plugins> </pluginManagement> </build> \uff08\u53ef\u9009\u64cd\u4f5c\uff09\u5728modules\u4e2d\uff0c\u53bb\u6389\u9664presto-base-elasticsearch\u548cpresto-elasticsearch6\u4ee5\u5916\u7684module\uff0c\u5176\u4ed6\u7684module\u8fd9\u91cc\u5e76\u4e0d\u9700\u8981\uff0c\u53ef\u4ee5\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4 \u8fdb\u5165presto-elasticsearch6\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-elasticsearch6 vi pom.xml \u5c06'elasticsearch.version'\u4fee\u6539\u4e3a6.1.3 \u5c06'elasticsearch-x-content'\u548c'elasticsearch-core'\u7684\u4f9d\u8d56\u6ce8\u91ca\u6389 \u56de\u5230presto-connectors-master\u76ee\u5f55\uff0c\u7f16\u8bd1presto-connectors mvn clean install -DskipTests \u7f16\u8bd1\u6210\u529f\u540e\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u83b7\u53d6'presto-connectors-master/presto-elasticsearch6/target'\u76ee\u5f55\u4e0b\u7684'presto-elasticsearch6-0.210'\u6587\u4ef6\u5939\uff0c\u5c06\u5176\u590d\u5236\u5230presto-server\u7684plugin\u76ee\u5f55\u4e0b cp -r /opt/presto-connectors-master/presto-elasticsearch6/target/presto-elasticsearch6-0.210 /opt/presto-server-0.210/plugin \u767b\u5f55\u96c6\u7fa4manager\u7ba1\u7406\u9875\u9762\uff0c\u5728'\u670d\u52a1\u7ba1\u7406->Elasticsearch \u670d\u52a1\u914d\u7f6e'\u9875\u9762\uff0c\u9009\u62e9\u5168\u90e8\u914d\u7f6e\uff0c\u641c\u7d22'port'\u5173\u952e\u8bcd,\u67e5\u770b'SERVER_PORT'\u914d\u7f6e\u4e3a24100\uff0c'TRANSPORT_TCP_PORT'\u914d\u7f6e\u4e3a24101 \u5728\u96c6\u7fa4\u5ba2\u6237\u7aef\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_evn curl -XGET http://172.21.3.115:24100/_cluster/health?pretty \u5176\u4e2d172.21.3.115\u662felasticsearch\u96c6\u7fa4\u8282\u70b9\uff0c24100\u4e3aSERVER_PORT\uff0c\u770b\u5230\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c { \"cluster_name\" : \"elasticsearch_cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 33, \"active_shards\" : 66, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } \u5728/opt/presto-server-0.210/etc/catalog\u76ee\u5f55\u4e0b\u521b\u5efaes.properties\u6587\u4ef6 connector.name=elasticsearch6 elasticsearch.cluster.name=elasticsearch_cluster elasticsearch.transport.hosts=172.21.3.115:24101 \u5176\u4e2d'elasticsearch.cluster.name'\u662f\u521a\u624d\u83b7\u53d6\u7684ES\u96c6\u7fa4\u7684\u540d\u5b57\uff0c'elasticsearch.transport.hosts'\u4e3aEsNode\u8282\u70b9IP\uff0c\u7aef\u53e3\u4e3a'TRANSPORT_TCP_PORT' \u91cd\u542fpresto-server sh /opt/presto-server-0.210/bin/launcher restart \u901a\u8fc7Presto CLI\u8fde\u63a5ElasticSearch \u00b6 \u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55,\u4f8b\u5982/opt \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog es \\ --schema default \\ --; catalog\u540e\u9762\u7684es\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684es.properties\u7684\u6587\u4ef6\u540d\u5339\u914d \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\u67e5\u8be2ES\u4e2d\u7684\u7d22\u5f15\u4fe1\u606f \u5f53\u524dconnector\u652f\u6301show,create,select,insert,drop\u64cd\u4f5c\uff0c\u6682\u4e0d\u652f\u6301delete,update,alter\u7b49\u64cd\u4f5c","title":"Presto0.210 <--> C80"},{"location":"SQL_Analytics/Presto_0.210/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.210/#_1","text":"Presto0.210 <-> FusionInsight HD V100R002C80SPC200 Presto0.210 <-> FusionInsight HD 6.5.0","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.210/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u8fdb\u884c\u5bf9\u63a5,\u5728Presto0.210\u7248\u672c\u4e2d\u652f\u6301\u4e0eFusionInsight\u7684ES\u8fdb\u884c\u5bf9\u63a5\u3002","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.210/#presto-server","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.210/presto-server-0.210.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.210 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C80SPC200\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto-0.210\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool -genkeypair -alias testuser -keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d,\u5e76\u4e14\u8981\u5ffd\u7565\u5927\u5c0f\u5199\uff0c\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u9009\u62e9\u7528\u6237\u7ec4(hadoop\u548chive\u7ec4)\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.210/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.210/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.210/etc/catalog/ \u5c06hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4fee\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.210\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.210","title":"\u83b7\u53d6\u5e76\u914d\u7f6epresto server"},{"location":"SQL_Analytics/Presto_0.210/#presto-cli","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.210/presto-cli-0.210-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.210-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h","title":"\u83b7\u53d6Presto CLI\u542f\u52a8\u5305"},{"location":"SQL_Analytics/Presto_0.210/#hive-connector","text":"\u8fdb\u5165\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://172.21.3.115:21088,thrift://172.21.3.116:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.210/etc/catalog/core-site.xml,/opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP\"\u7684\u503c\u4fee\u6539\u4e3a\u56fa\u5b9a\u7684\"auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.210.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.210/plugin/hive-hadoop2/presto-hive-0.210.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.210/bin/launcher start tailf /var/presto/data/var/log/server.log","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.210/#presto-clihive","text":"\u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982/opt\uff1b \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.210/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.210/presto-jdbc-0.210.jar \u53c2\u8003 https://prestodb.io/docs/0.210/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://172.21.3.48:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from adult limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/#elasticsearch-connector","text":"presto\u548cES\u5b98\u65b9\u90fd\u6ca1\u6709\u7ed9\u51fa\u9002\u914d\u7684\u6587\u6863\u4ecb\u7ecd\uff0c\u8fd9\u91cc\u6211\u4eec\u91c7\u7528\u5f00\u6e90\u7684\u9002\u914d\u5305\u8fdb\u884c\u9002\u914d\u3002\u5728https://github.com/harbby/presto-connectors \u4e0b\u8f7d\u9002\u914d\u5305\u6e90\u7801\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\uff0c\u89e3\u538b\u3002 \u4fee\u6539presto-elasticsearch-connectors\u6e90\u7801\u4ee5\u53ca\u914d\u7f6e \u8fdb\u5165/opt/presto-connectors-master/presto-elasticsearch6/src/main/java/com/facebook/presto/elasticsearch6/functions\u76ee\u5f55\uff0c\u53c2\u8003\u4e0b\u56fe\uff0c\u4fee\u6539MatchQueryFunction.java\u6587\u4ef6\uff0c\u6dfb\u52a0\u6784\u9020\u51fd\u6570\uff0c\u5c06\u51fd\u6570\u58f0\u660e\u4e3apublic \u8fdb\u5165presto-connectors-master\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-connectors-master vi pom.xml \u5728\u6700\u540e\u7684' '\u4e4b\u524d\uff0c\u6dfb\u52a0\u4ee5\u4e0bplugin\u4f9d\u8d56 <build> <pluginManagement> <plugins> <plugin> <groupId>pl.project13.maven</groupId> <artifactId>git-commit-id-plugin</artifactId> <configuration> <skip>true</skip> </configuration> </plugin> </plugins> </pluginManagement> </build> \uff08\u53ef\u9009\u64cd\u4f5c\uff09\u5728modules\u4e2d\uff0c\u53bb\u6389\u9664presto-base-elasticsearch\u548cpresto-elasticsearch6\u4ee5\u5916\u7684module\uff0c\u5176\u4ed6\u7684module\u8fd9\u91cc\u5e76\u4e0d\u9700\u8981\uff0c\u53ef\u4ee5\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4 \u8fdb\u5165presto-elasticsearch6\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-elasticsearch6 vi pom.xml \u5c06'elasticsearch.version'\u4fee\u6539\u4e3a6.1.3 \u5c06'elasticsearch-x-content'\u548c'elasticsearch-core'\u7684\u4f9d\u8d56\u6ce8\u91ca\u6389 \u56de\u5230presto-connectors-master\u76ee\u5f55\uff0c\u7f16\u8bd1presto-connectors mvn clean install -DskipTests \u7f16\u8bd1\u6210\u529f\u540e\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u83b7\u53d6'presto-connectors-master/presto-elasticsearch6/target'\u76ee\u5f55\u4e0b\u7684'presto-elasticsearch6-0.210'\u6587\u4ef6\u5939\uff0c\u5c06\u5176\u590d\u5236\u5230presto-server\u7684plugin\u76ee\u5f55\u4e0b cp -r /opt/presto-connectors-master/presto-elasticsearch6/target/presto-elasticsearch6-0.210 /opt/presto-server-0.210/plugin \u767b\u5f55\u96c6\u7fa4manager\u7ba1\u7406\u9875\u9762\uff0c\u5728'\u670d\u52a1\u7ba1\u7406->Elasticsearch \u670d\u52a1\u914d\u7f6e'\u9875\u9762\uff0c\u9009\u62e9\u5168\u90e8\u914d\u7f6e\uff0c\u641c\u7d22'port'\u5173\u952e\u8bcd,\u67e5\u770b'SERVER_PORT'\u914d\u7f6e\u4e3a24100\uff0c'TRANSPORT_TCP_PORT'\u914d\u7f6e\u4e3a24101 \u5728\u96c6\u7fa4\u5ba2\u6237\u7aef\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_evn curl -XGET http://172.21.3.115:24100/_cluster/health?pretty \u5176\u4e2d172.21.3.115\u662felasticsearch\u96c6\u7fa4\u8282\u70b9\uff0c24100\u4e3aSERVER_PORT\uff0c\u770b\u5230\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c { \"cluster_name\" : \"elasticsearch_cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 33, \"active_shards\" : 66, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } \u5728/opt/presto-server-0.210/etc/catalog\u76ee\u5f55\u4e0b\u521b\u5efaes.properties\u6587\u4ef6 connector.name=elasticsearch6 elasticsearch.cluster.name=elasticsearch_cluster elasticsearch.transport.hosts=172.21.3.115:24101 \u5176\u4e2d'elasticsearch.cluster.name'\u662f\u521a\u624d\u83b7\u53d6\u7684ES\u96c6\u7fa4\u7684\u540d\u5b57\uff0c'elasticsearch.transport.hosts'\u4e3aEsNode\u8282\u70b9IP\uff0c\u7aef\u53e3\u4e3a'TRANSPORT_TCP_PORT' \u91cd\u542fpresto-server sh /opt/presto-server-0.210/bin/launcher restart","title":"\u914d\u7f6eElasticSearch Connector"},{"location":"SQL_Analytics/Presto_0.210/#presto-clielasticsearch","text":"\u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55,\u4f8b\u5982/opt \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog es \\ --schema default \\ --; catalog\u540e\u9762\u7684es\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684es.properties\u7684\u6587\u4ef6\u540d\u5339\u914d \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\u67e5\u8be2ES\u4e2d\u7684\u7d22\u5f15\u4fe1\u606f \u5f53\u524dconnector\u652f\u6301show,create,select,insert,drop\u64cd\u4f5c\uff0c\u6682\u4e0d\u652f\u6301delete,update,alter\u7b49\u64cd\u4f5c","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5ElasticSearch"}]}