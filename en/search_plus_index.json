{"./":{"url":"./","title":"FusionInsight Ecosystem","keywords":"","body":"﻿ FusionInsight生态地图 FusionInsight支持开源标准的Hadoop接口，可以与以下第三方工具进行对接 Thrid Party Tools FusionInsight Category Name Version C30 C50 C60 C70 C80 Data Analysis SAS Access for Hadoop SAS HPA SAS EP 9.4M3 HDFS Hive HDFS Hive HDFSHive HDFSHive HDFSHive IBM SPSS Analytic Server 3.1.0 　 　 HDFSSpark 　 　 IBM Cognos 10.2.2fp4 　 　 Hive 　 　 Tableau 10.0.0 HiveSparkSQL HiveSparkSQL HiveSparkSQL 　 　 10.1.4 　 　 HiveSparkSQL 　 　 10.3.2 　 　 　 HiveSparkSQL 　 10.5.0 　 　 　 　 HiveSparkSQL QlikView 12 　 　 HiveSparkSQL HiveSparkSQL HiveSparkSQL QlikSense 3.2.4 　 　 　 Hive SparkSQL 　 Oracle BIEE 11g 　 　 HiveSparkSQL HiveSparkSQLELKLibrA 　 12c 　 　 HiveSparkSQL HiveSparkSQLELKLibrA 　 Alteryx 11.0.2.25199 　 　 HDFS Hive SparkSQL 　 HDFSHiveSparkSQL RapidMiner 8.2.001 　 　 　 　 HDFSHiveMapReduceSpark SmartBI 7.2.32464.17374 　 　 　 Hive SparkSQL 　 Yonghong One-stop Big Data Analysis Platform 7.1 　 　 Hive SparkSQL 　 　 Data Integration IBM InfoSphere DataStage 11.3.1.0 　 HDFSHiveSparkSQL 　 　 　 11.5.0.2 　 　 HDFSPhoenixHiveSparkSQLKafkaLibrA 　 　 IBM InfoSphere CDC 11.3.3.1 　 HDFS 　 　 　 Oracle GoldenGate 12.2.0.1.1 　 　 HDFSHBaseFlumeKafka 　 　 12.3.1.1.1 　 　 　 HDFSHBaseFlumeKafka HDFSHBaseFlumeKafka Informatica BDM(native) 10.0.0 　 HDFS HBase Hive HDFS HBase Hive 　 　 Informatica BDM(push down) 10.2.0 　 　 　 HDFS HBase Hive Yarn 　 Talend 6.3.1 　 　 HDFS HBase Hive 　 　 6.4.1 　 　 　 HDFSHBaseHive HDFSHBaseHive 7.0.1 　 　 　 HDFSHBaseHive HDFSHBaseHive Apache NiFi 1.7.1 　 　 　 　 HDFSHBaseHiveSparkKafka Kettle 6.1 　 　 HDFSHive HDFSHive HDFSHive Pentaho 7.1 　 　 　 HDFS Hive 　 8.0 　 　 HDFS Hive 　 　 Knime 3.6.1 　 　 　 　 HDFSHiveSpark kafka-manager 1.3.3.21 　 　 　 　 Kafka Informatica PWX CDC 10.2.0 　 　 　 　 Kafka Primeton MetaCube 6.1 　 HDFS HBase Hive 　 　 　 Unimas UTL 5.1 　 HDFS HBase Hive Kafka 　 　 　 Integrated Development RStudio 1.0.153 　 　 SparkR SparkR 　 Apache Zepplin 0.7.2 　 　 HBaseHiveSparkSparkR 　 　 0.7.3 　 　 　 HBaseHiveSparkSparkR HBaseHiveSparkSparkR 0.8.0 　 　 　 　 HBaseHiveSparkSparkR Jypyter Notebook 　 　 　 　 pySpark 　 DBeaver 4.0.8 　 　 PhoenixHiveSparkSQL 　 　 4.2.1 　 　 　 PhoenixHiveSparkSQL 　 DbVisualizer 9.5.7 　 　 PhoenixHiveSparkSQL 　 　 10.0.1 　 　 　 PhoenixHiveSparkSQL 　 Squirrel 3.7.1 　 　 PhoenixHiveSparkSQL 　 　 3.8.0 　 　 　 PhoenixHiveSparkSQL 　 Splunk 7.2.4 　 　 　 　 HDFSHive HUE 4.0.1 　 　 　 HDFS HBase Hive Spark 　 SQL Analysis Apache Kylin 1.6.0 　 　 HiveHBase 　 　 2.1.0 　 　 　 HiveHBase 　 2.3.0 　 　 　 HiveHBase 　 2.3.1 　 　 　 　 HiveHBase Kyligence 2.2 　 　 HiveHBase 　 　 2.3 　 　 HiveHBase 　 　 2.4 　 　 　 HiveHBase 　 2.5 　 　 　 HiveHBase 　 3.0 　 　 　 　 HiveHBaseYarn Presto 0.155 　 　 HDFSHive 　 　 0.184 　 　 　 HDFSHive 　 0.210 　 　 　 　 HDFSHiveElasticSearch Database SAP HANA 100_120_0-10009569 　 Hive Hive Hive Hive SAP VORA 2.0 　 　 　 Spark 　 2.1 　 　 　 Spark 　 Unimas UDB 6.1 　 LibrA 　 　 　 Other FUSE 2.8.3 　 　 HDFS 　 　 gis-tools-for-hadoop github 　 　 HiveMapReduce 　 　 IBM WAS 8.5.5.9 　 IBM_JDK 　 　 　 Apache Livy 0.5.0 　 　 　 　 Spark NeoKylin 6.9 　 　 　 OS 　 7.2 　 　 　 OS 　 "},"Business_Intelligence/":{"url":"Business_Intelligence/","title":"Data Analysis","keywords":"","body":" Data Analysis SAS (TBD) IBM SPSS (TBD) IBM Cognos (TBD) Alteryx Tableau QlikView Oracle BIEE RapidMiner Yonghong BI (TBD) "},"Business_Intelligence/Using_Alteryx_with_FusionInsight.html":{"url":"Business_Intelligence/Using_Alteryx_with_FusionInsight.html","title":"Alteryx","keywords":"","body":"Alteryx对接FusionInsight 适用场景 Alteryx 2018.2.5.48994 FusionInsight HD V100R002C80SPC200 配置Windows的kerberos认证 下载并安装MIT Kerberos 下载网址：http://web.mit.edu/kerberos/dist/#kfw-4.0 版本与操作系统位数保持一致，本文版本kfw-4.1-amd64.msi。 确认客户端机器的时间与FusionInsight HD集群的时间一致，时间差要小于5分钟 设置Kerberos的配置文件 在FusionInsight Manager创建一个角色与“人机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。角色需要根据业务需要授予Spark，Hive，HDFS的访问权限，并将用户加入角色。例如，创建用户“developuser”并下载对应的keytab文件user.keytab以及krb5.conf文件，把krb5.conf文件重命名为krb5.ini，并放到C:\\ProgramData\\MIT\\Kerberos5目录中。 设置Kerberos票据的缓存文件 创建存放票据的目录，例如“C:\\temp”。 设置Windows的系统环境变量，变量名为“KRB5CCNAME”，变量值为“C:\\temp\\krb5cache”。 重启机器。 在Windows上进行认证 使用Kerbers认证的用户名密码登录，用户名的格式为：用户名@Kerberos域名。 打开MIT Kerberos，单击“get Ticket”，在弹出的“MIT Kerberos: Get Ticket”窗口中，“Pricipal”输入用户名，“Password”输入密码，单击“OK”。 配置Spark ODBC 连接 在操作系统中配置Spark ODBC驱动 下载并安装ODBC驱动：https://www.tableau.com/support/drivers 根据操作系统类型选择对应的ODBC版本，下载并安装。 创建DSN(Data Source Name)：选择 开始 -> Simba Spark ODBC Driver -> ODBC Administrator。 选择 System DSN -> Add -> Simba Spark ODBC Driver -> Finish 按实际配置相应的变量， Mechanism：Kerberos Host FQDN：hadoop.hadoop.com Service Name：spark2x Realm：留空 点击“Advanced Options”，勾选如下选项： 点击OK，保存配置。 点击Test进行测试连接，如果出现下图，则表示Spark ODBC连接成功。 在Alteryx使用Spark数据源 Alteryx启动后选择Options->Advanced Options->Manage In-DB Connections 在弹出的界面中填写配置： DataSource：Apache Spark ODBC COnnection Type：System Connections: 首次使用选new Connection Name: 自定义 Read->Driver：Apache Spark ODBC Write->Driver: 默认 Connection String：New database connection，选择Spark DSN填写用户名密码 新建一个workflow，拖入Input Data工具，在左侧Connect a file or database 中点击下拉菜单，选择Other Databases->ODBC->Simba Spark Data Source Name 选择在配置ODBC驱动时新建的Spark DSN：Simba Spark （System），填入用户名密码： 点击OK，Alteryx会连接至集群,在弹出的对话框中显示的是集群中Spark中的数据表，选择一个数据表作为输入，例如Customer 导入成功后显示如下,Refresh之后在左侧可以看到数据预览： 再添加一个数据源，执行join操作，成功后结果如下： 配置Hive ODBC数据源 下载并安装Hive的ODBC驱动 ODBC驱动下载地址：下载地址 创建DSN(Data Source Name)：选择 开始 -> Simba Spark ODBC Driver -> ODBC Administrator。 选择 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish 按实际配置相应的变量 Host(s): Hive Service主节点 Port：Hive Service端口21066 Mechanism：Kerberos Host FQDN：hadoop.hadoop.com Service Name：hive Realm：留空 如下图： Advanced Options不需要进行配置默认的参数即可连接成功。 点击Test进行测试连接，如果出现下图，则表示ODBC连接Hive成功。 Alteryx使用Hive数据源 Alteryx启动后选择Options->Advanced Options->Manage In-DB Connections 在弹出的界面中填写配置： DataSource：Hive Connection Type：System Connections: 首次使用选new Connection Name: 自定义 Read->Driver：Hive ODBC Write->Driver: Hive ODBC Connection String：New database connection，选择Hive DSN，填写用户名密码 在主界面新建一个workflow，拖入Input Data工具，在左侧Connect a file or database 中点击下拉菜单，选择Other Databases->ODBC Data Source Name 选择在配置ODBC驱动时新建的Hive DSN：Sample Cloudera Hive DSN(System)，填入用户名密码： 点击OK，Alteryx会连接至集群,在弹出的对话框中显示的是集群中Hive中的数据表，选择一个数据表作为输入，例如Customer： 导入成功后显示如下,Refresh之后在左侧可以看到数据预览： 再添加一个数据源，执行join操作，成功后结果如下： 配置HDFS数据源 HDFS是通过WebHDFS连接，前提条件是获取MIT Kerberos Ticket，并在Manager中修改HDFS的配置： dfs.http.policy 修改为HTTP_AND_HTTPS，重启HDFS。 在Alteryx主界面新建一个workflow，拖入Input Data工具，在左侧Connect a file or database 中点击下拉菜单，选择Hadoop 在弹出的界面中填写配置： Server：WebHDFS Host： HDFS所在服务器IP Port: 配置文件中dfs.namenode.http.port对应端口 User Name & Password： Kerberos 认证用户名及密码 Kerberos: Kerberos MIT 点击Test，出现Connection successful 表明连接成功。 弹出集群中的HDFS文件系统内容，目前支持Avro和CSV格式的文件，需上传至HDFS文件系统中。 选择相应文件，连接成功，Refresh之后左侧菜单显示文件内容预览： Join 操作成功后显示如下: FAQ 找不到C:\\ProgramData\\MIT\\Kerberos5文件夹 C:\\ProgramData一般属于隐藏文件夹，设置文件夹隐藏可见或者使用搜索功能即可解决问题。 连接成功无数据库权限 连接所使用的用户需要有数据库的权限，否则将导致ODBC连接成功却无法读取数据库内容。 测试连接时出现Default Kerberos ticket is expired Kerberos MIT ticket 过期，需要重新获得，获取一次有效期为10h. "},"Business_Intelligence/Using_Tableau_with_FusionInsight.html":{"url":"Business_Intelligence/Using_Tableau_with_FusionInsight.html","title":"Tableau","keywords":"","body":"Connection Instruction between Tableau and FusionInsight Succeeded Case Tableau 10.0.0 FusionInsight HD V100R002C30 Tableau 10.0.0 FusionInsight HD V100R002C50 Tableau 10.0.0 FusionInsight HD V100R002C60U10 Tableau 10.1.4 FusionInsight HD V100R002C60U20 Tableau 10.3.2 FusionInsight HD V100R002C70SPC200 Tableau 10.5.0 FusionInsight HD V100R002C80SPC100 Configure the Kerberos on Windows Download and install MIT Kerberos from the following URL http://web.mit.edu/kerberos/dist/#kfw-4.0 Make sure the time differences between FusionInsight clusters and Tableau client is no longer than 5 minutes. Configure required Kerberos filesystem Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive privileges to this user. For example, create a user named developuser, download the user.keytab and krb5.conf files on the Tableau client, rename the krb5.conf file into krb5.ini and save it into the following directory C:\\ProgramData\\MIT\\Kerberos5 Configure Kerberos cache file Create a directory to save the Kerberos cache file, for example, C:\\temp Configure the Environment Variables, Variable name is KRB5CCNAME, Variable value is C:\\temp\\krb5cache restart the Tableau client Start Kerberos Authentication on Windows use the created username and password to log in, the Principal is equal to username@Kerberos Realm name Open MIT Kerberos, click on Get Ticket, and type in the right Principal and Password for authentication Connecting Tableau to Hive Configure the ODBC interface to connect FusionInsight HiveDriver Download and install the ODBC driver Download URL: http://www.cloudera.com/content/cloudera/en/downloads/connectors/hive/odbc/hive-odbc-v2-5-15.html and choose the right one depends on the OS and bit version Configure ODBC drivers Open ODBC Data Sources(64-bit) by searching the keyword ODBC on Windows click on User DSN tab, click on Add button, choose Cloudera ODBC Driver for Apache Hive and click on Finish to start to configure In detail: 1: Hive Server 2 2: No Service Discovery 3: 172.21.3.101 4: 21006 5: default 6: Kerberos 7: hadoop.hadoop.com 8: hive 9: SASL click on Test button to test the connection Open Tableau Click on More option, and choose ODBC by search in keyword Connection Configuration shown as bellow: click on Connect and then click on Sign In Search the Data Search the data from multiple tables Connecting Tableau to Spark Download and install the ODBC driver for Spark Download urlhttp://www.tableau.com/support/drivers Created DSN （Data Source Name） Open ODBC Data Sources(64-bit) Click on System DSN tab, click on Add, choose Simba Spark ODBC Driver and click on Finish Open the installed Driver directory, for example, C:\\Program Files\\Simba Spark ODBC Driver\\lib and open the DriverConfiguration64.exe to Configure In detail: 1：SparkThriftServer (Spark 1.1 and later) 2: Kerberos 3: hadoop.hadoop.com 4: spark 5: SASL click on Advanced Options and choose \"Driver Config Take Precedence\" Click on ok to save the configuration Open Tableau Click on More option, and choose Spark SQL by search in keyword Connection Configuration shown as bellow: Server info can be got from FusionInsight Manager Web UI Port info can be got from FusionInsight Manager Web UI as well Click on Sign In , to come into a new Page, choose Schema and Table shown as bellow Open Sheet to visulize the data Performance test Search the table web_sales which contains millions of records Search the table by multiple tables whose names are store_sales and item Add customer_address table Test outcome： FAQ Cannot find C:\\ProgramData\\MIT\\Kerberos5 This Folder is hidden, configure the windows can solve it Connection succeeded but permission denied Use the user who has the privilege to DATABASE "},"Business_Intelligence/Using_QlikView_with_FusionInsight.html":{"url":"Business_Intelligence/Using_QlikView_with_FusionInsight.html","title":"QlikView","keywords":"","body":"QlikView and FusionInsight Instruction Case QlikView 12 FusionInsight HD V100R002C60U20 QlikView 12 FusionInsight HD V100R002C70SPC200 QlikView 12 FusionInsight HD V100R002C80SPC100 Configuring kerberos authentication for Windows Download and install MIT Kerberos at: http://web.mit.edu/kerberos/dist/#kfw-4.0 The version is consistent with the number of operating system bits. This article is version kfw-4.1-amd64.msi. Verify that the time of the client machine is the same as that of the FusionInsight HD cluster. The time difference is less than 5 minutes. Set the Kerberos configuration file Create a role and machine user on the FusionInsight Manager. For details, see the chapter \"Creating Users\" in the FusionInsight HD Administrator Guide. The role needs to grant Hive access rights based on business needs and add users to the role. For example, create the user \"sparkdemo\" and download the corresponding keytab file user.keytab and krb5.conf file, rename the krb5.conf file to krb5.ini, and put it in the C:\\ProgramData\\MIT\\Kerberos5 directory. Set the cache file for the Kerberos ticket Create a directory to store the tickets, such as C:\\temp. Set the system environment variable of Windows, the variable name is \"KRB5CCNAME\", and the variable value is C:\\temp\\krb5cache Restart the machine. Authenticate on Windows Use the command line to go to the MIT Kerberos installation path and find the executable kinit.exe. For example, the path to this article is: C:\\Program Files\\MIT\\Kerberos\\bin Execute the following command: kinit -k -t /path_to_userkeytab/user.keytab UserName Path_to_userkeytab is the path where the user keytab file is stored, user.keytab is the user's keytab, and UserName is the user name. Configuring a Hive data source Hive data source in QlikView, docking Hive's ODBC interface Download and install the Hive ODBC driver Download the driver from the following address and select the corresponding ODBC version according to the operating system type, download and install: address Configuring user DSN * On the User DSN tab of the OBDC Data Source Manager page, click Add to configure the user data source. On the Create Data Source page, find the Cloudera ODBC Driver for Apache Hive, select it and click Finish. Configure the Hive data source. Data Source Name: is a custom parameter Host(s): HiveServer business ip Port: Hive Service port, 21066 Mechanism: Kerberos Host FQDN: hadoop.hadoop.com Service Name: hive Realm: Leave blank ![](assets/Using_QlikView_with_FusionInsight/image7.png) Click Test If the connection is successful, the configuration is successful. Click OK Connect to the Hive data source Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database, click *Connect; On the Connect to Data Source page, select the data source hive_odbc configured above, then click OK; On the Data tab of the Edit Script page, click the Select button In the Create Select statement page, select the Database table you want to import, select in the field, import the full table, import the corresponding table into the corresponding table, and click OK* (select in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website. Configuring a Spark data source Configure the Spark data source in QlivView and connect to the thrift interface of SparkSQL. Download and install Spark's ODBC driver Download the Spark ODBC driver on Simba's official website, select 32bit or 64bit according to the user's own operating system, and select Spark for Data Source. SQL, address: http://www.tableau.com/support/drivers * Install the client as prompted by the install client. Configuring user DSN On the OBDC Data Source Manager page, on the User DSN tab, click Add to configure the user data source. On the Create Data Source page, find Simba Spark ODBC Driver, select it and click Finish. Configure the Spark data source on the Simba Spark ODBC Driver DSN Setup page. Data Source Name： Custom Mechanism： Kerberos Host FQDN： hadoop.hadoop.com Service Name： spark Realm： Leave blank, Host(s)： JDBCServer (main) service ip, Port： SparkThriftServer client port number 23040. After setting, click Advanced Options, in the pop-up Advanced Options page, check Use Native Query and Get Tables With Query, then click OK Go back to the Simba Spark ODBC Driver DSN Setup, click Test to connect successfully, click OK to exit the page, otherwise a failure dialog will pop up. Go back to the Simba Spark ODBC Driver DSN Setup page, click OK, go back to the ODBC Data Source Manager page, click OK to complete and exit the configuration. Connect to a Spark data source Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database, click Connect; On the Connect to Data Sources page, select the data source spark_odbc configured above, then click OK; On the Data tab of the Edit Script page, click the Select button In the Create Select statement page, select the Database table you want to import, select in the field, import the full table, import the corresponding table into the corresponding table, and click OK** (select in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website. FAQ Cannot find C:\\ProgramData\\MIT\\Kerberos5 folder C:\\ProgramData is generally a hidden folder, setting the folder to be hidden or using the search function to solve the problem. Connection succeeded without database permissions The user used for the connection needs to have the permissions of the database, otherwise the ODBC connection will succeed but the database content cannot be read. ODBC connection failed The common situation is that the input data of Host(s), Port, Host FQDN is incorrect. Please enter according to the actual situation. "},"Business_Intelligence/Using_Oracle_BIEE_with_FusionInsight.html":{"url":"Business_Intelligence/Using_Oracle_BIEE_with_FusionInsight.html","title":"Oracle BIEE","keywords":"","body":"Oracle BIEE对接FusionInsight 适用场景 Oracle BIEE 11g FusionInsight HD V100R002C60U20 Oracle BIEE 11g FusionInsight HD V100R002C70SPC200 Oracle BIEE 12c FusionInsight HD V100R002C60U20 Oracle BIEE 12c FusionInsight HD V100R002C70SPC200 Linux环境安装OBIEE 安装OS 安装RedHat6.5操作系统，desktop版 创建用户oracle 安装jdk8 获取jdk8安装包，执行安装 安装Weblogic 创建oracle home目录： umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ 上传weblogic安装包，解压 以oracle用户登录图形界面 安装BI Server 上传OBIEE安装包，解压 chmod 755 bi_platform-12.2.1.2.0_linux64.bin 以oracle用户登录图形界面 ./bi_platform-12.2.1.2.0_linux64.bin 补齐lib包 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel 取消当前安装，重新运行安装程序 安装Oracle Database 12c 安装数据库软件 创建数据库安装目录 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle 下载Oracle Database 12c安装包，解压得到database文件夹 chmod -R 755 database/ cd database/ su oracle ./runInstaller 只安装单实例数据库软件 创建数据库实例 cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca 字符集选择AL32UTF8，不勾选“create as container database” 配置环境变量vi ~/.bash_profile ORACLE_BASE=/Oracle/database ORACLE_HOME=$ORACLE_BASE/product/12.1.0/dbhome_1 ORACLE_SID=orcl ORACLE_TERM=xterm PATH=$PATH:$ORACLE_HOME/bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH 导入环境变量 source ~/.bash_profile 配置监听程序和网络服务名 netca Listener端口设为默认值1521 网络服务名配置为 ORCL 启动数据库和监听程序 主机重启后，需要重新执行以下命令启动数据库和监听程序 su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus界面执行startup 使用RCU创建Schema 启动rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu 配置BI Server 执行配置 cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh 安装BI Client 在Win7(64 bit)系统上安装BI Client 对接Hive 配置客户端系统DSN 配置Kerberos认证 从http://web.mit.edu/kerberos/下载安装kfw-4.1 安装配置Hive ODBC Driver 下载安装Hive ODBC Driver（Windows版本），下载地址 在BI客户端所在的Windows机器上配置系统DSN 测试ODBC连接 BI 管理工具新建RDP Client端打开Oracle BI 管理工具 选择上一步配置的DSN，用户名口令任意输入，但不能为空 禁用BI Server高速缓存 登录Weblogic域管理界面http://162.1.115.81:9500/em 配置中禁用高速缓存 上传RPD文件到服务端 客户端 cmd 切换到E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin目录 执行命令上传RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi 配置服务端系统DSN 配置Kerberos认证 mv /etc/krb5.conf /etc/krb5.conf.bak 将FusionInsight集群的krb5.conf上传到/etc目录下 kerberos认证 su oracle kinit test_cn 安装配置Cloudera Hive ODBC Driver yum install -y unixODBC 下载Hive ODBC Driver（Linux版本）下载地址 安装Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm 修改DSN配置，与Client端生成的RPD文件的DSN名称和配置保持一致 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini 修改odbc配置文件 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ 配置环境变量vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini 导入环境变量source /etc/profile 测试ODBC连接 su oracle isql -v 'Sample Cloudera Hive DSN' BI域配置系统ODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini 重启OBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh 服务端分析Hive数据 打开BI Analytics界面http://162.1.115.81:9502/analytics 创建分析 选择待分析的列拖到右侧区域 点击“结果”页签，检索所选列数据 点击右上角的保存按钮，保存查询结果 创建可视分析器项目 添加数据源 选取数据显示形式 添加计算 对接Spark SQL 配置客户端系统DSN Kerberos认证 Kerberos获取认证票据 安装配置Simba Spark ODBC Driver 下载安装 Simba Spark ODBC Driver：下载地址 配置DSN： 测试ODBC连接： BI管理工具新建RDP 新建obiee-spark.rdp，DSN选择上一步配置的 Sample Simba Spark DSN 上传RDP文件到服务端 上传RDP 配置服务端系统DSN Kerberos认证 su oracle kinit test_cn 安装配置Simba Spark ODBC Driver 下载Simba Spark ODBC Driver：下载地址 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm 修改DSN配置，增加Sample Simba Spark DSN，与Client端配置相同 vi /etc/odbc.ini 修改odbcinst.ini，vi /etc/odbcinist.ini 配置环境变量 vi /etc/profile 导入环境变量 source /etc/profile 测试ODBC连接 su oracle isql -v 'Sample Simba Spark DSN' BI域配置系统ODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini 重启OBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh 服务端分析Spark数据 参考服务端分析Hive数据 对接LibrA/ELK 配置LibrA与ELK的方式没有区别，以下以对接ELK为例进行操作 配置客户端系统DSN 配置obiee客户端的ODBC驱动 按照ELK的产品文档的指导安装配置ELK的windows驱动 配置DSN，测试ODBC连接，保存ODBC连接 BI管理工具新建RDP 新建obiee-elk.rdp，DSN选择上一步配置的 PostgreSQL35W 上传RDP文件到服务端 上传RDP 配置服务端系统DSN 参考LibrA/ELK的产品文档的Linux下配置数据源章节，完成obiee节点下的ODBC驱动的安装 测试ODBC连接，确保ODBC驱动安装成功 isql -v PostgreSQL35W BI域配置系统ODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini 在ODBC Data Sources部分增加PostgreSQL35W的DSN 在文件末尾增加PostgreSQL35W的DSN的详细配置 PostgreSQL35W的DSN的详细配置最后一行DriverUnicodeType=1需要加上，否则obiee查询的时候会报错[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) 重启OBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh 服务端分析Spark数据 参考服务端分析Hive数据 "},"Business_Intelligence/Using_RapidMiner_with_FusionInsight.html":{"url":"Business_Intelligence/Using_RapidMiner_with_FusionInsight.html","title":"RapidMiner","keywords":"","body":"Connection between RapidMiner with FusionInsightHD Succeeded Case Rapidminer Studio 8.2.001 FusionInsight HD V100R002C80SPC200 preparation Download and install RapidMiner Studio, download site https://rapidminer.com/ Start rapidminer, on the top of the main menu, choose Extensions->Marketplace,type radoop,install it and restart rapidminer Configure the local host file，file path is C:\\Windows\\System32\\drivers\\etc，add the cluster node ip and host name and save the file. Configure Kerberos file Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive, Spark,HDFS privileges to this user. For example, create a user named developuser, download the user.keytab and krb5.conf of the user and save them in your computer. Prepare the FusionInsight client configuration filesystem and jar files In the Manager GUI，choose Service->Download Client->Only Configuration File Unzip the file,find the following files,copy them into a directory,like ../config. Open yarn-site.xml,delete the following property: audit.service.name Yarn Login to one of the cluster nodes, go to the following path\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars,download the file directory /jars,save it in your computer,likeC:/jars。 Configure the cluster Bind the UDP port Download the UDP port bind tool uredir,website is https://github.com/troglobit/uredir After building and installing, we get the executing file uredir,upload it to the KDC server nodes in the cluster,and run the following command,here IP refers to the node ip../uredir IP:88 IP:21732 Configure Radoop Jars Download Radoop jars in this addresshttps://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html,get the correct version。 Upload the jar files to each node of the cluster,eg,/usr/local/lib/radoop/ In the HiveServer node of the cluster,uplaod the Radoop jar files to the following path and change their owner and execution authority cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar In the FusionInsight Manager GUI, choose Service->Hive->Service Configurationadd the following configuration radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name Notice that there should be a | as seperater Save the configuration，restart HiveServer Create Radoop UDF functions Run the following command in the client node, login to the Hive database source /opt/hadoopclient、bigdata_env kinit developuser beeline create a database in Hive, for example rapidminer, and create functions,run the following commands in beeline mode create database rapidminer; use rapidminer; DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep'; RapidMiner Configuration In RapidMiner sStadio，choose Connections->Manage Radoop Connections in the top menu. choose New Connections->Import Hadoop Configuration Files,choose the configuration files downloaded from the cluster,click Import Configuration After the import, click Next, go to the Connection settings window,configure ad following: Global： Hadoop Version：Other（Hadoop 2X line） Additional Libraries Directory: Spark jar files downloaded from the cluster Client Principal: Kerberos user name@HADOOP.com Keytab File: the keytab file downloaded from manager KDC Address: the KDC server IP(see the krb5.conf file) REALM: HADOOP.COM Kerberos Config File: the krb5 file downloaded from manager Hadoop： At the filter in upper right corner, search split , uncheck mapreduce.input.fileinputformat.split.maxsize Search classpath,uncheckmapreduce.application.classpath Spark： Spark Version：Spark2.1 Spark Archive(or libs)Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy：Static，Default Configuration Advanced Spark Parameters：add the following two parameters for spark:park.driver.extraJavaOptions and spark.executor.extraJavaOptions The value can be found in manager GUI, choose Services->Spark2X Configuration->type all，search extraJavaOptions in the search bar, choose the parameters in Spark2x->SparkResource2x Copy the values into a text file, replace the relative path ./in the value with absolute path in the cluster, like /opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc, then copy the values into rapidminer spark configuration Hive: Hive Version: Hive Server2 Hive Server Address：Hive Server IP Hive Port: 21066 Database Name: the database name created in Hive,here is rapidminer Customer database for UDFs: same as before click OK->Proced Anyway->Save Test the Connection Click Configure, in Global tab, click Test，Test Results show as following: In Hadoop tab，click Test,Test Results show as following: In Spark tab,click Test,Test Results show as following: In Hive tab, click Test, Test Results show as following: Click Full test,Test Results show as following: Radoop Demo In RapidMiner Studio main menu,choose Help->Tutorials->User Hadoop->Rapidminer Radoop Run the demo accordding to the Tutorials, get the follwing results "},"Data_Integration/":{"url":"Data_Integration/","title":"Data Integration","keywords":"","body":" Data Integration IBM InfoSphere DataStage IBM InfoSphere CDC (TBD) Oracle GoldenGate informatica (TBD) Talend Apache NiFi Kettle Knime Kafka-manager Informatica PWX CDC Primeton MetaCube (TBD) Unimas UTL (TBD) "},"Data_Integration/Using_IBM_InfoSphere_DataStage_with_FusionInsight.html":{"url":"Data_Integration/Using_IBM_InfoSphere_DataStage_with_FusionInsight.html","title":"IBM InfoSphere DataStage","keywords":"","body":"IBM InfoSphere DataStage对接FusionInsight 适用场景 IBM InfoSphere DataStage 11.3.1.0 FusionInsight HD V100R002C50 IBM InfoSphere DataStage 11.5.0.2 FusionInsight HD V100R002C60U20 前提条件 已完成IBM InfoSphere DataStage 11.5.0.2的安装部署（本文部署在Centos7.2上） 已完成FusionInsight集群的部署，版本FusionInsight HD V100R002C60U20 准备工作 配置域名解析 使用vi /etc/hosts命令修改DataStage Server和Client的hosts文件，添加FI集群节点信息，如： 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3 配置Kerberos认证 在FI管理界面创建DataStage对接用户，并赋予该用户所需权限，下载认证凭据 解压下载的tar文件，得到Kerberos配置文件krb5.conf和用户的keytab文件。 以root登录DataStage Server节点，将FI集群的krb5.conf文件复制到/etc目录。 将用户的user.keytab文件上传到DataStage Server节点的任意目录，如/home/dsadm。 安装FusionInsight客户端 参考FI产品文档，在FI服务管理界面下载完整客户端，上传到DataStageServer，安装至自定义目录，如/opt/ficlient。 对接HDFS 导入FI集群的SSL证书 浏览器导出FI集群的根证书 浏览器打开FI管理界面，查看证书，点击“证书路径”页签，选择根路径，查看根证书，在“详细信息”页签下，点击“复制到文件”，导出为cer格式 证书导入DataStage的keystore文件 将导出的FI根证书fi-root-ca.cer上传到DataStage服务端，如/home/dsadm路径下，将证书导入到keystore文件，命令参考： /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks 生成并保存加密后的keystore密码 使用vi /home/dsadm/authenticate.properties命令新建配置文件，保存上一步骤生成的密文： password={iisenc}SvtJ2f/uNTrvbuh26XDzag== 执行chown dsadm:dstage /home/dsadm/ authenticate.properties修改配置文件的属主 导出truststore环境变量 使用vi /opt/IBM/InformationServer/Server/DSEngine/dsenv编辑DSEngine的环境变量，在最后添加 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties 重启DSEngine，参考命令 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 读取HDFS文件 创建作业 新建并行作业，保存为hdfs2sf 添加File_Connector组件和Sequential File组件，以及File_Connector到Sequential File链接 参考下图修改配置 编译运行 保存配置后，编译，运行 在菜单 Tools -> Run Director 中打开Director客户端，查看作业日志 查看读取的数据 写入HDFS文件 创建作业 新建并行作业，保存为hdfswrite 添加Row Generator组件和File Connector组件，以及Row Generator到File Connector链接 参考下图修改配置 编译运行 保存 — 编译 — 运行 ，查看作业日志： 查看写入数据 对接Hive 使用Hive Connector 说明：Hive Connector官方认证过的Hive JDBC Driver只有DataDirect Hive Driver(IShive.jar)，用DataStage 11.5.0.2中自带的IShive.jar连接FusionInsight的hive时，会有thrift protocol报错，需要咨询IBM技术支持提供的最新的IShive.jar 设置JDBC Driver配置文件 在$DSHOME路径下创建isjdbc.config文件，CLASSPATH变量中添加DataDirect Hive Driver (IShive.jar)的路径，CLASS_NAMES变量中添加com.ibm.isf.jdbc.hive.HiveDriver，参考命令： su - dsadm cd $DSHOME vi isjdbc.config 在isjdbc.config中添加如下信息: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver 配置Kerberos认证信息： 在IShive.jar所在目录下创建JDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf 文件内容如下： JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; 读取Hive数据 创建作业 修改配置 URL参考如下进行配置： jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; 其中JDBC_DRIVER_test_keytab为上一步指定的鉴权信息 编译运行 保存 — 编译 — 运行 ，查看作业日志： 查看读取的数据 数据写入Hive表 创建作业 修改配置 编译运行 保存 — 编译 — 运行 ，查看作业日志，写入10条数据，用时2’11” 查看Hive表数据： Hive Connector向Hive表写数据使用Insert语句，每插入一条数据会起一个MR任务，效率特别低，不推荐使用这种方式。可以将数据直接写入HDFS文件。 使用JDBC Connector 如果要使用FusionInsight的Hive JDBC驱动， 用isjdbc.config文件CLASSPATH中添加jdbc驱动和依赖包的方式，在运行作业时会有如下报错，此时需要用导出CLASSPATH环境变量的方式加载 而且只能用JDBC Connector，不能用Hive Connector，否则会有如下报错 设置CLASSPATH环境变量 Hive jdbc驱动包及依赖包位于Hive客户端lib目录下/opt/ficlient/Hive/Beeline/lib，若未安装客户端，也可单独上传这些jar包到任意目录。 设置CLASSPATH环境变量，添加上述jar包的完整路径，参考命令： su - dsadm vi $DSHOME/dsenv 文件最后添加相关的jar包（具体路径根据实际环境调整） export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar 导入环境变量 source $DSHOME/dsenv 重启DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 读取Hive数据 创建作业 修改配置 其中URL为： jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; 编译运行 数据写入Hive表 创建作业 修改配置 编译运行 写入5条数据，用时1’49” 数据导入Hive表的HDFS文件 创建作业 修改配置 编译运行 查看写入数据 hive表数据增量100 增量数据定期自动导入Hive表的HDFS文件 增量数据可以新增HDFS文件的方式导入hive，如果要定期自动化执行，导入的文件名中需要包含可变参数进行设置和区分，然后以命令或脚本方式运行作业，给该参数赋值。 创建作业 设置作业参数 点击“job properties”按钮，设置参数如下 修改配置 File Connector配置导出文件的名称，以“#”引用设置的参数 dsjob命令运行作业 保存编译作业，在DataStage Server上执行dsjob -run命令，格式为： dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME 命令参考: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append 查看HDFS文件： 查看Hive数据增量为200条 对接SparkSQL 与使用FI Hive JDBC驱动类似，可以用SparkSQL JDBC驱动连接Hive，同样需要导出CLASSPATH环境变量来加载驱动包及依赖包。 SparkSQL jdbc不支持insert into语句，只能用来读hive数据，不能插入数据到hive表。 设置CLASSPATH环境变量 SparkSQL jdbc驱动包及依赖包位于Spark客户端lib目录下/opt/ficlient/Spark/spark/lib/，若未安装客户端，也可单独上传所需jar包到任意目录。 设置CLASSPATH环境变量，添加上述jar包的完整路径，以及spark客户端配置文件路径（SparkSQL jdbc连接hive时需要读取hive-site.xml中的配置）： su - dsadm vi $DSHOME/dsenv 配置如下内容： export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf 导入环境变量 source $DSHOME/dsenv 重启DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 读取Hive表数据 创建作业 修改配置 URL参考： jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; 编译运行 对接Phoenix 使用Phoenix以JDBC方式访问HBase表，也需要导出CLASSPATH环境变量来加载驱动包及依赖包。 设置CLASSPATH环境变量 Phoenix相关的jar包位于HBase客户端lib目录下/opt/ficlient/HBase/hbase/lib，若未安装客户端，也可单独上传所需jar包到任意目录。 设置CLASSPATH环境变量，添加上述jar包的完整路径，以及HBase客户端配置文件路径（phoenix连接时需要读取hbase-site.xml中的配置）： su - dsadm vi $DSHOME/dsenv 配置如下内容： export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf 导入环境变量 source $DSHOME/dsenv 重启DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start 创建jaas配置文件 Phoenix连接需要查询zookeeper ，zookeeper的Kerberos认证需要指定jaas配置文件 su - admin vi /home/dsadm/jaas.conf 文件内容如下： Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; 读取Phoenix表数据 创建作业 修改配置 URL参考： jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab 配置JVM options为-Djava.security.auth.login.config=/home/dsadm/jaas.conf 编译运行 写入Phoenix表数据 Phoenix插入语句是upsert into，不支持Insert into 语句，所以不能用JDBC Connector在运行时自动生成SQL语句，需要自己填写，否则会报错： main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. 创建作业 修改配置 编译运行 对接Fiber 对接Fiber需要先安装FI客户端 修改JDBC Driver配置文件 修改$DSHOME路径的isjdbc.config文件，CLASSPATH变量中添加Fiber jdbc driver及依赖包的路径，CLASS_NAMES变量中添加com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver 参考命令： su - dsadm cd $DSHOME vi isjdbc.config 配置如下： CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver 修改Fiber配置文件 DataStage使用IBM jdk，需要新建Fiber配置文件给DataStage使用 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml 修改fiber_ibm.xml中phoenix,hive,spark各driver的以下两个参数： java.security.auth.login.config 修改为 /home/dsadm/jaas.conf zookeeper.kinit 修改为 /opt/IBM/InformationServer/jdk/jre/bin/kinit 文件/home/dsadm/jaas.conf的内容如下： Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; 其它配置项参考FI产品文档Fiber客户端配置指导修改。 使用Hive Driver读取数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive 编译运行 使用Hive Driver写入数据 创建作业 修改配置 编译运行 使用Spark Driver读取数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark 编译运行 使用Phoenix Driver读取数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix 编译运行 目前未能读取到数据，”The connector could not determine the value for the fetch size.”，问题正在确认中 使用Phoenix Driver写入数据 创建作业 修改配置 URL参考： jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix 编译运行 写入数据0行，问题正在确认中 对接Kafka 说明：kafka Connector不支持发送或者消费integer, float, double, numeric, decimal等数值类型的字段，需要转换成char, varchar, longvarchar等类型，否则会有如下报错： main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL). 安装kafka客户端 kafka Connector需要配置Kafka client Classpath，可以在DataStage节点安装kafka客户端来获取kafka-client jar包。安装步骤参考FusionInsight产品文档。 Kafka Client Classpath 需要提供kafka-client, log4j, slf4j-api 三个jar包的路径，如： /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar 发送消息到kafka 创建作业 修改配置 RowGenerator 生成数据 transformer数据类型转换： Kafka配置： 编译运行 读取Kafka消息 创建作业 修改配置 编译运行 查看读取的数据 对接MPPDB 获取MPPDB JDBC Driver 从MPPDB发布包中获取，包名为Gauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz 解压后得到gsjdbc4.jar，上传到DataStage Server 修改JDBC Driver配置文件 修改$DSHOME路径的isjdbc.config文件，CLASSPATH变量中添加MPPDB Driver 的路径，CLASS_NAMES变量中添加org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config 配置： CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver; 读取MPPDB表数据 创建作业 修改配置 URL格式为： jdbc:postgresql://host:port/database 编译运行 数据写入MPPDB表 创建作业 修改配置 URL格式为： jdbc:postgresql://host:port/database 编译运行 查看MPPDB表数据： "},"Data_Integration/Using_Oracle_GoldenGate_with_FusionInsight.html":{"url":"Data_Integration/Using_Oracle_GoldenGate_with_FusionInsight.html","title":"Oracle GoldenGate","keywords":"","body":"Oracle GoldenGate对接FusionInsight 适用场景 Oracle GoldenGate 12.2 FusionInsight HD V100R002C60U20 Oracle GoldenGate 12.3 FusionInsight HD V100R002C70SPC200 Oracle GoldenGate 12.3 FusionInsight HD V100R002C80SPC100 环境信息 软件信息 Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20 硬件信息 源端OGG VM: 162.1.115.68 Redhat6.5 （包含Oracle DB12c的数据库） 目标端OGG VM: 162.1.115.69 Redhat6.5（包含Hadoop的客户端） 拓朴结构 测试拓朴结构如下图所示： 测试表 源端测试表： 在源端Oracle的PDBORCL数据库的test用户下创建test1表，其中ID为主键 OGG for Oracle安装 前置条件：完成oracle12c数据库的安装（IP：162.1.115.68） 软件版本：linuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip 下载并安装OGG for Oracle 将fbo_ggs_Linux_x64_shiphome.zip上传至oracle客户端（ip：162.1.115.68）/home/oracle目录下，切换至oracle用户，解压生成bo_ggs_Linux_x64_shiphome目录。 在/home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1目录下，运行./runInstaller 安装成功，/home/orcle/OGG/是OGG for Oracle的安装目录。 配置环境变量 切换到oracle用户 su - oracle vi .bash_profile 文件.bash_profile内容如下： # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/bin export PATH PATH=$PATH:$HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib 运行OGG 打开数据库归档及开启最小附加日志 使用Sqlplus / as sysdba登陆Oracle源端数据库后打开Archive Log: shutdown immediate; startup mount; alter database archivelog; alter database open; archive log list; 源端数据库打开数据库级最小附加日志及force logging： SELECT supplemental_log_data_min, force_logging FROM v$database; alter database add supplemental log data; alter database force logging; 切换日志以使附加日志生效： ALTER SYSTEM switch logfile; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication; alter system set enable_goldengate_replication = true scope=both; 配置DB12c PDB的tnsname信息vi $ORACLE_HOME/network/admin/tnsnames.ora： 在数据库中创建ogg用户并赋予权限 使用sqlplus / as sysdba登陆数据库后创建ogg用户并赋予权限 create user c##ogg identified by welcome1; grant dba to c##ogg container=all; grant create session, connect, resource to c##ogg container=all; grant alter any table to c##ogg container=all; grant alter system to c##ogg container=all; exec dbms_goldengate_auth.grant_admin_privilege('c##ogg',container=>'all'); 配置GoldenGate 登陆数据库的别名 在GoldenGate中创建用户别名，用于登录Oracle数据库读取数据库日志： add credentialstore ALTER CREDENTIALSTORE ADD USER c##ogg PASSWORD welcome1 ALIAS ogg_src 这样就可以用别名ogg_src登陆数据库了： dblogin useridalias ogg_src C##ogg是Oracle DB12c的普通用户，可以访问多个数据库实例。 创建test用户和test1表 test用户是基于pdborcl数据库实例的： 登陆数据库 Sqlplus / as sysdba 创建用户 alter session set container=pdborcl; alter database open; create user test identified by welcome1; grant resource, connect to test; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500M UNIFORM SIZE 128k; alter user test quota unlimited on test; alter user test quota unlimited on users; 创建测试表 conn test/welcome1@pdborcl; create table test1(id number primary key, name varchar2(50)); 配置GoldenGate捕获进程 编辑eora.prm，在GGSCI命令行下运行edit param eora命令： GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume 编辑diroby/eora.oby文件，在GGSCI命令行下运行shell vi diroby/eora.oby命令：(shell之后接操作系统命令) 使用oracle用户创建diroby目录： cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby 注意进程名eora和数据文件dirdat/eo的对应关系 在GGSCI命令行下运行obey diroby/eora.oby命令，把捕获进程eora加入到管理者进程中： GGSCI> obey diroby/eora.oby 把捕获进程eora注册到pdborcl数据库中： GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) 为pdborcl.test下的所有表添加表级附加日志： GGSCI> add schematrandata pdborcl.test allcols 启动GoldenGate捕获进程eora: GGSCI> start eora 配置GoldenGate传输进程phdfs 配置GoldenGate传输进程phdfs，将OGG生成的数据文件传递给目标端GoldenGate HDFS处理。 编辑phdfs.prm，在GGSCI命令行下运行edit param phdfs命令： 编辑diroby/phdfs.oby文件，在GGSCI命令行下运行shell vi diroby/phdfs.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/phdfs.oby 注意进程名phdfs和数据文件dirdat/rs的对应关系 在GGSCI命令行下运行obey diroby/phdfs.oby命令，把捕获进程phdfs加入到管理者进程中： GGSCI> obey diroby/phdfs.oby 启动GoldenGate捕获进程phdfs: GGSCI> start phdfs 配置GoldenGate传输进程phbase 配置GoldenGate传输进程phbase，将OGG生成的数据文件传递给目标端GoldenGate HBASE处理。 编辑phbase.prm，在GGSCI命令行下运行edit param phbase命令： 编辑diroby/phbase.oby文件，在GGSCI命令行下运行shell vi diroby/phbase.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/phbase.oby 注意进程名phbase和数据文件dirdat/se的对应关系 在GGSCI命令行下运行obey diroby/phbase.oby命令，把捕获进程phbase加入到管理者进程中： GGSCI> obey diroby/phbase.oby 启动GoldenGate捕获进程phbase: GGSCI> start phbase 配置GoldenGate传输进程pflume 配置GoldenGate传输进程pflume，将OGG生成的数据文件传递给目标端GoldenGate FLUME处理。 编辑pflume.prm，在GGSCI命令行下运行edit param pflume命令： 编辑diroby/pflume.oby文件，在GGSCI命令行下运行shell vi diroby/pflume.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/pflume.oby 注意进程名pflume和数据文件dirdat/rf的对应关系 在GGSCI命令行下运行obey diroby/pflume.oby命令，把捕获进程pflume加入到管理者进程中： GGSCI> obey diroby/pflume.oby 启动GoldenGate捕获进程pflume: GGSCI> start pflume 配置GoldenGate传输进程pkafka 配置GoldenGate传输进程pkafka，将OGG生成的数据文件传递给目标端GoldenGate Kafka处理。 编辑pkafka.prm，在GGSCI命令行下运行edit param pkafka命令： 编辑diroby/pkafka.oby文件，在GGSCI命令行下运行shell vi diroby/pkafka.oby命令：(shell之后接操作系统命令) GGSCI> shell vi diroby/pkafka.oby 注意进程名pkafka和数据文件dirdat/rk的对应关系 在GGSCI命令行下运行obey diroby/pkafka.oby命令，把捕获进程pkafka加入到管理者进程中： GGSCI> obey diroby/ pkafka.oby 启动GoldenGate捕获进程pkafka: GGSCI> start pkafka 查看GoldenGate进程运行状态 查看GoldenGate进程状态：(EORCL是与ELK对接的进程) GGSCI> info all 查看某个进程的详细信息： GGSCI> info eora detail 查看GoldenGate的统计信息： GGSCI> stats eora, latest 查看GoldenGate进程报告，用于定位问题： GGSCI> view report eora OGG for Bigdata安装 环境准备 下载安装FusionInsight客户端 在Bigdata客户端机器上（ip：162.1.115.69）按照FusionInsight产品文档安装FusionInsight客户端。将客户端JDK替换成1.7版本。 下载并安装oracle JDK1.7 将krb5.conf放在/etc/目录下 下载并安装OGG for Bigdata 将122011_ggs_Adapters_Linux_x64.zip上传至客户端/opt目录下： unzip 122011_ggs_Adapters_Linux_x64.zip 将解压后的ggs_Adapters_Linux_x64.tar解压到/opt/OGG_HADOOP目录下： 配置环境变量 更改环境变量，编辑根目录下vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME=/usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib PATH=$JAVA_HOME/bin:$PATH:$HOME/bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH Source环境变量，source .bash_profile. 将/opt/OGG_HADOOP/AdapterExamples/big-data下的四个目录下的所有文件拷贝到/opt/OGG_HADOOP/dirprm目录下。 配置GoldenGate管理进程 编辑mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all 配置GoldenGate HDFS 复制进程 编辑rhdfs.prm，在GGSCI命令行下运行edit param rhdfs命令： GGSCI> edit param rhdfs 编辑hdfs.props, 在GGSCI命令行下运行shell vi dirprm/hdfs.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/hdfs.props 需要在HDFS中创建/ogg1目录。 将hdfs.keytab文件拷贝到/opt/OGG_HADOOP/dirprm目录中： 把GoldenGate复制进程rhdfs加入到GoldenGate管理者进程中： GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all 配置GoldenGate HBase 复制进程 编辑rhbase.prm，在GGSCI命令行下运行edit param rhbase命令： GGSCI> edit param rhbase 编辑hbase.props, 在GGSCI命令行下运行shell vi dirprm/hbase.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/hbase.props 拷贝hbase.keytab和jaas.conf到/opt/OGG_HADOOP/dirprm/下： jaas.conf 文件 把GoldenGate复制进程rhbase加入到GoldenGate管理者进程中： GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all 配置GoldenGate Kafka 复制进程 创建kafka消息，进入FusionInsight客户端/opt/hadoopclient/Kafka/kafka/bin Kafka创建消息： ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka查看消息： ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka给消息授权： ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test 编辑rkafka.prm，在GGSCI命令行下运行edit param rkafka命令： GGSCI> edit param rkafka 编辑kafka.props, 在GGSCI命令行下运行shell vi dirprm/kafka.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/kafka.props 其中 gg.handler.kafkahandler.BlockingSend 属性控制同步和异步，默认false，异步。 GGSCI> shell vi dirprm/custom_kafka_producer.properties 修改Kafka里的配置，将如下选项修改为True 把GoldenGate复制进程rkafka加入到GoldenGate管理者进程中： GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all 配置GoldenGate Flume 复制进程 安装Flume客户端，配置非加密传输 配置Server的配置文件properties.properties 导出的properties.properties文件，增加如下配置： 可以在HDFS中增加/ogg/flume目录 将此properties.properties文件上传至FusionInsight。 编辑rflume.prm，在GGSCI命令行下运行edit param rflume命令： GGSCI> edit param rflume 编辑flume.props, 在GGSCI命令行下运行shell vi dirprm/flume.props命令：(shell之后接操作系统命令) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false 控制DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false 相同SCHAME打包 GGSCI> shell vi dirprm/custom-flume-rpc.properties 拷贝flume.keytab文件到/opt/OGG_HADOOP/dirprm/目录下 把GoldenGate复制进程rflume加入到GoldenGate管理者进程中： GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all 测试结果 Oracle端启动所有的传输进程 确保所有传输进程均已经正常启动 在Oracle数据库源端做Insert操作 su – oracle source .bash_profile sqlplus test/welcome1@pdborcl 查看HDFS同步情况，hadoop fs –ls /ogg1 查看HBase同步情况 hbase shell 查看kafka结果，进入kafka客户端/opt/hadoopclient/Kafka/kafka/bin 执行以下命令： ./kafka-console-consumer.sh --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning 在HDFS中查看flume运行结果：查看/ogg/flume/下数据文件： 在Oracle数据库源端做Update操作 执行以下命令 su – oracle source .bash_profile sqlplus test/welcome1@pdborcl 查看HDFS同步情况，hadoop fs –ls /ogg1 查看HBase同步情况 hbase shell 查看kafka结果，进入kafka客户端/opt/hadoopclient/Kafka/kafka/bin 执行以下命令： ./kafka-console-consumer.sh --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning 在HDFS中查看flume运行结果：查看/ogg/flume/下数据文件： 在Oracle数据库源端做Delete操作 执行以下命令 su – oracle source .bash_profile sqlplus test/welcome1@pdborcl 查看HDFS同步情况，hadoop fs –ls /ogg1 查看HBase同步情况 hbase shell 查看kafka结果，进入kafka客户端/opt/hadoopclient/Kafka/kafka/bin 执行以下命令： ./kafka-console-consumer.sh --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning 在HDFS中查看flume运行结果：查看/ogg/flume/下数据文件： "},"Data_Integration/Using_Talend_with_FusionInsight.html":{"url":"Data_Integration/Using_Talend_with_FusionInsight.html","title":"Talend","keywords":"","body":"Connection Instruction between Talend and FusionInsight Succeeded Case Talend 7.0.1 FusionInsight HD V100R002C80SPC200(HDFS,HBase Component) Talend 6.4.1 FusionInsight HD V100R002C80SPC200(HDFS,HBase,Hive) Note: Because of the version bug of Talend 7.0.1, Hive cannot be successfully connected. Using Talend 6.4.1 for substitution. Installing Talend Purpose Installing Talend 7.0.1 Prerequisites Installing FusionInsight HD cluster and its client completed Procedure Configure the JAVA_HOME into Path Environment Variables Configure Kerberos Get Kerberos related userkeytab and krb5.conf files by login into the FusionInsight HD manager web UI and put them into the following directory C:\\ProgramData\\Kerberos. In addition, create a new file named krb5.ini with the same content of krb5.conf, put the krb5.ini file into the following directory C:\\Windows Download TOS from the following web pages https://www.talend.com/products/big-data/big-data-open-studio/ , create the jaas.conf file for zookeeper connection with its content shown as follows Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; Sart TOS_BD by clicking TOS_BD-win-x86_64.exe Installing additional Talend Packages Connecting Talend to HDFS Purpose Configuring Talend related HDFS processor to connect FusionInsight HD HDFS Prerequisites Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed HDFS Connection Procedure Add the tHDFSConnection component with its configuration shown as follows: In detail： 1: Cloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" Test completed： HDFS Get Procedure The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSGet component shown as follows Note: Put the out.csv into the HDFS filesystem with the following directory /tmp/talend_test, C:/SOFT is the local folder for file output TEST completed： Check the test outcome by coming into the local directory C:/SOFT HDFS Put Procedure The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSPut component shown as follows Note: Before the test starts, create HDFSPut.txt located at the directory C:/SOFT with its content shown as follows It is created on a local PC. Test Completed： Login into the cluster to check the test outcome: Connecting Talend to Hive Purpose Configuring Talend related Hive processor to connect FusionInsight HD Hive Prerequisites Installing Talend 6.4.1 completed Installing FusionInsight HD cluster and its client completed Hive Connection Procedure The Talend version for Hive connection is 6.4.1 The whole process is shown as the following pic: The configuration of tHiveConnection component shown as follows 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" Note: Need to click the button which besides the Distribution to import the required jar files of FusionInsight HD. If there still need to add extra jar files, you can complete this step either by Talend itself or manually add these jar files. Test Completed： Hive Create Table & Load Procedure The configuration of tHiveConnection component does not change The configuration of tHiveCreateTable component shown as follows Note: It is required to Edit schema of the table The configuration of tHiveLoad component shown as follows Note: Before the test starts, the file out.csv need to be uploaded into the hdfs filesystem directory /tmp/talend_test/ The content of out.csv shown as follows ： 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHiveClose component shown as follows Test Completed： Check the table createdTableTalend by login into the cluster Hive Input Procedure The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveInput component shown as follows Note: It is required to Edit schema of the hive table The configuration of tLogRow keeps by default The configuration of tHiveClose component shown as follows Test Completed： Hive Row Procedure The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveRow component shown as follows Note: It is required to Edit schema of hive table Test Completed： Check the cluster outcome by login into the FusionInsight Cluster Connecting Talend to HBase Purpose Configuring Talend related HBase processor to connect FusionInsight HD HBase Prerequisites Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed HBase Connection Procedure The whole process is shown as the following pic: Using eclipse to export the LoginUtil which from HBase sample project code of FusionInsight HD client (Sample project code in this time can be found by following directory C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example) Find the tHbaseConnection component by Palette The configuration of tHbaseConnection shown as the following pic: Note: It is required to import the jar files of HBase sample project and the exported hbase_loginUtil.jar hbase-example required jar faile can be located by the following directory C:\\FusionInsight_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.0.2.tar.gz\\hbase\\lib The configuration of tLibraryLoad shown as folloing pic: Click on Advanced settings and add the java code import com.huawei.hadoop.security.LoginUtil; shown as follows: Use tJava component to customize the tHBaseConnection component The content of the Java code shown as follows： org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); Test Completed HBase Input Output Procedure The content of the Java code shown as follows： The configuration of tLibraryLoad，tHBaseConnection，tJava, tHBaseClose do not change The configuration of tFileInputDelimited shown as following pic: Note: It is required to Edit schema of out.csv The content of out.csv shown as follows: 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHBaseOutput shown as folloing pic: Note: It is required to Edit Schema of table: The configuration of tHBaseInput shown as folloing pic: The configuration of tLogRow keeps by default Test Completed: Login into the FusinInsight HD cluster and check the HBase table hbaseInputOutputTest by using following comands: hbase shell scan 'hbaseInputOutputTest' "},"Data_Integration/Using_Kettle_6.1_with_FusionInsight_HD_C60U10.html":{"url":"Data_Integration/Using_Kettle_6.1_with_FusionInsight_HD_C60U10.html","title":"Kettle","keywords":"","body":"Kettle对接FusionInsight 适用场景 Kettle 6.1 FusionInsight HD V100R002C60U10 Kettle 6.1 FusionInsight HD V100R002C60U10 Kettle 6.1 FusionInsight HD V100R002C60U10 环境准备 Linux平台 安装操作系统 安装CentOS6.5 Desktop 禁用防火墙，SELinux 添加本地主机名解析 使用vi /etc/hosts添加本地主机名解析 162.1.115.89 kettle 安装FusionInsight HD客户端 下载完整客户端，安装至目录/opt/hadoopclient 使用vi /etc/profile编辑以下内容插入到文件末尾 source /opt/hadoopclient/bigdata_env 将krb5.conf放在/etc目录下 cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Windows平台 安装JDK8 配置系统环境变量 JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 在PATH环境变量添加 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 获取Kerberos配置文件 在FI管理界面下载用户的认证凭据 解压后得到Kerberos配置文件krb5.conf和用户密钥文件user.keytab 将krb5.conf文件复制C:\\Windows目录下，重命名为krb5.ini 添加系统环境变量KRB5_CONFIG（可选步骤） KRB5_CONFIG=C:\\Windows 配置并启动Kettle 从以下地址 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ 下载Kettle6.1版本 解压得到data-integration目录 替换pentaho-big-data-plugin下的配置文件 下载FusionInsightHD客户端并解压 用解压目录下Hive/jdbc-examples/conf/core-site.xml文件 替换data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23目录下的core-site.xml文件 替换Hive相关jar包 将data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib下的hive相关的jar包 替换成Hive客户端下jdbc-examples/lib目录下的以下jar包 获取用户keytab文件 在FI管理界面下载用户的keytab文件到本地 Kerberos认证（可选步骤） 在对接Hive时，可以使用本地缓存的认证票据，或者在jdbc URL中指定principal和keytab文件进行认证（对接HDFS时，只能使用本地缓存的票据） 如果使用本地缓存的票据，需要在启动kettle前先完成认证。 使用本地缓存票据存在以下问题：kettle只在启动时读取一次票据，而不是连接时实时读取当前票据信息，所以当kettle启动时获取的票据过期以后，连接Hive会失败，必须重启kettle。 启动kettle Linux平台 VNC登录CentOS桌面，打开Terminal cd /opt/data-integration/ ./spoon.sh Windows平台 双击data-integration目录下的Spoon.bat 对接Hive 创建Hive连接 选择 文件 -> 新建 -> 转换 点击 主对象树 页签，在页签中选择 转换 -> DB连接，右键选择 新建 连接类型选择Hive 2，填写主机名、端口号、数据库名 点击左侧 选项，如果使用本地缓存票据，填写以下参数： 如果要在连接Hive时使用keytab文件认证，增加user.principal和user.keytab两个参数： 测试连接时，Hadoop版本选用HDP2.3 连接测试成功后，点击 确认 保存连接 读取Hive数据 以hive -> postgresql为例 将上面创建的转换保存为hive2postgres.ktr 创建postgresql连接 添加转换步骤 在 核心对象 页签下，拖动 输入 -> 表输入，和 输出 -> 表输出 两个步骤到工作区，并连接这两个步骤。 修改Hive表输入配置 双击 表输入 步骤， 数据库连接 选择前面创建的hive连接，点击 获取SQL查询语句 ，选择需要导入的hive表 修改postgresql表输出配置 双击 表输出 步骤，数据库连接中 选择前面创建的postgresql连接，点击 获取目标表配置 如下（需要先在postgresql数据库创建目标表） 运行转换 保存配置，点击 执行 按钮，选择 本地执行 执行结果： postgresql数据库查看： 写入Hive数据 以oracle -> hive为例 添加Oracle JDBC Driver 从http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html 下载对应版本的jdbc Driver，放到data-integration/lib目录下，重启kettle 新建转换，保存为oracle2hive.ktr 创建Oracle连接 参考上面章节创建hive连接 创建待导入的Hive表 CREATE TABLE IF NOT EXISTS kettle_export ( id int, name string ); 添加转换步骤 修改步骤配置 Oracle表输入配置 Hive表输出配置 运行转换 保存配置，点击 执行 按钮，选择 本地执行 执行结果：向Hive表写入13条数据，用时4min+ 查看Hive表数据： 说明：向Hive表中写入数据，每插入一条数据会起一个MR任务，所以效率特别低，不推荐用这种方式，可以将数据写入HDFS文件 对接HDFS 创建Hadoop Cluster 选择 文件 -> 新建 -> 转换，点击 主对象树 页签，在 Hadoop Clusters 右键选择 New Cluster HDFS的Hostname填写NameNode主节点的IP，端口号是25000，如果NaneNode发生主备切换，需要修改IP JobTracker的Hostname 填写 Yarn ResourceManager主节点的IP，端口号是26004，如果ResourceManager发生主备切换，需要修改IP。 点击 测试 kettle6.1不支持HDFS NameNode和Yarn ResourceManager的HA配置 导入HDFS文件 以postgresql -> HDFS为例 将上面创建的转换保存为postgres2hdfs.ktr 参考前面章节创建postgresql连接 添加转换步骤 在 核心对象 页签下，拖动 输入 -> 表输入 ，和 Big Data -> Hadoop File Output 两个步骤到工作区，并连接这两个步骤。 创建待导入的Hive表 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string, description string, total_emp int, salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES (\"field.delim\"=\"[,]\") STORED AS TEXTFILE; 如果数据中含有”,”，列分隔符不可以使用默认的”,”，本样例使用多字节分隔符”[,]” 修改postgresql表输入配置 双击 表输入 步骤，数据库连接 选择前面创建的postgresql连接，点击 获取SQL查询语句，选择需要导入的表 修改Hadoop File Output配置 双击 Hadoop File Output 步骤，在 文件 页签下，Hadoop Cluster 选择前面创建的集群，Folder/File 选择到hive表对应的hdfs目录，文件名可以任意指定 点击 内容 页签，分隔符设置与前面创建的Hive表相同，勾选 快速数据存储（无格式）（否则保存的文件中会按字段长度填充空格） 点击 字段 页签，获取字段 运行转换 保存配置，点击 执行 按钮，选择 本地执行 。 执行结果： 查看导入的HDFS文件： 查看Hive表数据： 读取HDFS文件 以HDFS -> Excel为例 新建转换，保存为hdfs2excel.ktr 添加转换步骤 在 核心对象 页签下，拖动 Big Data -> Hadoop File Input 和 输出 -> Microsoft Excel 输出，两个步骤到工作区，并连接这两个步骤。 修改 Hadoop File Input配置 双击 Hadoop File Input 步骤，文件 页签，选择待导出的文件，文件类型支持CSV（txt也可以）和Fixed（固定列宽） 点击 内容 页签，选择文件类型、分隔符、编码方式等 点击 字段 页签，获取字段 kettle会自动扫描文件中的字段类型和长度 可以手动修改字段名称和长度 点击 确定 按钮，保存配置 修改Microsoft Excel输出配置 双击 Microsoft Excel 输出 步骤，选择文件保存位置和文件名 点击 内容 页签，获取字段 运行转换 保存配置，点击 执行 按钮，选择 本地执行 执行结果 查看导出的excel文件 "},"Data_Integration/Using_Nifi_1.7.1_with_FusionInsight_HD_C80spc200.html":{"url":"Data_Integration/Using_Nifi_1.7.1_with_FusionInsight_HD_C80spc200.html","title":"Apache NiFi","keywords":"","body":"Connection Instruction between Apache NiFi and FusionInsight Succeeded Case Apache NiFi 1.7.1 FusionInsight HD V100R002C80SPC200 Installing Apache NiFi Purpose Installing Apache NiFi 1.7.1 Prerequisites Installing FusionInsight HD cluster and its client completed Procedure Get JAVA_HOME configuration by execute source command on client sidesource /opt/hadoopclient/bigdata_env echo $JAVA_HOME Download NiFi installation file from https://nifi.apache.org/download.html, move the file to client side by using tool WinSCP , execute command unzip nifi-1.7.1-bin.zip to unzip the installation file to the following directory /usr/nifi/nifi-1.7.1 Configure NiFi server IP address and port by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 Start and Stop NiFi server cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop Start NiFi Server bin/nifi.sh start Configuring Kerberos authentication within NiFi Purpose Configuring Kerberos authentication within NiFi server for the later connection usage Prerequisites Installing Apache NiFi completed Installing FusionInsight HD cluster and its client completed Create a developuser for connection Procedure Download the required Kerberos authentication files user.keytab and krb5.conf from FusionInsight HD Manager site, save the files into the following directory /opt/developuser Configure Kerberos authentication by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file Detailed Configuration： nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find KeytabCredentialsService and click ADD Click on gear icon to configure Click on lightning icon to enable and save the KeytabCredentialsService Completed Connecting NiFi to HDFS Purpose Configuring NiFi related HDFS processor to connect FusionInsight HD HDFS Prerequisites Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed PutHDFS Procedure Find and Copy the hdfs-site.xml，core-site.xml files which located in FusionInsight HD client to the following directory /usr/nifi/nifi-1.7.1/conf Make an adjustment to the content of hdfs-site.xml that is to delete the following property dfs.client.failover.proxy.provider.hacluster org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider Make an adjustment to the content of core-site.xml that is to change halcluster into detailed namenode ip with its port fs.defaultFS hdfs://172.21.3.102:25000 The whole process shown as the following pic: The configuration of processor GetFile In detail： 1: /home/dataset The configuration of processor PutHDFS In detail： 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest The configuration of the connection between two former processors Move the file nifiHDFS.csv into the following directory /home/dataset before test start Content of nifiHDFS.csv： 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq Test completed Log into FusionInsight HDFS to check the test outcome by using the following command hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS Procedure The whole process shown as the following pic: The configuration of processor GetHDFS In detail： 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest/HDFS The configuration of processor PutFile In detail： 1: /home/dataset/HDFS Move the file nifiHDFS.csv into HDFS directory /tmp/nifitest/HDFS Test completed Log into the FusionInsight HD client side to check the outcome with the directory /home/dataset/HDFS ListHDFS & FetchHDFS Procedure The whole process shown as the following pic: The configuration of processor ListHDFS In detail： 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest The configuration of processor RouteOnAttribute Note: Add one customized property requiredfilenames with the value ${filename:matches('sanguo.*')} by clicking on plus icon In detail： 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} The relationship configuration between processor RouteOnAttribute and upper processor FetchHDFS shown as the following pic The relationship configuration between processor RouteOnAttribute and lower processor FetchHDFS shown as the following pic The configuration of processor FetchHDFS In detail： 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService The configuration of upper processor PutFile The configuration of lower processor PutFile Check the files on FusionInsight HDFS by executing command hdfs dfs -ls /tmp/nifitest Test completed Log into FusionInsight HD client side to check the outcomes separately Connecting NiFi to Hive Purpose Configuring NiFi Hive processor to connect FusionInsight HD Hive Prerequisites Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed HiveConnectionPool Procedure Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HiveConnectionPool and click ADD Click on gear icon to configure In detail 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService Click on lightning icon to enable and save the HiveConnectionPool Completed Create jaas.conf file which located at directory /usr/nifi/nifi-1.7.1/conf wit the following content Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; Make an adjustment to the bootstrap.conf file by executing following command vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true Make an adjustment to the nifi.properties file by executing following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true Execute the following command to come into the directory of NiFi Hive related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar SelectHiveQL read Hive table Procedure The whole process shown as the following pic: The configuration of processor SelectHiveQL In detail： 1: HiveConnectionPool 2: select * from default.t2 3. CSV The configuration of processor PutFile Log into FusionInsight cluster to check table t2 on hive Completed Check the outcome by log into the following directory /home/dataset/HIVE PutHiveQL load whole table Procedure The whole process shown as the following pic: The configuration of processor GetFile In detail： 1： /home/dataset/ 2: iris.txt Content of iris.txt： 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa The configuration of processor PutHDFS In detail： 1： /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2： KeytabCredentialsService 3: /tmp/nifitest/loadhive The configuration of processor ReplaceText In detail： 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; The configuration of processor PutHiveQL Move the data file iris.txt into the following directory /home/dataset/ before test Completed: Login the HIVE to check the test outcome PutHiveQL Load the table by rows Procedure The whole process shown as the following pic: The configuration of processor GetFile In detail： 1： /home/dataset/ 2： iris_add.txt Content of iris_add.txt： \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" The configuration of processor SplitText There is no change for the configuration of processor ExtractText The configuration of processor ReplaceText The configuration of processor PutHiveQL Move the data file iris_add.txt into the following directory /home/dataset/ before test Completed： Login the HIVE to check the test outcome： Connecting NiFi to HBase Purpose Configuring NiFi HBase processor to connect FusionInsight HD HBase Prerequisites Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed HBase_1_1_2_ClientService Procedure Move the hbase related configuration file hbase-site.xml which is within the FusionInsight HD client side into the following directory /usr/nifi/nifi-1.7.1/conf Execute the following command to come into the directory of NiFi HBase related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HBase_1_1_2_ClientService and click ADD Click on gear icon to configure In detail： 1： /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2： KeytabCredentialsService Click on lightining icon to enable and save the HBase_1_1_2_ClientService Completed PutHBaseJSON load the table Procedure The whole process shown as the following pic: The configuration of processor GetFile Content of hbase_test.csv： 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica The configuration of processor InverAvroSchema In detail： 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data The configuration of processor ConvertCSVToAvro The configuration of processor ConvertAvroToJSON The configuration of processor SplitJson The configuration of processor PutHBaseJSON In detail: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data Move the data file hbase_test.csv into the following directory /home/dataset/HBASE before test In addition, execute following command to create a HBase table hbase shell create 'HBase_test','data' Completed： Login into the FusionInsight HD cluster to check the outcome: GetHbase Procedure The whole process shown as the following pic: The configuration of processor GetHBase The configuration of processor PutFile Completed Login into the following directory /home/dataset/GetHBase_test to check the test outcome Connecting NiFi to Spark Purpose Configuring NiFi Livy Session processor to connect FusionInsight HD Spark Prerequisites Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed Installing and configuring Apache Livy 0.5.0 (Apache Livy can be installed on test host or any other host as long as they can connect to each other including FusionInsight HD cluster) There exist connection instruction between Apache Livy and FusionInsight, please check the FusionInsight ecosystem LivySessionController Procedure Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure In detail： 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: spark 4：KeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_PySpark In detail： 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: pysaprk 4：KeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_SparkR In detail： 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: sparkr 4：KeytabCredentialsService Click on lightining icon to enable and save the LivySessionController,LivySessionController_PySpark,LivySessionController_SparkR Completed Spark Sample Procedure The whole process shown as the following pic: The configuration of processor GetFile In detail： 1: /home/dataset/sparkTest 2: code1.txt Content of code1.txt： 1+2 The configuration of processor ExtractText Click plus icon to add a Property code1 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail： 1: LivySessionController 2: ${code1} Move the code file code1.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed： Log into the Livy server to check the outcome PySpark Sample Procedure The whole process shown as the following pic: The configuration of processor GetFile In detail： 1: /home/dataset/sparkTest 2: code2.txt Content of code2.txt： import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y The configuration of processor ExtractText Click plus icon to add a Property code2 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail： 1: LivySessionController_PySpark 2: ${code2} Move the code file code2.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome SparkR Sample Procedure The whole process shown as the following pic: Note: It's different by comparing to example of former Spark and PySpark The configuration of processor GetFile In detail： 1: /home/dataset/sparkTest 2: code3.txt Content of code3.txt： piR The configuration of processor ExecuteSparkInteractive In detail： 1: /home/dataset/sparkTest 2: code content of code3.txt Move the code file code3.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome Connecting NiFi to Kafka Purpose Configuring NiFi Kafka processor to connect FusionInsight HD Kafka Prerequisites Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed GetHTTP & PutKafka Procedure The whole process shown as the following pic: The configuration of processor GetHTTP In detail： 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv The configuration of processor PutKafka In detail： 1： 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2： nifi-kafka-test-demo 3： nifi Before test： Log into the Kafka component within FusionInsightHD client side and create a Topic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 Completed： Log into the kafka component within FusionInsightHD client side to check the outcome cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning ConsumeKafka_0_11 Procedure The whole process shown as the following pic: The configuration of processor ConsumeKafka_0_11 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer The configuration of processor PutFile Before test： Open the kafka-examples which provided by FusionInsightHD client in eclipse, configure the kafka-examples so that it can be successfully ran and produce messages to kafka Note: There must be a producer when testing the NiFi ConsumeKafka_0_11 processor, run NewProducer.java within kafka-examples at first and then start to test NiFi ConsumeKafka_0_11 Completed： Log into the follow directory /home/dataset/Kafka to check the test outcome "},"Data_Integration/Using_Knime_3.6.1_with_FusionInsight_HD_C80SPC200.html":{"url":"Data_Integration/Using_Knime_3.6.1_with_FusionInsight_HD_C80SPC200.html","title":"Knime","keywords":"","body":"Knime 对接FusionInsight 适用场景 Knime 3.6.1 FusionInsight HD V100R002C80SPC200 环境准备以及Knime下载 环境准备 安装JDK8 配置系统环境变量 JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 在PATH环境变量添加 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 下载Knime 在Knime官网https://www.knime.com/downloads/download-knime选择合适的安装包进行下载. 版本与操作系统位数保持一致，本文版本kfw-4.1-amd64.msi。 * 确认客户端机器的时间与FusionInsight HD集群的时间一致，时间差要小于5分钟 * 设置Kerberos的配置文件 在FusionInsight Manager创建一个角色与“人机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。角色需要根据业务需要授予Spark，Hive，HDFS的访问权限，并将用户加入角色。下载该用户对应的Kerberos认证文件，包括user.keytab以及krb5.conf文件，保存在本地。 * 设置Kerberos票据的缓存文件 * 创建存放票据的目录，例如“C:\\temp”。 * 设置Windows的系统环境变量，变量名为“KRB5CCNAME”，变量值为“C:\\temp\\krb5cc_tmp”。 ![](assets/Using_Knime_3.6.1_with_FusionInsight_HD_C80spc200/fb1a2.png) * 重启机器 * 在Windows上进行认证 * 使用Kerbers认证的用户名密码登录，用户名的格式为：用户名@Kerberos域名。 * 打开MIT Kerberos，单击“get Ticket”，在弹出的“MIT Kerberos: Get Ticket”窗口中，“Pricipal”输入用户名，“Password”输入密码，单击“OK”,获得票据。 ![](assets/Using_Knime_3.6.1_with_FusionInsight_HD_C80spc200/00465.png) --> 配置Knime 获取集群的hdfs-site.xml和core-site.xml文件，保存在本地. 在Knime的安装目录中，修改配置文件“knime.ini”,在末尾添加一行 `Djava.security.krb5.conf=path\\to\\krb5.conf` 双击Knime.exe，启动Knime 在菜单栏选择File->Preference->KNIME->Big Data->Hadoop，在Hadoop Configuration中填入本地保存的HDFS的hdfs-site.xml和core-site.xml文件,点击Apply and Close保存配置。 在菜单栏选择File->Preference->KNIME->Big Data->Kerberos，填入kerberos认证用户名和本地keytab文件的路径，并选择Enable Kerberos Logging,点击Apply and Close保存配置。 Knime连接HDFS 前提条件 已经完成Knime 3.6.1的安装 已完成FusionInsight HD和客户端的安装，包含HDFS组件 已完成本机的Kerberos认证 建立HDFS连接 在Knime菜单栏中选择File->New->New KNIME Workflow,命名后保存。 在工作区中拖入一个HDFS Connection 节点 双击HDFS Connection 节点，填写如下配置： Host: HDFS主节点 Port: 25000 Authentication: Kerberos 点击Test connection,显示如下，表示连接成功 点击Apply，保存配置 读取HDFS文件 在工作区中拖入Download 节点，将其与HDFS Connection相连 双击Download 节点，选择要从HDFS文件系统下载的文件以及文件的本地保存路径 点击Apply，保存配置 点击菜单栏中的 执行任务 查看本地文件 上传文件至HDFS 将要上传的文件放在本地的一个文件夹中，例如C:\\KnimeData 在工作区中拖入List Files,String to URI 以及Upload 节点，将其进行如下连接 双击List Files 节点，选择要上传文件的本地路径，点击Apply，保存配置 双击Upload 节点，选择在HDFS中文件保存的路径，点击Apply，保存配置 点击菜单栏中的 执行任务 在服务器上查看HDFS文件系统中所上传的文件 Knime连接Hive 前提条件 已经完成Knime 3.6.1的安装 已完成FusionInsight HD和客户端的安装，包含Hive组件 已完成本机的Kerberos认证 建立Hive连接 在Knime菜单栏中选择File->New->New KNIME Workflow,命名后保存。 在工作区中拖入一个Hive Connector 节点 双击Hive Connector 节点，填写如下配置： Hostname: HIve主节点 Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos 点击Apply，保存配置 写入Hive表 在工作区中拖入以下几个节点，并进行如下连接 其中HDFS Connection 节点配置参考上节中建立HDFS连接，点击Apply保存配置 File Reader节点中选择本地将要上传的文件，点击Apply保存配置 Hive Loader节点中选择文件要上传至服务器的路径以及表名，点击Apply保存配置 点击菜单栏中的 ![](assets/Using_Knime_3.6.1_with_FusionInsight_HD_C80spc200/image011.png)执行任务 在服务器上查看导入Hive中的表 Knime连接Spark 前提条件 已经完成Knime 3.6.1的安装 已完成FusionInsight HD和客户端的安装，包含Spark2x组件 已完成本机的Kerberos认证 安装spark-job-server 此部分可参考KNIME官方文档https://download.knime.org/store/3.6/knime_extension_for_apache_spark_2.3.0.pdf 打开https://www.knime.com/knime-extension-for-apache-spark，根据集群以及操作系统版本获取对应的spark-job-server安装包，上传至服务器节点，例如/opt目录下。 对于FusionInsight集群，spark版本为1.5和2.1，根据要使用的spark版本选择对应的spark-job-server进行安装，这里以spark2.1为例 对于使用spark2x的集群，执行以下命令进行安装配置 LINKNAME=spark2-job-server useradd -d /opt/${LINKNAME}/ -M -r -s /bin/false spark-job-server su -l -c \"hdfs dfs -mkdir -p /user/spark-job-server ; hdfs dfs -chown -R spark-job-server /user/spark-job-server\" hdfs cp /path/to/spark-job-server-xxx.tar.gz /opt cd /opt tar xzf spark-job-server-xxx.tar.gz ln -s spark-job-server-xxx ${LINKNAME} chown -R spark-job-server:spark-job-server ${LINKNAME} spark-job-server-xxx/ 对于RHEL 6.x/CentOS 6.x操作系统，执行：ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} chkconfig --levels 2345 ${LINKNAME} on 修改environment.conf文件,设置 master = yarn-client，以yarn-client模式运行spark. 修改settings.sh文件，设置SPARK_HOME=/opt/hadoopclient/Spark2x/spark 配置Kerberos安全认证 将Kerberos用户的配置文件上传至服务器节点，并执行一下命令 chown spark-job-server:spark-job-server /path/to/keytab chmod go= /path/to/keytab 在environment.conf文件中，进行如下设置 spark { jobserver { context-per-jvm = true } } shiro { authentication = on config.path = \"shiro.ini\" use-as-proxy-user = on } 在setting.sh文件中，编辑以下几行 export JOBSERVER_KEYTAB=/path/to/keytab export JOBSERVER_PRINCIPAL=user/host@REALM 在FusionInsight的manager管理页面，修改HDFS的core-site.xml文件配置，主页面选择服务管理->HDFS->服务配置,参数配置选择全部服务，在左侧选择HDFS->自定义,添加以下两个参数 hadoop.proxyuser.spark-job-server.hosts = * hadoop.proxyuser.spark-job-server.groups = * 保存配置，重启相关服务。 启动和停止Spark-job-server 启动Spark-job-server cd /etc/init.d ./spark2-job-server start 停止Spark-job-server cd /etc/init.d ./spark2-job-server stop 建立Spark连接 在Knime菜单栏中选择File->New->New KNIME Workflow,命名后保存,在工作区中拖入一个Create Spark Context 节点，双击后进行如下配置 在Context Settings页面 Spark version:集群中使用的Spark版本 Context name：建立的Spark Context 名字 在Connection Settings页面 Jobserver URL:http://ip:8090/ Authentication: None 点击Apply，保存配置 可点击菜单栏按钮，测试连接是否有错，若显示如下，表明节点配置无误。 在浏览器中打开Jobserver URL中配置的地址，可以进入Spark Job Server UI界面，可以看到刚才建立的Spark Context，显示如下： Spark应用实例 Spark应用实例下载地址https://www.knime.com/nodeguide/big-data/spark-executor Hive to Spark to Hive 下载完成打开应用实例，配置HDFS Connection，File Reader，Hive Connector，Hive Loader，Create Spark Context和Spark to Hive 节点，具体配置如下： 点击菜单栏中的 ![](assets/Using_Knime_3.6.1_with_FusionInsight_HD_C80spc200/image011.png)执行任务 登录节点，执行beeline进入Hive界面，执行show tables;查看导入的表. 可以看到，通过Hive Loader节点导入的表contactdata以及Spark to Hive节点导入的表sparktohivetable均已导入Hive。 "},"Data_Integration/Using_kafka-manager_with_FusionInsight_HD_C80SPC200.html":{"url":"Data_Integration/Using_kafka-manager_with_FusionInsight_HD_C80SPC200.html","title":"Kafka-manager","keywords":"","body":"kafka-manager 对接FusionInsight 适用场景 /kafka-manager-1.3.3.21 FusionInsight HD V100R002C80SPC200 环境准备 安装JDK1.8及以上版本 下载kafka-manager源码 下载地址为 https://github.com/yahoo/kafka-manager 解压后得到安装目录kafka-manager yum安装sbt yum install sbt 安装FusionInsight客户端，安装目录为/opt/hadoopclient 获取kafka用户的认证文件,登录FusionInsight集群节点,将/opt/huawei/Bigdata/om-server_V100R002C80SPC200/apache-tomcat-8.5.28/conf/security/kafka.keytab文件下载到本地，并上传至客户端节点/opt/hadoopclient/目录下 通过FusionInsight HD的管理页面创建一个“机机”用户，具体请参见《FusionInsight HD管理员指南》的 创建用户 章节。例如，创建用户kafkauser，并选择kafka和kafkaadmin用户组，下载对应的秘钥文件,将krb5.conf文件上传到客户端节点的/opt/hadoopclient/目录下 kafka-manager编译及配置 修改源码 进入安装目录/kafka-manager/app/kafka/manager/jmx，修改KafkaJMX.scala文件中写死的jmx连接字符串，将'jmxrmi'修改为'kafka',如下图： 编译kafka-manager,获取压缩包 cd /opt/kafka-manager ./sbt clean dist cp /opt/kafka-manager/target/universal/kafka-manager-1.3.3.21.zip /opt cd /opt uzip kafka-manager-1.3.3.21.zip cd /opt/kafka-manager-1.3.3.21 修改配置文件 修改配置文件conf/application.conf,'kafka-manager.zkhosts'修改为zookeeper集群节点IP:端口,FI集群默认端口为24002kafka-manager.zkhosts=\"172.21.3.115:24002\" 新建conf/jaas.conf文件，配置如下： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; 将kafka-manager的lib库中zookeeper的jar包替换为FusionInsight客户端中zookeeper的jar包,并重命名 cp /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar /opt/kafka-manager-1.3.3.21/lib cd /opt/kafka-manager-1.3.3.21/lib rm org.apache.zookeeper.zookeeper-3.4.10.jar mv zookeeper-3.5.1.jar org.apache.zookeeper.zookeeper-3.4.10.jar kafka-manager使用 启动kafka-manager cd /opt/kafka-manager-1.3.3.21 nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Djava.security.auth.login.config=conf/jaas.conf -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com & 可通过-Dhttp.port=port指定访问端口,默认为9000 在浏览器地址栏输入172.21.3.48:9000，即可访问kafka-manager 点击cluster->Add Cluster，进行如下填写 Cluster Name:自定义 Cluster Zookeeper Hosts:ZooKeeper集群节点信息,可写多个或者一个节点,一定要加上kafka后缀 Kafka Version:当前FI集群中使用的是0.11.0.1，选择最接近的0.11.0.2即可 勾选Enable JMX复选框 将以下几个size大小设置为大于等于2 其他设置可以保持默认或者根据需要修改,点击Save可以看到集群创建成功 点击Go to cluster view，可以看到集群相关信息 在Brokers菜单栏可以看到当前集群的brokers情况 在Topic->Create菜单栏可以创建新的topic 在Topic->List菜单栏可以看到当前集群所有的topic "},"Data_Integration/Using_Informatica_PWX_CDC_with_FusionInsight.html":{"url":"Data_Integration/Using_Informatica_PWX_CDC_with_FusionInsight.html","title":"Informatica PWX CDC","keywords":"","body":"Connection Instruction Between Informatica PowerExchange CDC and FusionInsight Succeeded Case Informatica PowerexChange CDC 10.2.0 FusionInsight HD 6.5 Environment Information Informatica PowerExchange CDC 10.2.0 Linux & Windows version Informatica PowerExchange Publisher 10.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka client Architecture A data source, oracle database One Linux machine, installed with Informatica PWX CDC, start the listener and logger service, then install the PWX Publisher which can transfer the log data captured by PWX CDC to the kafka topic. One Linux machine, installed with FusionInsight HD Kafka client, consume the data transferred from PWX Publisher (optional) One Windows machine, installed with PWX CDC, start the listener service, use navigator to see the data captured by PWX CDC. database configuration >This part can refer to the Informatica PowerExchange CDC user guide https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html login to the system as oracle user, use Sqlplus / as sysdba login to Oracle database, open Archive Log: SHUTDOWN IMMEDIATE; STARTUP MOUNT; ALTER DATABASE ARCHIVELOG; ALTER DATABASE OPEN; SHUTDOWN IMMEDIATE: STARTUP; archive log list; Tips:Back up your database after both SHUTDOWN commands.. Set Up Oracle Minimal Global Supplemental Logging SELECT supplemental_log_data_min, force_logging FROM v$database; alter database add supplemental log data; alter database force logging; ALTER SYSTEM switch logfile; Copy the Oracle Catalog to the Archived Logs EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Define a CDC User and Grant User Privileges create a test table and insert some data Install Informatica PWX CDC & PWX Publisher Install Informatica PWX CDC in Linux Get the installation package pwx1020_linux_em64t.tar. untar the package and run ./install.sh, configure the installation path here is /opt/PowerExchange/10.2.0. Configure the environment open environment file vi ~/.bash_profile add the following configuration export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK run source ~/.bash_profile run dtlinfo,check the installation Configure dbmover.cfg and pwxccl.cfg file Configure dbmover.cfg as following nodeln is the self defined listener node name the second ORCL in ORACLEID is the database name to be listened. CAPT_PATH is the CDC control file path, the path should be created previously define the SVCNODE and CMDNODE name Configure pwxccl.cfg as following CONDENSENAME should be the same as SVCNODE in dbmover.cfg DBID is the database SID CAPTURE_NODE is the capture node name CAPTURE_NODE_UID is the database user name CAPTURE_NODE_PWD is the database user password Start listener and logger services Use PWX CDC capture ORACLE log data install Informatica PWX CDC in Windows machine Get the installation package and double click to install, add environment variable PWX_CONFIG,configured as the dbmover.cfg file in PWX Configure dbmover.cfg file set listener name, add listener Information in server side set the listened database name set the control file path start the listener start Navigator In Navigator create a new registeration group as following: NEXT chick next,we can see the test table created in oracle, double click the table name, choose all columns chick next, change state to active, check boxrun DDL immediately, click finish In Extraction Groups, double click the orcl11 created before, right click, add Extract Defination, set the map name and table name click next, can see the capture created before click add, finish click the icon, run row test, the captured data is shown as following Use PWX CDC publisher to connect Kafka Change kafka configuration file Configure producer.properties, add the following configurationsasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer Configure jaas.conf as following create a kafka topic, named pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic Install Informatica PWX Publisher Get the package pwxcdcpub120_linux_x64.tar.gz,untar it Login as root，add the following configuration in ~/.bash_profile file export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key source the environment, kerberos认证source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser Copy all the files in directory samples to instanceA/config Configuration for PWX Publisher can refer to the Informatica user guide https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html Configure cdcPublisherAvro.cfg Configure cdcPublisherCommon.cfg Configure cdcPublisherKafka.cfg, set kafka topic name and the properties file path Configure cdcPowerExchange.cfg Extract.pwxCapiConnectionName is the CAPI_CONNECTION in dbmover.cfg file Extract.pwxExtractionMapSchemaName is the schema name in pwx extraction, here is u8orcl Extract.pwxNodeLocation is pwx node name Extract.pwxNodeUserId/Extract.pwxNodePwd and Extract.pwxXmapUserId/Extract.pwxXmappassword is database user name and pasword Change the PwxCDCPublisher.sh file in installation path bin,add the following RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" Start pwx CDC Publisher,run sh PwxCDCPublisher.sh Start kafka consumer Insert data in oracle, the captured data in kafka is the following Update data in oracle, the captured data in kafka is the following Delete data in oracle, the captured data in kafka is the following Q&A 1.Failed to start pwxccl A:Run the following script in oracle exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Then grant C##PWX sysdba right grant sysdba to C##PWX "},"Integrated_Development_Environment/":{"url":"Integrated_Development_Environment/","title":"Integrated Development","keywords":"","body":" Integrated Development R RStudio Notebook Like IDE Apache Zeppelin Zeppelin0.7.2 FusionInsight HD V100R002C60U20 Zeppelin0.7.3 FusionInsight HD V100R002C70SPC100 Zeppelin0.8.0 FusionInsight HD V100R002C80SPC200 Jupyter Notebook Splunk SQL Like IDE DBeaver DbVisualizer Squirrel "},"Integrated_Development_Environment/Using_RStudio_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_RStudio_with_FusionInsight.html","title":"RStudio","keywords":"","body":"RSutdio对接FusionInsight Spark 适用场景 R-3.4.1 FusionInsight HD V100R002C60U10 R-3.4.1 FusionInsight HD V100R002C70SPC100 对接方式 RStudio与Spark集成有两种方式： 通过RStudio官方发布的sparklyr与Spark进行集成 通过Apache Spark社区发布的SparkR进行集成 本文档包含了两种方式对接的步骤, 相关对接步骤如下： 安装R 安装RStudio Server 安装FusionInsight客户端 使用SparkR与RStudio集成进行分析 在RStudio中使用SparkR进行数据分析 使用RStudio Sparklyr和Spark集成进行分析 使用sparklyr结合spark进行数据分析babynames数据集 使用sparklyr结合spark进行数据分析航空公司飞行数据(必须配套Spark2x) 安装R 由于Spark的Executor上也需要执行R，所以除了在RStudio的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 本文使用的RStudio节点为Redhat7.1，FusionInsight集群节点为Redhat6.6 配置redhat的yum源，国内可以配置aliyun的源或者163的源 配置EPEL的源 安装R-3.4.1 配置aliyun的源 配置好Redhat7.1的yum源 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache 配置163的源 配置好Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 配置EPEL的源 安装EPEL源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm 更新cache yum clean all yum makecache 安装R-3.4.1 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 安装RStudio Server 下载并安装RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm 使用 vi /etc/rstudio/rserver.conf 修改RStudio的配置文件，指定RStudio Server使用的R的路径 rsession-which-r=/usr/bin/R 重启rstudio-server后，查看服务是否正常 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server 服务正常启动如下 由于RStudio Server不允许使用root用户登陆，需要新建一个普通用户用于Web界面的登陆 useradd -d /home/test -m test passwd test 用户新建完成后，关闭防火墙，然后使用本机ip:8787端口访问RStudio Server，使用新建的test用户登陆即可进入RStudio的Web开发界面 sudo systemctl stop firewalld 安装FusionInsight客户端 登录FusionInsight Manager系统，单击 服务管理 ，在菜单栏中单击 下载客户端, 客户端类型勾选 完整客户端, 是否在集群的节点中生成客户端文件选择 否 使用WinSCP工具将下载下来的软件包上传到Linux服务器的目录，例如 /tmp/client 切换到新建的test用户 su test 解压软件包。进入安装包所在目录，例如 /tmp/client 。执行如下命令解压安装包到本地目录 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar 进入安装包所在目录，执行如下命令安装客户端到指定目录（绝对路径），例如安装到 /home/test/hadoopclient 目录 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient 客户端将被安装到 /home/test/hadoopclient 目录中 检查客户端节点与FusionInsight集群时间同步（差距不能超过5分钟） 检查SparkR是否可用 使用sparkuser进行Kerberos认证(sparkuser为FusionInsight中创建的拥有Spark访问权限的人机用户) cd /home/test/hadoopclient source bigdata_env kinit sparkuser 执行sparkR启动SparkR, 正常启动出现以下界面 使用SparkR与RStudio集成进行分析 使用新建的用户登陆即可进入RStudio的Web开发界面 选择 Tools 菜单下的 Shell 进入登陆用户的shell进行kerberos认证 cd /home/test/hadoopclient source bigdata_env kinit sparkuser 在RStudio界面中配置环境变量，初始化SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc 初始化成功后如下图 在Yarn的ResourceManager界面可以看到sparkuser在集群启动了一个SparkR的应用 在RStudio中使用SparkR进行数据分析 R DataFrame 转化为SparkR DataFrame df 通过JSON文件加载数据进行分析处理 将测试数据put到HDFS中 wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ 执行文件加载分析 people 从Hive表中加载数据进行分析 hiveContext DataFrame Operations Selecting rows, columns df Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts Operating on Columns df$waiting_secs Running SQL Queries from SparkR people = 13 AND age Machine Learning df 使用RStudio Sparklyr和Spark集成进行分析 选择 Tools 菜单下的 Shell 进入登陆用户的shell进行kerberos认证 cd /home/test/hadoopclient source bigdata_env kinit sparkuser 在RStudio中执行下面的命令，安装所需的library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") 通过spark_connect连接spark集群 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc 这里如果SPARK_HOME指向/home/test/hadoopclient/Spark/spark，同时设置version为1.6.1，则会对接上1.5.1的Spark sparklyr官方支持是1.6.1以上的Spark，这里强制指定version为1.6.1，主要功能均正常，部分Spark1.6.1支持而1.5.1不支持的特性执行会失败 启动成功后，在FusionInsgiht的Yarn的ResourceManager页面可以看到sparklyr的任务已经启动 在RStudio的Spark面板刷新一下，可以看到所有hive的表 选择hive表右边的数据图表可以预览表中的数据 使用sparklyr结合spark进行数据分析babynames数据集 Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl % mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl % filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl % filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl % group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl % filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank % arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly % inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl % filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank % arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly % inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName % mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE) 使用sparklyr结合spark进行数据分析航空公司飞行数据(必须配套Spark2x) Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data % filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay % filter(arrdelay > -60 & arrdelay % filter(year >= 2003 & year % left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition % sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 % ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) Assess model performance Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles % mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles % ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 % filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay % filter(arrdelay > -60 & arrdelay % filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier % group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights. FAQ FusionInsight集群不允许访问网络，如何安装R 在集群外同版本的Redhat版本下按照配置EPEL的源安装R进行操作，最后一步不要执行yum install R 执行yum install yum-utils安装yumdownloader 执行yumdownloader R --resolve --destdir=/tmp/packages把所有的rpm安装包下载到/tmp/packages中 将/tmp/packages中的所有rpm包复制到集群每个节点的/tmp/packages中 切换到集群每个节点的/tmp/packages中，执行yum localinstall *.rpm完成安装 安装sparklyr报错configuration failed for package ‘openssl’ 操作系统需要执行yum install openssl-devel安装openssl-devel 如何获取本文中使用sparklyr分析的源数据 执行以下shell脚本获取待分析的数据 # Make download directory mkdir /tmp/flights # Download flight data by year for i in {2006..2008} do echo \"$(date) $i Download\" fnam=$i.csv.bz2 wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam echo \"$(date) $i Unzip\" bunzip2 /tmp/flights/$fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat 将下载下来的/tmp/flights目录以及/tmp/airlines.csv，/tmp/airports.csv文件上传到HDFS的/user/sparkuser目录中，然后在Hive中创建三张表，将数据加载到对应的表中 hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int, month int, dayofmonth int, dayofweek int, deptime int, crsdeptime int, arrtime int, crsarrtime int, uniquecarrier string, flightnum int, tailnum string, actualelapsedtime int, crselapsedtime int, airtime string, arrdelay int, depdelay int, origin string, dest string, distance int, taxiin string, taxiout string, cancelled int, cancellationcode string, diverted int, carrierdelay string, weatherdelay string, nasdelay string, securitydelay string, lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES(\"skip.header.line.count\"=\"1\"); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string, Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,', \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties(\"skip.header.line.count\"=\"1\"); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string, name string, city string, country string, faa string, icao string, lat double, lon double, alt int, tz_offset double, dst string, tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,', \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports; "},"Integrated_Development_Environment/Using_Zeppelin_with_FusionInsight_HD.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_with_FusionInsight_HD.html","title":"Apache Zeppelin","keywords":"","body":"Zeppelin对接FusionInsight HD Zeppelin0.7.2 FusionInsight HD V100R002C60U20 Zeppelin0.7.3 FusionInsight HD V100R002C70SPC100 Zeppelin0.8.0 FusionInsight HD V100R002C80SPC200 "},"Integrated_Development_Environment/Using_Zeppelin_0.7.2_with_FusionInsight_HD_C60U20.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_0.7.2_with_FusionInsight_HD_C60U20.html","title":"Zeppelin0.7.2 <-> FusionInsight HD V100R002C60U20","keywords":"","body":"Zeppelin对接FusionInsight HD 适用场景 Zeppelin 0.7.2 FusionInsight HD V100R002C60U20 安装Zeppelin 操作场景 安装Zeppelin0.7.2 前提条件 已完成FusionInsight HD客户端的安装。 操作步骤 将软件包zeppelin-0.7.2-bin-all.tgz上传至/opt目录下，解压生成zeppelin-0.7.2-bin-all目录。 tar -zxvf zeppelin-0.7.2-bin-all.tgz 启动和停止Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop 配置Zeppelin环境变量，在profile文件中加入如下变量 vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh 加入如下内容： export JAVA_HOME=/opt/jdk1.7.0_51/ 编辑zeppelin-site.xml文件，位置/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml 将zeppelin-site.xml中端口8080替换成18081（可自定义，也可以不改） sed -i 's/8080/18081/' zeppelin-site.xml 运行zeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start 在浏览器中输入地址zeppelin_ip:18081登陆，zeppelin_ip为安装zeppelin的虚拟机IP。 根据产品文档创建用户test，并赋予足够权限，下载用户test的keytab文件user.keytab，上传至/opt/目录下。 编辑zeppelin-site.xml文件，将zeppelin.anonymous.allowed参数的true修改为false。 编辑shiro.ini文件，位置/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc表示对任何url访问都需要验证 [users]下增加用户test，密码Huawei@123 重启zeppelin。 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart 使用test用户登陆Zeppelin Zeppelin连接Hive 操作场景 Zeppelin中配置JDBC解析器，对接Hive的JDBC接口。 前提条件 已经完成Zeppelin 0.7.2的安装； 已完成FusionInsight HD客户端的安装，包含Hive组件。 操作步骤 将/opt/hadoopclient/Hive/Beeline/lib/下的jar包拷贝至/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/目录下。 将从新拷贝过来的jar包的属主和权限修改为和/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/下原有的jar包相同 chown 501:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.2-bin-all/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择JDBC，点击 edit 编辑，修改default.driver和default.url参数，点击 save 保存 default.driver：org.apache.hive.jdbc.HiveDriver default.url：jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hive 编辑note，点击右侧“执行”按钮。 %jdbc Show tables; Select * from workers_info; 查看结果 Zeppelin连接HBase 操作场景 Zeppelin中配置Hbase解析器，对接Hbase 前提条件 已经完成Zeppelin 0.7.2的安装； 已完成FusionInsight HD客户端的安装，包含HBase组件。 操作步骤 将/opt/hadoopclient/HBase/hbase/lib/以下的jar包拷贝至/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/目录下，overwrite选择n 在/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/下新建目录zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar 将/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/下与FusionInsight冲突的38个jar包移动到zeppelin_hbase_jar目录中 commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar 最终/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/有152个jar包 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.2-bin-all/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择hbase，点击 edit 编辑，修改hbase.home参数，点击 save 保存 hbase.home：/opt/hadoopclient/HBase/hbase 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hbase 编辑note，点击右侧 执行 按钮 %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' 在FusionInsight的客户端下可以看到创建的hbase表test2和数据 Zeppelin连接Spark 操作场景 Zeppelin中配置Spark解析器 前提条件 完成Zeppelin0.7.2的安装； 已完成FusionInsight HD V100R002C60U20和客户端的安装，包含Spark组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 将/opt/zeppelin-0.7.2-bin-all/lib/目录下的原有的相关的jar包删除 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar 将/opt/hadoopclient/Spark/adapter/dev_lib/下的以下jar包拷贝到/opt/zeppelin-0.7.2-bin-all/lib/目录下 hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar 将/opt/zeppelin-0.7.2-bin-all/lib/下的jackson的相关jar包删除 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar 将/opt/hadoopclient/Spark/adapter/dev_lib/下的jackson相关的jar包拷贝到/opt/zeppelin-0.7.2-bin-all/lib/下 jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar 将步骤1和步骤2所有从spark客户端拷贝过来的jar包的属主和权限修改为和/opt/zeppelin-0.7.2-bin-all/lib/下原有的jar包相同 chown 501:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.2-bin-all/conf，加入以下内容 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择Spark，点击 edit 编辑，将 Master 参数改为 yarn-client，点击 save 保存 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart 执行zeppelin的spark样例代码zeppelin Tutorial -> Basic Features(Spark) 样例代码需要访问Internet上的资源，所以保证zeppelin所在的节点可以联网，检测是否能打开以下链接 执行zeppelin的spark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) 安装python-matplotlib yum install python-matplotlib 安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh 配置环境变量PATH，将python换成安装Anaconda安装目录中的python export PATH=/root/anaconda2/bin/:$PATH 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.pyspark.python 参数改为Anaconda安装目录中的python，点击 save 保存 执行zeppelin的pyspark样例代码Zeppelin Tutorial -> Matplotlib Zeppelin连接SparkR 操作场景 Zeppelin中配置Spark解析器，连接SparkR 前提条件 完成Zeppelin0.7.2的安装； 已完成FusionInsight HD V100R002C60U20和客户端的安装，包含Spark组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 由于Spark的Executor上也需要执行R，所以除了在Zeppelin的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 不同OS配置yum源时下载的文件路径有所不同，下面以Redhat6.6安装R为例 如果安装R的节点无法访问互联网，参考FAQ进行R的安装 配置Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 配置EPEL的源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm 更新cache yum clean all yum makecache 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 FusionInsight客户端下测试是否可以使用sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR 正常启动如下图所示 参考http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter 在R的命令行中安装sparkR样例需要的R的libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.R.cmd 参数改为R的启动文件，点击 save 保存 重启zeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart 在Zeppelin中执行Zeppelin Tutorial -> R (SparkR)样例 FAQ FusionInsight集群不允许访问网络，如何安装R 在集群外同版本的Redhat版本下按照本文中yum源的方式进行安装R的操作，最后一步不要执行yum install R 执行yum install yum-utils安装yumdownloader 执行yumdownloader R --resolve --destdir=/tmp/packages把所有的rpm安装包下载到/tmp/packages中 将/tmp/packages中的所有rpm包复制到集群每个节点的/tmp/packages中 切换到集群每个节点的/tmp/packages中，执行yum localinstall *.rpm完成安装 连接hbase出现AuthFialed for /hwbackup/hbase 原因：zeppelin的原理hbase的jar包与从FusionInsight客户端下拷贝过来的jar冲突。 解决：将zeppelin中原有的重名jar包移走或删除，全部用FusionInsight客户端下的相关jar包。 Zeppelin连接spark是报如下NoSuchMethodError 原因：jar包冲突 解决：删除/opt/zeppelin-0.7.2-bin-all/lib/下原有jar包scala-reflect-2.11.7.jar，替换为FusionInsight客户端下的jar包，重启zeppelin Zeppelin执行Spark样例代码时报GC overhead limit exceeded 原因：内存不够 解决：安装Zeppelin的节点的内存需要16G以上 执行zeppelin的样例代码Zeppelin Tutorial/Matplotlib (Python PySpark)报如下错误 原因：python版本问题 解决：安装Anaconda2-4.4 "},"Integrated_Development_Environment/Using_Zeppelin_0.7.3_with_FusionInsight_HD_C70SPC100.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_0.7.3_with_FusionInsight_HD_C70SPC100.html","title":"Zeppelin0.7.3 <-> FusionInsight HD V100R002C70SPC100","keywords":"","body":"Zeppelin对接FusionInsight HD 适用场景 Zeppelin 0.7.3 FusionInsight HD V100R002C70SPC100 (Spark2.x) Zeppelin 0.7.3 FusionInsight HD V100R002C80SPC200 (Spark2.x) 编译Zeppelin 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v 安装git yum install -y git 安装nodejs： wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin 导入环境变量 source /etc/profile 执行npm -v 安装bower npm install -g bower 配置bower允许root用户执行 echo '{ \"allow_root\": true }' > /root/.bowerrc 执行bower -v 获取Zeppelin0.7.3的版本 git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 修改scala版本，适配FusionInsight_HD_V100R002C70SPC100的Hadoop版本 在zeppelin代码根目录执行vi ./dev/change_scala_version.sh，修改下图的SCALA_LIB_VERSION为2.11.8 执行命令完成scala版本的修改 ./dev/change_scala_version.sh 2.11 执行vi pom.xml文件的修改为0.9.3 执行vi hbase/pom.xml修改hbase版本和hadoop版本 编译Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests 编译完成后在zeppelin-distribution/target目录下生成zeppelin-0.7.3.tar.gz文件 安装Zeppelin 操作场景 安装Zeppelin0.7.3 前提条件 已完成FusionInsight HD客户端的安装。 操作步骤 将编译好的zeppelin-0.7.3.tar.gz上传放到/opt目录下，解压生成zeppelin-0.7.3目录。 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz 配置Zeppelin环境变量，在profile文件中加入如下变量 vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh 加入如下内容： export JAVA_HOME=/opt/hadoopclient/JDK/jdk 编辑zeppelin-site.xml文件，位置/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml 将zeppelin-site.xml中端口8080替换成18081（可自定义，也可以不改） sed -i 's/8080/18081/' zeppelin-site.xml 运行zeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start 在浏览器中输入地址zeppelin_ip:18081登陆，zeppelin_ip为安装zeppelin的虚拟机IP。 根据产品文档创建用户test，并赋予足够权限，下载用户test的keytab文件user.keytab，上传至/opt/目录下。 编辑zeppelin-site.xml文件，将zeppelin.anonymous.allowed参数的true修改为false。 编辑shiro.ini文件，位置/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc表示对任何url访问都需要验证 [users]下增加用户test，密码Huawei@123 重启zeppelin。 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart 使用test用户登陆Zeppelin Zeppelin连接Hive 操作场景 Zeppelin中配置JDBC解析器，对接Hive的JDBC接口。 前提条件 已经完成Zeppelin 0.7.3的安装； 已完成FusionInsight HD客户端的安装，包含Hive组件。 操作步骤 将/opt/hadoopclient/Hive/Beeline/lib/下的jar包拷贝至/opt/zeppelin-0.7.3/interpreter/jdbc/目录下。 将从新拷贝过来的jar包的属主和权限修改为和/opt/zeppelin-0.7.3/ interpreter/jdbc/下原有的jar包相同 chown 501:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.3/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择JDBC，点击 edit 编辑，修改default.driver和default.url参数，点击 save 保存 default.driver：org.apache.hive.jdbc.HiveDriver default.url：jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hive 编辑note，点击右侧“执行”按钮。 %jdbc Show tables; Select * from workers_info; 查看结果 Zeppelin连接HBase 操作场景 Zeppelin中配置Hbase解析器，对接Hbase 前提条件 已经完成Zeppelin 0.7.3的安装； 已完成FusionInsight HD客户端的安装，包含HBase组件。 操作步骤 将/opt/zeppelin-0.7.3/interpreter/hbase/目录下旧的jar包移走cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar 将/opt/hadoopclient/HBase/hbase/lib/以下的jar包拷贝至/opt/zeppelin-0.7.3/interpreter/hbase/目录下 cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase 从FusionInsight客户端下载用户test的user.keytab和krb5.conf文件，将krb5.conf文件放在/etc/下 使用vi /opt/zeppelin-0.7.3/conf/新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的test用户，将test的keytab文件user.key放在/opt/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择hbase，点击 edit 编辑，修改hbase.home参数，点击 save 保存 hbase.home：/opt/hadoopclient/HBase/hbase 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hbase 编辑note，点击右侧 执行 按钮 %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' 在FusionInsight的客户端下可以看到创建的hbase表test2和数据 Zeppelin连接Spark 操作场景 Zeppelin中配置Spark解析器 前提条件 完成Zeppelin0.7.3的安装； 已完成FusionInsight HD V100R002C70SPC100和客户端的安装，包含Spark2x组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下内容 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择Spark，点击 edit 编辑，将 Master 参数改为 yarn-client，点击 save 保存 重启zeppelin source /opt/hadoopclient/bigdata_env kinit –kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart 执行zeppelin的sparkSQL语句 执行zeppelin的spark样例代码zeppelin Tutorial -> Basic Features(Spark) 样例代码需要访问Internet上的资源，所以保证zeppelin所在的节点可以联网，检测是否能打开以下链接 执行zeppelin的spark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) 安装python-matplotlib yum install python-matplotlib 安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh 配置环境变量PATH，将python换成安装Anaconda安装目录中的python export PATH=/root/anaconda2/bin/:$PATH 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.pyspark.python 参数改为Anaconda安装目录中的python，点击 save 保存 执行zeppelin的pyspark样例代码Zeppelin Tutorial -> Matplotlib Zeppelin连接SparkR 操作场景 Zeppelin中配置Spark解析器，连接SparkR 前提条件 完成Zeppelin0.7.3的安装； 已完成FusionInsight HD V100R002C70SPC100和客户端的安装，包含Spark组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 由于Spark的Executor上也需要执行R，所以除了在Zeppelin的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 不同OS配置yum源时下载的文件路径有所不同，下面以Redhat6.6安装R为例 如果安装R的节点无法访问互联网，参考FAQ进行R的安装 配置Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 配置EPEL的源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm 更新cache yum clean all yum makecache 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 FusionInsight客户端下测试是否可以使用sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR 正常启动如下图所示 参考http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter 在R的命令行中安装sparkR样例需要的R的libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.R.cmd 参数改为R的启动文件，点击 save 保存 重启zeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart 在Zeppelin中执行Zeppelin Tutorial -> R (SparkR)样例 FAQ FusionInsight集群不允许访问网络，如何安装R 在集群外同版本的Redhat版本下按照本文中yum源的方式进行安装R的操作，最后一步不要执行yum install R 执行yum install yum-utils安装yumdownloader 执行yumdownloader R --resolve --destdir=/tmp/packages把所有的rpm安装包下载到/tmp/packages中 将/tmp/packages中的所有rpm包复制到集群每个节点的/tmp/packages中 切换到集群每个节点的/tmp/packages中，执行yum localinstall *.rpm完成安装 连接hbase出现AuthFialed for /hwbackup/hbase 原因：zeppelin的原理hbase的jar包与从FusionInsight客户端下拷贝过来的jar冲突。 解决：将zeppelin中原有的重名jar包移走或删除，全部用FusionInsight客户端下的相关jar包。 Zeppelin连接spark是报如下NoSuchMethodError 原因：jar包冲突 解决：删除/opt/zeppelin-0.7.3/lib/下原有jar包scala-reflect-2.11.7.jar，替换为FusionInsight客户端下的jar包，重启zeppelin Zeppelin执行Spark样例代码时报GC overhead limit exceeded 原因：内存不够 解决：安装Zeppelin的节点的内存需要16G以上 执行zeppelin的样例代码Zeppelin Tutorial/Matplotlib (Python PySpark)报如下错误 原因：python版本问题 解决：安装Anaconda2-4.4 "},"Integrated_Development_Environment/Using_Zeppelin_0.8.0_with_FusionInsight_HD_C80SPC200.html":{"url":"Integrated_Development_Environment/Using_Zeppelin_0.8.0_with_FusionInsight_HD_C80SPC200.html","title":"Zeppelin0.8.0 <-> FusionInsight HD V100R002C80SPC200","keywords":"","body":"Zeppelin对接FusionInsight HD 适用场景 Zeppelin 0.8.0 FusionInsight HD V100R002C80SPC200 (Spark2.x) 安装Zeppelin 操作场景 安装Zeppelin0.8.0 前提条件 已完成FusionInsight HD和客户端的安装。 操作步骤 安装Zeppelin 0.8.0,在网址https://zeppelin.apache.org/download.html下载安装包，使用WinSCP导入主机并用tar -zxvf zeppelin-0.8.0-bin-all.tgz安装生成zeppelin-0.8.0-bin-all目录。 启动和停止Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop 执行source命令到客户端，获取java配置信息 source /opt/hadoopclient/bigdata_env echo $JAVA_HOME 配置Zeppelin环境变量，在profile文件中加入如下变量 vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH 编辑zeppelin-env.sh文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh 加入如下内容： export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 编辑zeppelin-site.xml文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml 将zeppelin-site.xml中端口8080替换成18081（可自定义，也可以不改） sed -i 's/8080/18081/' zeppelin-site.xml 运行zeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start 在浏览器中输入地址zeppelin_ip:18081登陆，zeppelin_ip为安装zeppelin的虚拟机IP 根据产品文档创建用户developuser，并赋予足够权限，下载用户developuser的keytab文件user.keytab，上传至/usr/zeppelin/zeppelin-0.8.0-bin-all目录下 编辑zeppelin-site.xml文件，将zeppelin.anonymous.allowed参数的true修改为false 编辑shiro.ini文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc表示对任何url访问都需要验证 [users]下增加用户developuser，密码Huawei@123，权限admin 重启zeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 使用账户developuser登陆zeppelin Zeppelin连接Hive 操作场景 Zeppelin中配置JDBC解析器，对接Hive的JDBC接口。 前提条件 已经完成Zeppelin 0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Hive组件。 操作步骤 将/opt/hadoopclient/Hive/Beeline/lib/下的jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/目录下。 将从新拷贝过来的jar包的属主和权限修改为和/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/下原有的jar包相同 chown 502:wheel *.jar chmod 644 *.jar 编辑zeppelin-env.sh文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/，加入以下三个配置内容export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 从FusionInsight客户端下载用户developuser的user.keytab和krb5.conf文件，将krb5.conf文件放在/opt/developuser/下 在/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/路径下新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的developuser用户，将developuser的keytab文件user.key放在/opt/developuser/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择JDBC，点击 edit 编辑，修改default.driver和default.url参数，点击 save 保存 default.driver：org.apache.hive.jdbc.HiveDriver default.url：jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hive_test 编辑note，点击右侧“执行”按钮。 %jdbc Show tables; %jdbc select * from t2 查看结果 Zeppelin连接HBase 操作场景 Zeppelin中配置Hbase解析器，对接Hbase 前提条件 已经完成Zeppelin 0.8.0的安装 已完成FusionInsight HD和客户端的安装，包含HBase组件 操作步骤 将/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/目录下旧的jar包移走cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar 将/opt/hadoopclient/HBase/hbase/lib/以下的jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/目录下 cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase 编辑zeppelin-env.sh文件，位置/usr/zeppelin/zeppelin-0.8.0-bin-all/conf，加入以下三个配置内容 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase 从FusionInsight客户端下载用户developuser的user.keytab和krb5.conf文件，将krb5.conf文件放在/opt/developuser下 在/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/路径下新建hbase的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的developuser用户，将developuser的keytab文件user.key放在/opt/developuser/目录下 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择hbase，点击 edit 编辑，修改hbase.home参数，点击 save 保存 hbase.home：/opt/hadoopclient/HBase/hbase 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 页面选择Notebook -> Create new note 自定义note名称，例如hbase_test 编辑note，点击右侧“执行”按钮 %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' 在FusionInsight的客户端下可以看到创建的hbase表test4和数据 Zeppelin连接Spark 操作场景 Zeppelin中配置Spark解析器 前提条件 完成Zeppelin0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Spark2x组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 将/opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars路径下所有的jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark 将/opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars路径下libfb303-0.9.3.jar和libthrift-0.9.3.jar两个jar包拷贝至/usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep路径下 确保/usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter路径下有且仅有libthrift-0.9.3.jar这个版本的jar包 编辑zeppelin-env.sh文件，位置/opt/zeppelin-0.7.3/conf，加入以下内容 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择Spark，点击 edit 编辑，将 master 参数改为 yarn-client，并且检查zeppelin.spark.useHiveContext项，使其值为false，点击 save 保存 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 执行zeppelin的spark样例代码，参考网址 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ 样例代码需要访问Internet上的资源，所以保证zeppelin所在的节点可以联网，检测是否能打开以下链接 执行zeppelin的spark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) 安装python-matplotlib yum install python-matplotlib 安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh 配置环境变量PATH，将python换成安装Anaconda安装目录中的python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.pyspark.python 参数改为Anaconda安装目录中的python，点击 save 保存 执行zeppelin的pyspark样例代码Zeppelin Tutorial -> Matplotlib (Python • PySpark) Zeppelin连接SparkR 操作场景 Zeppelin中配置Spark解析器，连接SparkR 前提条件 完成Zeppelin0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Spark2x组件。 参考http://zeppelin.apache.org/docs/latest/interpreter/spark.html 操作步骤 由于Spark的Executor上也需要执行R，所以除了在Zeppelin的节点上安装R以外，所有FusionInsight集群节点上也要安装同版本的R，安装步骤如下： 不同OS配置yum源时下载的文件路径有所不同，下面以Redhat6.6安装R为例 配置Redhat6.6的yum源 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache 如果遇到源yum-plugin-fastestmirror无法下载时，可在网址https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror下选择相应的版本代替下载安装 配置EPEL的源 Redhat 6.x 使用下面命令安装 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm 更新cache yum clean all yum makecache 执行yum install R 安装R的相关的包 执行R，检查R是否可用 正常启动如下图所示 FusionInsight客户端下测试是否可以使用sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR 参考http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter 在R的命令行中安装sparkR样例需要的R的libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') 在zeppelin的界面中，选择右上角的 Interpreter 选择Spark，点击 edit 编辑，将 zeppelin.R.cmd 参数改为R的启动文件，点击 save 保存 重启zeppelin。 source /opt/hadoopclient/bigdata_env kinit –kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart 在Zeppelin中执行Zeppelin Tutorial -> R (SparkR)样例 Zeppelin连接Apache Livy 操作场景 Zeppelin中配置Livy解析器，连接Livy 前提条件 完成Zeppelin0.8.0的安装； 已完成FusionInsight HD和客户端的安装，包含Spark2x组件。 完成Apache Livy 0.5.0的安装 可参考《Apache Livy对接FusionInsight》对接文档完成Apache Livy的安装 操作步骤 用如下命令启动Livy服务 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start 登陆Zeppelin，选择右上角菜单中的 Interpreter 选择livy，点击 edit 编辑zeppelin.livy.url的值为http://172.21.3.43:8998（可以不更改），点击 save 保存 页面选择Notebook -> Create new note 自定义note名称，例如livy_connection_test 在Zeppelin中执行Spark样例代码 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y 在Zeppelin中执行PySpark样例代码 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y 在Zeppelin中执行SparkR样例代码 %livy.sparkr hello "},"Integrated_Development_Environment/Using_Jupyter_Notebook_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_Jupyter_Notebook_with_FusionInsight.html","title":"Jypyter Notebook","keywords":"","body":"Jupyter_Notebook对接FusionInsight 安装Jupyter notebook Jupyter notebook的安装依赖于Python，且涉及到许多工具的依赖包，相互之间还存在版本依赖关系，比较麻烦，通常可以直接安装Anaconda包，里面包含了Python、Jupyter Notebook，以及众多的科学工具包，这里我们直接安装Anaconda 从Anaconda官网下载并安装Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh 生成Jupyter notebook的配置文件 jupyter notebook --generate-config --allow-root 修改Jupyter notebook的配置IPc.NotebookApp.ip为本机IP地址 vi /root/.jupyter/jupyter_notebook_config.py 启动Jupyter notebook:: jupyter notebook --allow-root 出现如下提示表示Jupyter notebook启动成功 [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 使用 Ctrl+C 可以退出Jupyter notebook 安装FusionInsight Client 参考FusionInsight的产品文档完成Linux下的FusionInsight客户端的安装，安装到/opt/hadoopclient目录 完成Kerberos认证 使用sparkuser进行Kerberos认证(sparkuser为FusionInsight中创建的拥有Spark访问权限的人机用户)cd /opt/hadoopclient/ source bigdata_env kinit sparkuser 导入ipython相关环境变量 执行以下命令导入环境变量，或者将下面两行添加到/opt/hadoopclient/bigdata_env文件，后续source bigdata_env时可以自动将环境变量导入export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" Jupyter notebook中使用pyspark进行分析 执行pyspark会自动启动Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 打开上述链接，可以进行数据分析 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc % summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10) Jupyter notebook中使用R语言进行分析 TBD "},"Integrated_Development_Environment/Using_DBeaver_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_DBeaver_with_FusionInsight.html","title":"DBeaver","keywords":"","body":"DBeaver对接FusionInsight 适用场景 DBeaver 4.0.8 FusionInsight HD V100R002C60U20 DBeaver 4.2.1 FusionInsight HD V100R002C70SPC200 说明 SQL开发工具，如DbVisualizer、DBeaver、Squirrel是数据库开发的常用选择，虽然这些工具大多不提供原生Hive、SparkSQL、Phoenix的支持，但是通过它们支持的自定义JDBC的能力，我们可以与FusionInsignt提供的Fiber组件的JDBC接口进行对接，实现这Hive、SparkSQL、Phoenix组件的统一SQL查询。 Fiber架构图 本文介绍了DBeaver与FusionInsight的Fiber对接的操作步骤 Linux下DBeaver连接Fiber 操作场景 以安全模式为例，使用DBeaver通过Fiber访问Hive、Spark、Phoenix 前提条件 已经安装好Linux（Redhat Linux Enterprise 6.5 64bit）Desktop操作系统； 已经安装好的Linux机器的时间与FusionInsight HD集群的时间要保持一致，时间差小于5分钟。 已完成FusionInsight HD V100R002C60U20安全集群的安装，已安装好Fiber客户端。 操作步骤 安装jdk1.8，DBeaver4.0.8需要jdk1.8以上版本 tar -xvf jdk-8u112-linux-x64.tar.gz 配置环境变量/etc/profile，加入如下内容，source环境变量 #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH 下载地址：http://dbeaver.jkiss.org/download/, 软件dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz，安装DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz 安装FusionInsight客户端，具体请参见《FusionInsight HD 产品文档》的 安装客户端 章节，客户端安装目录为/opt/hadoopclient/，其中Fiber客户端目录/opt/hadoopclient/Fiber/。 修改Fiber的配置文件/opt/hadoopclient/Fiber/conf/fiber.xml，将其中hive、spark、phoenix的认证方式改为安全模式keytab认证方式，具体配置方法参考 产品文档 -> 管理员指南 -> 业务操作指南 -> 统一SQL(Fiber) -> 客户端配置 章节。 Hive JDBC连接配置 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback /opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab java.security.krb5.conf /opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf java.security.auth.login.config /opt/jaas.conf zookeeper.server.principal {HIVE_CLIENT_ZK_PRINCIPAL} zookeeper.kinit /opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit Spark连接配置 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback /opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab java.security.krb5.conf /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf java.security.auth.login.config /opt/jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit /opt/hadoopclient/KrbClient/kerberos/bin/kinit Phoenix连接配置 phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback /opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab java.security.krb5.conf /opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf java.security.auth.login.config /opt/jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit /opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit jaas.conf文件： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 打开DBeaver，进入DBeaver的安装目录执行./dbeaver，启动dbeaver 进入DBeaver界面，菜单选择 Database -> 新建DriverManager，在弹出的对话框中点击 New 填写如下信息，点击 OK Driver Name：Fiber（自定义） Class Name：com.huawei.fiber.FiberDriver URL Template：jdbc:fiber:// Default Port：2345（可随便写） Category：Hadoop 点击 Add File 按钮，将Fiber客户端/opt/hadoopclient/Fiber/lib/下的jar包添加进来 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar 在Connection Properties中加入以下属性： 菜单栏选择 File -> New -> Database Connection, 类型选择Fiber User name和Password可不填写 配置Driver properties里面的defaultDirver，可按需求填写hive或spark或phoenix，点击next Network页面保持默认，点击 next 输入自定义Connection name后，点击 finish, 连接建立完成 测试hive链接 查看Hive表中数据 测试spark链接, 把driver切换为spark，连接右键选择 Edit Connection 使用spark driver查看表中数据 测试phoenix连接，把driver切换为phoenix，连接右键选择 Edit Connection 查看phoenix表中数据 Windows下DBeaver连接Fiber 操作场景 以安全模式为例，使用DBeaver通过Fiber访问Hive、Spark、Phoenix 前提条件 Windows上已经安装好jdk1.8以上版本，并完成jdk环境变量配置 客户端机器的时间与FusionInsight HD集群的时间要保持一致，时间差小于5分钟。 从http://dbeaver.jkiss.org/download/下载DBeaver软件，完成windows上的安装 已完成FusionInsight HD V100R002C60U20安全集群的安装，已安装好Fiber客户端。 已将集群的节点主机名与IP的映射关系加入到windows的hosts文件中C:\\Windows\\System32\\drivers\\etc\\hosts 操作步骤 Fiber的安全认证可以用kinit和keytab两种方式，具体参数配置说明可参考 产品文档 -> 管理员指南 -> 业务操作指南 -> 统一SQL(Fiber) -> 客户端配置 章节。kinit认证的有效期是24小时，keytab认证方式长期有效 使用kinit认证方式配置 使用keytab认证方式配置 使用kinit认证方式配置 下载对应操作系统架构的MIT Kerberos，并安装 http://web.mit.edu/kerberos/dist/#kfw-4.0 确认客户端机器的时间与FusionInsight HD集群的时间一致，时间差要小于5分钟 设置Kerberos的配置文件 在FusionInsight Manager创建角色和人机用户，具体请参见 产品文档 -> 管理员指南 -> 系统设置 -> 权限管理 -> 用户管理 -> 创建用户 章节。角色需要根据业务需要授予Hive的访问权限，并将用户加入角色，创建用户“test” 下载对应的keytab文件user.keytab以及krb5.conf文件，把krb5.conf文件重命名为krb5.ini，并放到C:\\ProgramData\\MIT\\Kerberos5目录中 设置Kerberos票据的缓存文件 创建存放票据的目录，例如C:\\temp 设置Windows的系统环境变量，变量名为KRB5CCNAME，变量值为C:\\temp\\krb5cache 在Windows上进行认证 打开MIT Kerberos，单击 get Ticket ，在弹出的MIT Kerberos: Get Ticket窗口中，Pricipal 输入用户名(如：test@HADOOP.COM)，Password 输入密码，单击 OK 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; 修改fiber.xml文件，位置C:\\Fiber\\conf\\fiber.xml Hive的JDBC连接 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接 phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe DBeaver连接前确认kerberos认证有效 使用keytab认证方式配置 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下，principal和keytab按实际填写 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 修改fiber.xml文件配置，位置C:\\Fiber\\conf\\fiber.xml Hive的JDBC连接 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接，需要增加属性hbase.myclient.keytab和hbase.myclient.principal phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf hbase.myclient.keytab C:\\\\Fiber\\\\conf\\\\user.keytab hbase.myclient.principal test zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe DBeaver连接Fiber 将Hive、Spark、Phoenix的JDBC配置中classPath中的文件拷贝至Fiber文件夹中 进入DBeaver界面，菜单选择 Database -> 新建DriverManager，在弹出的对话框中点击 New 填写如下信息，点击 OK Driver Name：Fiber（自定义） Class Name：com.huawei.fiber.FiberDriver URL Template：jdbc:fiber:// Default Port：2345（可随便写） Category：Hadoop 点击 Add File 按钮，将Fiber客户端（/opt/hadoopclient/Fiber/lib/）下的jar包添加进来 在Connection Properties中加入以下属性 菜单栏选择 File -> New -> Database Connection User name和Password可不填写 确认defaultDirver，可按需求填写hive或spark或phoenix。 Network保持默认，点击 next 自定义Connection name，点击finish 连接建立完成 测试hive连接 查看Hive表中数据 测试spark连接, 把driver切换为spark，连接右键选择 Edit Connection 使用spark driver查看表中数据 测试phoenix连接，把driver切换为phoenix，连接右键选择 Edit Connection 查看phoenix表中数据 DBeaver对接Fiber功能验证 Hive增加查看数据 将JDBC的defaultDrive切换至Hive Hive查询数据：菜单栏选择 SQL Editor -> New SQL Editor，编辑脚本，点击左上角执行按钮。 SELECT * FROM workers_info Hive增加数据： 编辑数据文件data_input.txt，上传至集群的hdfs目录中，例如/tmp/下，文本内容如下： 编辑脚本，点击左上角执行按钮。 查看更新后数据： Spark增加查看数据 将JDBC 的defaultDriver切换至Spark Spark查询数据：编辑脚本，点击左上角执行按钮。 SELECT * FROM workers_info Spark增加数据： 编辑数据文件data_input.txt，上传至Spark的JDBCServer(主)实例所在的节点的/opt/目录下 文本内容如下： 编辑脚本，点击左上角执行按钮。 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info 查看结果： Phoenix增删改查数据 将JDBC 的defaultDrive切换至Phoenix Phoenix增加数据 菜单栏选择 SQL Editor -> New SQL Editor，编辑脚本，点击左上角 执行 按钮。 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') 查看增加的数据： Phoenix删除数据 页面上删除：选择待删除的列，然后点击下方 删除 按钮，然后点击 save 按钮： 脚本删除：编辑脚本，点击左上方 执行 按钮 delete from TB_PHOENIX where ID=104; 查看输出后的数据 Phoenix更新数据, 编辑更新脚本，点击左上方 执行 按钮 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') 查看更新后的数据： 查看数据：编辑查询脚本，点击左上方 执行 按钮。 SELECT * FROM TB_PHOENIX "},"Integrated_Development_Environment/Using_DbVisualizer_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_DbVisualizer_with_FusionInsight.html","title":"DbVisualizer","keywords":"","body":"DbVisualizer对接FusionInsight 适用场景 DbVisualizer 9.5.7 FusionInsight HD V100R002C60U20 DbVisualizer 10.0.1 FusionInsight HD V100R002C70SPC200 说明 SQL开发工具，如DbVisualizer、DBeaver、Squirrel是数据库开发的常用选择，虽然这些工具大多不提供原生Hive、SparkSQL、Phoenix的支持，但是通过它们支持的自定义JDBC的能力，我们可以与FusionInsignt提供的Fiber组件的JDBC接口进行对接，实现这Hive、SparkSQL、Phoenix组件的统一SQL查询。 Fiber架构图 本文介绍了DbVisualizer与FusionInsight的Fiber对接的操作步骤 DbVisualizer安装 DbVisualizer9.5.7需要jdk1.8，下载安装jdk1.8，配置环境变量。 参考FusionInsight产品文档安装FusionInsight客户端，位置/opt/hadoopclient 修改C:\\Windows\\System32\\drivers\\etc\\hosts文件，加入FusionInsight集群信息 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber。 参考FusionInsight产品文档创建用户test，并赋予足够的权限，下载test的keytab文件user.keytab，拷贝到C:\\Fiber\\conf\\文件夹下。 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下，principal和keytab按实际填写： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 修改fiber.xml文件配置，位置C:\\Fiber\\conf\\fiber.xml Hive的JDBC连接 hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接 spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接，需要增加属性 hbase.myclient.keytab 和 hbase.myclient.principal phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf hbase.myclient.keytab C:\\\\Fiber\\\\conf\\\\user.keytab hbase.myclient.principal test zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe 将Hive、Spark、Phoenix的JDBC配置中classPath中的文件拷贝至Fiber文件夹中。 下载DbVisualizer，地址：http://www.dbvis.com/download/，下载软件dbvis_windows-x64_9_5_7_jre.exe 双击dbvis_windows-x64_9_5_7_jre.exe安装 DbVisualizer连接Fiber 配置DbVisualizer通过Fiber连接FusionInsight的Hive、Spark、Phoenix组件。 打开DbVisualizer9.5.7，点击 Cancel 菜单栏选择 ToolsDriver Manager 新建driver Name：Fiber(自定义) URL Format：jdbc:fiber:// User Specified：将C:\\Fiber\\lib\\下所有的jar包加入 Driver Class：加入jar包后选择com.huawei.fiber.FiberDriver 菜单栏 Database -> Create Database Connection 选择 Use Wizard {width=\"4.2in\" height=\"1.4in\"} 自定义连接名称，例如Fiber 选择Driver Fiber 填写URL：jdbc:fiber:// 点击 Finish 查询Hive表数据 打开 Properties 面板，填写defaultDriver和fiberconfig属性，点击 Apply 。 打开 Connection 面板，点击 Connect 按钮，可以在左侧看到hive数据表。 菜单栏选择 File -> New SQL Commander ，编辑SQL，点击 执行 按钮，查看查询结果。 查询SparkSQL中的数据 将defaultDriver切换为spark：将 Properties 中的defaultDriver值改为spark，点击 Apply 。 打开Connection面板，点击 Reconnect ，连接成功，可以看到SparkSQL中的数据表。 菜单栏选择 File -> New SQL Commander，编辑SQL，点击 执行 按钮，查看查询结果。 查询Phoenix中的数据 将defaultDriver切换为phoenix，将 Properties 中的defaultDriver值改为phoenix，点击 Apply 。 打开 Connection 面板，点击 Reconnect，连接成功，可以看到phoenix数据表 查看phoenix表TB_PHOENIX中的数据。 菜单栏选择 File -> New SQL Commander，编辑SQL，点击 执行 按钮，查看查询结果。 Phoenix的增加删除更新数据 Phoenix的增加删除更新数据，需要在Fiber中hbase的配置文件hbase-site.xml中加入如下参数，否则不会自动Commit 修改Hbase-site.xml文件，位置C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml，然后重启DbVisualizer。 phoenix.connection.autoCommit true Phoenix表增加数据 UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix表删除数据 delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix表更新数据 UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix; "},"Integrated_Development_Environment/Using_Squirrel_with_FusionInsight.html":{"url":"Integrated_Development_Environment/Using_Squirrel_with_FusionInsight.html","title":"Squirrel","keywords":"","body":"Squirrel对接FusionInsight 适用场景 Squirrel 3.7.1 FusionInsight HD V100R002C60U20 Squirrel 3.8.0 FusionInsight HD V100R002C70SPC200 说明 SQL开发工具，如DbVisualizer、DBeaver、Squirrel是数据库开发的常用选择，虽然这些工具大多不提供原生Hive、SparkSQL、Phoenix的支持，但是通过它们支持的自定义JDBC的能力，我们可以与FusionInsignt提供的Fiber组件的JDBC接口进行对接，实现这Hive、SparkSQL、Phoenix组件的统一SQL查询。 Fiber架构图 本文介绍了Squirrel与FusionInsight的Fiber对接的操作步骤 Squirrel安装 安装jdk1.8，配置环境变量。 参考FusionInsight产品文档安装FusionInsight客户端，位置/opt/hadoopclient。 修改C:\\Windows\\System32\\drivers\\etc\\hosts文件，加入FusionInsight集群信息。 在本地PC机上新建一个目录，将FusionInsight客户端下的fiber客户端文件夹Fiber拷贝至本地，例如C:\\Fiber。 参考FusionInsight产品文档创建用户test，并赋予足够的权限，下载test的keytab文件user.keytab，拷贝到C:\\Fiber\\conf\\文件夹下。 将FusionInsight客户端下jaas.conf文件和krb5.conf拷贝到C:\\Fiber\\conf目录下，文档内容如下，principal和keytab按实际填写： Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; 修改fiber.xml文件配置，位置C:\\Fiber\\conf\\fiber.xml。 Hive的JDBC连接： hive hive jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Spark的JDBC连接： spark spark jdbc configuration org.apache.hive.jdbc.HiveDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe Phoenix的JDBC连接，需要增加属性 hbase.myclient.keytab 和 hbase.myclient.principal： phoenix phoenix jdbc configuration org.apache.phoenix.jdbc.PhoenixDriver com.huawei.fiber.DefaultAuthenticationCallback C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase java.security.krb5.conf C:\\\\Fiber\\\\conf\\\\krb5.conf java.security.auth.login.config C:\\\\Fiber\\\\conf\\\\jaas.conf hbase.myclient.keytab C:\\\\Fiber\\\\conf\\\\user.keytab hbase.myclient.principal test zookeeper.server.principal zookeeper/hadoop.hadoop.com zookeeper.kinit C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe 将Hive、Spark、Phoenix的JDBC配置中classPath中的文件拷贝至Fiber文件夹中。 下载Squirrel，地址：http://www.squirrelsql.org/#installation，选择Install jar of SQuirreL 3.7.1 for Windows/Linux/others，下载软件squirrel-sql-3.7.1-standard.jar 双击squirrel-sql-3.7.1-standard.jar安装 在这里可以选择要安装哪些环境，使用的数据库插件，语言包。 Squirrel连接Fiber 使用SQuirreL SQL Client通过Fiber连接FusionInsight的Hive、SparkSQL、Phoenix组件。 打开SQuirreL SQL Client，选择Drivers，点击 +。 填写Driver信息，点击 OK。 Name：Fiber（自定义） Example URL：jdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path：将Fiber/lib下的jar包都添加进来 ClassName：com.huawei.fiber.FiberDriver 可以看到添加完成的Driver Fiber。 对接Hive 点击 Aliases，点击 + 在弹出框中填写信息 Name：Fiber（自定义） Driver：选择Fiber User Name：test Password：密码 点击 Connect 连接成功，点击 OK 点击 Connect 查看hive中数据表 点击 SQL面板，编辑SQL语句，点击 执行 按钮，在下方可以看到查询结果。 Hive增加数据： 编辑数据文件data_input.txt，上传至集群的hdfs目录中，例如/tmp/下，文本内容如下： 编辑脚本，点击 执行 按钮： load data inpath ‘/tmp/data_input.txt’ overwrite into table workers_info 查看结果： 对接SparkSQL 将defaultDriver切换为spark，点击 Test 点击 Connect 连接成功，点击 OK 双击Fiber，点击 Connet，将driver切换为spark 可以看到数据表 点击 SQL面板，编辑SQL语句，点击 执行 按钮，在下方可以看到查询结果。 Spark增加数据 编辑数据文件data_input.txt，上传至集群的hdfs目录中，例如/tmp/下，文本内容如下： 编辑脚本，点击 执行 按钮： load data inpath ‘/tmp/data_input.txt’ overwrite into table workers_info 查看结果： 对接Phoenix 将defaultDriver切换为phoenix，点击 Test 点击 Connect 连接成功，点击 OK 双击 Fiber，点击 Connect，将driver切换为phoenix 可以看到数据phoenix表 点击 SQL面板 ，编辑SQL语句，点击 执行 按钮，在下方可以看到查询结果。 select * from tb_phoenix 点击 SQL面板，编辑SQL语句，向phoenix表中增加一条数据，点击 执行 按钮。 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') 查询结果： select * from tb_phoenix 点击 SQL面板，编辑SQL语句，删除一条数据，点击 执行 按钮。 delete from TB_PHOENIX where ID=109; 查看结果： select * from tb_phoenix 点击 SQL面板，编辑SQL语句，更新一条数据，点击 执行 按钮。 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') 查看结果 "},"Integrated_Development_Environment/Using_splunk7.2.4_with_FusionInsight_HD_C80SPC200.html":{"url":"Integrated_Development_Environment/Using_splunk7.2.4_with_FusionInsight_HD_C80SPC200.html","title":"Splunk","keywords":"","body":"Splunk对接FusionInsight HD 适用场景 Splunk7.2.4 FusionInsight HD V100R002C80SPC200 安装与启动Splunk,获取配置文件 关闭主机防火墙systemctl stop firewalld 安装Splunk7.2.4,在网址https://www.splunk.com/en_us/download/splunk-enterprise.html下载Linux平台安装包，使用WinSCP导入主机并用tar -zxvf splunk-7.2.4-8a94541dcfac-Linux-x86_64.tgz解压出splunk目录。 Splunk 对接Hadoop集群需要使用Splunk Analytics for Hadoop 组件，该组件不支持Windows版本的Splunk Enterprise，需下载Linux版本Splunk 启动和停止splunk,进入splunk目录执行 ./bin/splunk start ./bin/splunk stop 第一次启动会显示Licence Agreement页面，输入y,然后输入用户名和密码 启动成功后显示如下 在浏览器输入http://ip:8080，输入用户名密码即可进入splunk页面。 在集群服务端，获取mapred用户的keytab文件以及集群的krb5.conf文,上传至splunk主机中,例如/opt/splunk/目录下 新建提供程序 进入splunk主界面，点击右上角设置，选择虚拟索引 新建提供程序，填写相关信息 名称：自定义 提供程序序列：hadoop Java主页：集群中环境变量JAVA_HOME的值 Hadoop主页：集群中环境变量HADOOP_HOME的值 填写Hadoop集群信息 Hadoop版本：Hadoop2.x(Yarn) 文件系统：hdfs://hacluster 勾选启用通过身份验证 资源管理器地址:resourcemanager服务所在节点ip或主机名,端口为26004,在集群manager界面,选择服务管理->yarn->resourcemanager可查看resourcemanager服务所在节点ip,在服务配置中，可查看resourcemanager服务端口 资源计划程序地址:节点同resourcemanager,端口可在服务配置中查看 HDFS工作目录：自行制定 填写安全设置信息 勾选添加安全集群，安全模式选择kerberos Kerberos服务器配置选择配置文件路径,填写路径 kerberos主体名称:mapred/hadoop.hadoop.com@HADOOP.com kerberos密钥即为keytab文件 HDFS主体:hdfa/hadoop.hadoop.com@HADOOP.com MapreReduce主体为:mapred/hadoop.hadoop.com@HADOOP.com 资源管理器主体与节点管理器主体可不填 其他保持默认，点击“保存”。 新建虚拟索引 在新建索引界面，自定义索引名称，提供程序选择刚才新建的提供程序，HDFS 中数据的路径根据需要搜索的路径进行填写，勾选递归处理目录，点击保存。 使用搜索程序 在splunk主页面，点击浏览数据 选择已创建的提供程序和虚拟索引 点击下一步，选择要搜索的文件 在数据预览中，选择数据来源类型，根据数据类型进行选择 在上下文配置中选择应用程序的上下文，点击下一步 点击完成 点击搜索可以进入对此文件的搜索页面 可以根据查询需要进行一些可视化展示 读取Hive表 通过Splunk读取Hive表，需要在提供程序中添加以下两个配置： vix.splunk.search.splitter = HiveSplitGenerator vix.splunk.search.splitter.hive.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe 在虚拟索引中配置要搜索的表的信息，包括数据库名称，表名，表头，字段类型，文件类型，分隔符，换行符 目前仅能正确读取rcfile格式的文件 然后在虚拟索引处点击搜索，进入搜索页面，并在搜索框前选择所有时间，即可看到表中数据 "},"SQL_Analytics_Engine/":{"url":"SQL_Analytics_Engine/","title":"SQL Analysis","keywords":"","body":"SQL Analysis Apache Kylin Apache Kylin 1.6.0 FusionInsight HD V100R002C60U20 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC200 Apache Kylin 2.3.1 FusionInsight HD V100R002C80SPC200 Kyligence Presto Presto0.155 FusionInsight HD V100R002C60U20 Presto0.184 FusionInsight HD V100R002C70SPC100 Presto0.210 FusionInsight HD V100R002C80SPC200 "},"SQL_Analytics_Engine/Using_Kylin_with_FusionInsight.html":{"url":"SQL_Analytics_Engine/Using_Kylin_with_FusionInsight.html","title":"Apache Kylin","keywords":"","body":"Apache Kylin对接FusionInsight HD Apache Kylin 1.6.0 FusionInsight HD V100R002C60U20 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC200 Apache Kylin 2.3.1 FusionInsight HD V100R002C80SPC200 "},"SQL_Analytics_Engine/Using_Kylin1.6.0_with_FusionInsight_HD_C60U20.html":{"url":"SQL_Analytics_Engine/Using_Kylin1.6.0_with_FusionInsight_HD_C60U20.html","title":"Kylin 1.6.0 <-> C60U20","keywords":"","body":"Apache Kylin对接FusionInsight 适用场景 Apache Kylin 1.6.0 (基于HBase1.0.2的分支版本：yang21-hbase102) FusionInsight HD V100R002C60U20 说明 Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 Apache Kylin主要与FusionInsight的Hive和HBase进行对接 环境准备 修改/etc/hosts 添加本机主机名解析 162.1.115.89 kylin 配置NTP服务 使用vi /etc/ntp.conf增加NTP服务的配置 server 162.2.200.200 nomodify notrap nopeer noquery 启动NTP服务 service ntpd start chkconfig ntpd on 安装Hadoop client 在FusionInsight Manager服务管理页面下载客户端，上传到kylin安装主机，解压 cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient 安装JDK rpm -Uvh jdk-8u112-linux-x64.rpm 编译Kylin 可直接下载的二进制文件的Kylin-1.6.0主版本是基于HBase1.1.1编译的，而FusionInsight使用的HBase版本是1.0.2，这两个版本部分类和方法不兼容，需要配套1.0.2的HBase重新编译Kylin。 下载Kylin-1.6.0基于HBase1.0.2版本的源码https://github.com/apache/kylin/tree/yang21-hbase102得到kylin-yang21-hbase102.zip 安装编译工具 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v 安装git yum install -y git 安装nodejs： wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin 导入环境变量 source /etc/profile 执行npm -v 编译打包 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh 等待编译完成，得到Kylin二进制安装包 启动Kylin 解压二进制包 解压上一步骤生成的安装包tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt 配置环境变量 配置环境变量：vi /etc/profile，增加以下配置 export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin 导入环境变量 source /etc/profile Kylin启动还需要配置HIVE_CONF、HCAT_HOME，使用vi /opt/hadoopclient/Hive/component_env，在文件最后增加 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog 导入环境变量 source /opt/hadoopclient/bigdata_env 进行kerberos认证 kinit test_cn Kylin检查环境设置： cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh 修改Kylin配置 修改kylin.properties： vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties 配置Hive client使用beeline： kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' 配置获取任务状态时使用kerberos 鉴权： kylin.job.status.with.kerberos=true 去掉不允许修改的配置 FusionInsight不允许修改dfs.replication, mapreduce.job.split.metainfo.maxsize的参数，需要注释掉Kylin所有配置文件中的相关参数，否则构建Cube时会报如下错误： 修改以下文件： kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase配置 将/opt/hadoopclient/Hive/config/hivemetastore-site.xml中的配置合并到hive-site.xml 将/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml中的配置合并到/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib路径 kylin的find-hive-dependency.sh默认Hive lib路径为大数据集群中Hive的安装路径，若Kylin安装在集群节点上不会有问题，否则需要修改为客户端路径。 编辑find-hive-dependency.sh：vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'` 启动Kylin 使用./kylin.sh start启动Kylin 输入默认用户名密码：ADMIN/KYLIN登陆 Demo测试 导入Demo数据 执行以下命令导入sample数据 cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh 选择菜单 System -> Actions -> Reload Metadata 构建Cube 选择learn_kylin工程，构建默认的kylin_sales_cube Cube构建成功，状态变为READY 查询表数据 在Insight页面执行查询 "},"SQL_Analytics_Engine/Using_Kylin2.1.0_with_FusionInsight_HD_C70.html":{"url":"SQL_Analytics_Engine/Using_Kylin2.1.0_with_FusionInsight_HD_C70.html","title":"Kylin 2.1.0 <-> C70","keywords":"","body":"Apache Kylin2.1.0对接FusionInsight_HD_C70 适用场景 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC100 Apache Kylin 2.1.0 FusionInsight HD V100R002C70SPC200 说明 Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 Apache Kylin主要与FusionInsight的Hive和HBase进行对接 环境准备 修改/etc/hosts 添加本机主机名解析 172.16.52.86 kylin 配置NTP服务 使用vi /etc/ntp.conf增加NTP服务的配置 server 172.18.0.18 nomodify notrap nopeer noquery 启动NTP服务 service ntpd start chkconfig ntpd on 安装Hadoop client 在FusionInsight Manager服务管理页面下载客户端，上传到kylin安装主机，解压 ./install.sh /opt/hadoopclient 安装JDK rpm -Uvh jdk-8u112-linux-x64.rpm 编译Kylin 可直接下载的二进制文件的Kylin-2.1.0主版本是基于HBase1.1.1编译的，而FusionInsight使用的HBase版本是1.0.2，这两个版本部分类和方法不兼容，需要重新编译Kylin。 下载Kylin-2.1.0基于HBase1.1.1版本的源码码https://github.com/apache/kylin/tree/2.1.x得到kylin-2.1.x.zip 安装编译工具 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v 安装git yum install -y git 安装nodejs： wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin 导入环境变量 source /etc/profile 执行npm -v 修改kylin源码 修改HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java 修改pom.xml vi /opt/kylin-2.1.x/pom.xml 将HBase、Hive、Hadoop版本改成与FusionInsight HD对应的版本 编译打包 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh 等待编译完成，得到Kylin二进制安装包 启动Kylin 解压二进制包 解压上一步骤生成的安装包tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt 配置环境变量 配置环境变量：vi /etc/profile，增加以下配置 export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin 导入环境变量 source /etc/profile Kylin启动还需要配置HIVE_CONF、HCAT_HOME，使用vi /opt/hadoopclient/Hive/component_env，在文件最后增加 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog 导入环境变量 source /opt/hadoopclient/bigdata_env 进行kerberos认证 kinit test Kylin检查环境设置： cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh 修改Kylin配置 修改kylin.properties： vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties 配置Hive client使用beeline： kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' 配置获取任务状态时使用kerberos 鉴权： kylin.job.status.with.kerberos=true 去掉不允许修改的配置 FusionInsight不允许修改dfs.replication, mapreduce.job.split.metainfo.maxsize的参数，需要注释掉Kylin所有配置文件中的相关参数，否则构建Cube时会报如下错误： 需要修改以下文件： kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase配置 将/opt/hadoopclient/Hive/config/hivemetastore-site.xml中的配置合并到hive-site.xml 将/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml中的配置合并到/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib路径 kylin的/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh默认Hive lib路径为大数据集群中Hive的安装路径，若Kylin安装在集群节点上不会有问题，否则需要修改为客户端路径。 启动Kylin 使用./kylin.sh start启动Kylin 输入默认用户名密码：ADMIN/KYLIN登陆 Demo测试 导入Demo数据 执行以下命令导入sample数据 cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh 选择菜单 System -> Actions -> Reload Metadata 选择菜单 System -> Model 构建Cube 构建默认的kylin_sales_cube 选择End Data（Exclude）时间： 点击Monitor可以查看build状态： Build完成： Cube构建成功，状态变为READY 查询表数据 在Insight页面执行查询 "},"SQL_Analytics_Engine/Using_Kylin2.3.1_with_FusionInsight_HD_C80.html":{"url":"SQL_Analytics_Engine/Using_Kylin2.3.1_with_FusionInsight_HD_C80.html","title":"Kylin 2.3.1 <-> C80","keywords":"","body":"Apache Kylin2.3.1对接FusionInsight_HD_C80 适用场景 Apache Kylin 2.3.1 FusionInsight HD V100R002C80SPC100 说明 Apache Kylin™是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 Apache Kylin主要与FusionInsight的Hive和HBase进行对接 环境准备 修改/etc/hosts 添加本机主机名解析 172.16.52.86 kylin 配置NTP服务 使用vi /etc/ntp.conf增加NTP服务的配置,时间与FusionInsight集群同步 server 172.18.0.18 nomodify notrap nopeer noquery 启动NTP服务 service ntpd start chkconfig ntpd on 参考FusionInsight产品文档在Kylin节点安装FusionInsight客户端 在FusionInsight Manager服务管理页面下载客户端，上传到kylin节点安装FusionInsight客户端到/opt/hadoopclient目录 ./install.sh /opt/hadoopclient 安装JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm 下载Kylin Fusioninsight配套的HBase是1.3.0，Apache Kylin可直接下载apache-kylin-2.3.1-hbase1x-bin.tar.gz主版本二进制包，无需编译Apache kylin 下载解压Kylin 下载Kylin-2.3.1基于HBase1.x版本的二进制包， http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz 上传apache-kylin-2.3.1-hbase1x-bin.tar.gz到Apache kylin节点的/opt目录 解压上一步骤的安装包 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt 配置Kylin 配置环境变量 配置环境变量：vi /etc/profile，增加以下配置 export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin 导入环境变量 source /etc/profile Kylin启动还需要配置HIVE_CONF、HCAT_HOME，使用vi /opt/hadoopclient/Hive/component_env，在文件最后增加 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog 导入环境变量 source /opt/hadoopclient/bigdata_env 进行kerberos认证 kinit test Kylin检查环境设置： cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh 修改FusionInsight的Hive配置项 在hive.security.authorization.sqlstd.confwhitelist.append参数最后追加一下参数配置，保存配置，重启影响的服务 |mapreduce\\.job\\..*|dfs\\..* 修改Kylin配置 获取Hive的JDBC字符串 执行Beeline查看Hive的JDBC字符串 source bigdata_env kinit test beeline 修改kylin.properties： vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties 配置Hive client使用beeline： kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC字符串使用上一步骤获取的字符串 注意：kylin.source.hive.beeline-params参数里面原有的 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' 要去掉 修改Hive/HBase配置 将/opt/hadoopclient/Hive/config/hivemetastore-site.xml中的配置合并到hive-site.xml 将/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml中的配置合并到/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib路径 kylin的/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh默认Hive lib路径为大数据集群中Hive的安装路径，需要修改为客户端路径 启动Kylin 使用./kylin.sh start启动Kylin 输入默认用户名密码：ADMIN/KYLIN登陆 Demo测试 导入Demo数据 执行以下命令导入sample数据 cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh 选择菜单 System -> Actions -> Reload Metadata 选择菜单 System -> Model 构建Cube 构建默认的kylin_sales_cube 选择End Data（Exclude）时间： 点击Monitor可以查看build状态： Build完成： Cube构建成功，状态变为READY 查询表数据 在Insight页面执行查询 "},"SQL_Analytics_Engine/Using_Kyligence_with_FusionInsight.html":{"url":"SQL_Analytics_Engine/Using_Kyligence_with_FusionInsight.html","title":"Kyligence","keywords":"","body":"Kyligence Analytics Platform对接FusionInsight 适用场景 Kyligence Analytics Platform v2.2 FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.3 FusionInsight HD V100R002C60U20 Kyligence Analytics Platform v2.4 FusionInsight HD V100R002C70SPC200 Kyligence Analytics Platform v2.5 FusionInsight HD V100R002C70SPC200 说明 参考 官方产品文档 的 快速安装 章节 "},"SQL_Analytics_Engine/Using_Presto_with_FusionInsight.html":{"url":"SQL_Analytics_Engine/Using_Presto_with_FusionInsight.html","title":"Presto","keywords":"","body":"Presto对接FusionInsight HD Presto0.155 FusionInsight HD V100R002C60U20 Presto0.184 FusionInsight HD V100R002C70SPC100 "},"SQL_Analytics_Engine/Using_Presto0.155_with_FusionInsight_HD_C60U20.html":{"url":"SQL_Analytics_Engine/Using_Presto0.155_with_FusionInsight_HD_C60U20.html","title":"Presto0.146 <-> C60U20","keywords":"","body":"Apache Presto对接FusionInsight 适用场景 Presto0.155 FusionInsight HD V100R002C60U20 说明 Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。 Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题 Presto主要与FusionInsight的Hive和HDFS进行对接 配置Hive Connector Presto集群包括coordinator节点和不限数量的worker节点(coordinator节点也可同时为worker节点)，其中只需要在coordinator节点上配置Hive Connector即可。 本文档中配置coordinator节点同时也是worker节点。 从该链接下载presto-server的安装包，并上传到presto coordinator的节点 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz 将该压缩包解压缩后得到目录/opt/presto-server-0.155。 在presto节点上安装华为FusionInsight HD的客户端，默认安装目录/opt/hadoopclient presto该0.155版本要求jdk至少在1.8u60+以上，修改/etc/profile文件方式配置系统默认的java为FusionInsight HD客户端的jdk，并source环境变量，命令参考如下 在/etc/profile中增加以下行 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source 环境变量 source /etc/profile 创建Java keystore File for TLS，(后续步骤默认都在presto节点上执行)参考如下命令 source /opt/hadoopclient/bigdata_env keytool –genkeypair –alias testuser –keyalg RSA –keystore /opt/presto.jks alias后的值必须要跟后面创建的用户名称一致 first and last name必须写成presto节点的主机名 通过FusionInsight HD的管理页面创建一个“机机”用户，具体请参见《FusionInsight HD管理员指南》的 创建用户 章节。例如，创建用户testuser，并选择hadoop和hive用户组，下载对应的秘钥文件user.keytab以及krb5.conf文件，并上传到presto节点的/opt/hadoopclient目录下，将user.keytab改名为testuser.keytab。 参考如下命令在Huawei FusionInsight HD的Kerberos中创建一个新的principal，其名称为“testuser/presto-server”，其中presto-server为presto的coordinator节点的主机名，导出该principal的秘钥文件为/opt/presto.keytab。 执行kadmin –p kadmin/admin命令时初始密码Admin@123，修改后需严格牢记新密码。 创建目录/opt/presto-server-0.155/etc，在该目录下创建如下文件 config.properties参考如下 coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config参考如下内容 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties参考如下内容 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties参考如下内容 com.facebook.presto=INFO 创建目录/opt/presto-server-0.155/etc/catalog，在该目录下创建hive.properties文件 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml 其中hive.metastore.uri的值从/opt/hadoopclient/Hive/config/hive-site.xml中查找 修改/etc/hosts文件，将本机的IP与主机名解析以及Huawei FusionInsight HD集群节点的IP与主机名解析添加进去，例如 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v可以正确输出mvn版本 安装git yum install -y git 参考如下命令，下载presto-server-0.155的源码 git clone https://github.com/prestodb/presto.git git checkout 0.155 修改presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java的代码，将代码中\"Sasl.QOP=auth\"修改为\"Sasl.QOP=auth-conf\" 重新编译presto cd presto-hive mvn clean install -DskipTests 将编译后target目录下的presto-hive-0.155.jar文件替换/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar文件 启动presto server，跟踪/var/presto/data/var/log/server.log查看启动日志 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log 检查FusionInsight Manager中HDFS服务配置中hadoop.rpc.protection的配置，必须设置为authentacation。 通过Presto CLI连接Hive 使用Presto CLI连接Huawei FusionInsight HD的Hive，使用presto自带的命令行工具执行SQL语句。 通过如下链接下载presto cli启动的jar包 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar 并将该jar包上传到可与presto节点网络互通的节点上(也可将presto coordinator节点作为cli使用节点)。 配置cli节点的jdk为1.8u60+以上版本 配置cli节点的/etc/hosts文件，将FI集群和presto coordinator节点的IP与主机名关系配置到cli节点 从presto节点拷贝presto.jks、presto.keytab、krb5.conf以及连接HDFS所需的core-site.xml和hdfs-site.xml文件到cli节点 将presto-cli-0.155-executable.jar包改为可执行文件 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h 创建presto cli启动脚本，类似如下，注意将相关文件的路径按实际位置替换 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog后面的hive是和presto coordinator节点配置的hive.properties的文件名匹配的，如果hive.properties改名为hivetest.properties，则这里改为hivetest 通过cli执行SQL语句，其他SQL语法请参考https://prestodb.io/docs/0.155/sql.html 查询表workers_info中数据： 百万记录数表web_sales查询： 通过Presto JDBC连接Hive 使用Presto JDBC接口连接Huawei FusionInsight HD Hive 从如下链接下载jdbc的驱动包 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar 参考https://prestodb.io/docs/0.155/installation/jdbc.html设置JDBC URL，用户名为任意字符，密码为空，在eclipse中调通的示例如下: import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; public class PrestoTest { public static void main(String[] args) throws SQLException, ClassNotFoundException { Class.forName(\"com.facebook.presto.jdbc.PrestoDriver\"); Connection connection =DriverManager.getConnection (\"jdbc:presto://162.1.115.71:8080/hive/default\",\"root\",null); Statement stmt =connection.createStatement(); ResultSet rs = stmt.executeQuery(\"select * from workers_info limit 10\"); int col = rs.getMetaData().getColumnCount(); while(rs.next()) { for (int i = 1; i 测试结果： 百万记录数表web_sales查询： "},"SQL_Analytics_Engine/Using_Presto0.184_with_FusionInsight_HD_C70SPC100.html":{"url":"SQL_Analytics_Engine/Using_Presto0.184_with_FusionInsight_HD_C70SPC100.html","title":"Presto0.184 <-> C70SPC100","keywords":"","body":"Apache Presto对接FusionInsight 适用场景 Presto0.184 FusionInsight HD V100R002C70SPC100 Presto0.196 FusionInsight HD V100R002C80SPC100 说明 Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。 Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题 Presto主要与FusionInsight的Hive和HDFS进行对接 配置Hive Connector Presto集群包括coordinator节点和不限数量的worker节点(coordinator节点也可同时为worker节点)，其中只需要在coordinator节点上配置Hive Connector即可。 本文档中配置coordinator节点同时也是worker节点。 从该链接下载presto-server的安装包，并上传到presto coordinator的节点 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz 将该压缩包解压缩后得到目录/opt/presto-server-0.184。 在presto节点上安装华为FusionInsight HD V100R002C70SPC100的客户端，默认安装目录/opt/hadoopclient presto该0.184版本要求jdk至少在1.8u60+以上，修改/etc/profile文件方式配置系统默认的java为FusionInsight HD客户端的jdk，并source环境变量，命令参考如下 在/etc/profile中增加以下行 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source 环境变量 source /etc/profile 创建Java keystore File for TLS，(后续步骤默认都在presto节点上执行)参考如下命令 source /opt/hadoopclient/bigdata_env keytool –genkeypair –alias testuser –keyalg RSA –keystore /opt/presto.jks alias后的值必须要跟后面创建的用户名称一致 first and last name必须写成presto节点的主机名 通过FusionInsight HD的管理页面创建一个“机机”用户，具体请参见《FusionInsight HD管理员指南》的 创建用户 章节。例如，创建用户testuser，并选择hadoop和hive用户组，下载对应的秘钥文件user.keytab以及krb5.conf文件，并上传到presto节点的/opt/hadoopclient目录下，将user.keytab改名为testuser.keytab。 参考如下命令在Huawei FusionInsight HD的Kerberos中创建一个新的principal，其名称为“testuser/presto-server”，其中presto-server为presto的coordinator节点的主机名，导出该principal的秘钥文件为/opt/presto.keytab。 执行kadmin –p kadmin/admin命令时初始密码Admin@123，修改后需严格牢记新密码。 创建目录/opt/presto-server-0.184/etc，在该目录下创建如下文件 config.properties参考如下 coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config参考如下内容 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties参考如下内容 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties参考如下内容 com.facebook.presto=INFO 创建目录/opt/presto-server-0.184/etc/catalog，在该目录下创建hive.properties文件 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml 其中hive.metastore.uri的值从/opt/hadoopclient/Hive/config/hive-site.xml中查找 将FusionInsight HD客户端中的core-site.xml和hdfs-site.xml复制到/opt/presto-server-0.184/etc/catalog中 cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ 按照下图修改hdfs-site.xml文件中的dfs.client.failover.proxy.provider.hacluster属性为org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml 修改/etc/hosts文件，将本机的IP与主机名解析以及Huawei FusionInsight HD集群节点的IP与主机名解析添加进去，例如 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v可以正确输出mvn版本 安装git yum install -y git 参考如下命令，下载presto-server-0.184的源码 git clone https://github.com/prestodb/presto.git git checkout 0.184 修改presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java的代码，将代码中\"Sasl.QOP=auth\"修改为\"Sasl.QOP=auth-conf\" 重新编译presto cd presto-hive mvn clean install -DskipTests 将编译后target目录下的presto-hive-0.184.jar文件替换/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar文件 启动presto server，跟踪/var/presto/data/var/log/server.log查看启动日志 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log 通过Presto CLI连接Hive 使用Presto CLI连接Huawei FusionInsight HD的Hive，使用presto自带的命令行工具执行SQL语句。 通过如下链接下载presto cli启动的jar包 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar 并将该jar包上传到可与presto节点网络互通的节点上(也可将presto coordinator节点作为cli使用节点)。 配置cli节点的jdk为1.8u60+以上版本 配置cli节点的/etc/hosts文件，将FI集群和presto coordinator节点的IP与主机名关系配置到cli节点 从presto节点拷贝presto.jks、presto.keytab、krb5.conf以及连接HDFS所需的core-site.xml和hdfs-site.xml文件到cli节点 将presto-cli-0.184-executable.jar包改为可执行文件 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h 创建presto cli启动脚本，类似如下，注意将相关文件的路径按实际位置替换 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog后面的hive是和presto coordinator节点配置的hive.properties的文件名匹配的，如果hive.properties改名为hivetest.properties，则这里改为hivetest 通过cli执行SQL语句，其他SQL语法请参考https://prestodb.io/docs/0.184/sql.html 查询表workers_info中数据： 百万记录数表web_sales查询： 通过Presto JDBC连接Hive 使用Presto JDBC接口连接Huawei FusionInsight HD Hive 从如下链接下载jdbc的驱动包 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar 参考https://prestodb.io/docs/0.184/installation/jdbc.html设置JDBC URL，用户名为任意字符，密码为空，在eclipse中调通的示例如下: import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; public class PrestoTest { public static void main(String[] args) throws SQLException, ClassNotFoundException { Class.forName(\"com.facebook.presto.jdbc.PrestoDriver\"); Connection connection =DriverManager.getConnection (\"jdbc:presto://162.1.115.71:8080/hive/default\",\"root\",null); Statement stmt =connection.createStatement(); ResultSet rs = stmt.executeQuery(\"select * from workers_info limit 10\"); int col = rs.getMetaData().getColumnCount(); while(rs.next()) { for (int i = 1; i 测试结果： 百万记录数表web_sales查询： "},"SQL_Analytics_Engine/Using_Presto0.210_with_FusionInsight_HD_C80SPC200.html":{"url":"SQL_Analytics_Engine/Using_Presto0.210_with_FusionInsight_HD_C80SPC200.html","title":"Presto0.210 <-> C80SPC200","keywords":"","body":"Apache Presto对接FusionInsight 适用场景 Presto0.210 FusionInsight HD V100R002C80SPC200 说明 Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节。 Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题 Presto主要与FusionInsight的Hive进行对接,在Presto0.210版本中支持与FusionInsight的ES进行对接。 获取并配置presto server Presto集群包括coordinator节点和不限数量的worker节点(coordinator节点也可同时为worker节点)，其中只需要在coordinator节点上配置Hive Connector即可。 本文档中配置coordinator节点同时也是worker节点。 从该链接下载presto-server的安装包，并上传到presto coordinator的节点 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.210/presto-server-0.210.tar.gz 将该压缩包解压缩后得到目录/opt/presto-server-0.210。 在presto节点上安装华为FusionInsight HD V100R002C80SPC200的客户端，默认安装目录/opt/hadoopclient presto-0.210版本要求jdk至少在1.8u60+以上，修改/etc/profile文件方式配置系统默认的java为FusionInsight HD客户端的jdk，并source环境变量，命令参考如下 在/etc/profile中增加以下行 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source 环境变量 source /etc/profile 创建Java keystore File for TLS，(后续步骤默认都在presto节点上执行)参考如下命令 source /opt/hadoopclient/bigdata_env keytool -genkeypair -alias testuser -keyalg RSA –keystore /opt/presto.jks alias后的值必须要跟后面创建的用户名称一致 first and last name必须写成presto节点的主机名,并且要忽略大小写，统一使用小写字母 通过FusionInsight HD的管理页面创建一个“机机”用户，具体请参见《FusionInsight HD管理员指南》的 创建用户 章节。例如，创建用户testuser，并根据业务需求选择用户组(hadoop和hive组)，下载对应的秘钥文件user.keytab以及krb5.conf文件，并上传到presto节点的/opt/hadoopclient目录下，将user.keytab改名为testuser.keytab。 参考如下命令在Huawei FusionInsight HD的Kerberos中创建一个新的principal，其名称为“testuser/presto-server”，其中presto-server为presto的coordinator节点的主机名，导出该principal的秘钥文件为/opt/presto.keytab。 执行kadmin –p kadmin/admin命令时初始密码Admin@123，修改后需严格牢记新密码。 创建目录/opt/presto-server-0.210/etc，在该目录下创建如下文件 config.properties参考如下 coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config参考如下内容 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties参考如下内容 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties参考如下内容 com.facebook.presto=INFO 创建目录/opt/presto-server-0.210/etc/catalog,将FusionInsight HD客户端中的core-site.xml和hdfs-site.xml复制到/opt/presto-server-0.210/etc/catalog中 cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.210/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.210/etc/catalog/ 将hdfs-site.xml文件中的dfs.client.failover.proxy.provider.hacluster属性修改为org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.210/etc/catalog/hdfs-site.xml 修改/etc/hosts文件，将本机的IP与主机名解析以及Huawei FusionInsight HD集群节点的IP与主机名解析添加进去，例如 安装maven： wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ 修改profile文件vi /etc/profile,增加以下配置 export PATH=$PATH:/opt/apache-maven-3.3.9/bin 导入环境变量 source /etc/profile 执行mvn -v可以正确输出mvn版本 安装git yum install -y git 参考如下命令，下载presto-server-0.210的源码 git clone https://github.com/prestodb/presto.git git checkout 0.210 获取Presto CLI启动包 使用Presto CLI连接Huawei FusionInsight HD的Hive，使用presto自带的命令行工具执行SQL语句。 通过如下链接下载presto cli启动的jar包 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.210/presto-cli-0.210-executable.jar 并将该jar包上传到可与presto节点网络互通的节点上(也可将presto coordinator节点作为cli使用节点)。 配置cli节点的jdk为1.8u60+以上版本 配置cli节点的/etc/hosts文件，将FI集群和presto coordinator节点的IP与主机名关系配置到cli节点 从presto节点拷贝presto.jks、presto.keytab、krb5.conf以及连接HDFS所需的core-site.xml和hdfs-site.xml文件到cli节点 将presto-cli-0.210-executable.jar包改为可执行文件 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h 配置Hive Connector 进入目录/opt/presto-server-0.210/etc/catalog,在该目录下创建hive.properties文件 connector.name=hive-hadoop2 hive.metastore.uri=thrift://172.21.3.115:21088,thrift://172.21.3.116:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.210/etc/catalog/core-site.xml,/opt/presto-server-0.210/etc/catalog/hdfs-site.xml 其中hive.metastore.uri的值从/opt/hadoopclient/Hive/config/hive-site.xml中查找 修改presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java的代码，将代码中\"Sasl.QOP\"的值修改为固定的\"auth-conf\" 重新编译presto cd presto-hive mvn clean install -DskipTests 将编译后target目录下的presto-hive-0.210.jar文件替换/opt/presto-server-0.210/plugin/hive-hadoop2/presto-hive-0.210.jar文件 启动presto server，跟踪/var/presto/data/var/log/server.log查看启动日志 sh /opt/presto-server-0.210/bin/launcher start tailf /var/presto/data/var/log/server.log 通过Presto CLI连接Hive 进入Presto CLI启动包所在目录，例如/opt； 创建presto cli启动脚本，类似如下，注意将相关文件的路径按实际位置替换 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; catalog后面的hive是和presto coordinator节点配置的hive.properties的文件名匹配的，如果hive.properties改名为hivetest.properties，则这里改为hivetest 通过cli执行SQL语句，其他SQL语法请参考https://prestodb.io/docs/0.210/sql.html 查询表workers_info中数据： 通过Presto JDBC连接Hive 使用Presto JDBC接口连接Huawei FusionInsight HD Hive 从如下链接下载jdbc的驱动包 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.210/presto-jdbc-0.210.jar 参考https://prestodb.io/docs/0.210/installation/jdbc.html设置JDBC URL，用户名为任意字符，密码为空，在eclipse中调通的示例如下: import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; public class PrestoTest { public static void main(String[] args) throws SQLException, ClassNotFoundException { Class.forName(\"com.facebook.presto.jdbc.PrestoDriver\"); Connection connection =DriverManager.getConnection (\"jdbc:presto://172.21.3.48:8080/hive/default\",\"root\",null); Statement stmt =connection.createStatement(); ResultSet rs = stmt.executeQuery(\"select * from adult limit 10\"); int col = rs.getMetaData().getColumnCount(); while(rs.next()) { for (int i = 1; i 测试结果： 配置ElasticSearch Connector presto和ES官方都没有给出适配的文档介绍，这里我们采用开源的适配包进行适配。在https://github.com/harbby/presto-connectors 下载适配包源码，上传至服务器，解压。 修改presto-elasticsearch-connectors源码以及配置 进入/opt/presto-connectors-master/presto-elasticsearch6/src/main/java/com/facebook/presto/elasticsearch6/functions目录，参考下图，修改MatchQueryFunction.java文件，添加构造函数，将函数声明为public 进入presto-connectors-master目录，修改pom.xml文件 cd presto-connectors-master vi pom.xml 在最后的''之前，添加以下plugin依赖 pl.project13.maven git-commit-id-plugin true （可选操作）在modules中，去掉除presto-base-elasticsearch和presto-elasticsearch6以外的module，其他的module这里并不需要，可以减少编译时间 进入presto-elasticsearch6目录，修改pom.xml文件 cd presto-elasticsearch6 vi pom.xml 将'elasticsearch.version'修改为6.1.3 将'elasticsearch-x-content'和'elasticsearch-core'的依赖注释掉 回到presto-connectors-master目录，编译presto-connectors mvn clean install -DskipTests 编译成功后，显示如下： 获取'presto-connectors-master/presto-elasticsearch6/target'目录下的'presto-elasticsearch6-0.210'文件夹，将其复制到presto-server的plugin目录下 cp -r /opt/presto-connectors-master/presto-elasticsearch6/target/presto-elasticsearch6-0.210 /opt/presto-server-0.210/plugin 登录集群manager管理页面，在'服务管理->Elasticsearch 服务配置'页面，选择全部配置，搜索'port'关键词,查看'SERVER_PORT'配置为24100，'TRANSPORT_TCP_PORT'配置为24101 在集群客户端节点执行如下命令 source /opt/hadoopclient/bigdata_evn curl -XGET http://172.21.3.115:24100/_cluster/health?pretty 其中172.21.3.115是elasticsearch集群节点，24100为SERVER_PORT，看到返回如下结果 { \"cluster_name\" : \"elasticsearch_cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 33, \"active_shards\" : 66, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } 在/opt/presto-server-0.210/etc/catalog目录下创建es.properties文件 connector.name=elasticsearch6 elasticsearch.cluster.name=elasticsearch_cluster elasticsearch.transport.hosts=172.21.3.115:24101 其中'elasticsearch.cluster.name'是刚才获取的ES集群的名字，'elasticsearch.transport.hosts'为EsNode节点IP，端口为'TRANSPORT_TCP_PORT' 重启presto-server sh /opt/presto-server-0.210/bin/launcher restart 通过Presto CLI连接ElasticSearch 进入Presto CLI启动包所在目录,例如/opt 创建presto cli启动脚本，类似如下，注意将相关文件的路径按实际位置替换 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog es \\ --schema default \\ --; catalog后面的es是和presto coordinator节点配置的es.properties的文件名匹配 通过cli执行SQL语句查询ES中的索引信息 当前connector支持show,create,select,insert,drop操作，暂不支持delete,update,alter等操作 "},"Database/":{"url":"Database/","title":"Database","keywords":"","body":" Database SAP HANA (TBD) "},"Other/":{"url":"Other/","title":"Other","keywords":"","body":" Other FUSE Gis-Tools-For-Hadoop Apache Livy IBM WAS (TBD) "},"Other/Using_FUSE_with_FusionInsight.html":{"url":"Other/Using_FUSE_with_FusionInsight.html","title":"FUSE","keywords":"","body":"FUSE对接FusionInsight HDFS 适用场景 fuse 2.8.3 FusionInsight HD V100R002C60U20（非安全模式） 说明 通过使用FUSE组件，可以使用将远端的HDFS文件系统mount到本端的Linux系统中使用 配置对接 安装jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz 配置环境变量/etc/profile，加入如下内容，source环境变量 export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH source /etc/profile 安装rpm包 yum install fuse fuse-devel fuse-libs 安装FusionInsight客户端，具体请参见产品文档的 安装客户端 章节，例如客户端安装目录为/opt/hadoopclient/ 下载Hadoop-2.7.2源码hadoop-2.7.2-src.tar.gz，编译fuse_dfs，将编译好的fuse_dfs拷贝到/opt目录下 将源码下的fuse_dfs_wrapper.sh脚本拷贝至/opt目录下，并根据实际情况作如下修改(修改HADOOP_PREFIX和JAVA_HOME的配置)： export HADOOP_PREFIX=/opt/hadoopclient/HDFS/hadoop if [ \"$OS_ARCH\" = \"\" ]; then export OS_ARCH=amd64 fi if [ \"$JAVA_HOME\" = \"\" ]; then export JAVA_HOME=/opt/jdk1.8.0_112 fi if [ \"$LD_LIBRARY_PATH\" = \"\" ]; then export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:$HADOOP_PREFIX/lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX/share ]; then export HADOOP_PREFIX=$HADOOP_PREFIX for f in ${HADOOP_PREFIX}/share/hadoop/hdfs/*.jar ; do export CLASSPATH=$CLASSPATH:$f done for f in $HADOOP_PREFIX/share/hadoop/hdfs/lib/*.jar; do export CLASSPATH=$CLASSPATH:$f done for f in ${HADOOP_PREFIX}/share/hadoop/common/lib/*.jar; do export CLASSPATH=$CLASSPATH:$f done for f in ${HADOOP_PREFIX}/share/hadoop/common/*.jar; do export CLASSPATH=$CLASSPATH:$f done export PATH=/opt:$PATH export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:$HADOOP_PREFIX/lib/native:/usr/local/lib:$LD_LIBRARY_PATH fi fuse_dfs $@ 更改文件权限 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source环境变量 source /opt/hadoopclient/bigdata_env 本地创建mount目录 mkdir –p /mnt/hdfs 执行挂载脚本，其中162-1-95-196是HDFS的NameNode(hacluster,主)的主机名 ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ 查看/mnt/hdfs目录 如果需要卸载挂载点，执行umount /mnt/hdfs即可 验证对接 hdfsclient写 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient读 hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse写 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse读 dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null "},"Other/Using_GIS_Tools_for_Hadoop_with_FusionInsight.html":{"url":"Other/Using_GIS_Tools_for_Hadoop_with_FusionInsight.html","title":"Gis-Tools-For-Hadoop","keywords":"","body":"GIS Tools for Hadoop对接FusionInsight 适用场景 GIS Tools for Hadoop FusionInsight HD V100R002C60U20 aggregation-hive 参考GIS说明https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive中关于集成Hive的示例，在华为FusionInsight HD中执行该示例。 获取gis源代码https://github.com/Esri/gis-tools-for-hadoop/ 完成FusionInsight HD V100R002C60U20的安装，包含Hive组件。 在FusionInsight Manager创建一个HiveAdmin角色，具体请参加《FusionInsight HD 管理员指南》的 创建Hive角色 章节。 在FusionInsight Manager创建一个“机机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。将用户加入上面创建的角色HiveAdmin。例如，创建用户 testuser 并下载对应的keytab文件user.keytab以及krb5.conf文件 安装FusionInsight HD的客户端，具体请参见《FusionInsight HD 管理员指南》的安装和使用客户端章节。 将下载的gis tools源码通过WinSCP工具上传到安装有FusionInsight HD客户端所在节点的/opt目录下，上传源码目录为gis-tools-for-hadoop-master 将下载的gis tools源码通过FusionInsight HD的客户端上传到HDFS文件系统中，将目录gis-tools-for-hadoop-master直接放到HDFS的根目录下，命令参考 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master 修改执行hive示例的sql文件，修改后的文件如下 set role admin; add jar hdfs:///gis-tools-for-hadoop-master/samples/lib/esri-geometry-api.jar; add jar hdfs:///gis-tools-for-hadoop-master/samples/lib/spatial-sdk-hadoop.jar; reload function; DROP TABLE earthquakes; DROP TABLE counties; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point'; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains'; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING, latitude DOUBLE, longitude DOUBLE, depth DOUBLE, magnitude DOUBLE, magtype string, mbstations string, gap string, distance string, rms string, source string, eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data'; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string, Perimeter string, State string, County string, Name string, BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data'; SELECT counties.name, count(*) cnt FROM counties JOIN earthquakes WHERE ST_Contains(counties.boundaryshape, ST_Point(earthquakes.longitude, earthquakes.latitude)) GROUP BY counties.name ORDER BY cnt desc; 使用FusionInsight HD客户端执行修改后的sql文件，命令参考 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql 执行结果如下，与GIS开源网站描述一致 aggregation-mr 参考GIS说明https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr中关于集成MR的示例，在华为FusionInsight HD中执行该示例。 获取gis源代码https://github.com/Esri/gis-tools-for-hadoop/ 完成FusionInsight HD V100R002C60U20的安装，包含Hive组件。 在FusionInsight Manager创建一个“机机”用户，具体请参见《FusionInsight HD 管理员指南》的创建用户章节。将用户加入上面创建的角色HiveAdmin。例如，创建用户“testuser”并下载对应的keytab文件user.keytab以及krb5.conf文件 安装FusionInsight HD的客户端，具体请参见《FusionInsight HD 管理员指南》的安装和使用客户端章节。 将下载的gis tools源码通过WinSCP工具上传到安装有FusionInsight HD客户端所在节点的/opt目录下，上传源码目录为gis-tools-for-hadoop-master 修改/opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh如下，其中26004为yarn配置的yarn.resourcemanager.port端口 #!/bin/bash NAME_NODE_URL=hdfs://hacluster JOB_TRACKER_URL=162.1.93.103:26004 SAMPLE_DIR=/tmp/gistest JOB_DIR=$SAMPLE_DIR/job LIB_DIR=$SAMPLE_DIR/lib DATA_DIR=$SAMPLE_DIR/data OUTPUT_DIR=$SAMPLE_DIR/output 修改/opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh的执行权限，并执行 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh 执行完毕得到如下结果文件result.txt "},"Other/Using_Livy_with_FusionInsight.html":{"url":"Other/Using_Livy_with_FusionInsight.html","title":"Apache Livy","keywords":"","body":"Apache Livy对接FusionInsight 适用场景 Apache Livy 0.5.0-incubating FusionInsight HD V100R002C80SPC200 (Spark2.x) 安装Livy 操作场景 安装 Apache Livy 0.5.0 前提条件 已完成FusionInsight HD和客户端的安装。 操作步骤 安装Apache Livy 0.5.0-incubating，在网址https://livy.incubator.apache.org/download/下载安装包，使用WinSCP导入主机并用unzip livy-0.5.0-incubating-bin.zip解压生成livy-0.5.0-incubating-bin目录 执行source命令到客户端，获取java配置信息 source /opt/hadoopclient/bigdata_env echo $JAVA_HOME 根据产品文档创建用户developuser，并赋予足够权限，下载用户developuser的keytab文件user.keytab，上传至/opt/developuser目录下 在/usr/livy/livy-0.5.0-incubating-bin/conf路径下新建livy的认证文件jaas.conf，内容如下: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; 其中用户为在FusionInsight Manager中创建的developuser用户，将developuser的keytab文件user.key放在/opt/developuser/目录下 配置Livy环境变量，在profile文件中加入如下变量 vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH 编辑livy.conf文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf 加入如下内容： livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true livy.server.auty.type=kerberos livy.server.auth.kerberos.keytab=/opt/developuser/user.keytab livy.server.auth.kerberos.principal=developuser@HADOOP.COM livy.server.launch.kerberos.keytab=/opt/developuser/user.keytab livy.server.launch.kerberos.principal=developuser@HADOOP.COM 编辑livy-client.conf文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf 加入如本机ip地址： livy.rsc.rpc.server.address =172.16.52.190 编辑livy-env.sh文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh 加入如下内容： export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 编辑spark-blacklist.conf文件，位置/usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf 注销掉如下内容： spark.master spark.submit.deployMode 启动和停止Livy，在路径/usr/livy/livy-0.5.0-incubating-bin下 bin/livy-server start 启动成功后可以在http://172.16.52.190:8998访问到Livy服务器： 测试运行Livy样例代码 操作场景 测试运行Livy样例代码，包括Spark Shell，PySpark，SparkR 样例代码参考网址https://livy.incubator.apache.org/examples/ 前提条件 已完成FusionInsight HD和客户端的安装。 已完成Anaconda和R在客户端主机上的安装。 若没有安装Anaconda和R，请参考Zeppelin0.8.0对接FusionInsight HD V100R002C80SPC200 (Spark2.x)指导文档中连接Spark和SparkR部分相关内容 运行Spark样例操作步骤 输入命令python启动Anaconda 输入如下python代码启动一个Livy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) 当一个session完成启动后， 它将会变为闲置状态 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() 下面通过传递一个简单JSON命令行的方式来执行Scala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) 可以在Session0状态栏看到之前运行的样例代码以及结果 也可以在终端看到以JSON格式返回的结果 更新Scala再次运行 data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y 可以在Session0状态栏看到之前运行的样例代码以及结果 关闭session0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers) 运行PySpark样例操作步骤 继续接着上面的步骤，更改类型为pyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() 可以在Session状态栏看到新启动的Session1 通过传递JSON命令的方式执行Python样例代码，注意要更改statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y 可以在Session1状态栏看到之前运行的样例代码以及结果 关闭session1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers) 运行SparkR样例操作步骤 继续接着上面的步骤，更改类型为sparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() 可以在Session状态栏看到新启动的Session2 通过传递JSON命令的方式执行R样例代码，注意要更改statements_url data = { 'code': textwrap.dedent(\"\"\" hello 可以在Session2状态栏看到之前运行的样例代码以及结果 关闭session2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers) "}}