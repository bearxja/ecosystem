{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FusionInsight MRS Ecosystem \u00b6 FusionInsight support opensource Hadoop interface, it can integrate with the following thridparty tools Thrid Party Tools FusionInsight Category Name Version C50 C60 C70 C80 6.5 Business Intelligence QlikView 12 Hive SparkSQL Hive SparkSQL Hive SparkSQL SSRS 2017 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Data Analysis Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark Data Integration Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL 11.5.0.2 HDFS Hive Phoenix SparkSQL Kafka GaussDB IBM InfoSphere CDC 11.3.3.1 HDFS Informatica PowerCenter 10.2.0 HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Talend 6.4.1 HDFS HBase Hive 7.0.1 HDFS HBase Development Jupyter Notebook 5.7.8 Hive ELK Spark2x Hive ELK Spark2x Other Apache Livy 0.5.0 Spark2x 0.6.0 Spark2x","title":"Home"},{"location":"#fusioninsight-mrs-ecosystem","text":"FusionInsight support opensource Hadoop interface, it can integrate with the following thridparty tools Thrid Party Tools FusionInsight Category Name Version C50 C60 C70 C80 6.5 Business Intelligence QlikView 12 Hive SparkSQL Hive SparkSQL Hive SparkSQL SSRS 2017 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Data Analysis Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark Data Integration Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL 11.5.0.2 HDFS Hive Phoenix SparkSQL Kafka GaussDB IBM InfoSphere CDC 11.3.3.1 HDFS Informatica PowerCenter 10.2.0 HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Talend 6.4.1 HDFS HBase Hive 7.0.1 HDFS HBase Development Jupyter Notebook 5.7.8 Hive ELK Spark2x Hive ELK Spark2x Other Apache Livy 0.5.0 Spark2x 0.6.0 Spark2x","title":"FusionInsight MRS Ecosystem"},{"location":"Business_Intelligence/","text":"Business Intelligence \u00b6 QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 SSRS 2017 \u2194 6.5 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80","title":"Index"},{"location":"Business_Intelligence/#business-intelligence","text":"QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 SSRS 2017 \u2194 6.5 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80","title":"Business Intelligence"},{"location":"Business_Intelligence/QlikView/","text":"QlikView and FusionInsight Instruction \u00b6 Case \u00b6 QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Configuring kerberos authentication for Windows \u00b6 Download and install MIT Kerberos at: http://web.mit.edu/kerberos/dist/#kfw-4.0 The version is consistent with the number of operating system bits. This article is version kfw-4.1-amd64.msi. Verify that the time of the client machine is the same as that of the FusionInsight HD cluster. The time difference is less than 5 minutes. Set the Kerberos configuration file Create a role and machine user on the FusionInsight Manager. For details, see the chapter \"Creating Users\" in the FusionInsight HD Administrator Guide. The role needs to grant Hive access rights based on business needs and add users to the role. For example, create the user \"sparkdemo\" and download the corresponding keytab file user.keytab and krb5.conf file, rename the krb5.conf file to krb5.ini, and put it in the C:\\ProgramData\\MIT\\Kerberos5 directory. Set the cache file for the Kerberos ticket Create a directory to store the tickets, such as C:\\temp . Set the system environment variable of Windows, the variable name is \"KRB5CCNAME\", and the variable value is C:\\temp\\krb5cache Restart the machine. Authenticate on Windows Use the command line to go to the MIT Kerberos installation path and find the executable kinit.exe. For example, the path to this article is: C:\\Program Files\\MIT\\Kerberos\\bin Execute the following command: kinit -k -t /path_to_userkeytab/user.keytab UserName Path_to_userkeytab is the path where the user keytab file is stored, user.keytab is the user's keytab, and UserName is the user name. Configuring a Hive data source \u00b6 Hive data source in QlikView, docking Hive's ODBC interface Download and install the Hive ODBC driver \u00b6 Download the driver from the following address and select the corresponding ODBC version according to the operating system type, download and install: address Configuring user DSN \u00b6 * On the User DSN tab of the OBDC Data Source Manager page, click Add to configure the user data source. On the Create Data Source page, find the Cloudera ODBC Driver for Apache Hive , select it and click Finish . Configure the Hive data source. Data Source Name: is a custom parameter Host(s): HiveServer business ip Port: Hive Service port, 21066 Mechanism: Kerberos Host FQDN: hadoop.hadoop.com Service Name: hive Realm: Leave blank Click Test If the connection is successful, the configuration is successful. Click OK Connect to the Hive data source \u00b6 Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar * In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Source page, select the data source hive_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button * In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website. Configuring a Spark data source \u00b6 Configure the Spark data source in QlivView and connect to the thrift interface of SparkSQL. Download and install Spark's ODBC driver \u00b6 Download the Spark ODBC driver on Simba's official website, select 32bit or 64bit according to the user's own operating system, and select Spark for Data Source. SQL, address: http://www.tableau.com/support/drivers * Install the client as prompted by the install client. Configuring user DSN \u00b6 On the OBDC Data Source Manager page, on the User DSN tab, click Add to configure the user data source. On the Create Data Source page, find Simba Spark ODBC Driver , select it and click Finish . Configure the Spark data source on the Simba Spark ODBC Driver DSN Setup page. Data Source Name\uff1a Custom Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a Leave blank, Host(s)\uff1a JDBCServer (main) service ip, Port\uff1a SparkThriftServer client port number 23040. After setting, click Advanced Options , in the pop-up Advanced Options page, check Use Native Query and Get Tables With Query , then click OK Go back to the Simba Spark ODBC Driver DSN Setup , click Test to connect successfully, click OK to exit the page, otherwise a failure dialog will pop up. Go back to the Simba Spark ODBC Driver DSN Setup page, click OK , go back to the ODBC Data Source Manager page, click OK to complete and exit the configuration. Connect to a Spark data source \u00b6 Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Sources page, select the data source spark_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website. FAQ \u00b6 Cannot find C:\\ProgramData\\MIT\\Kerberos5 folder C:\\ProgramData is generally a hidden folder, setting the folder to be hidden or using the search function to solve the problem. Connection succeeded without database permissions The user used for the connection needs to have the permissions of the database, otherwise the ODBC connection will succeed but the database content cannot be read. ODBC connection failed The common situation is that the input data of Host(s) , Port , Host FQDN is incorrect. Please enter according to the actual situation.","title":"12 <--> C80"},{"location":"Business_Intelligence/QlikView/#qlikview-and-fusioninsight-instruction","text":"","title":"QlikView and FusionInsight Instruction"},{"location":"Business_Intelligence/QlikView/#case","text":"QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL)","title":"Case"},{"location":"Business_Intelligence/QlikView/#configuring-kerberos-authentication-for-windows","text":"Download and install MIT Kerberos at: http://web.mit.edu/kerberos/dist/#kfw-4.0 The version is consistent with the number of operating system bits. This article is version kfw-4.1-amd64.msi. Verify that the time of the client machine is the same as that of the FusionInsight HD cluster. The time difference is less than 5 minutes. Set the Kerberos configuration file Create a role and machine user on the FusionInsight Manager. For details, see the chapter \"Creating Users\" in the FusionInsight HD Administrator Guide. The role needs to grant Hive access rights based on business needs and add users to the role. For example, create the user \"sparkdemo\" and download the corresponding keytab file user.keytab and krb5.conf file, rename the krb5.conf file to krb5.ini, and put it in the C:\\ProgramData\\MIT\\Kerberos5 directory. Set the cache file for the Kerberos ticket Create a directory to store the tickets, such as C:\\temp . Set the system environment variable of Windows, the variable name is \"KRB5CCNAME\", and the variable value is C:\\temp\\krb5cache Restart the machine. Authenticate on Windows Use the command line to go to the MIT Kerberos installation path and find the executable kinit.exe. For example, the path to this article is: C:\\Program Files\\MIT\\Kerberos\\bin Execute the following command: kinit -k -t /path_to_userkeytab/user.keytab UserName Path_to_userkeytab is the path where the user keytab file is stored, user.keytab is the user's keytab, and UserName is the user name.","title":"Configuring kerberos authentication for Windows"},{"location":"Business_Intelligence/QlikView/#configuring-a-hive-data-source","text":"Hive data source in QlikView, docking Hive's ODBC interface","title":"Configuring a Hive data source"},{"location":"Business_Intelligence/QlikView/#download-and-install-the-hive-odbc-driver","text":"Download the driver from the following address and select the corresponding ODBC version according to the operating system type, download and install: address","title":"Download and install the Hive ODBC driver"},{"location":"Business_Intelligence/QlikView/#configuring-user-dsn","text":"* On the User DSN tab of the OBDC Data Source Manager page, click Add to configure the user data source. On the Create Data Source page, find the Cloudera ODBC Driver for Apache Hive , select it and click Finish . Configure the Hive data source. Data Source Name: is a custom parameter Host(s): HiveServer business ip Port: Hive Service port, 21066 Mechanism: Kerberos Host FQDN: hadoop.hadoop.com Service Name: hive Realm: Leave blank Click Test If the connection is successful, the configuration is successful. Click OK","title":"Configuring user DSN"},{"location":"Business_Intelligence/QlikView/#connect-to-the-hive-data-source","text":"Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar * In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Source page, select the data source hive_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button * In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website.","title":"Connect to the Hive data source"},{"location":"Business_Intelligence/QlikView/#configuring-a-spark-data-source","text":"Configure the Spark data source in QlivView and connect to the thrift interface of SparkSQL.","title":"Configuring a Spark data source"},{"location":"Business_Intelligence/QlikView/#download-and-install-sparks-odbc-driver","text":"Download the Spark ODBC driver on Simba's official website, select 32bit or 64bit according to the user's own operating system, and select Spark for Data Source. SQL, address: http://www.tableau.com/support/drivers * Install the client as prompted by the install client.","title":"Download and install Spark's ODBC driver"},{"location":"Business_Intelligence/QlikView/#configuring-user-dsn_1","text":"On the OBDC Data Source Manager page, on the User DSN tab, click Add to configure the user data source. On the Create Data Source page, find Simba Spark ODBC Driver , select it and click Finish . Configure the Spark data source on the Simba Spark ODBC Driver DSN Setup page. Data Source Name\uff1a Custom Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a Leave blank, Host(s)\uff1a JDBCServer (main) service ip, Port\uff1a SparkThriftServer client port number 23040. After setting, click Advanced Options , in the pop-up Advanced Options page, check Use Native Query and Get Tables With Query , then click OK Go back to the Simba Spark ODBC Driver DSN Setup , click Test to connect successfully, click OK to exit the page, otherwise a failure dialog will pop up. Go back to the Simba Spark ODBC Driver DSN Setup page, click OK , go back to the ODBC Data Source Manager page, click OK to complete and exit the configuration.","title":"Configuring user DSN"},{"location":"Business_Intelligence/QlikView/#connect-to-a-spark-data-source","text":"Open QlikView 12, New a document Close the pop-up wizard Open the Edit Script button in the toolbar In the pop-up Edit Script page, click the Data tab, find OCBC in the drop-down bar of Database , click Connect ; On the Connect to Data Sources page, select the data source spark_odbc configured above, then click OK ; On the Data tab of the Edit Script page, click the Select button In the Create Select statement page, select the Database table you want to import, select * in the ** field**, import the full table, import the corresponding table into the corresponding table, and click * OK* (select * in the example); Go back to the Edit Script page and click OK Go back to the QlikView worksheet page and click Reload to import the database table into QlikView. The data can then be processed by cartographic tabulation analysis. For specific steps, please refer to the QlikView official website.","title":"Connect to a Spark data source"},{"location":"Business_Intelligence/QlikView/#faq","text":"Cannot find C:\\ProgramData\\MIT\\Kerberos5 folder C:\\ProgramData is generally a hidden folder, setting the folder to be hidden or using the search function to solve the problem. Connection succeeded without database permissions The user used for the connection needs to have the permissions of the database, otherwise the ODBC connection will succeed but the database content cannot be read. ODBC connection failed The common situation is that the input data of Host(s) , Port , Host FQDN is incorrect. Please enter according to the actual situation.","title":"FAQ"},{"location":"Business_Intelligence/SSRS/","text":"Connect SSRS to FusionInsight \u00b6 Applicable Scene \u00b6 SSRS 2017 <-> FusionInsight HD 6.5 (Hive/SparkSQL) SSRS Connection Guide \u00b6 Download and install Microsoft ODBC driver with 32bit and 64 bit Microsoft Hive ODBC Driver download: Click to download Microsoft Spark ODBC Driver download address: Click to download Download and install the MIT Kerberos authentication client: Click to download Configure Kerberos authentication Download and install MIT Kerberos. Remember the installation path, for example: \"C:\\program Files\\MIT\\Kerberos\". Refer to the FusionInsight HD 6.5 document, \"Application Development Guide-> Security Mode-> Security Authentication\" to configure the connection account and related permissions. Use the connection account to log in to the FusionInsight management interface, hover the mouse over the upper right corner to display the user, select \"Download user credentials\" in the drop-down display box, select the cluster and confirm the download. After downloading and decompressing, there are two krb5.conf and user.keytab files. Rename the krb5.conf file to krb5.ini and copy it to the \"C:\\ProgramData\\MIT\\Kerberos5\" directory. The \"C:\\ProgramData\" directory is usually hidden, and you need to set to show hidden files. Set the cache file for Kerberos tickets. Create a directory for your tickets, such as \"C:\\temp\". Set the system environment variable for Windows. The variable name is \"KRB5CCNAME\" and the variable value is \"C:\\temp\\krb5cache\". Set the Windows system environment variable. The variable name is \"KRB5_CONFIG\" and the variable value is \"C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini\".Restart the server. Authenticate on Windows. Open MIT Kerberos, click \"Get Ticket\", and in the pop-up \"MIT Kerberos: Get Ticket\" window, enter the username in \"Privileg\", enter the password in \"Password\", and click \"OK\". Configure Spark and Hive ODBC DSN Configure Spark ODBC DSN Open the Windows ODBC configuration tool, and configure \"Sample Microsoft Hive DSN\" and \"Sample Microsoft Spark DSN\" in the System DSN. For the related configuration, refer to the following figure and replace the HOST address according to the actual environment. Disable SSL verification in \"SSL OPTIONS\", the settings are as follows: Configure HIVE ODBC DSN HIVE DSN configuration refer to the following figure, replace the HOST address according to the actual environment Functional Verification Prepare test data Create a database and data table through beeline, and insert some test data. In this test scenario, a sales test table is created, including employee ID, name, quarter, sales, etc. Part of the test data is inserted, and the SSRS report is used to display the sales of each quarter of the employee and a comparison histogram. Create Hive data source Open Report Builder, right-click \"Data Sources\", set the Data Source Name, select \"use a connection embedded in my report\", select Connection Type as ODBC, click \"build\", in the pop-up box, select \"Data Source Specification\" use connection string \", click the\" build \"button, select\" Machine Data Source \"in the pop-up box, select\" Sample Microsoft Hive DSN \", and click\" OK \"to complete the configuration. Create a Spark data source Open Report Builder, right-click \"Data Sources\", set the Data Source Name, select \"use a connection embedded in my report\", select Connection Type as ODBC, click \"build\", in the pop-up box, select \"Data Source Specification\" use connection string \", click the\" build \"button, select\" Machine Data Source \"in the pop-up box, select\" Sample Microsoft Spark DSN \", and click\" OK \"to complete the configuration. Create dataset In the left view of Report Builder, right-click \"Data Sets\" to add a dataset, set the dataset name, select \"use a dataset embedded in my report\", and select data sources as the previously configured Hive or Spark Data Sources. Enter the SQL statement in the Query to filter the data columns, such as \"select * from database.table\", click \"OK\" to complete the configuration. After completing this operation, the columns of the data set will appear on the left for subsequent data analysis. Design report The report is designed according to the requirements. The report designed for this test are as follows. Two data tables are designed based on Hive and Spark, as shown below: Click the \"RUN\" button on the upper left to test. The test results are as follows: Post to SSRS After the test is completed, select \"File-> Publish to Report Server\" in the upper left corner. The report server address is http://{ip}/ReportServer. After uploading, open the report server website http://{ip}/ ReportServer, and select the corresponding path. View the report results as follows, you can view the report data in the browser. The verification is completed. The SSRS report obtains the data through Hive and Spark ODBC and displays it correctly.","title":"2017 <--> 6.5"},{"location":"Business_Intelligence/SSRS/#connect-ssrs-to-fusioninsight","text":"","title":"Connect SSRS to FusionInsight"},{"location":"Business_Intelligence/SSRS/#applicable-scene","text":"SSRS 2017 <-> FusionInsight HD 6.5 (Hive/SparkSQL)","title":"Applicable Scene"},{"location":"Business_Intelligence/SSRS/#ssrs-connection-guide","text":"Download and install Microsoft ODBC driver with 32bit and 64 bit Microsoft Hive ODBC Driver download: Click to download Microsoft Spark ODBC Driver download address: Click to download Download and install the MIT Kerberos authentication client: Click to download Configure Kerberos authentication Download and install MIT Kerberos. Remember the installation path, for example: \"C:\\program Files\\MIT\\Kerberos\". Refer to the FusionInsight HD 6.5 document, \"Application Development Guide-> Security Mode-> Security Authentication\" to configure the connection account and related permissions. Use the connection account to log in to the FusionInsight management interface, hover the mouse over the upper right corner to display the user, select \"Download user credentials\" in the drop-down display box, select the cluster and confirm the download. After downloading and decompressing, there are two krb5.conf and user.keytab files. Rename the krb5.conf file to krb5.ini and copy it to the \"C:\\ProgramData\\MIT\\Kerberos5\" directory. The \"C:\\ProgramData\" directory is usually hidden, and you need to set to show hidden files. Set the cache file for Kerberos tickets. Create a directory for your tickets, such as \"C:\\temp\". Set the system environment variable for Windows. The variable name is \"KRB5CCNAME\" and the variable value is \"C:\\temp\\krb5cache\". Set the Windows system environment variable. The variable name is \"KRB5_CONFIG\" and the variable value is \"C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini\".Restart the server. Authenticate on Windows. Open MIT Kerberos, click \"Get Ticket\", and in the pop-up \"MIT Kerberos: Get Ticket\" window, enter the username in \"Privileg\", enter the password in \"Password\", and click \"OK\". Configure Spark and Hive ODBC DSN Configure Spark ODBC DSN Open the Windows ODBC configuration tool, and configure \"Sample Microsoft Hive DSN\" and \"Sample Microsoft Spark DSN\" in the System DSN. For the related configuration, refer to the following figure and replace the HOST address according to the actual environment. Disable SSL verification in \"SSL OPTIONS\", the settings are as follows: Configure HIVE ODBC DSN HIVE DSN configuration refer to the following figure, replace the HOST address according to the actual environment Functional Verification Prepare test data Create a database and data table through beeline, and insert some test data. In this test scenario, a sales test table is created, including employee ID, name, quarter, sales, etc. Part of the test data is inserted, and the SSRS report is used to display the sales of each quarter of the employee and a comparison histogram. Create Hive data source Open Report Builder, right-click \"Data Sources\", set the Data Source Name, select \"use a connection embedded in my report\", select Connection Type as ODBC, click \"build\", in the pop-up box, select \"Data Source Specification\" use connection string \", click the\" build \"button, select\" Machine Data Source \"in the pop-up box, select\" Sample Microsoft Hive DSN \", and click\" OK \"to complete the configuration. Create a Spark data source Open Report Builder, right-click \"Data Sources\", set the Data Source Name, select \"use a connection embedded in my report\", select Connection Type as ODBC, click \"build\", in the pop-up box, select \"Data Source Specification\" use connection string \", click the\" build \"button, select\" Machine Data Source \"in the pop-up box, select\" Sample Microsoft Spark DSN \", and click\" OK \"to complete the configuration. Create dataset In the left view of Report Builder, right-click \"Data Sets\" to add a dataset, set the dataset name, select \"use a dataset embedded in my report\", and select data sources as the previously configured Hive or Spark Data Sources. Enter the SQL statement in the Query to filter the data columns, such as \"select * from database.table\", click \"OK\" to complete the configuration. After completing this operation, the columns of the data set will appear on the left for subsequent data analysis. Design report The report is designed according to the requirements. The report designed for this test are as follows. Two data tables are designed based on Hive and Spark, as shown below: Click the \"RUN\" button on the upper left to test. The test results are as follows: Post to SSRS After the test is completed, select \"File-> Publish to Report Server\" in the upper left corner. The report server address is http://{ip}/ReportServer. After uploading, open the report server website http://{ip}/ ReportServer, and select the corresponding path. View the report results as follows, you can view the report data in the browser. The verification is completed. The SSRS report obtains the data through Hive and Spark ODBC and displays it correctly.","title":"SSRS Connection Guide"},{"location":"Business_Intelligence/Tableau/","text":"Connection Instruction between Tableau and FusionInsight \u00b6 Succeeded Case \u00b6 Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Configure the Kerberos on Windows \u00b6 Download and install MIT Kerberos from the following URL http://web.mit.edu/kerberos/dist/#kfw-4.0 Make sure the time differences between FusionInsight clusters and Tableau client is no longer than 5 minutes. Configure required Kerberos filesystem Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf files on the Tableau client, rename the krb5.conf file into krb5.ini and save it into the following directory C:\\ProgramData\\MIT\\Kerberos5 Configure Kerberos cache file Create a directory to save the Kerberos cache file, for example, C:\\temp Configure the Environment Variables, Variable name is KRB5CCNAME , Variable value is C:\\temp\\krb5cache restart the Tableau client Start Kerberos Authentication on Windows use the created username and password to log in, the Principal is equal to username@Kerberos Realm name Open MIT Kerberos , click on Get Ticket, and type in the right Principal and Password for authentication Connecting Tableau to Hive \u00b6 Configure the ODBC interface to connect FusionInsight HiveDriver Download and install the ODBC driver Download URL: http://www.cloudera.com/content/cloudera/en/downloads/connectors/hive/odbc/hive-odbc-v2-5-15.html and choose the right one depends on the OS and bit version Configure ODBC drivers Open ODBC Data Sources(64-bit) by searching the keyword ODBC on Windows click on User DSN tab, click on Add button, choose Cloudera ODBC Driver for Apache Hive and click on Finish to start to configure In detail: 1: Hive Server 2 2: No Service Discovery 3: 172.21.3.101 4: 21006 5: default 6: Kerberos 7: hadoop.hadoop.com 8: hive 9: SASL click on Test button to test the connection Open Tableau Click on More option, and choose ODBC by search in keyword Connection Configuration shown as bellow: click on Connect and then click on Sign In Search the Data Search the data from multiple tables Connecting Tableau to Spark \u00b6 Download and install the ODBC driver for Spark Download url http://www.tableau.com/support/drivers Created DSN \uff08Data Source Name\uff09 Open ODBC Data Sources(64-bit) Click on System DSN tab, click on Add, choose Simba Spark ODBC Driver and click on Finish Open the installed Driver directory, for example, C:\\Program Files\\Simba Spark ODBC Driver\\lib and open the DriverConfiguration64.exe to Configure In detail: 1\uff1aSparkThriftServer (Spark 1.1 and later) 2: Kerberos 3: hadoop.hadoop.com 4: spark 5: SASL click on Advanced Options and choose \"Driver Config Take Precedence\" Click on ok to save the configuration Open Tableau Click on More option, and choose Spark SQL by search in keyword Connection Configuration shown as bellow: Server info can be got from FusionInsight Manager Web UI Port info can be got from FusionInsight Manager Web UI as well Click on Sign In , to come into a new Page, choose Schema and Table shown as bellow Open Sheet to visulize the data Performance test Search the table web_sales which contains millions of records Search the table by multiple tables whose names are store_sales and item Add customer_address table Test outcome\uff1a FAQ \u00b6 Cannot find C:\\ProgramData\\MIT\\Kerberos5 This Folder is hidden, configure the windows can solve it Connection succeeded but permission denied Use the user who has the privilege to DATABASE","title":"10.5.0 <--> C80"},{"location":"Business_Intelligence/Tableau/#connection-instruction-between-tableau-and-fusioninsight","text":"","title":"Connection Instruction between Tableau and FusionInsight"},{"location":"Business_Intelligence/Tableau/#succeeded-case","text":"Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL)","title":"Succeeded Case"},{"location":"Business_Intelligence/Tableau/#configure-the-kerberos-on-windows","text":"Download and install MIT Kerberos from the following URL http://web.mit.edu/kerberos/dist/#kfw-4.0 Make sure the time differences between FusionInsight clusters and Tableau client is no longer than 5 minutes. Configure required Kerberos filesystem Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf files on the Tableau client, rename the krb5.conf file into krb5.ini and save it into the following directory C:\\ProgramData\\MIT\\Kerberos5 Configure Kerberos cache file Create a directory to save the Kerberos cache file, for example, C:\\temp Configure the Environment Variables, Variable name is KRB5CCNAME , Variable value is C:\\temp\\krb5cache restart the Tableau client Start Kerberos Authentication on Windows use the created username and password to log in, the Principal is equal to username@Kerberos Realm name Open MIT Kerberos , click on Get Ticket, and type in the right Principal and Password for authentication","title":"Configure the Kerberos on Windows"},{"location":"Business_Intelligence/Tableau/#connecting-tableau-to-hive","text":"Configure the ODBC interface to connect FusionInsight HiveDriver Download and install the ODBC driver Download URL: http://www.cloudera.com/content/cloudera/en/downloads/connectors/hive/odbc/hive-odbc-v2-5-15.html and choose the right one depends on the OS and bit version Configure ODBC drivers Open ODBC Data Sources(64-bit) by searching the keyword ODBC on Windows click on User DSN tab, click on Add button, choose Cloudera ODBC Driver for Apache Hive and click on Finish to start to configure In detail: 1: Hive Server 2 2: No Service Discovery 3: 172.21.3.101 4: 21006 5: default 6: Kerberos 7: hadoop.hadoop.com 8: hive 9: SASL click on Test button to test the connection Open Tableau Click on More option, and choose ODBC by search in keyword Connection Configuration shown as bellow: click on Connect and then click on Sign In Search the Data Search the data from multiple tables","title":"Connecting Tableau to Hive"},{"location":"Business_Intelligence/Tableau/#connecting-tableau-to-spark","text":"Download and install the ODBC driver for Spark Download url http://www.tableau.com/support/drivers Created DSN \uff08Data Source Name\uff09 Open ODBC Data Sources(64-bit) Click on System DSN tab, click on Add, choose Simba Spark ODBC Driver and click on Finish Open the installed Driver directory, for example, C:\\Program Files\\Simba Spark ODBC Driver\\lib and open the DriverConfiguration64.exe to Configure In detail: 1\uff1aSparkThriftServer (Spark 1.1 and later) 2: Kerberos 3: hadoop.hadoop.com 4: spark 5: SASL click on Advanced Options and choose \"Driver Config Take Precedence\" Click on ok to save the configuration Open Tableau Click on More option, and choose Spark SQL by search in keyword Connection Configuration shown as bellow: Server info can be got from FusionInsight Manager Web UI Port info can be got from FusionInsight Manager Web UI as well Click on Sign In , to come into a new Page, choose Schema and Table shown as bellow Open Sheet to visulize the data Performance test Search the table web_sales which contains millions of records Search the table by multiple tables whose names are store_sales and item Add customer_address table Test outcome\uff1a","title":"Connecting Tableau to Spark"},{"location":"Business_Intelligence/Tableau/#faq","text":"Cannot find C:\\ProgramData\\MIT\\Kerberos5 This Folder is hidden, configure the windows can solve it Connection succeeded but permission denied Use the user who has the privilege to DATABASE","title":"FAQ"},{"location":"Data_Analysis/","text":"Data Analysis \u00b6 Rapidminer Studio 8.2.001 \u2194 C80","title":"Index"},{"location":"Data_Analysis/#data-analysis","text":"Rapidminer Studio 8.2.001 \u2194 C80","title":"Data Analysis"},{"location":"Data_Analysis/RapidMiner/","text":"Connection between RapidMiner with FusionInsightHD \u00b6 Succeeded Case \u00b6 Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark) preparation \u00b6 Download and install RapidMiner Studio, download site https://rapidminer.com/ Start rapidminer, on the top of the main menu, choose Extensions->Marketplace ,type radoop ,install it and restart rapidminer Configure the local host file\uff0cfile path is C:\\Windows\\System32\\drivers\\etc \uff0cadd the cluster node ip and host name and save the file. Configure Kerberos file Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive, Spark,HDFS privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf of the user and save them in your computer. Prepare the FusionInsight client configuration filesystem and jar files In the Manager GUI\uff0cchoose Service->Download Client->Only Configuration File Unzip the file,find the following files,copy them into a directory,like ../config . Open yarn-site.xml ,delete the following property: <property> <name>audit.service.name</name> <value>Yarn</value> </property> Login to one of the cluster nodes, go to the following path \\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars ,download the file directory /jars ,save it in your computer,like C:/jars \u3002 Configure the cluster \u00b6 Bind the UDP port Download the UDP port bind tool uredir ,website is https://github.com/troglobit/uredir After building and installing, we get the executing file uredir ,upload it to the KDC server nodes in the cluster,and run the following command,here IP refers to the node ip. ./uredir IP:88 IP:21732 Configure Radoop Jars Download Radoop jars in this address https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,get the correct version\u3002 Upload the jar files to each node of the cluster,eg, /usr/local/lib/radoop/ In the HiveServer node of the cluster,uplaod the Radoop jar files to the following path and change their owner and execution authority ``` cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar `` * In the FusionInsight Manager GUI, choose Service->Hive->Service Configuration`add the following configuration radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name - Notice that there should be a | as seperater Save the configuration\uff0crestart HiveServer Create Radoop UDF functions Run the following command in the client node, login to the Hive database source /opt/hadoopclient\u3001bigdata_env kinit developuser beeline create a database in Hive, for example rapidminer , and create functions,run the following commands in beeline mode create database rapidminer; use rapidminer; DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep'; RapidMiner Configuration \u00b6 In RapidMiner sStadio\uff0cchoose Connections->Manage Radoop Connections in the top menu. choose New Connections->Import Hadoop Configuration Files ,choose the configuration files downloaded from the cluster,click Import Configuration After the import, click Next , go to the Connection settings window,configure ad following: Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory: Spark jar files downloaded from the cluster Client Principal: Kerberos user name @HADOOP.com Keytab File: the keytab file downloaded from manager KDC Address: the KDC server IP(see the krb5.conf file) REALM: HADOOP.COM Kerberos Config File: the krb5 file downloaded from manager Hadoop\uff1a At the filter in upper right corner, search split , uncheck mapreduce.input.fileinputformat.split.maxsize Search classpath ,uncheck mapreduce.application.classpath Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive(or libs)Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1aadd the following two parameters for spark: park.driver.extraJavaOptions and spark.executor.extraJavaOptions The value can be found in manager GUI, choose Services->Spark2X Configuration->type all \uff0csearch extraJavaOptions in the search bar, choose the parameters in Spark2x->SparkResource2x Copy the values into a text file, replace the relative path ./ in the value with absolute path in the cluster, like /opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc , then copy the values into rapidminer spark configuration Hive: Hive Version: Hive Server2 Hive Server Address\uff1aHive Server IP Hive Port: 21066 Database Name: the database name created in Hive,here is rapidminer Customer database for UDFs: same as before click OK->Proced Anyway->Save Test the Connection \u00b6 Click Configure, in Global tab, click Test\uff0cTest Results show as following: In Hadoop tab\uff0cclick Test,Test Results show as following: In Spark tab,click Test,Test Results show as following: In Hive tab, click Test, Test Results show as following: Click Full test,Test Results show as following: Radoop Demo \u00b6 In RapidMiner Studio main menu,choose Help->Tutorials->User Hadoop->Rapidminer Radoop Run the demo accordding to the Tutorials, get the follwing results","title":"8.2.001 <--> C80"},{"location":"Data_Analysis/RapidMiner/#connection-between-rapidminer-with-fusioninsighthd","text":"","title":"Connection between RapidMiner with FusionInsightHD"},{"location":"Data_Analysis/RapidMiner/#succeeded-case","text":"Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark)","title":"Succeeded Case"},{"location":"Data_Analysis/RapidMiner/#preparation","text":"Download and install RapidMiner Studio, download site https://rapidminer.com/ Start rapidminer, on the top of the main menu, choose Extensions->Marketplace ,type radoop ,install it and restart rapidminer Configure the local host file\uff0cfile path is C:\\Windows\\System32\\drivers\\etc \uff0cadd the cluster node ip and host name and save the file. Configure Kerberos file Created a user with \"Human-Machine\" as its type( For detail, take product documentation as a reference ), grant the Hive, Spark,HDFS privileges to this user. For example, create a user named developuser , download the user.keytab and krb5.conf of the user and save them in your computer. Prepare the FusionInsight client configuration filesystem and jar files In the Manager GUI\uff0cchoose Service->Download Client->Only Configuration File Unzip the file,find the following files,copy them into a directory,like ../config . Open yarn-site.xml ,delete the following property: <property> <name>audit.service.name</name> <value>Yarn</value> </property> Login to one of the cluster nodes, go to the following path \\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars ,download the file directory /jars ,save it in your computer,like C:/jars \u3002","title":"preparation"},{"location":"Data_Analysis/RapidMiner/#configure-the-cluster","text":"Bind the UDP port Download the UDP port bind tool uredir ,website is https://github.com/troglobit/uredir After building and installing, we get the executing file uredir ,upload it to the KDC server nodes in the cluster,and run the following command,here IP refers to the node ip. ./uredir IP:88 IP:21732 Configure Radoop Jars Download Radoop jars in this address https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,get the correct version\u3002 Upload the jar files to each node of the cluster,eg, /usr/local/lib/radoop/ In the HiveServer node of the cluster,uplaod the Radoop jar files to the following path and change their owner and execution authority ``` cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar `` * In the FusionInsight Manager GUI, choose Service->Hive->Service Configuration`add the following configuration radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name - Notice that there should be a | as seperater Save the configuration\uff0crestart HiveServer Create Radoop UDF functions Run the following command in the client node, login to the Hive database source /opt/hadoopclient\u3001bigdata_env kinit developuser beeline create a database in Hive, for example rapidminer , and create functions,run the following commands in beeline mode create database rapidminer; use rapidminer; DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep';","title":"Configure the cluster"},{"location":"Data_Analysis/RapidMiner/#rapidminer-configuration","text":"In RapidMiner sStadio\uff0cchoose Connections->Manage Radoop Connections in the top menu. choose New Connections->Import Hadoop Configuration Files ,choose the configuration files downloaded from the cluster,click Import Configuration After the import, click Next , go to the Connection settings window,configure ad following: Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory: Spark jar files downloaded from the cluster Client Principal: Kerberos user name @HADOOP.com Keytab File: the keytab file downloaded from manager KDC Address: the KDC server IP(see the krb5.conf file) REALM: HADOOP.COM Kerberos Config File: the krb5 file downloaded from manager Hadoop\uff1a At the filter in upper right corner, search split , uncheck mapreduce.input.fileinputformat.split.maxsize Search classpath ,uncheck mapreduce.application.classpath Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive(or libs)Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1aadd the following two parameters for spark: park.driver.extraJavaOptions and spark.executor.extraJavaOptions The value can be found in manager GUI, choose Services->Spark2X Configuration->type all \uff0csearch extraJavaOptions in the search bar, choose the parameters in Spark2x->SparkResource2x Copy the values into a text file, replace the relative path ./ in the value with absolute path in the cluster, like /opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc , then copy the values into rapidminer spark configuration Hive: Hive Version: Hive Server2 Hive Server Address\uff1aHive Server IP Hive Port: 21066 Database Name: the database name created in Hive,here is rapidminer Customer database for UDFs: same as before click OK->Proced Anyway->Save","title":"RapidMiner Configuration"},{"location":"Data_Analysis/RapidMiner/#test-the-connection","text":"Click Configure, in Global tab, click Test\uff0cTest Results show as following: In Hadoop tab\uff0cclick Test,Test Results show as following: In Spark tab,click Test,Test Results show as following: In Hive tab, click Test, Test Results show as following: Click Full test,Test Results show as following:","title":"Test the Connection"},{"location":"Data_Analysis/RapidMiner/#radoop-demo","text":"In RapidMiner Studio main menu,choose Help->Tutorials->User Hadoop->Rapidminer Radoop Run the demo accordding to the Tutorials, get the follwing results","title":"Radoop Demo"},{"location":"Data_Integration/","text":"Data Integration \u00b6 Apache NiFi 1.7.1 \u2194 C80 Informatica PowerCenter 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 6.5 Talend 6.4.1 \u2194 C80 7.0.1 \u2194 C80","title":"Index"},{"location":"Data_Integration/#data-integration","text":"Apache NiFi 1.7.1 \u2194 C80 Informatica PowerCenter 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 6.5 Talend 6.4.1 \u2194 C80 7.0.1 \u2194 C80","title":"Data Integration"},{"location":"Data_Integration/Apache_NiFi/","text":"Connection Instruction between Apache NiFi and FusionInsight \u00b6 Succeeded Case \u00b6 Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka/Solr) Installing Apache NiFi \u00b6 Purpose \u00b6 Installing Apache NiFi 1.7.1 Prerequisites \u00b6 Installing FusionInsight HD cluster and its client completed Procedure \u00b6 Get JAVA_HOME configuration by execute source command on client side source /opt/hadoopclient/bigdata_env echo $JAVA_HOME Download NiFi installation file from https://nifi.apache.org/download.html , move the file to client side by using tool WinSCP , execute command unzip nifi-1.7.1-bin.zip to unzip the installation file to the following directory /usr/nifi/nifi-1.7.1 Configure NiFi server IP address and port by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 Start and Stop NiFi server cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop Start NiFi Server bin/nifi.sh start Configuring Kerberos authentication within NiFi \u00b6 Purpose \u00b6 Configuring Kerberos authentication within NiFi server for the later connection usage Prerequisites \u00b6 Installing Apache NiFi completed Installing FusionInsight HD cluster and its client completed Create a developuser for connection Procedure \u00b6 Download the required Kerberos authentication files user.keytab and krb5.conf from FusionInsight HD Manager site, save the files into the following directory /opt/developuser Configure Kerberos authentication by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file Detailed Configuration\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find KeytabCredentialsService and click ADD Click on **gear** icon to configure ![](assets/Apache_NiFi/markdown-img-paste-20180912174747644.png) ![](assets/Apache_NiFi/markdown-img-paste-2018091217482271.png) Click on **lightning** icon to enable and save the KeytabCredentialsService ![](assets/Apache_NiFi/markdown-img-paste-20180912174904147.png) ![](assets/Apache_NiFi/markdown-img-paste-20180912175037790.png) Completed Connecting NiFi to HDFS \u00b6 Purpose \u00b6 Configuring NiFi related HDFS processor to connect FusionInsight HD HDFS Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed PutHDFS Procedure \u00b6 Find and Copy the hdfs-site.xml \uff0c core-site.xml files which located in FusionInsight HD client to the following directory /usr/nifi/nifi-1.7.1/conf Make an adjustment to the content of hdfs-site.xml that is to delete the following property <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> Make an adjustment to the content of core-site.xml that is to change halcluster into detailed namenode ip with its port <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset The configuration of processor PutHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest The configuration of the connection between two former processors Move the file nifiHDFS.csv into the following directory /home/dataset before test start Content of nifiHDFS.csv \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - Test completed Log into FusionInsight HDFS to check the test outcome by using the following command hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest/HDFS The configuration of processor PutFile In detail\uff1a 1: /home/dataset/HDFS Move the file nifiHDFS.csv into HDFS directory /tmp/nifitest/HDFS Test completed Log into the FusionInsight HD client side to check the outcome with the directory /home/dataset/HDFS ListHDFS & FetchHDFS Procedure \u00b6 The whole process shown as the following pic: The configuration of processor ListHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest The configuration of processor RouteOnAttribute Note: Add one customized property requiredfilenames with the value ${filename:matches('sanguo.*')} by clicking on plus icon In detail\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} The relationship configuration between processor RouteOnAttribute and upper processor FetchHDFS shown as the following pic The relationship configuration between processor RouteOnAttribute and lower processor FetchHDFS shown as the following pic The configuration of processor FetchHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService The configuration of upper processor PutFile The configuration of lower processor PutFile Check the files on FusionInsight HDFS by executing command hdfs dfs -ls /tmp/nifitest Test completed Log into FusionInsight HD client side to check the outcomes separately Connecting NiFi to Hive \u00b6 Purpose \u00b6 Configuring NiFi Hive processor to connect FusionInsight HD Hive Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed HiveConnectionPool Procedure \u00b6 Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HiveConnectionPool and click ADD Click on gear icon to configure In detail 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService Click on lightning icon to enable and save the HiveConnectionPool Completed Create jaas.conf file which located at directory /usr/nifi/nifi-1.7.1/conf wit the following content Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; Make an adjustment to the bootstrap.conf file by executing following command vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true Make an adjustment to the nifi.properties file by executing following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true Execute the following command to come into the directory of NiFi Hive related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar SelectHiveQL read Hive table Procedure \u00b6 The whole process shown as the following pic: The configuration of processor SelectHiveQL In detail\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV The configuration of processor PutFile Log into FusionInsight cluster to check table t2 on hive Completed Check the outcome by log into the following directory /home/dataset/HIVE PutHiveQL load whole table Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2: iris.txt Content of iris.txt \uff1a 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa The configuration of processor PutHDFS In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive The configuration of processor ReplaceText In detail\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; The configuration of processor PutHiveQL Move the data file iris.txt into the following directory /home/dataset/ before test Completed: Login the HIVE to check the test outcome PutHiveQL Load the table by rows Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt Content of iris_add.txt \uff1a \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" The configuration of processor SplitText There is no change for the configuration of processor ExtractText The configuration of processor ReplaceText The configuration of processor PutHiveQL Move the data file iris_add.txt into the following directory /home/dataset/ before test Completed\uff1a Login the HIVE to check the test outcome\uff1a Connecting NiFi to HBase \u00b6 Purpose \u00b6 Configuring NiFi HBase processor to connect FusionInsight HD HBase Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed HBase_1_1_2_ClientService Procedure \u00b6 Move the hbase related configuration file hbase-site.xml which is within the FusionInsight HD client side into the following directory /usr/nifi/nifi-1.7.1/conf Execute the following command to come into the directory of NiFi HBase related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HBase_1_1_2_ClientService and click ADD Click on gear icon to configure In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService Click on lightining icon to enable and save the HBase_1_1_2_ClientService Completed PutHBaseJSON load the table Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile Content of hbase_test.csv \uff1a 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica The configuration of processor InverAvroSchema In detail\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data The configuration of processor ConvertCSVToAvro The configuration of processor ConvertAvroToJSON The configuration of processor SplitJson The configuration of processor PutHBaseJSON In detail: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data Move the data file hbase_test.csv into the following directory /home/dataset/HBASE before test In addition, execute following command to create a HBase table hbase shell create 'HBase_test','data' Completed\uff1a Login into the FusionInsight HD cluster to check the outcome: GetHbase Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetHBase The configuration of processor PutFile Completed Login into the following directory /home/dataset/GetHBase_test to check the test outcome Connecting NiFi to Spark \u00b6 Purpose \u00b6 Configuring NiFi Livy Session processor to connect FusionInsight HD Spark Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed Installing and configuring Apache Livy 0.5.0 (Apache Livy can be installed on test host or any other host as long as they can connect to each other including FusionInsight HD cluster) There exist connection instruction between Apache Livy and FusionInsight, please check the FusionInsight ecosystem LivySessionController Procedure \u00b6 Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: spark 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_PySpark In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: pysaprk 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_SparkR In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: sparkr 4\uff1aKeytabCredentialsService Click on lightining icon to enable and save the LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR Completed Spark Sample Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code1.txt Content of code1.txt \uff1a 1+2 The configuration of processor ExtractText Click plus icon to add a Property code1 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController 2: ${code1} Move the code file code1.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed\uff1a Log into the Livy server to check the outcome PySpark Sample Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code2.txt Content of code2.txt \uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) The configuration of processor ExtractText Click plus icon to add a Property code2 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController_PySpark 2: ${code2} Move the code file code2.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome SparkR Sample Procedure \u00b6 The whole process shown as the following pic: Note: It's different by comparing to example of former Spark and PySpark The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code3.txt Content of code3.txt \uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: /home/dataset/sparkTest 2: code content of code3.txt Move the code file code3.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome Connecting NiFi to Kafka \u00b6 Purpose \u00b6 Configuring NiFi Kafka processor to connect FusionInsight HD Kafka Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed GetHTTP & PutKafka Procedure \u00b6 The whole process shown as the following pic: The configuration of processor GetHTTP In detail\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv The configuration of processor PutKafka In detail\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi Before test\uff1a Log into the Kafka component within FusionInsightHD client side and create a Topic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 Completed\uff1a Log into the kafka component within FusionInsightHD client side to check the outcome cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning ConsumeKafka_0_11 Procedure \u00b6 The whole process shown as the following pic: The configuration of processor ConsumeKafka_0_11 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer The configuration of processor PutFile Before test\uff1a Open the kafka-examples which provided by FusionInsightHD client in eclipse, configure the kafka-examples so that it can be successfully ran and produce messages to kafka Note: There must be a producer when testing the NiFi ConsumeKafka_0_11 processor, run NewProducer.java within kafka-examples at first and then start to test NiFi ConsumeKafka_0_11 Completed\uff1a Log into the follow directory /home/dataset/Kafka to check the test outcome Connecting NiFi to Kafka with security mode \u00b6 Purpose \u00b6 Configuring NiFi Kafka processor to connect FusionInsight HD Kafka with port 21007 Prerequisites \u00b6 Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Complete NiFi Kerberos configuration nifi host ip: 172.16.2.119, FI HD ip: 172.16.6.10-12 Kerberos authentication related operation steps \u00b6 Create a jaas.conf file in the nifi host /opt path, with the content\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; Stop nifi with the command bin/nifi.sh stop Find the corresponding kafka client jar package in the kafka client of FI HD\uff0cfor example: /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar Find the original kafka client jar package on /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-kafka-1-0-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies with the name kafka-clients-1.1.0.jar. Rename it into kafka-clients-0.11.0.1.jar.org. And copy the FI HD corresponding kafka-clients-1.1.0.jar into this directory: Login to nifi server\uff0cuse the following command at first to load the environment variables source /opt/hadoopclient/bigdata_env \uff0cThen use the following command to load the jvm parameters for java running: export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf\" /etc/krb5.conf is the authentication krb5.conf file corresponding to the connecting FI cluster After completing the above steps, you can use the command java -version to check whether the jvm parameters are successfully loaded: Use command bin/nifi.sh start to start nifi: PublishKafka_1_0 Sample \u00b6 The entire workflow is\uff1a The configuration of the processor GetHTTP is as follows\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - The configuration of the processor PublishKafka_1_0 is as follows\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: Kafka 4: KeytabCredentialsService 5: testtopic 6: Guarantee Replicated Delivery Running the entire workflow\uff1a Check the outcome on kafka: ConsumeKafka_1_0 Sample \u00b6 The entire workflow is\uff1a The configuration of the processor ConsumeKafka_1_0 is as follows\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: kafka 4: KeytabCredentialsService 5: testtopic 6: Demo The configuration of the processor PutFile is as follows\uff1a Running the entire workflow\uff1a Use command on Kafka client to insert some data: ./bin/kafka-console-producer.sh --broker-list 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 --topic testtopic --producer.config config/producer.properties Login to nifi host /opt/nifikafka21007 to check the outcome NiFi connect to Solr with security mode \u00b6 Test environmental description \u00b6 FI HD: 172.16.4.121-123 NIFI: 172.17.2.124 FI HD related configuration: \u00b6 Refer to the solr section of the product documentation and do the following configuration (optional)\uff1a Use the following curl command to create a collection named nifi_test on FusionInsight Solr curl --negotiate -k -v -u : \"https://172.16.4.122:21101/solr/admin/collections?action=CREATE&name=nifi_test&collection.configName=confWithSchema&numShards=3&replicationFactor=1\" Log in to the fusioninsight client, log in to kadmin as shown in the screenshot below, and add the HTTP service principal of 3 nodes. Note 1: Follow the steps shown as the pic below Note 2: The initial password is Admin@123 when executing the kadmin \u2013p kadmin/admin command, the new password must be kept in mind after modification Log in to the oracle official website to obtain JCE, and adapt it to the FI HD cluster Explanation: Because kerberos checksum encryption and decryption uses a key length that far beyond the default secure character length of jre, you need to download the Java Cryptography Extension (JCE) from the java official website: https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html , then extract it to %JAVA_HOME%/jre/lib/security and replace the corresponding file. The specific operation is as follows: decompress the downloaded jce, and copy the two decompressed jar packages US_export_policy.jar and local_policy.jar to /opt/huawei/Bigdata/common/runtime0/jdk-8u201/jre/lib/security/ path of every FI HD node(172.16.4.121,172.16.4.122,172.16.4.123), and restart the entire solr service NiFi related configuration: \u00b6 nifi.properties file configuration changes\uff1a web properties part\uff1a kerberos part\uff1a Add sasl configuration\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true bootstrap.conf file configuration changes\uff1a Add one jvm parameter: java.arg.17=-Djava.security.auth.login.config=/opt/jaas.conf The jaas.conf content is: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; Because it is connected to solr, we need to find the path of nifi's solr dependency package directory, taking my machine as an example, it is\uff1a /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-solr-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Rename the original zookeeper-3.4.6.jar to zookeeper-3.4.6.jar.org, and copy the FI HD corresponding zookeeper-3.5.1.jar into this directory SSL certificate related configuration \u00b6 Note: Because the FI HD solr is deployed in a secure mode, when using the rest interface to interact, it is required to pass the SSL layer authentication. You need to create a corresponding certificate (truststore). After completion, you need to use spnego to interact with the cluster solr. The following describes two methods of obtaining authentication certificates, which correspond to the two connect methods of Solr, Cloud and Standard, respectively. huawei-huawei certificate Login to the Linux background (requires openssl), use the following command: openssl s_client -host 172.16.4.122 -port 21101 -prexit -showcerts\\ There will be three segments of certificates: huawei-huawei, huawei-FusionInsight, FusionInsight-172.16.4.122. We need the huawei-huawei part. Copy part of huawei-huawei to a new file: /opt/ssltest/huawei-huawei.pem Use the command keytool -import -alias gca -file /opt/ssltest/huawei-huawei.pem -keystore /opt/ssltest/truststore to add the content of the huawei-huawei.pem certificate generated in the previous step to /opt/ssltest/truststore file, the password entered during the process is changeit , enter yes at the last step Certificate chain\uff0cIt is known from the previous step that a certificate chain will be generated, which contains three segments. This step is to generate a new truststore_huawei file for the entire certificate chain(all three segements). Use the following command echo \"\" | openssl s_client -host 172.16.4.122 -port 21101 -showcerts | awk '/BEGIN CERT/ {p=1} ; p==1; /END CERT/ {p=0}' > /opt/ssltest/allcerts122.pem to redirect the entire certificate chain into the following pem file: /opt/ssltest/allcerts122.pem Use the command keytool -import -alias gca -file /opt/ssltest/allcerts122.pem -keystore /opt/ssltest/truststore_chain to add the content of the allcerts122.pem certificate generated in the previous step to /opt/ssltest/truststore_chain file\uff0che password entered during the process is changeit , enter yes at the last step NiFi PutSolrContentStream STANDARD mode Configuration \u00b6 Note: Standard mode directly connects to Solr service through HTTPS. SSL requires certificate chain truststore_chain Configure KeytabCredentialsService Configure StandardRestrictedSSLContextService Note: Rename this contoller to CHAINStandardRestrictedSSLContextService for easy differentiation 1. /opt/ssltest/truststore_chain 2. changeit 3. JKS 4. TLS The entire PutSolrContentStream workflow is shown in Figure\uff1a GenerateFlowFile Configuration: { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream Configuration\uff1a 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService Start the entire workflow Log in to the Cluster Manager interface and log in to the solradmin webUI to view the results\uff1a NiFi PutSolrContentStream CLOUD mode Configuration \u00b6 Note: Cloud mode first connects to the cluster zookeeper service and then reads the solr connection information to connect to the solr service. SSL only requires the huawei-huawei certificate truststore. Configure KeytabCredentialsService Configure StandardRestrictedSSLContextService 1. /opt/ssltest/truststore 2. changeit 3. JKS 4. TLS The entire PutSolrContentStream workflow is shown in Figure\uff1a GenerateFlowFile Configuration\uff1a { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream Configuration: 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService Start the entire workflow Log in to the Cluster Manager interface and log in to the solradmin webUI to view the results\uff1a NIFI QuerySolr Configuration \u00b6 Note: QuerySolr can be connected to Solr in either Standard or Cloud mode. The difference is that the requested Solor Location and SSL certificate configuration are different. The others are the same. The entire PutSolrContentStream workflow is shown in Figure\uff1a QuerySolr Standard mode Configuration: 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService QuerySolr Cloud mode Configuration: 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService PutFile Configuration: Start the entire workflow: Log in to the background to view the results:","title":"1.7.1 <--> C80"},{"location":"Data_Integration/Apache_NiFi/#connection-instruction-between-apache-nifi-and-fusioninsight","text":"","title":"Connection Instruction between Apache NiFi and FusionInsight"},{"location":"Data_Integration/Apache_NiFi/#succeeded-case","text":"Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka/Solr)","title":"Succeeded Case"},{"location":"Data_Integration/Apache_NiFi/#installing-apache-nifi","text":"","title":"Installing Apache NiFi"},{"location":"Data_Integration/Apache_NiFi/#purpose","text":"Installing Apache NiFi 1.7.1","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites","text":"Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#procedure","text":"Get JAVA_HOME configuration by execute source command on client side source /opt/hadoopclient/bigdata_env echo $JAVA_HOME Download NiFi installation file from https://nifi.apache.org/download.html , move the file to client side by using tool WinSCP , execute command unzip nifi-1.7.1-bin.zip to unzip the installation file to the following directory /usr/nifi/nifi-1.7.1 Configure NiFi server IP address and port by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 Start and Stop NiFi server cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop Start NiFi Server bin/nifi.sh start","title":"Procedure"},{"location":"Data_Integration/Apache_NiFi/#configuring-kerberos-authentication-within-nifi","text":"","title":"Configuring Kerberos authentication within NiFi"},{"location":"Data_Integration/Apache_NiFi/#purpose_1","text":"Configuring Kerberos authentication within NiFi server for the later connection usage","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_1","text":"Installing Apache NiFi completed Installing FusionInsight HD cluster and its client completed Create a developuser for connection","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#procedure_1","text":"Download the required Kerberos authentication files user.keytab and krb5.conf from FusionInsight HD Manager site, save the files into the following directory /opt/developuser Configure Kerberos authentication by execute following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties and adjust the propeties within the nifi.properties file Detailed Configuration\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find KeytabCredentialsService and click ADD Click on **gear** icon to configure ![](assets/Apache_NiFi/markdown-img-paste-20180912174747644.png) ![](assets/Apache_NiFi/markdown-img-paste-2018091217482271.png) Click on **lightning** icon to enable and save the KeytabCredentialsService ![](assets/Apache_NiFi/markdown-img-paste-20180912174904147.png) ![](assets/Apache_NiFi/markdown-img-paste-20180912175037790.png) Completed","title":"Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-hdfs","text":"","title":"Connecting NiFi to HDFS"},{"location":"Data_Integration/Apache_NiFi/#purpose_2","text":"Configuring NiFi related HDFS processor to connect FusionInsight HD HDFS","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_2","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#puthdfs-procedure","text":"Find and Copy the hdfs-site.xml \uff0c core-site.xml files which located in FusionInsight HD client to the following directory /usr/nifi/nifi-1.7.1/conf Make an adjustment to the content of hdfs-site.xml that is to delete the following property <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> Make an adjustment to the content of core-site.xml that is to change halcluster into detailed namenode ip with its port <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset The configuration of processor PutHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest The configuration of the connection between two former processors Move the file nifiHDFS.csv into the following directory /home/dataset before test start Content of nifiHDFS.csv \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - Test completed Log into FusionInsight HDFS to check the test outcome by using the following command hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv","title":"PutHDFS Procedure"},{"location":"Data_Integration/Apache_NiFi/#gethdfs-procedure","text":"The whole process shown as the following pic: The configuration of processor GetHDFS In detail\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: Choose KeytabCredentialsService which was completed in previous section 3: /tmp/nifitest/HDFS The configuration of processor PutFile In detail\uff1a 1: /home/dataset/HDFS Move the file nifiHDFS.csv into HDFS directory /tmp/nifitest/HDFS Test completed Log into the FusionInsight HD client side to check the outcome with the directory /home/dataset/HDFS","title":"GetHDFS Procedure"},{"location":"Data_Integration/Apache_NiFi/#listhdfs-fetchhdfs-procedure","text":"The whole process shown as the following pic: The configuration of processor ListHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest The configuration of processor RouteOnAttribute Note: Add one customized property requiredfilenames with the value ${filename:matches('sanguo.*')} by clicking on plus icon In detail\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} The relationship configuration between processor RouteOnAttribute and upper processor FetchHDFS shown as the following pic The relationship configuration between processor RouteOnAttribute and lower processor FetchHDFS shown as the following pic The configuration of processor FetchHDFS In detail\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService The configuration of upper processor PutFile The configuration of lower processor PutFile Check the files on FusionInsight HDFS by executing command hdfs dfs -ls /tmp/nifitest Test completed Log into FusionInsight HD client side to check the outcomes separately","title":"ListHDFS &amp; FetchHDFS Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-hive","text":"","title":"Connecting NiFi to Hive"},{"location":"Data_Integration/Apache_NiFi/#purpose_3","text":"Configuring NiFi Hive processor to connect FusionInsight HD Hive","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_3","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#hiveconnectionpool-procedure","text":"Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HiveConnectionPool and click ADD Click on gear icon to configure In detail 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService Click on lightning icon to enable and save the HiveConnectionPool Completed Create jaas.conf file which located at directory /usr/nifi/nifi-1.7.1/conf wit the following content Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; Make an adjustment to the bootstrap.conf file by executing following command vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true Make an adjustment to the nifi.properties file by executing following command vi /usr/nifi/nifi-1.7.1/conf/nifi.properties nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true Execute the following command to come into the directory of NiFi Hive related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar","title":"HiveConnectionPool Procedure"},{"location":"Data_Integration/Apache_NiFi/#selecthiveql-read-hive-table-procedure","text":"The whole process shown as the following pic: The configuration of processor SelectHiveQL In detail\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV The configuration of processor PutFile Log into FusionInsight cluster to check table t2 on hive Completed Check the outcome by log into the following directory /home/dataset/HIVE","title":"SelectHiveQL read Hive table Procedure"},{"location":"Data_Integration/Apache_NiFi/#puthiveql-load-whole-table-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2: iris.txt Content of iris.txt \uff1a 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa The configuration of processor PutHDFS In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive The configuration of processor ReplaceText In detail\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; The configuration of processor PutHiveQL Move the data file iris.txt into the following directory /home/dataset/ before test Completed: Login the HIVE to check the test outcome","title":"PutHiveQL load whole table Procedure"},{"location":"Data_Integration/Apache_NiFi/#puthiveql-load-the-table-by-rows-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt Content of iris_add.txt \uff1a \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" The configuration of processor SplitText There is no change for the configuration of processor ExtractText The configuration of processor ReplaceText The configuration of processor PutHiveQL Move the data file iris_add.txt into the following directory /home/dataset/ before test Completed\uff1a Login the HIVE to check the test outcome\uff1a","title":"PutHiveQL Load the table by rows Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-hbase","text":"","title":"Connecting NiFi to HBase"},{"location":"Data_Integration/Apache_NiFi/#purpose_4","text":"Configuring NiFi HBase processor to connect FusionInsight HD HBase","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_4","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#hbase_1_1_2_clientservice-procedure","text":"Move the hbase related configuration file hbase-site.xml which is within the FusionInsight HD client side into the following directory /usr/nifi/nifi-1.7.1/conf Execute the following command to come into the directory of NiFi HBase related library cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Substitute zookeeper-3.5.1.jar which is from FusionInsight HD client side for the original zookeeper-3.4.6.jar Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find HBase_1_1_2_ClientService and click ADD Click on gear icon to configure In detail\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService Click on lightining icon to enable and save the HBase_1_1_2_ClientService Completed","title":"HBase_1_1_2_ClientService Procedure"},{"location":"Data_Integration/Apache_NiFi/#puthbasejson-load-the-table-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile Content of hbase_test.csv \uff1a 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica The configuration of processor InverAvroSchema In detail\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data The configuration of processor ConvertCSVToAvro The configuration of processor ConvertAvroToJSON The configuration of processor SplitJson The configuration of processor PutHBaseJSON In detail: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data Move the data file hbase_test.csv into the following directory /home/dataset/HBASE before test In addition, execute following command to create a HBase table hbase shell create 'HBase_test','data' Completed\uff1a Login into the FusionInsight HD cluster to check the outcome:","title":"PutHBaseJSON load the table Procedure"},{"location":"Data_Integration/Apache_NiFi/#gethbase-procedure","text":"The whole process shown as the following pic: The configuration of processor GetHBase The configuration of processor PutFile Completed Login into the following directory /home/dataset/GetHBase_test to check the test outcome","title":"GetHbase Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-spark","text":"","title":"Connecting NiFi to Spark"},{"location":"Data_Integration/Apache_NiFi/#purpose_5","text":"Configuring NiFi Livy Session processor to connect FusionInsight HD Spark","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_5","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed Installing and configuring Apache Livy 0.5.0 (Apache Livy can be installed on test host or any other host as long as they can connect to each other including FusionInsight HD cluster) There exist connection instruction between Apache Livy and FusionInsight, please check the FusionInsight ecosystem","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#livysessioncontroller-procedure","text":"Enter NiFi Web UI site, right click on canvas and click on Configure icon Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: spark 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_PySpark In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: pysaprk 4\uff1aKeytabCredentialsService Click on plus icon to add the service Find LivySessionController and click ADD Click on gear icon to configure Change the name of Controller as LivySessionController_SparkR In detail\uff1a 1: 172.21.3.43 (host ip for Apache Livy) 2: 8998 (Livy default port, can be changed) 3: sparkr 4\uff1aKeytabCredentialsService Click on lightining icon to enable and save the LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR Completed","title":"LivySessionController Procedure"},{"location":"Data_Integration/Apache_NiFi/#spark-sample-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code1.txt Content of code1.txt \uff1a 1+2 The configuration of processor ExtractText Click plus icon to add a Property code1 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController 2: ${code1} Move the code file code1.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed\uff1a Log into the Livy server to check the outcome","title":"Spark Sample Procedure"},{"location":"Data_Integration/Apache_NiFi/#pyspark-sample-procedure","text":"The whole process shown as the following pic: The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code2.txt Content of code2.txt \uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) The configuration of processor ExtractText Click plus icon to add a Property code2 with its Value as $ The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: LivySessionController_PySpark 2: ${code2} Move the code file code2.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome","title":"PySpark Sample Procedure"},{"location":"Data_Integration/Apache_NiFi/#sparkr-sample-procedure","text":"The whole process shown as the following pic: Note: It's different by comparing to example of former Spark and PySpark The configuration of processor GetFile In detail\uff1a 1: /home/dataset/sparkTest 2: code3.txt Content of code3.txt \uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) The configuration of processor ExecuteSparkInteractive In detail\uff1a 1: /home/dataset/sparkTest 2: code content of code3.txt Move the code file code3.txt into the following directory /home/dataset/sparkTest before test Start the Livy server Completed Log into the Livy server to check the outcome","title":"SparkR Sample Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-kafka","text":"","title":"Connecting NiFi to Kafka"},{"location":"Data_Integration/Apache_NiFi/#purpose_6","text":"Configuring NiFi Kafka processor to connect FusionInsight HD Kafka","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_6","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Configuring Kerberos authentication within NiFi completed","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#gethttp-putkafka-procedure","text":"The whole process shown as the following pic: The configuration of processor GetHTTP In detail\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv The configuration of processor PutKafka In detail\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi Before test\uff1a Log into the Kafka component within FusionInsightHD client side and create a Topic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 Completed\uff1a Log into the kafka component within FusionInsightHD client side to check the outcome cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning","title":"GetHTTP &amp; PutKafka Procedure"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_0_11-procedure","text":"The whole process shown as the following pic: The configuration of processor ConsumeKafka_0_11 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer The configuration of processor PutFile Before test\uff1a Open the kafka-examples which provided by FusionInsightHD client in eclipse, configure the kafka-examples so that it can be successfully ran and produce messages to kafka Note: There must be a producer when testing the NiFi ConsumeKafka_0_11 processor, run NewProducer.java within kafka-examples at first and then start to test NiFi ConsumeKafka_0_11 Completed\uff1a Log into the follow directory /home/dataset/Kafka to check the test outcome","title":"ConsumeKafka_0_11 Procedure"},{"location":"Data_Integration/Apache_NiFi/#connecting-nifi-to-kafka-with-security-mode","text":"","title":"Connecting NiFi to Kafka with security mode"},{"location":"Data_Integration/Apache_NiFi/#purpose_7","text":"Configuring NiFi Kafka processor to connect FusionInsight HD Kafka with port 21007","title":"Purpose"},{"location":"Data_Integration/Apache_NiFi/#prerequisites_7","text":"Installing NiFi 1.7.1 completed Installing FusionInsight HD cluster and its client completed Complete NiFi Kerberos configuration nifi host ip: 172.16.2.119, FI HD ip: 172.16.6.10-12","title":"Prerequisites"},{"location":"Data_Integration/Apache_NiFi/#kerberos-authentication-related-operation-steps","text":"Create a jaas.conf file in the nifi host /opt path, with the content\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; Stop nifi with the command bin/nifi.sh stop Find the corresponding kafka client jar package in the kafka client of FI HD\uff0cfor example: /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar Find the original kafka client jar package on /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-kafka-1-0-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies with the name kafka-clients-1.1.0.jar. Rename it into kafka-clients-0.11.0.1.jar.org. And copy the FI HD corresponding kafka-clients-1.1.0.jar into this directory: Login to nifi server\uff0cuse the following command at first to load the environment variables source /opt/hadoopclient/bigdata_env \uff0cThen use the following command to load the jvm parameters for java running: export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf\" /etc/krb5.conf is the authentication krb5.conf file corresponding to the connecting FI cluster After completing the above steps, you can use the command java -version to check whether the jvm parameters are successfully loaded: Use command bin/nifi.sh start to start nifi:","title":"Kerberos authentication related operation steps"},{"location":"Data_Integration/Apache_NiFi/#publishkafka_1_0-sample","text":"The entire workflow is\uff1a The configuration of the processor GetHTTP is as follows\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - The configuration of the processor PublishKafka_1_0 is as follows\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: Kafka 4: KeytabCredentialsService 5: testtopic 6: Guarantee Replicated Delivery Running the entire workflow\uff1a Check the outcome on kafka:","title":"PublishKafka_1_0 Sample"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_1_0-sample","text":"The entire workflow is\uff1a The configuration of the processor ConsumeKafka_1_0 is as follows\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: kafka 4: KeytabCredentialsService 5: testtopic 6: Demo The configuration of the processor PutFile is as follows\uff1a Running the entire workflow\uff1a Use command on Kafka client to insert some data: ./bin/kafka-console-producer.sh --broker-list 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 --topic testtopic --producer.config config/producer.properties Login to nifi host /opt/nifikafka21007 to check the outcome","title":"ConsumeKafka_1_0 Sample"},{"location":"Data_Integration/Apache_NiFi/#nifi-connect-to-solr-with-security-mode","text":"","title":"NiFi connect to Solr with security mode"},{"location":"Data_Integration/Apache_NiFi/#test-environmental-description","text":"FI HD: 172.16.4.121-123 NIFI: 172.17.2.124","title":"Test environmental description"},{"location":"Data_Integration/Apache_NiFi/#fi-hd-related-configuration","text":"Refer to the solr section of the product documentation and do the following configuration (optional)\uff1a Use the following curl command to create a collection named nifi_test on FusionInsight Solr curl --negotiate -k -v -u : \"https://172.16.4.122:21101/solr/admin/collections?action=CREATE&name=nifi_test&collection.configName=confWithSchema&numShards=3&replicationFactor=1\" Log in to the fusioninsight client, log in to kadmin as shown in the screenshot below, and add the HTTP service principal of 3 nodes. Note 1: Follow the steps shown as the pic below Note 2: The initial password is Admin@123 when executing the kadmin \u2013p kadmin/admin command, the new password must be kept in mind after modification Log in to the oracle official website to obtain JCE, and adapt it to the FI HD cluster Explanation: Because kerberos checksum encryption and decryption uses a key length that far beyond the default secure character length of jre, you need to download the Java Cryptography Extension (JCE) from the java official website: https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html , then extract it to %JAVA_HOME%/jre/lib/security and replace the corresponding file. The specific operation is as follows: decompress the downloaded jce, and copy the two decompressed jar packages US_export_policy.jar and local_policy.jar to /opt/huawei/Bigdata/common/runtime0/jdk-8u201/jre/lib/security/ path of every FI HD node(172.16.4.121,172.16.4.122,172.16.4.123), and restart the entire solr service","title":"FI HD related configuration:"},{"location":"Data_Integration/Apache_NiFi/#nifi-related-configuration","text":"nifi.properties file configuration changes\uff1a web properties part\uff1a kerberos part\uff1a Add sasl configuration\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true bootstrap.conf file configuration changes\uff1a Add one jvm parameter: java.arg.17=-Djava.security.auth.login.config=/opt/jaas.conf The jaas.conf content is: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; Because it is connected to solr, we need to find the path of nifi's solr dependency package directory, taking my machine as an example, it is\uff1a /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-solr-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies Rename the original zookeeper-3.4.6.jar to zookeeper-3.4.6.jar.org, and copy the FI HD corresponding zookeeper-3.5.1.jar into this directory","title":"NiFi related configuration:"},{"location":"Data_Integration/Apache_NiFi/#ssl-certificate-related-configuration","text":"Note: Because the FI HD solr is deployed in a secure mode, when using the rest interface to interact, it is required to pass the SSL layer authentication. You need to create a corresponding certificate (truststore). After completion, you need to use spnego to interact with the cluster solr. The following describes two methods of obtaining authentication certificates, which correspond to the two connect methods of Solr, Cloud and Standard, respectively. huawei-huawei certificate Login to the Linux background (requires openssl), use the following command: openssl s_client -host 172.16.4.122 -port 21101 -prexit -showcerts\\ There will be three segments of certificates: huawei-huawei, huawei-FusionInsight, FusionInsight-172.16.4.122. We need the huawei-huawei part. Copy part of huawei-huawei to a new file: /opt/ssltest/huawei-huawei.pem Use the command keytool -import -alias gca -file /opt/ssltest/huawei-huawei.pem -keystore /opt/ssltest/truststore to add the content of the huawei-huawei.pem certificate generated in the previous step to /opt/ssltest/truststore file, the password entered during the process is changeit , enter yes at the last step Certificate chain\uff0cIt is known from the previous step that a certificate chain will be generated, which contains three segments. This step is to generate a new truststore_huawei file for the entire certificate chain(all three segements). Use the following command echo \"\" | openssl s_client -host 172.16.4.122 -port 21101 -showcerts | awk '/BEGIN CERT/ {p=1} ; p==1; /END CERT/ {p=0}' > /opt/ssltest/allcerts122.pem to redirect the entire certificate chain into the following pem file: /opt/ssltest/allcerts122.pem Use the command keytool -import -alias gca -file /opt/ssltest/allcerts122.pem -keystore /opt/ssltest/truststore_chain to add the content of the allcerts122.pem certificate generated in the previous step to /opt/ssltest/truststore_chain file\uff0che password entered during the process is changeit , enter yes at the last step","title":"SSL certificate related configuration"},{"location":"Data_Integration/Apache_NiFi/#nifi-putsolrcontentstream-standard-mode-configuration","text":"Note: Standard mode directly connects to Solr service through HTTPS. SSL requires certificate chain truststore_chain Configure KeytabCredentialsService Configure StandardRestrictedSSLContextService Note: Rename this contoller to CHAINStandardRestrictedSSLContextService for easy differentiation 1. /opt/ssltest/truststore_chain 2. changeit 3. JKS 4. TLS The entire PutSolrContentStream workflow is shown in Figure\uff1a GenerateFlowFile Configuration: { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream Configuration\uff1a 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService Start the entire workflow Log in to the Cluster Manager interface and log in to the solradmin webUI to view the results\uff1a","title":"NiFi PutSolrContentStream STANDARD mode Configuration"},{"location":"Data_Integration/Apache_NiFi/#nifi-putsolrcontentstream-cloud-mode-configuration","text":"Note: Cloud mode first connects to the cluster zookeeper service and then reads the solr connection information to connect to the solr service. SSL only requires the huawei-huawei certificate truststore. Configure KeytabCredentialsService Configure StandardRestrictedSSLContextService 1. /opt/ssltest/truststore 2. changeit 3. JKS 4. TLS The entire PutSolrContentStream workflow is shown in Figure\uff1a GenerateFlowFile Configuration\uff1a { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream Configuration: 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService Start the entire workflow Log in to the Cluster Manager interface and log in to the solradmin webUI to view the results\uff1a","title":"NiFi PutSolrContentStream CLOUD mode Configuration"},{"location":"Data_Integration/Apache_NiFi/#nifi-querysolr-configuration","text":"Note: QuerySolr can be connected to Solr in either Standard or Cloud mode. The difference is that the requested Solor Location and SSL certificate configuration are different. The others are the same. The entire PutSolrContentStream workflow is shown in Figure\uff1a QuerySolr Standard mode Configuration: 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService QuerySolr Cloud mode Configuration: 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService PutFile Configuration: Start the entire workflow: Log in to the background to view the results:","title":"NIFI QuerySolr Configuration"},{"location":"Data_Integration/IBM_InfoSphere_CDC/","text":"IBM InfoSphere CDC connects to FusionInsight \u00b6 Applicable scene \u00b6 IBM InfoSphere CDC 11.3.3.1 \u2194 FusionInsight HD V100R002C50 (HDFS)","title":"11.3.3.1 <--> C50"},{"location":"Data_Integration/IBM_InfoSphere_CDC/#ibm-infosphere-cdc-connects-to-fusioninsight","text":"","title":"IBM InfoSphere CDC connects to FusionInsight"},{"location":"Data_Integration/IBM_InfoSphere_CDC/#applicable-scene","text":"IBM InfoSphere CDC 11.3.3.1 \u2194 FusionInsight HD V100R002C50 (HDFS)","title":"Applicable scene"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/","text":"IBM InfoSphere DataStage connects to FusionInsight \u00b6 Applicable scene \u00b6 IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 (HDFS/Hive/SparkSQL) IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Phoenix/SparkSQL/Kafka/GaussDB) Prerequisites \u00b6 The installation and deployment of IBM InfoSphere DataStage 11.5.0.2 has been completed (this article is deployed on Centos7.2) The deployment of FusionInsight cluster has been completed, version FusionInsight HD V100R002C60U20 Ready to work \u00b6 Configure domain name resolution \u00b6 Use the vi /etc/hosts command to modify the hosts files of DataStage Server and Client, and add FI cluster node information, such as: 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3 Configure Kerberos authentication \u00b6 Create a DataStage docking user in the FI management interface, grant the user the required permissions, and download the authentication credentials Decompress the downloaded tar file to get the Kerberos configuration file krb5.conf and the user's keytab file. Log in to the DataStage Server node as root, and copy the krb5.conf file of the FI cluster to the /etc directory. Upload the user's user.keytab file to any directory of the DataStage Server node, such as /home/dsadm . Install FusionInsight Client \u00b6 Refer to FI product documentation, download the complete client on the FI service management interface, upload it to DataStageServer, and install it to a custom directory, such as /opt/ficlient . Docking HDFS \u00b6 Import the SSL certificate of the FI cluster \u00b6 The browser exports the root certificate of the FI cluster The browser opens the FI management interface, view the certificate, click the \"Certificate Path\" tab, select the root path, view the root certificate, under the \"Details\" tab, click \"Copy to File\" to export to cer format Import the certificate into the keystore file of DataStage Upload the exported FI root certificate fi-root-ca.cer to the DataStage server, for example, under the path /home/dsadm , import the certificate into the keystore file, command reference: /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca. cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks Generate and save the encrypted keystore password Use the vi /home/dsadm/authenticate.properties command to create a new configuration file and save the cipher text generated in the previous step: password={iisenc}SvtJ2f/uNTrvbuh26XDzag== Execute chown dsadm:dstage /home/dsadm/ authenticate.properties to modify the owner of the configuration file Export truststore environment variables Use vi /opt/IBM/InformationServer/Server/DSEngine/dsenv to edit the environment variables of DSEngine and add at the end export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties Restart DSEngine, refer to the command su-dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start Read HDFS file \u00b6 Create assignment Create a new parallel job and save it as hdfs2sf Add File_Connector component and Sequential File component, and File_Connector to Sequential File link Refer to the figure below to modify the configuration Compile and run After saving the configuration, compile and run Open the Director client in the menu Tools -> Run Director to view the job log View the read data Write HDFS file \u00b6 Create assignment Create a new parallel job and save it as hdfswrite Add Row Generator component and File Connector component, and Row Generator to File Connector link Refer to the figure below to modify the configuration Compile and run Save \u2014 Compile \u2014 Run , view the job log: View written data Docking Hive \u00b6 Use Hive Connector \u00b6 Note: The Hive JDBC Driver officially certified by Hive Connector is only DataDirect Hive Driver (IShive.jar). When using the IShive.jar included in DataStage 11.5.0.2 to connect to FusionInsight's hive, there will be a thrift protocol error. You need to consult IBM technology. Support the latest IShive.jar provided Set JDBC Driver configuration file \u00b6 Create the isjdbc.config file under the $DSHOME path, add the path of DataDirect Hive Driver (IShive.jar) to the CLASSPATH variable, and add com.ibm.isf.jdbc.hive.HiveDriver to the CLASS_NAMES variable, refer to the command: su-dsadm cd $DSHOME vi isjdbc.config Add the following information in isjdbc.config: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver Configure Kerberos authentication information: Create JDBCDriverLogin.conf in the directory where IShive.jar is located cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf The contents of the file are as follows: JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; Read Hive data \u00b6 Create assignment Change setting URL reference is configured as follows: jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; Where JDBC_DRIVER_test_keytab is the authentication information specified in the previous step Compile and run Save \u2014 Compile \u2014 Run , view the job log: View the read data Write data to Hive table \u00b6 Create assignment Change setting Compile and run Save \u2014 Compile \u2014 Run , view the job log, write 10 pieces of data, 2\u201911\u201d View Hive table data: Hive Connector uses the Insert statement to write data to the Hive table. Every time a piece of data is inserted, an MR task will be started. The efficiency is particularly low. This method is not recommended. You can write data directly to HDFS files. Use JDBC Connector \u00b6 If you want to use FusionInsight's Hive JDBC driver, add the jdbc driver and dependent packages in the CLASSPATH of the isjdbc.config file. The following error will be reported when you run the job. At this time, you need to load it by exporting the CLASSPATH environment variable. And you can only use JDBC Connector, not Hive Connector, otherwise the following error will be reported Set the CLASSPATH environment variable \u00b6 The Hive jdbc driver package and dependent packages are located in the lib directory of the Hive client /opt/ficlient/Hive/Beeline/lib . If the client is not installed, these jar packages can be uploaded to any directory separately. Set the CLASSPATH environment variable, add the full path of the above jar package, refer to the command: su-dsadm vi $DSHOME/dsenv Add the relevant jar package at the end of the file (the specific path is adjusted according to the actual environment) export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/ Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3. jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive /Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7 .2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar :/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/ Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc- 1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastor e-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar: /opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient /Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/ opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j- 1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt /ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper -3.5.1.jar Import environment variables source $DSHOME/dsenv Restart DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start Read Hive data \u00b6 Create assignment Change setting The URL is: jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM; user.keytab=/home/dsadm/user.keytab; Compile and run Write data to Hive table \u00b6 Create assignment Change setting Compile and run Write 5 pieces of data, 1\u201949\" Import data into HDFS file of Hive table \u00b6 Create assignment Change setting Compile and run View written data Hive table data increment 100 Incremental data is regularly and automatically imported into HDFS files of Hive tables \u00b6 Incremental data can be imported into hive by adding HDFS files. If it is to be automatically executed on a regular basis, the imported file name needs to include variable parameters for setting and distinguishing, and then run the job in command or script mode to assign values \u200b\u200bto the parameters. Create assignment Set job parameters Click the \"job properties\" button and set the parameters as follows Change setting File Connector configures the name of the export file, and refers to the set parameters with \"#\" dsjob command to run the job Save the compilation job and execute the dsjob -run command on the DataStage Server in the format: dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME Command reference: su-dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append View HDFS files: View Hive data increment is 200 Docking SparkSQL \u00b6 Similar to using the FI Hive JDBC driver, you can use the SparkSQL JDBC driver to connect to Hive. You also need to export the CLASSPATH environment variable to load the driver package and dependent packages. SparkSQL jdbc does not support the insert into statement, it can only be used to read hive data, but cannot insert data into hive tables. Set the CLASSPATH environment variable \u00b6 The SparkSQL jdbc driver package and dependent packages are located in the Spark client lib directory /opt/ficlient/Spark/spark/lib/ . If the client is not installed, you can also upload the required jar package separately to any directory. Set the CLASSPATH environment variable, add the full path of the above jar package, and the spark client configuration file path (SparkSQL jdbc needs to read the configuration in hive-site.xml when connecting to hive): su-dsadm vi $DSHOME/dsenv Configure the following: export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/ spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7. 1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark /spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce -client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2 .1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark .jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/ opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Sp ark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17. jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark /spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf Import environment variables source $DSHOME/dsenv Restart DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start Read Hive table data \u00b6 Create assignment Change setting URL reference: jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP. COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; Compile and run Docking Phoenix \u00b6 To use Phoenix to access HBase tables in JDBC mode, you also need to export the CLASSPATH environment variable to load the driver package and dependent packages. Set the CLASSPATH environment variable \u00b6 Phoenix-related jar packages are located in the lib directory of the HBase client /opt/ficlient/HBase/hbase/lib . If the client is not installed, you can also upload the required jar packages separately to any directory. Set the CLASSPATH environment variable, add the full path of the above jar package, and the HBase client configuration file path (phoenix needs to read the configuration in hbase-site.xml when connecting): su-dsadm vi $DSHOME/dsenv Configure the following: export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons -collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30. jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib /hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2 .jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol- 1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-seconda ryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating. jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib /httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9 .13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/ lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6. jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt /ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib /solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf Import environment variables source $DSHOME/dsenv Restart DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start Create jaas configuration file \u00b6 Phoenix connection needs to query zookeeper, and the Kerberos authentication of zookeeper needs to specify the jaas configuration file su-admin vi /home/dsadm/jaas.conf The contents of the file are as follows: Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; Read Phoenix table data \u00b6 Create assignment Change setting URL reference: jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab Configure JVM options as -Djava.security.auth.login.config=/home/dsadm/jaas.conf Compile and run Write Phoenix table data \u00b6 The Phoenix insert statement is upsert into and does not support the Insert into statement, so you cannot use the JDBC Connector to automatically generate SQL statements at runtime. You need to fill in it yourself, otherwise an error will be reported: main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 ( 42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. Create assignment Change setting Compile and run Docking Fiber \u00b6 To connect to Fiber, you need to install the FI client first Modify JDBC Driver configuration file \u00b6 Modify the isjdbc.config file in the $DSHOME path, add the path of the Fiber jdbc driver and dependent packages to the CLASSPATH variable, add com.huawei.fiber.FiberDriver; org.apache.hive.jdbc.HiveDriver; org.apache. phoenix.jdbc.PhoenixDriver Reference command: su-dsadm cd $DSHOME vi isjdbc.config The configuration is as follows: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar; /opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive -common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10. jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org. apache.phoenix.jdbc.PhoenixDriver Modify Fiber Configuration File \u00b6 DataStage uses IBM jdk, you need to create a new Fiber configuration file for DataStage to use cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml Modify the following two parameters of the phoenix, hive, and spark drivers in fiber_ibm.xml: -java.security.auth.login.config is modified to /home/dsadm/jaas.conf -zookeeper.kinit is modified to /opt/IBM/InformationServer/jdk/jre/bin/kinit The content of the file /home/dsadm/jaas.conf is as follows: Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; For other configuration items, please refer to FI product document Fiber Client Configuration Guide to modify. Use Hive Driver to read data \u00b6 Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive Compile and run Use Hive Driver to write data \u00b6 Create assignment Change setting Compile and run Use Spark Driver to read data \u00b6 Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark Compile and run Use Phoenix Driver to read data \u00b6 Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix Compile and run Currently unable to read the data,\" The connector could not determine the value for the fetch size.\", the problem is being confirmed Use Phoenix Driver to write data \u00b6 Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix Compile and run Write 0 rows of data, the problem is being confirmed Docking with Kafka \u00b6 Note: Kafka Connector does not support sending or consuming numeric fields such as integer, float, double, numeric, decimal, etc., and needs to be converted to char, varchar, longvarchar, etc., otherwise the following error will be reported: main_program: APT_PMsectionLeader(2, node2), player 2-Unexpected termination by Unix signal 9(SIGKILL). Install kafka client \u00b6 Kafka Connector needs to configure Kafka client Classpath. You can install the kafka client on the DataStage node to get the kafka-client jar package. For installation steps, refer to FusionInsight product documentation. Kafka Client Classpath needs to provide the paths of three jar packages of kafka-client, log4j, and slf4j-api, such as: /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs /slf4j-api-1.7.21.jar Send message to kafka \u00b6 Create assignment Change setting RowGenerator generates data Transformer data type conversion: Kafka configuration: Compile and run Read Kafka messages \u00b6 Create assignment Change setting Compile and run View the read data Docking MPPDB \u00b6 Obtain MPPDB JDBC Driver \u00b6 Obtained from the MPPDB release package, the package name is Gauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz After decompression, get gsjdbc4.jar, upload it to DataStage Server Modify JDBC Driver configuration file \u00b6 Modify the isjdbc.config file of the $DSHOME path, add the path of MPPDB Driver in the CLASSPATH variable, and add org.postgresql.Driver in the CLASS_NAMES variable su-dsadm cd $DSHOME vi isjdbc.config Configuration: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver; Read MPPDB table data \u00b6 Create assignment Change setting The URL format is: jdbc:postgresql://host:port/database Compile and run Write data to MPPDB table \u00b6 Create assignment Change setting The URL format is: jdbc:postgresql://host:port/database Compile and run View MPPDB table data:","title":"11.5.0.2 <--> C60"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#ibm-infosphere-datastage-connects-to-fusioninsight","text":"","title":"IBM InfoSphere DataStage connects to FusionInsight"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#applicable-scene","text":"IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 (HDFS/Hive/SparkSQL) IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Phoenix/SparkSQL/Kafka/GaussDB)","title":"Applicable scene"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#prerequisites","text":"The installation and deployment of IBM InfoSphere DataStage 11.5.0.2 has been completed (this article is deployed on Centos7.2) The deployment of FusionInsight cluster has been completed, version FusionInsight HD V100R002C60U20","title":"Prerequisites"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#ready-to-work","text":"","title":"Ready to work"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#configure-domain-name-resolution","text":"Use the vi /etc/hosts command to modify the hosts files of DataStage Server and Client, and add FI cluster node information, such as: 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3","title":"Configure domain name resolution"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#configure-kerberos-authentication","text":"Create a DataStage docking user in the FI management interface, grant the user the required permissions, and download the authentication credentials Decompress the downloaded tar file to get the Kerberos configuration file krb5.conf and the user's keytab file. Log in to the DataStage Server node as root, and copy the krb5.conf file of the FI cluster to the /etc directory. Upload the user's user.keytab file to any directory of the DataStage Server node, such as /home/dsadm .","title":"Configure Kerberos authentication"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#install-fusioninsight-client","text":"Refer to FI product documentation, download the complete client on the FI service management interface, upload it to DataStageServer, and install it to a custom directory, such as /opt/ficlient .","title":"Install FusionInsight Client"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#docking-hdfs","text":"","title":"Docking HDFS"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#import-the-ssl-certificate-of-the-fi-cluster","text":"The browser exports the root certificate of the FI cluster The browser opens the FI management interface, view the certificate, click the \"Certificate Path\" tab, select the root path, view the root certificate, under the \"Details\" tab, click \"Copy to File\" to export to cer format Import the certificate into the keystore file of DataStage Upload the exported FI root certificate fi-root-ca.cer to the DataStage server, for example, under the path /home/dsadm , import the certificate into the keystore file, command reference: /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca. cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks Generate and save the encrypted keystore password Use the vi /home/dsadm/authenticate.properties command to create a new configuration file and save the cipher text generated in the previous step: password={iisenc}SvtJ2f/uNTrvbuh26XDzag== Execute chown dsadm:dstage /home/dsadm/ authenticate.properties to modify the owner of the configuration file Export truststore environment variables Use vi /opt/IBM/InformationServer/Server/DSEngine/dsenv to edit the environment variables of DSEngine and add at the end export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties Restart DSEngine, refer to the command su-dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"Import the SSL certificate of the FI cluster"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#read-hdfs-file","text":"Create assignment Create a new parallel job and save it as hdfs2sf Add File_Connector component and Sequential File component, and File_Connector to Sequential File link Refer to the figure below to modify the configuration Compile and run After saving the configuration, compile and run Open the Director client in the menu Tools -> Run Director to view the job log View the read data","title":"Read HDFS file"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#write-hdfs-file","text":"Create assignment Create a new parallel job and save it as hdfswrite Add Row Generator component and File Connector component, and Row Generator to File Connector link Refer to the figure below to modify the configuration Compile and run Save \u2014 Compile \u2014 Run , view the job log: View written data","title":"Write HDFS file"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#docking-hive","text":"","title":"Docking Hive"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#use-hive-connector","text":"Note: The Hive JDBC Driver officially certified by Hive Connector is only DataDirect Hive Driver (IShive.jar). When using the IShive.jar included in DataStage 11.5.0.2 to connect to FusionInsight's hive, there will be a thrift protocol error. You need to consult IBM technology. Support the latest IShive.jar provided","title":"Use Hive Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#set-jdbc-driver-configuration-file","text":"Create the isjdbc.config file under the $DSHOME path, add the path of DataDirect Hive Driver (IShive.jar) to the CLASSPATH variable, and add com.ibm.isf.jdbc.hive.HiveDriver to the CLASS_NAMES variable, refer to the command: su-dsadm cd $DSHOME vi isjdbc.config Add the following information in isjdbc.config: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver Configure Kerberos authentication information: Create JDBCDriverLogin.conf in the directory where IShive.jar is located cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf The contents of the file are as follows: JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"Set JDBC Driver configuration file"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#read-hive-data","text":"Create assignment Change setting URL reference is configured as follows: jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; Where JDBC_DRIVER_test_keytab is the authentication information specified in the previous step Compile and run Save \u2014 Compile \u2014 Run , view the job log: View the read data","title":"Read Hive data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#write-data-to-hive-table","text":"Create assignment Change setting Compile and run Save \u2014 Compile \u2014 Run , view the job log, write 10 pieces of data, 2\u201911\u201d View Hive table data: Hive Connector uses the Insert statement to write data to the Hive table. Every time a piece of data is inserted, an MR task will be started. The efficiency is particularly low. This method is not recommended. You can write data directly to HDFS files.","title":"Write data to Hive table"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#use-jdbc-connector","text":"If you want to use FusionInsight's Hive JDBC driver, add the jdbc driver and dependent packages in the CLASSPATH of the isjdbc.config file. The following error will be reported when you run the job. At this time, you need to load it by exporting the CLASSPATH environment variable. And you can only use JDBC Connector, not Hive Connector, otherwise the following error will be reported","title":"Use JDBC Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#set-the-classpath-environment-variable","text":"The Hive jdbc driver package and dependent packages are located in the lib directory of the Hive client /opt/ficlient/Hive/Beeline/lib . If the client is not installed, these jar packages can be uploaded to any directory separately. Set the CLASSPATH environment variable, add the full path of the above jar package, refer to the command: su-dsadm vi $DSHOME/dsenv Add the relevant jar package at the end of the file (the specific path is adjusted according to the actual environment) export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/ Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3. jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive /Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7 .2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar :/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/ Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc- 1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastor e-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar: /opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient /Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/ opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j- 1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt /ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper -3.5.1.jar Import environment variables source $DSHOME/dsenv Restart DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"Set the CLASSPATH environment variable"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#read-hive-data_1","text":"Create assignment Change setting The URL is: jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM; user.keytab=/home/dsadm/user.keytab; Compile and run","title":"Read Hive data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#write-data-to-hive-table_1","text":"Create assignment Change setting Compile and run Write 5 pieces of data, 1\u201949\"","title":"Write data to Hive table"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#import-data-into-hdfs-file-of-hive-table","text":"Create assignment Change setting Compile and run View written data Hive table data increment 100","title":"Import data into HDFS file of Hive table"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#incremental-data-is-regularly-and-automatically-imported-into-hdfs-files-of-hive-tables","text":"Incremental data can be imported into hive by adding HDFS files. If it is to be automatically executed on a regular basis, the imported file name needs to include variable parameters for setting and distinguishing, and then run the job in command or script mode to assign values \u200b\u200bto the parameters. Create assignment Set job parameters Click the \"job properties\" button and set the parameters as follows Change setting File Connector configures the name of the export file, and refers to the set parameters with \"#\" dsjob command to run the job Save the compilation job and execute the dsjob -run command on the DataStage Server in the format: dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME Command reference: su-dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append View HDFS files: View Hive data increment is 200","title":"Incremental data is regularly and automatically imported into HDFS files of Hive tables"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#docking-sparksql","text":"Similar to using the FI Hive JDBC driver, you can use the SparkSQL JDBC driver to connect to Hive. You also need to export the CLASSPATH environment variable to load the driver package and dependent packages. SparkSQL jdbc does not support the insert into statement, it can only be used to read hive data, but cannot insert data into hive tables.","title":"Docking SparkSQL"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#set-the-classpath-environment-variable_1","text":"The SparkSQL jdbc driver package and dependent packages are located in the Spark client lib directory /opt/ficlient/Spark/spark/lib/ . If the client is not installed, you can also upload the required jar package separately to any directory. Set the CLASSPATH environment variable, add the full path of the above jar package, and the spark client configuration file path (SparkSQL jdbc needs to read the configuration in hive-site.xml when connecting to hive): su-dsadm vi $DSHOME/dsenv Configure the following: export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/ spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7. 1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark /spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce -client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2 .1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark .jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/ opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Sp ark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17. jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark /spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf Import environment variables source $DSHOME/dsenv Restart DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"Set the CLASSPATH environment variable"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#read-hive-table-data","text":"Create assignment Change setting URL reference: jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP. COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; Compile and run","title":"Read Hive table data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#docking-phoenix","text":"To use Phoenix to access HBase tables in JDBC mode, you also need to export the CLASSPATH environment variable to load the driver package and dependent packages.","title":"Docking Phoenix"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#set-the-classpath-environment-variable_2","text":"Phoenix-related jar packages are located in the lib directory of the HBase client /opt/ficlient/HBase/hbase/lib . If the client is not installed, you can also upload the required jar packages separately to any directory. Set the CLASSPATH environment variable, add the full path of the above jar package, and the HBase client configuration file path (phoenix needs to read the configuration in hbase-site.xml when connecting): su-dsadm vi $DSHOME/dsenv Configure the following: export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons -collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30. jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib /hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2 .jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol- 1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-seconda ryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating. jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib /httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9 .13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/ lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6. jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt /ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib /solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf Import environment variables source $DSHOME/dsenv Restart DSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"Set the CLASSPATH environment variable"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#create-jaas-configuration-file","text":"Phoenix connection needs to query zookeeper, and the Kerberos authentication of zookeeper needs to specify the jaas configuration file su-admin vi /home/dsadm/jaas.conf The contents of the file are as follows: Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"Create jaas configuration file"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#read-phoenix-table-data","text":"Create assignment Change setting URL reference: jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab Configure JVM options as -Djava.security.auth.login.config=/home/dsadm/jaas.conf Compile and run","title":"Read Phoenix table data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#write-phoenix-table-data","text":"The Phoenix insert statement is upsert into and does not support the Insert into statement, so you cannot use the JDBC Connector to automatically generate SQL statements at runtime. You need to fill in it yourself, otherwise an error will be reported: main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 ( 42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. Create assignment Change setting Compile and run","title":"Write Phoenix table data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#docking-fiber","text":"To connect to Fiber, you need to install the FI client first","title":"Docking Fiber"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#modify-jdbc-driver-configuration-file","text":"Modify the isjdbc.config file in the $DSHOME path, add the path of the Fiber jdbc driver and dependent packages to the CLASSPATH variable, add com.huawei.fiber.FiberDriver; org.apache.hive.jdbc.HiveDriver; org.apache. phoenix.jdbc.PhoenixDriver Reference command: su-dsadm cd $DSHOME vi isjdbc.config The configuration is as follows: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar; /opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive -common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10. jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org. apache.phoenix.jdbc.PhoenixDriver","title":"Modify JDBC Driver configuration file"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#modify-fiber-configuration-file","text":"DataStage uses IBM jdk, you need to create a new Fiber configuration file for DataStage to use cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml Modify the following two parameters of the phoenix, hive, and spark drivers in fiber_ibm.xml: -java.security.auth.login.config is modified to /home/dsadm/jaas.conf -zookeeper.kinit is modified to /opt/IBM/InformationServer/jdk/jre/bin/kinit The content of the file /home/dsadm/jaas.conf is as follows: Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; For other configuration items, please refer to FI product document Fiber Client Configuration Guide to modify.","title":"Modify Fiber Configuration File"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#use-hive-driver-to-read-data","text":"Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive Compile and run","title":"Use Hive Driver to read data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#use-hive-driver-to-write-data","text":"Create assignment Change setting Compile and run","title":"Use Hive Driver to write data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#use-spark-driver-to-read-data","text":"Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark Compile and run","title":"Use Spark Driver to read data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#use-phoenix-driver-to-read-data","text":"Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix Compile and run Currently unable to read the data,\" The connector could not determine the value for the fetch size.\", the problem is being confirmed","title":"Use Phoenix Driver to read data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#use-phoenix-driver-to-write-data","text":"Create assignment Change setting URL reference: jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix Compile and run Write 0 rows of data, the problem is being confirmed","title":"Use Phoenix Driver to write data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#docking-with-kafka","text":"Note: Kafka Connector does not support sending or consuming numeric fields such as integer, float, double, numeric, decimal, etc., and needs to be converted to char, varchar, longvarchar, etc., otherwise the following error will be reported: main_program: APT_PMsectionLeader(2, node2), player 2-Unexpected termination by Unix signal 9(SIGKILL).","title":"Docking with Kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#install-kafka-client","text":"Kafka Connector needs to configure Kafka client Classpath. You can install the kafka client on the DataStage node to get the kafka-client jar package. For installation steps, refer to FusionInsight product documentation. Kafka Client Classpath needs to provide the paths of three jar packages of kafka-client, log4j, and slf4j-api, such as: /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs /slf4j-api-1.7.21.jar","title":"Install kafka client"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#send-message-to-kafka","text":"Create assignment Change setting RowGenerator generates data Transformer data type conversion: Kafka configuration: Compile and run","title":"Send message to kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#read-kafka-messages","text":"Create assignment Change setting Compile and run View the read data","title":"Read Kafka messages"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#docking-mppdb","text":"","title":"Docking MPPDB"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#obtain-mppdb-jdbc-driver","text":"Obtained from the MPPDB release package, the package name is Gauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz After decompression, get gsjdbc4.jar, upload it to DataStage Server","title":"Obtain MPPDB JDBC Driver"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#modify-jdbc-driver-configuration-file_1","text":"Modify the isjdbc.config file of the $DSHOME path, add the path of MPPDB Driver in the CLASSPATH variable, and add org.postgresql.Driver in the CLASS_NAMES variable su-dsadm cd $DSHOME vi isjdbc.config Configuration: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;","title":"Modify JDBC Driver configuration file"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#read-mppdb-table-data","text":"Create assignment Change setting The URL format is: jdbc:postgresql://host:port/database Compile and run","title":"Read MPPDB table data"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#write-data-to-mppdb-table","text":"Create assignment Change setting The URL format is: jdbc:postgresql://host:port/database Compile and run View MPPDB table data:","title":"Write data to MPPDB table"},{"location":"Data_Integration/Informatica_PWX_CDC/","text":"Connection Instruction Between Informatica PowerExchange CDC and FusionInsight \u00b6 Succeeded Case \u00b6 Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD 6.5 (Kafka) Environment Information \u00b6 Informatica PowerExchange CDC 10.2.0 Linux & Windows version Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka client Architecture \u00b6 A data source, oracle database One Linux machine, installed with Informatica PWX CDC, start the listener and logger service, then install the PWX Publisher which can transfer the log data captured by PWX CDC to the kafka topic. One Linux machine, installed with FusionInsight HD Kafka client, consume the data transferred from PWX Publisher (optional) One Windows machine, installed with PWX CDC, start the listener service, use navigator to see the data captured by PWX CDC. database configuration \u00b6 >This part can refer to the Informatica PowerExchange CDC user guide https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html login to the system as oracle user, use Sqlplus / as sysdba login to Oracle database, open Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >Tips:Back up your database after both SHUTDOWN commands.. Set Up Oracle Minimal Global Supplemental Logging SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; ALTER SYSTEM switch logfile ; Copy the Oracle Catalog to the Archived Logs EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Define a CDC User and Grant User Privileges create a test table and insert some data Install Informatica PWX CDC & PWX Publisher \u00b6 Install Informatica PWX CDC in Linux \u00b6 Get the installation package pwx1020_linux_em64t.tar . untar the package and run ./install.sh , configure the installation path here is /opt/PowerExchange/10.2.0 . Configure the environment \u00b6 open environment file vi ~/.bash_profile add the following configuration export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * run source ~/.bash_profile * run dtlinfo ,check the installation Configure dbmover.cfg and pwxccl.cfg file \u00b6 Configure dbmover.cfg as following nodeln is the self defined listener node name the second ORCL in ORACLEID is the database name to be listened. CAPT_PATH is the CDC control file path, the path should be created previously define the SVCNODE and CMDNODE name Configure pwxccl.cfg as following CONDENSENAME should be the same as SVCNODE in dbmover.cfg DBID is the database SID CAPTURE_NODE is the capture node name CAPTURE_NODE_UID is the database user name CAPTURE_NODE_PWD is the database user password Start listener and logger services Use PWX CDC capture ORACLE log data \u00b6 ### install Informatica PWX CDC in Windows machine Get the installation package and double click to install, add environment variable PWX_CONFIG ,configured as the dbmover.cfg file in PWX Configure dbmover.cfg file set listener name, add listener Information in server side - set the listened database name - set the control file path * start the listener * start Navigator In Navigator create a new registeration group as following: NEXT chick next,we can see the test table created in oracle, double click the table name, choose all columns chick next, change state to active , check box run DDL immediately , click finish In Extraction Groups, double click the orcl11 created before, right click, add Extract Defination, set the map name and table name click next, can see the capture created before click add, finish click the icon, run row test, the captured data is shown as following Use PWX CDC publisher to connect Kafka \u00b6 ### Change kafka configuration file * Configure producer.properties , add the following configuration sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * Configure jaas.conf as following ![](assets/Informatica_PWX_CDC/14cae.png) create a kafka topic, named pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### Install Informatica PWX Publisher * Get the package pwxcdcpub120_linux_x64.tar.gz ,untar it Login as root\uff0cadd the following configuration in ~/.bash_profile file export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key source the environment, kerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser Copy all the files in directory samples to instanceA/config > Configuration for PWX Publisher can refer to the Informatica user guide https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html Configure cdcPublisherAvro.cfg - Configure cdcPublisherCommon.cfg - Configure cdcPublisherKafka.cfg , set kafka topic name and the properties file path - Configure cdcPowerExchange.cfg * Extract.pwxCapiConnectionName is the CAPI_CONNECTION in dbmover.cfg file * Extract.pwxExtractionMapSchemaName is the schema name in pwx extraction, here is u8orcl * Extract.pwxNodeLocation is pwx node name * Extract.pwxNodeUserId/Extract.pwxNodePwd and Extract.pwxXmapUserId/Extract.pwxXmappassword is database user name and pasword Change the PwxCDCPublisher.sh file in installation path bin,add the following RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" Start pwx CDC Publisher,run sh PwxCDCPublisher.sh Start kafka consumer \u00b6 In FusionInsight HD Kafka client, run the following command to start consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties Insert data in oracle, the captured data in kafka is the following Update data in oracle, the captured data in kafka is the following Delete data in oracle, the captured data in kafka is the following Q&A \u00b6 1.Failed to start pwxccl A:Run the following script in oracle exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Then grant C##PWX sysdba right grant sysdba to C##PWX","title":"10.2.0 <--> 6.5"},{"location":"Data_Integration/Informatica_PWX_CDC/#connection-instruction-between-informatica-powerexchange-cdc-and-fusioninsight","text":"","title":"Connection Instruction Between Informatica PowerExchange CDC and FusionInsight"},{"location":"Data_Integration/Informatica_PWX_CDC/#succeeded-case","text":"Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD 6.5 (Kafka)","title":"Succeeded Case"},{"location":"Data_Integration/Informatica_PWX_CDC/#environment-information","text":"Informatica PowerExchange CDC 10.2.0 Linux & Windows version Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka client","title":"Environment Information"},{"location":"Data_Integration/Informatica_PWX_CDC/#architecture","text":"A data source, oracle database One Linux machine, installed with Informatica PWX CDC, start the listener and logger service, then install the PWX Publisher which can transfer the log data captured by PWX CDC to the kafka topic. One Linux machine, installed with FusionInsight HD Kafka client, consume the data transferred from PWX Publisher (optional) One Windows machine, installed with PWX CDC, start the listener service, use navigator to see the data captured by PWX CDC.","title":"Architecture"},{"location":"Data_Integration/Informatica_PWX_CDC/#database-configuration","text":">This part can refer to the Informatica PowerExchange CDC user guide https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html login to the system as oracle user, use Sqlplus / as sysdba login to Oracle database, open Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >Tips:Back up your database after both SHUTDOWN commands.. Set Up Oracle Minimal Global Supplemental Logging SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; ALTER SYSTEM switch logfile ; Copy the Oracle Catalog to the Archived Logs EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Define a CDC User and Grant User Privileges create a test table and insert some data","title":"database configuration"},{"location":"Data_Integration/Informatica_PWX_CDC/#install-informatica-pwx-cdc-pwx-publisher","text":"","title":"Install Informatica PWX CDC &amp; PWX Publisher"},{"location":"Data_Integration/Informatica_PWX_CDC/#install-informatica-pwx-cdc-in-linux","text":"Get the installation package pwx1020_linux_em64t.tar . untar the package and run ./install.sh , configure the installation path here is /opt/PowerExchange/10.2.0 .","title":"Install Informatica PWX CDC in Linux"},{"location":"Data_Integration/Informatica_PWX_CDC/#configure-the-environment","text":"open environment file vi ~/.bash_profile add the following configuration export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * run source ~/.bash_profile * run dtlinfo ,check the installation","title":"Configure the environment"},{"location":"Data_Integration/Informatica_PWX_CDC/#configure-dbmovercfg-and-pwxcclcfg-file","text":"Configure dbmover.cfg as following nodeln is the self defined listener node name the second ORCL in ORACLEID is the database name to be listened. CAPT_PATH is the CDC control file path, the path should be created previously define the SVCNODE and CMDNODE name Configure pwxccl.cfg as following CONDENSENAME should be the same as SVCNODE in dbmover.cfg DBID is the database SID CAPTURE_NODE is the capture node name CAPTURE_NODE_UID is the database user name CAPTURE_NODE_PWD is the database user password Start listener and logger services","title":"Configure dbmover.cfg and pwxccl.cfg file"},{"location":"Data_Integration/Informatica_PWX_CDC/#use-pwx-cdc-capture-oracle-log-data","text":"### install Informatica PWX CDC in Windows machine Get the installation package and double click to install, add environment variable PWX_CONFIG ,configured as the dbmover.cfg file in PWX Configure dbmover.cfg file set listener name, add listener Information in server side - set the listened database name - set the control file path * start the listener * start Navigator In Navigator create a new registeration group as following: NEXT chick next,we can see the test table created in oracle, double click the table name, choose all columns chick next, change state to active , check box run DDL immediately , click finish In Extraction Groups, double click the orcl11 created before, right click, add Extract Defination, set the map name and table name click next, can see the capture created before click add, finish click the icon, run row test, the captured data is shown as following","title":"Use PWX CDC capture ORACLE log data"},{"location":"Data_Integration/Informatica_PWX_CDC/#use-pwx-cdc-publisher-to-connect-kafka","text":"### Change kafka configuration file * Configure producer.properties , add the following configuration sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * Configure jaas.conf as following ![](assets/Informatica_PWX_CDC/14cae.png) create a kafka topic, named pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### Install Informatica PWX Publisher * Get the package pwxcdcpub120_linux_x64.tar.gz ,untar it Login as root\uff0cadd the following configuration in ~/.bash_profile file export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key source the environment, kerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser Copy all the files in directory samples to instanceA/config > Configuration for PWX Publisher can refer to the Informatica user guide https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html Configure cdcPublisherAvro.cfg - Configure cdcPublisherCommon.cfg - Configure cdcPublisherKafka.cfg , set kafka topic name and the properties file path - Configure cdcPowerExchange.cfg * Extract.pwxCapiConnectionName is the CAPI_CONNECTION in dbmover.cfg file * Extract.pwxExtractionMapSchemaName is the schema name in pwx extraction, here is u8orcl * Extract.pwxNodeLocation is pwx node name * Extract.pwxNodeUserId/Extract.pwxNodePwd and Extract.pwxXmapUserId/Extract.pwxXmappassword is database user name and pasword Change the PwxCDCPublisher.sh file in installation path bin,add the following RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" Start pwx CDC Publisher,run sh PwxCDCPublisher.sh","title":"Use PWX CDC publisher to connect Kafka"},{"location":"Data_Integration/Informatica_PWX_CDC/#start-kafka-consumer","text":"In FusionInsight HD Kafka client, run the following command to start consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties Insert data in oracle, the captured data in kafka is the following Update data in oracle, the captured data in kafka is the following Delete data in oracle, the captured data in kafka is the following","title":"Start kafka consumer"},{"location":"Data_Integration/Informatica_PWX_CDC/#qa","text":"1.Failed to start pwxccl A:Run the following script in oracle exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); Then grant C##PWX sysdba right grant sysdba to C##PWX","title":"Q&amp;A"},{"location":"Data_Integration/Informatica_PowerCenter/","text":"Connection Instruction Between Informatica PowerCenter and FusionInsight HD \u00b6 Succeeded Case \u00b6 Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive) Environment Information \u00b6 Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD client Architecture \u00b6 One Linux machine, installed with Informatica Server and FusionInsight HD client One Windows machine, installed with Informatica PowerCenter Client Install and config FusionInsight HD client \u00b6 Install the FusionInsight client\uff0cinstallation path is /opt/hadoopclient Create a user from FusionInsight HD manager\uff0crefer to . For example, create a user named developuser\uff0cand assign him all rights for HDFS and Hive. Download the keytab file, put krb5.conf file to the /opt/ path of client node. Install Oracle database and Informatica Server \u00b6 create user oracle\uff0cinstall oracle database create user infa,login to oracle use sqlplus / as sysdba , run the following sql create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; Get Informatica Server installation package and upload to server node, run ./install.sh as user infa, the installation path is /home/infa/Informatica/10.2.0 . Visit ip:6008 in a browser, open the Administrator tool, input the user name and password. Informatica Server configuration \u00b6 Create PowerCenter Repository Service In Services and Nodes, right click domain, Create a PowerCenter Repository Service - Set Name and node, next - Set database information, finish Enable the Repository Service,and create contents In repository Properties, set the Operating Mode to Mormal, and recycle the service Create PowerCenter Integration Service In Services and Nodes, right click domain, Create a PowerCenter Integration Service - Set Name and node, next - Set Repository information, finish\uff0cenable the service Create developuser in infa server InSecurity tab, create a user, named as developuser\uff0cthe same as user in Hadoop cluster Edit the user privileges and groups Infa Server configuration for Hadoop Copy krb5.conf file in /opt to /etc and ${INFA_HOME}java/jre/lib/security/ , and give the read right to user infa. Login to node as user infa, create a directory for the configuration file, such as /opt/pwx-hadoop/conf Get the follwoing configuration fie from FusionInsight HD client, put them into /opt/pwx-hadoop/conf and change the file right to 775 - Do Kerberos authentication, and set cache,the infa user should have read and write rights for the cache file source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - Edit the core-site.xml file in /opt/pwx-hadoop/conf add the following property <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - In Administrator tool, add an Environment variable for pwc_DIS , recycle the service delete jar files related to hive in /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ and copy jar files related to hive in /opt/hadoopclient/Hive/Beeline/lib to the path, change the file rights rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* PowerCenter Client configuration \u00b6 PowerCenter Repository Manager configuration \u00b6 Get PowerCenter Client installation package,install PowerCenter Client,start PowerCenter Repository Manager, in tool bar, choose Repository->Configure domain ,input the domain information, then we can see the repository created before. Double click the repository, input user name and password, connect In folder, create a folder PowerCenter Designer configuration \u00b6 Open PowerCenter Designer, right click the folder, click open - Click tool bar, Sources->import from databases\uff0ccreate a system DSN in ODBC source,choose Oracle Driver,input database information. - Choose the data source created just now, input database user name and password, connect, get the table in the database - Choose target designer\uff0cdrag in the table in sources - Double click the table, set database type to Flat File In mapping configuration, create a new mapping, drag in the source and target table and link them PowerCenter Workflow Manager \u00b6 In tool bar Task, create a new task,name it and choose the mapping created just now Create a workflow, drag in the task, link them In tool bar connection, create a application connection, choose Hadoop HDFS Connection HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser Double click the task created, in mapping tab, click Targets , set the Writers to HDFS Flat Write , set connection value to the connection created just now In properties, config as following Save the workflow, right click, start the workflow Open PowerCenter Workflow Monitor, the run information is shown. In HDFS, check if the data is uploaded. In the task properties, choose Generate And Load Hive Table , Overwrite Hive Table ,input the table created in hive, start workflow Open PowerCenter Workflow Monitor, the run information is shown. In Hive, check if the data is loaded into the table","title":"10.2.0 <--> 6.5"},{"location":"Data_Integration/Informatica_PowerCenter/#connection-instruction-between-informatica-powercenter-and-fusioninsight-hd","text":"","title":"Connection Instruction Between Informatica PowerCenter and FusionInsight HD"},{"location":"Data_Integration/Informatica_PowerCenter/#succeeded-case","text":"Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive)","title":"Succeeded Case"},{"location":"Data_Integration/Informatica_PowerCenter/#environment-information","text":"Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD client","title":"Environment Information"},{"location":"Data_Integration/Informatica_PowerCenter/#architecture","text":"One Linux machine, installed with Informatica Server and FusionInsight HD client One Windows machine, installed with Informatica PowerCenter Client","title":"Architecture"},{"location":"Data_Integration/Informatica_PowerCenter/#install-and-config-fusioninsight-hd-client","text":"Install the FusionInsight client\uff0cinstallation path is /opt/hadoopclient Create a user from FusionInsight HD manager\uff0crefer to . For example, create a user named developuser\uff0cand assign him all rights for HDFS and Hive. Download the keytab file, put krb5.conf file to the /opt/ path of client node.","title":"Install and config FusionInsight HD client"},{"location":"Data_Integration/Informatica_PowerCenter/#install-oracle-database-and-informatica-server","text":"create user oracle\uff0cinstall oracle database create user infa,login to oracle use sqlplus / as sysdba , run the following sql create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; Get Informatica Server installation package and upload to server node, run ./install.sh as user infa, the installation path is /home/infa/Informatica/10.2.0 . Visit ip:6008 in a browser, open the Administrator tool, input the user name and password.","title":"Install Oracle database and Informatica Server"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-server-configuration","text":"Create PowerCenter Repository Service In Services and Nodes, right click domain, Create a PowerCenter Repository Service - Set Name and node, next - Set database information, finish Enable the Repository Service,and create contents In repository Properties, set the Operating Mode to Mormal, and recycle the service Create PowerCenter Integration Service In Services and Nodes, right click domain, Create a PowerCenter Integration Service - Set Name and node, next - Set Repository information, finish\uff0cenable the service Create developuser in infa server InSecurity tab, create a user, named as developuser\uff0cthe same as user in Hadoop cluster Edit the user privileges and groups Infa Server configuration for Hadoop Copy krb5.conf file in /opt to /etc and ${INFA_HOME}java/jre/lib/security/ , and give the read right to user infa. Login to node as user infa, create a directory for the configuration file, such as /opt/pwx-hadoop/conf Get the follwoing configuration fie from FusionInsight HD client, put them into /opt/pwx-hadoop/conf and change the file right to 775 - Do Kerberos authentication, and set cache,the infa user should have read and write rights for the cache file source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - Edit the core-site.xml file in /opt/pwx-hadoop/conf add the following property <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - In Administrator tool, add an Environment variable for pwc_DIS , recycle the service delete jar files related to hive in /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ and copy jar files related to hive in /opt/hadoopclient/Hive/Beeline/lib to the path, change the file rights rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive*","title":"Informatica Server configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-client-configuration","text":"","title":"PowerCenter Client configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-repository-manager-configuration","text":"Get PowerCenter Client installation package,install PowerCenter Client,start PowerCenter Repository Manager, in tool bar, choose Repository->Configure domain ,input the domain information, then we can see the repository created before. Double click the repository, input user name and password, connect In folder, create a folder","title":"PowerCenter Repository Manager configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-designer-configuration","text":"Open PowerCenter Designer, right click the folder, click open - Click tool bar, Sources->import from databases\uff0ccreate a system DSN in ODBC source,choose Oracle Driver,input database information. - Choose the data source created just now, input database user name and password, connect, get the table in the database - Choose target designer\uff0cdrag in the table in sources - Double click the table, set database type to Flat File In mapping configuration, create a new mapping, drag in the source and target table and link them","title":"PowerCenter Designer configuration"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-workflow-manager","text":"In tool bar Task, create a new task,name it and choose the mapping created just now Create a workflow, drag in the task, link them In tool bar connection, create a application connection, choose Hadoop HDFS Connection HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser Double click the task created, in mapping tab, click Targets , set the Writers to HDFS Flat Write , set connection value to the connection created just now In properties, config as following Save the workflow, right click, start the workflow Open PowerCenter Workflow Monitor, the run information is shown. In HDFS, check if the data is uploaded. In the task properties, choose Generate And Load Hive Table , Overwrite Hive Table ,input the table created in hive, start workflow Open PowerCenter Workflow Monitor, the run information is shown. In Hive, check if the data is loaded into the table","title":"PowerCenter Workflow Manager"},{"location":"Data_Integration/Talend/","text":"Connection Instruction between Talend and FusionInsight \u00b6 Succeeded Case \u00b6 Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) Note: Because of the version bug of Talend 7.0.1, Hive cannot be successfully connected. Using Talend 6.4.1 for substitution. Installing Talend \u00b6 Purpose \u00b6 Installing Talend 7.0.1 Prerequisites \u00b6 Installing FusionInsight HD cluster and its client completed Procedure \u00b6 Configure the JAVA_HOME into Path Environment Variables Configure Kerberos Get Kerberos related userkeytab and krb5.conf files by login into the FusionInsight HD manager web UI and put them into the following directory C:\\ProgramData\\Kerberos . In addition, create a new file named krb5.ini with the same content of krb5.conf, put the krb5.ini file into the following directory C:\\Windows Download TOS from the following web pages https://www.talend.com/products/big-data/big-data-open-studio/ , create the jaas.conf file for zookeeper connection with its content shown as follows Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; Sart TOS_BD by clicking TOS_BD-win-x86_64.exe Installing additional Talend Packages Connecting Talend to HDFS \u00b6 Purpose \u00b6 Configuring Talend related HDFS processor to connect FusionInsight HD HDFS Prerequisites \u00b6 Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed HDFS Connection Procedure \u00b6 Add the tHDFSConnection component with its configuration shown as follows: In detail\uff1a 1: Cloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - Test completed\uff1a HDFS Get Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSGet component shown as follows Note: Put the out.csv into the HDFS filesystem with the following directory /tmp/talend_test , C:/SOFT is the local folder for file output TEST completed\uff1a Check the test outcome by coming into the local directory C:/SOFT HDFS Put Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSPut component shown as follows Note: Before the test starts, create HDFSPut.txt located at the directory C:/SOFT with its content shown as follows It is created on a local PC. Test Completed\uff1a Login into the cluster to check the test outcome: Connecting Talend to Hive \u00b6 Purpose \u00b6 Configuring Talend related Hive processor to connect FusionInsight HD Hive Prerequisites \u00b6 Installing Talend 6.4.1 completed Installing FusionInsight HD cluster and its client completed Hive Connection Procedure \u00b6 The Talend version for Hive connection is 6.4.1 The whole process is shown as the following pic: The configuration of tHiveConnection component shown as follows 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" Note: Need to click the button which besides the Distribution to import the required jar files of FusionInsight HD. If there still need to add extra jar files, you can complete this step either by Talend itself or manually add these jar files. Test Completed\uff1a Hive Create Table & Load Procedure \u00b6 The configuration of tHiveConnection component does not change The configuration of tHiveCreateTable component shown as follows Note: It is required to Edit schema of the table The configuration of tHiveLoad component shown as follows Note: Before the test starts, the file out.csv need to be uploaded into the hdfs filesystem directory /tmp/talend_test/ The content of out.csv shown as follows \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHiveClose component shown as follows Test Completed\uff1a Check the table createdTableTalend by login into the cluster Hive Input Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveInput component shown as follows Note: It is required to Edit schema of the hive table The configuration of tLogRow keeps by default The configuration of tHiveClose component shown as follows Test Completed\uff1a Hive Row Procedure \u00b6 The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveRow component shown as follows Note: It is required to Edit schema of hive table Test Completed\uff1a Check the cluster outcome by login into the FusionInsight Cluster Connecting Talend to HBase \u00b6 Purpose \u00b6 Configuring Talend related HBase processor to connect FusionInsight HD HBase Prerequisites \u00b6 Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed HBase Connection Procedure \u00b6 The whole process is shown as the following pic: Using eclipse to export the LoginUtil which from HBase sample project code of FusionInsight HD client (Sample project code in this time can be found by following directory C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example ) Find the tHbaseConnection component by Palette The configuration of tHbaseConnection shown as the following pic: Note: It is required to import the jar files of HBase sample project and the exported hbase_loginUtil.jar hbase-example required jar faile can be located by the following directory C:\\FusionInsight_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.0.2.tar.gz\\hbase\\lib The configuration of tLibraryLoad shown as folloing pic: Click on Advanced settings and add the java code import com.huawei.hadoop.security.LoginUtil; shown as follows: Use tJava component to customize the tHBaseConnection component The content of the Java code shown as follows\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - Test Completed HBase Input Output Procedure \u00b6 The content of the Java code shown as follows\uff1a The configuration of tLibraryLoad \uff0c tHBaseConnection \uff0c tJava , tHBaseClose do not change The configuration of tFileInputDelimited shown as following pic: Note: It is required to Edit schema of out.csv The content of out.csv shown as follows: 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHBaseOutput shown as folloing pic: Note: It is required to Edit Schema of table: The configuration of tHBaseInput shown as folloing pic: The configuration of tLogRow keeps by default Test Completed: Login into the FusinInsight HD cluster and check the HBase table hbaseInputOutputTest by using following comands: hbase shell scan 'hbaseInputOutputTest'","title":"7.0.1 <--> C80"},{"location":"Data_Integration/Talend/#connection-instruction-between-talend-and-fusioninsight","text":"","title":"Connection Instruction between Talend and FusionInsight"},{"location":"Data_Integration/Talend/#succeeded-case","text":"Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) Note: Because of the version bug of Talend 7.0.1, Hive cannot be successfully connected. Using Talend 6.4.1 for substitution.","title":"Succeeded Case"},{"location":"Data_Integration/Talend/#installing-talend","text":"","title":"Installing Talend"},{"location":"Data_Integration/Talend/#purpose","text":"Installing Talend 7.0.1","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites","text":"Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#procedure","text":"Configure the JAVA_HOME into Path Environment Variables Configure Kerberos Get Kerberos related userkeytab and krb5.conf files by login into the FusionInsight HD manager web UI and put them into the following directory C:\\ProgramData\\Kerberos . In addition, create a new file named krb5.ini with the same content of krb5.conf, put the krb5.ini file into the following directory C:\\Windows Download TOS from the following web pages https://www.talend.com/products/big-data/big-data-open-studio/ , create the jaas.conf file for zookeeper connection with its content shown as follows Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; Sart TOS_BD by clicking TOS_BD-win-x86_64.exe Installing additional Talend Packages","title":"Procedure"},{"location":"Data_Integration/Talend/#connecting-talend-to-hdfs","text":"","title":"Connecting Talend to HDFS"},{"location":"Data_Integration/Talend/#purpose_1","text":"Configuring Talend related HDFS processor to connect FusionInsight HD HDFS","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites_1","text":"Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#hdfs-connection-procedure","text":"Add the tHDFSConnection component with its configuration shown as follows: In detail\uff1a 1: Cloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - Test completed\uff1a","title":"HDFS Connection Procedure"},{"location":"Data_Integration/Talend/#hdfs-get-procedure","text":"The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSGet component shown as follows Note: Put the out.csv into the HDFS filesystem with the following directory /tmp/talend_test , C:/SOFT is the local folder for file output TEST completed\uff1a Check the test outcome by coming into the local directory C:/SOFT","title":"HDFS Get Procedure"},{"location":"Data_Integration/Talend/#hdfs-put-procedure","text":"The whole process is shown as the following pic: The configuration of tHDFSConnection component does not change The configuration of tHDFSPut component shown as follows Note: Before the test starts, create HDFSPut.txt located at the directory C:/SOFT with its content shown as follows It is created on a local PC. Test Completed\uff1a Login into the cluster to check the test outcome:","title":"HDFS Put Procedure"},{"location":"Data_Integration/Talend/#connecting-talend-to-hive","text":"","title":"Connecting Talend to Hive"},{"location":"Data_Integration/Talend/#purpose_2","text":"Configuring Talend related Hive processor to connect FusionInsight HD Hive","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites_2","text":"Installing Talend 6.4.1 completed Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#hive-connection-procedure","text":"The Talend version for Hive connection is 6.4.1 The whole process is shown as the following pic: The configuration of tHiveConnection component shown as follows 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" Note: Need to click the button which besides the Distribution to import the required jar files of FusionInsight HD. If there still need to add extra jar files, you can complete this step either by Talend itself or manually add these jar files. Test Completed\uff1a","title":"Hive Connection Procedure"},{"location":"Data_Integration/Talend/#hive-create-table-load-procedure","text":"The configuration of tHiveConnection component does not change The configuration of tHiveCreateTable component shown as follows Note: It is required to Edit schema of the table The configuration of tHiveLoad component shown as follows Note: Before the test starts, the file out.csv need to be uploaded into the hdfs filesystem directory /tmp/talend_test/ The content of out.csv shown as follows \uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHiveClose component shown as follows Test Completed\uff1a Check the table createdTableTalend by login into the cluster","title":"Hive Create Table &amp; Load Procedure"},{"location":"Data_Integration/Talend/#hive-input-procedure","text":"The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveInput component shown as follows Note: It is required to Edit schema of the hive table The configuration of tLogRow keeps by default The configuration of tHiveClose component shown as follows Test Completed\uff1a","title":"Hive Input Procedure"},{"location":"Data_Integration/Talend/#hive-row-procedure","text":"The whole process is shown as the following pic: The configuration of tHiveConnection component does not change The configuration of tHiveRow component shown as follows Note: It is required to Edit schema of hive table Test Completed\uff1a Check the cluster outcome by login into the FusionInsight Cluster","title":"Hive Row Procedure"},{"location":"Data_Integration/Talend/#connecting-talend-to-hbase","text":"","title":"Connecting Talend to HBase"},{"location":"Data_Integration/Talend/#purpose_3","text":"Configuring Talend related HBase processor to connect FusionInsight HD HBase","title":"Purpose"},{"location":"Data_Integration/Talend/#prerequisites_3","text":"Installing Talend 7.0.1 completed Installing FusionInsight HD cluster and its client completed","title":"Prerequisites"},{"location":"Data_Integration/Talend/#hbase-connection-procedure","text":"The whole process is shown as the following pic: Using eclipse to export the LoginUtil which from HBase sample project code of FusionInsight HD client (Sample project code in this time can be found by following directory C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example ) Find the tHbaseConnection component by Palette The configuration of tHbaseConnection shown as the following pic: Note: It is required to import the jar files of HBase sample project and the exported hbase_loginUtil.jar hbase-example required jar faile can be located by the following directory C:\\FusionInsight_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.0.2.tar.gz\\hbase\\lib The configuration of tLibraryLoad shown as folloing pic: Click on Advanced settings and add the java code import com.huawei.hadoop.security.LoginUtil; shown as follows: Use tJava component to customize the tHBaseConnection component The content of the Java code shown as follows\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - Test Completed","title":"HBase Connection Procedure"},{"location":"Data_Integration/Talend/#hbase-input-output-procedure","text":"The content of the Java code shown as follows\uff1a The configuration of tLibraryLoad \uff0c tHBaseConnection \uff0c tJava , tHBaseClose do not change The configuration of tFileInputDelimited shown as following pic: Note: It is required to Edit schema of out.csv The content of out.csv shown as follows: 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq The configuration of tHBaseOutput shown as folloing pic: Note: It is required to Edit Schema of table: The configuration of tHBaseInput shown as folloing pic: The configuration of tLogRow keeps by default Test Completed: Login into the FusinInsight HD cluster and check the HBase table hbaseInputOutputTest by using following comands: hbase shell scan 'hbaseInputOutputTest'","title":"HBase Input Output Procedure"},{"location":"Database/","text":"Database \u00b6","title":"Index"},{"location":"Database/#database","text":"","title":"Database"},{"location":"Development/","text":"Development \u00b6 Jupyter Notebook 5.7.8 \u2194 C80 5.7.8 \u2194 6.5","title":"Index"},{"location":"Development/#development","text":"Jupyter Notebook 5.7.8 \u2194 C80 5.7.8 \u2194 6.5","title":"Development"},{"location":"Development/EN_JupyterNotebook_update/","text":"Connection Instruction between Jupyter Notebook and FusionInsight \u00b6 Succeeded Case \u00b6 Jupyter Notebook 5.7.8 \u2194 FusionInsight HD V100R002C80SPC200 (Hive/ELK/Spark2x) Jupyter Notebook 5.7.8 \u2194 FusionInsight HD 6.5 (Hive/ELK/Spark2x) Install Anaconda \u00b6 Refer to Anaconda official documentation to install Anaconda corresponding to Linux\uff1a https://docs.anaconda.com/anaconda/install/linux/ Use the command wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh to download the Linux-related installation package Use the command bash Anaconda2-2019.03-Linux-x86_64.sh to start the installation Press the Enter to view License Agreement Type yes Choose the installation location to /opt/anaconda2 After the installation is complete, select yes to initialize, and write the initialization settings to the ~/.bashrc file. Use the command cp ~/.bashrc ~/.bashrc.anaconda to copy the generated .basrc file into a new named file .bashrc.anaconda with the following detail info: The red box is the initial configuration added after installing anaconda Use the command vi ~/.bashrc to modify the .bashrc file, delete the conda initialization part\uff1a Use the command source ~/.bashrc.anaconda to initialize the Environment Use the command jupyter notebook --generate-config --allow-root to generate the jupyter notebook configuration file Use the following command vi /root/.jupyter/jupyter_notebook_config.py to modify the configuration parameters: Change the IP Change the port(optional) save Install the FI HD client on the jupyter notebook host. Using the following command to start jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & Copy and paste the corresponding address directly to login to the jupyter notebook web UI\uff1a Connect to Spark2x \u00b6 Note: Use pySpark interface to connect Spark2x HD components Use the previous section command to start jupyter notebook and enter weibUI Go to the following link to obtain the required data file airlines.csv, and upload the data file to the /tmp path of the hdfs: https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv Create a new notebook and enter python code from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.2.118') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) Check the tasks on yarn: Connect to Hive \u00b6 Note: Configure the jdbc interface to connect to the cluster Hive Stop the running jupyter notebook Find the anaconda installation directory/bin/pip executable file, you need to install two python packages related to jdbc, use the following command to install: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall Note: The version of JayDeBeApi and Jpepe1 must be consistent with the above, otherwise a version mismatching error will be reported. Those who have installed these two packages can check the version by using the following command: ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi Put the cluster authentication file user.keytab to the jupyter notebook host's /opt directory for Hive authentication, and put the authentication-related krb5.conf file to the /etc/ path Create a jaas.conf configuration file in the jupyter notebook host /opt directory, the content is as follows: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; Use the following command to load the JVM parameters\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" Use the command java -version to see if the loading is successful\uff1a Start jupyter notebook with the following command source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & Note: If you don't need to interact with Spark2x components, you can directly use the command jupyter notebook --allow-root to directly start jupyter notebook Create a new notebook and enter the following code: import jaydebeapi import jpype import os # this worked conn = jaydebeapi.connect( \"org.apache.hive.jdbc.HiveDriver\", [\"jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/user.keytab\" , \"developuser\", \"Huawei@123\"], [ '/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/ant-1.10.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cglib-3.2.10.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/common-0.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections-3.2.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections4-4.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration-1.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration2-2.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-io-2.4.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang-2.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang3-3.3.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-logging-1.1.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-net-3.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/crypter-0.0.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-client-2.12.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-framework-2.12.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-core-3.1.16.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-frontend-jaxrs-3.1.16.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-transports-http-3.1.16.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/FMS-v1r2c60-20160429.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/guava-19.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-auth-3.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-common-3.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-mapreduce-client-core-3.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/HA-v1r2c60-20160429.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-common-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-jdbc-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-metastore-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-serde-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-rpc-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-0.23-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-common-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-standalone-metastore-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpclient-4.5.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpcore-4.4.4.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-annotations-2.9.8.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-2.9.8.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-asl-1.9.13.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-databind-2.9.8.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-jaxrs-1.9.13.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-mapper-asl-1.9.13.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.annotation-api-1.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.ws.rs-api-2.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jdbc_pom.xml','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jettison-1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jsch-0.1.54.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/libthrift-0.9.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/log4j-1.2.17.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/mockito-all-1.10.19.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/netty-all-4.1.17.Final.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-controller-api-0.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-monitor-plugin-0.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/pms-v1r2c60-20160429.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/protobuf-java-2.5.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-api-1.7.10.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-log4j12-1.7.5.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-aop-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-beans-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-context-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-core-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-expression-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax2-api-3.1.4.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax-api-1.0-2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-5.0.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-asl-4.4.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xercesImpl-2.9.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlpull-1.1.3.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlschema-core-2.2.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xpp3_min-1.1.4c.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xstream-1.4.10.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar']) import pandas as pd sql = \"Select * From drill_iris\" df_hive = pd.read_sql(sql, conn) df_hive conn.close() Note: jaydebeapi.connect () is the jdbc connection method. Jaydebeapi.connect (\"Driver Main Class\", [\"Connecting URL\", \"User\", \"Password\"], \"Path to JDBC driver\"), to connect hive, you need to connect the client All jar packages in the hive jdbc sample are imported Connect to ELK \u00b6 Note\uff1aConfigure jdbc interface to connect to the cluster ELK ELK related configuration Create a database user joe with a password of Bigdata@123 and give user joe all permissions Create HDFS tablespace Create database db_tpcds Create a table named \u201chdfs_001\u201d and insert some test data Refer to the ELK product document to configure the ELK whitelist open for jupyter notebook host Stop the running jupyter notebook Find the anaconda installation directory/bin/pip executable file, you need to install two python packages related to jdbc, use the following command to install: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall Note: The version of JayDeBeApi and Jpepe1 must be consistent with the above, otherwise a version mismatching error will be reported. Those who have installed these two packages can check the version by using the following command: ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi Start jupyter notebook with the following command source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & Note: If you don't need to interact with Spark2x components, you can directly use the command jupyter notebook --allow-root to directly start jupyter notebook Put ELK JDBC driver jar package gsjdbc4.jar in the jupyter notebook host /opt directory Create a new notebook and enter the following code: import jaydebeapi import jpype # this worked conn2 = jaydebeapi.connect( 'org.postgresql.Driver', [\"jdbc:postgresql://172.16.4.121:25108/db_tpcds\" , \"joe\", \"Bigdata@123\"], \"/opt/gsjdbc4.jar\" ) import pandas as pd sql = \"Select * From hdfs_001\" df = pd.read_sql(sql, conn2) df conn2.close() F&Q \u00b6 Encountered the following problems when using pySpark: ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /opt/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 solution\uff1a run sc.stop() Encountered the error when connecting to ELK: solution\uff1aConfigure ELK whitelist","title":"5.7.8 <--> 6.5"},{"location":"Development/EN_JupyterNotebook_update/#connection-instruction-between-jupyter-notebook-and-fusioninsight","text":"","title":"Connection Instruction between Jupyter Notebook and FusionInsight"},{"location":"Development/EN_JupyterNotebook_update/#succeeded-case","text":"Jupyter Notebook 5.7.8 \u2194 FusionInsight HD V100R002C80SPC200 (Hive/ELK/Spark2x) Jupyter Notebook 5.7.8 \u2194 FusionInsight HD 6.5 (Hive/ELK/Spark2x)","title":"Succeeded Case"},{"location":"Development/EN_JupyterNotebook_update/#install-anaconda","text":"Refer to Anaconda official documentation to install Anaconda corresponding to Linux\uff1a https://docs.anaconda.com/anaconda/install/linux/ Use the command wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh to download the Linux-related installation package Use the command bash Anaconda2-2019.03-Linux-x86_64.sh to start the installation Press the Enter to view License Agreement Type yes Choose the installation location to /opt/anaconda2 After the installation is complete, select yes to initialize, and write the initialization settings to the ~/.bashrc file. Use the command cp ~/.bashrc ~/.bashrc.anaconda to copy the generated .basrc file into a new named file .bashrc.anaconda with the following detail info: The red box is the initial configuration added after installing anaconda Use the command vi ~/.bashrc to modify the .bashrc file, delete the conda initialization part\uff1a Use the command source ~/.bashrc.anaconda to initialize the Environment Use the command jupyter notebook --generate-config --allow-root to generate the jupyter notebook configuration file Use the following command vi /root/.jupyter/jupyter_notebook_config.py to modify the configuration parameters: Change the IP Change the port(optional) save Install the FI HD client on the jupyter notebook host. Using the following command to start jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & Copy and paste the corresponding address directly to login to the jupyter notebook web UI\uff1a","title":"Install Anaconda"},{"location":"Development/EN_JupyterNotebook_update/#connect-to-spark2x","text":"Note: Use pySpark interface to connect Spark2x HD components Use the previous section command to start jupyter notebook and enter weibUI Go to the following link to obtain the required data file airlines.csv, and upload the data file to the /tmp path of the hdfs: https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv Create a new notebook and enter python code from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.2.118') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) Check the tasks on yarn:","title":"Connect to Spark2x"},{"location":"Development/EN_JupyterNotebook_update/#connect-to-hive","text":"Note: Configure the jdbc interface to connect to the cluster Hive Stop the running jupyter notebook Find the anaconda installation directory/bin/pip executable file, you need to install two python packages related to jdbc, use the following command to install: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall Note: The version of JayDeBeApi and Jpepe1 must be consistent with the above, otherwise a version mismatching error will be reported. Those who have installed these two packages can check the version by using the following command: ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi Put the cluster authentication file user.keytab to the jupyter notebook host's /opt directory for Hive authentication, and put the authentication-related krb5.conf file to the /etc/ path Create a jaas.conf configuration file in the jupyter notebook host /opt directory, the content is as follows: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; Use the following command to load the JVM parameters\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" Use the command java -version to see if the loading is successful\uff1a Start jupyter notebook with the following command source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & Note: If you don't need to interact with Spark2x components, you can directly use the command jupyter notebook --allow-root to directly start jupyter notebook Create a new notebook and enter the following code: import jaydebeapi import jpype import os # this worked conn = jaydebeapi.connect( \"org.apache.hive.jdbc.HiveDriver\", [\"jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/user.keytab\" , \"developuser\", \"Huawei@123\"], [ '/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/ant-1.10.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cglib-3.2.10.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/common-0.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections-3.2.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections4-4.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration-1.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration2-2.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-io-2.4.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang-2.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang3-3.3.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-logging-1.1.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-net-3.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/crypter-0.0.6.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-client-2.12.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-framework-2.12.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-core-3.1.16.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-frontend-jaxrs-3.1.16.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-transports-http-3.1.16.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/FMS-v1r2c60-20160429.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/guava-19.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-auth-3.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-common-3.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-mapreduce-client-core-3.1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/HA-v1r2c60-20160429.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-common-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-jdbc-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-metastore-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-serde-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-rpc-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-0.23-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-common-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-standalone-metastore-3.1.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpclient-4.5.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpcore-4.4.4.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-annotations-2.9.8.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-2.9.8.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-asl-1.9.13.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-databind-2.9.8.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-jaxrs-1.9.13.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-mapper-asl-1.9.13.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.annotation-api-1.2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.ws.rs-api-2.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jdbc_pom.xml','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jettison-1.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jsch-0.1.54.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/libthrift-0.9.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/log4j-1.2.17.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/mockito-all-1.10.19.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/netty-all-4.1.17.Final.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-controller-api-0.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-monitor-plugin-0.0.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/pms-v1r2c60-20160429.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/protobuf-java-2.5.0.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-api-1.7.10.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-log4j12-1.7.5.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-aop-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-beans-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-context-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-core-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-expression-4.3.20.RELEASE.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax2-api-3.1.4.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax-api-1.0-2.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-5.0.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-asl-4.4.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xercesImpl-2.9.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlpull-1.1.3.1.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlschema-core-2.2.3.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xpp3_min-1.1.4c.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xstream-1.4.10.jar','/opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar']) import pandas as pd sql = \"Select * From drill_iris\" df_hive = pd.read_sql(sql, conn) df_hive conn.close() Note: jaydebeapi.connect () is the jdbc connection method. Jaydebeapi.connect (\"Driver Main Class\", [\"Connecting URL\", \"User\", \"Password\"], \"Path to JDBC driver\"), to connect hive, you need to connect the client All jar packages in the hive jdbc sample are imported","title":"Connect to Hive"},{"location":"Development/EN_JupyterNotebook_update/#connect-to-elk","text":"Note\uff1aConfigure jdbc interface to connect to the cluster ELK ELK related configuration Create a database user joe with a password of Bigdata@123 and give user joe all permissions Create HDFS tablespace Create database db_tpcds Create a table named \u201chdfs_001\u201d and insert some test data Refer to the ELK product document to configure the ELK whitelist open for jupyter notebook host Stop the running jupyter notebook Find the anaconda installation directory/bin/pip executable file, you need to install two python packages related to jdbc, use the following command to install: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall Note: The version of JayDeBeApi and Jpepe1 must be consistent with the above, otherwise a version mismatching error will be reported. Those who have installed these two packages can check the version by using the following command: ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi Start jupyter notebook with the following command source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & Note: If you don't need to interact with Spark2x components, you can directly use the command jupyter notebook --allow-root to directly start jupyter notebook Put ELK JDBC driver jar package gsjdbc4.jar in the jupyter notebook host /opt directory Create a new notebook and enter the following code: import jaydebeapi import jpype # this worked conn2 = jaydebeapi.connect( 'org.postgresql.Driver', [\"jdbc:postgresql://172.16.4.121:25108/db_tpcds\" , \"joe\", \"Bigdata@123\"], \"/opt/gsjdbc4.jar\" ) import pandas as pd sql = \"Select * From hdfs_001\" df = pd.read_sql(sql, conn2) df conn2.close()","title":"Connect to ELK"},{"location":"Development/EN_JupyterNotebook_update/#fq","text":"Encountered the following problems when using pySpark: ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /opt/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 solution\uff1a run sc.stop() Encountered the error when connecting to ELK: solution\uff1aConfigure ELK whitelist","title":"F&amp;Q"},{"location":"Other/","text":"Other \u00b6 Apache Livy 0.5.0 \u2194 C80 0.6.0 \u2194 6.5","title":"Index"},{"location":"Other/#other","text":"Apache Livy 0.5.0 \u2194 C80 0.6.0 \u2194 6.5","title":"Other"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/","text":"Connection Instruction between Apache Livy and FusionInsight \u00b6 Succeeded Case \u00b6 Apache Livy 0.6.0 \u2194 FusionInsight HD 6.5 (Spark2x) Apache Livy 0.5.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x) Deploy the externally verified livy service and submit tasks using session and batch methods \u00b6 Scenario Description \u00b6 In some complex scenarios, access control is required for users who submit tasks. The livy service supports Kerberos SPNEGO authentication for external access. The following is a specific test scenario Connection with FI HD cluster: 172.16.6.10-12, three-node deployment Apache Livy server side\uff1a 172.16.2.118, Install the FI HD client on this node and complete the download and installation of Livy by referring to the previous chapter. Client: 172.16.2.119, Submit a task request using the curl command on this node. You need to install an FI HD client and check whether curl supports SPNEGO authentication by using the command 'curl -V' Two authenticated users are used in this scenario, user developuser, and user livy User livy is the user who needs to actually submit a spark task request to the FI HD cluster for the livy service User developuser is the user used by the client to submit tasks to the Livy server The entire business process is actually the proxy user developuser submits the spark task to the FI HD cluster in the name of the user livy, but before the task is performed, the user developuser needs to pass the Kerberos authentication of the FI HD cluster. In this way, the Apache Livy server access control is implemented Kerberos authentication-related configuration \u00b6 Log in to the FI HD manager to create the users developuser, livy to be used in the test. And download the user livy authentication information (user.keytab, krb5.conf) Log in to kadmin with the FI HD client, and create a new principal for FI HD's Kerberos authentication to the Livy HTTP service. The principal is \"HTTP/host-172-16-2-118\". host-172-16-2-118 is the hostname of the node where Apache Livy is deployed. When you execute the command \"kadmin \u2013p kadmin/admin\", the initial password is \"Admin@123\", and the new password must be kept in mind after modification. Pass the generated http2.keytab (keytab file name can be customized) authentication file to the /opt path of the livy server and use the \"kinit -kt\" command to check whether the authentication is successful kinit -kt /opt/http2.keytab HTTP/host-172-16-2-118@HADOOP.COM Use the command \"kdestroy\" to clear the cached notes when done To log in to the cluster, click Service Management-> Yarn-> Service Configuration-> Select All Configuration-> Custom. Add the following configuration under the corresponding parameter file as core-site.xml: hadoop.proxyuser.livy.hosts = * hadoop.proxyuser.livy.groups = * Follow the same method above to add the same configuration to the core-site.xml file on both hdfs and hive service: Client related checks \u00b6 Use \"curl -V\" command to check if client curl command supports Kerberos Spnego Install the corresponding client for FI HD cluster Check that client time and cluster time are less than 5 minutes Livy server configuration \u00b6 Install the corresponding client for FI HD cluster Check that client time and cluster time are less than 5 minutes Check the livy.conf file configuration Note\uff1a \"livy.file.local-dir-whitelist=/opt/\". This configuration parameter is to use the livy batch method to submit the task locally, you need to open the whitelist of the local path. launch.kerberos.* related configuration parameters are required for Livy to actually interact with the FI HD cluster auth.kerberos.* related configuration parameters are used for external access control to livy web UI Check the livy-client.conf file configuration Check the livy-env.sh file configuration Check spark-blacklist.conf file configuration Add the following configuration to the log4j.properties file to adjust the log level (optional) log4j.logger.org.eclipse.jetty=DEBUG Submit tasks using Livy session \u00b6 The livy session mode corresponds to the spark console interactive mode. The spark task is submitted and ran by using the detailed code Log into the livy server and use bin / livy-server start to start the livy service Open Livy-end livy-root-server.out log to see if livy starts successfully Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket Use the following command to start a pyspark session in livy curl --negotiate -k -v -u developuser : -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/sessions Submit a piece of code for session/0 using the following command curl --negotiate -k -v -u developuser : -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://host-172-16-2-118:8998/sessions/0/statements Use the following command to view the results: curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0/statements | python -m json.tool Use the following command to close the session curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0 -X DELETE Log in to the connected cluster's yarn view the results: Besides, after the client (172.16.2.119) completes the curl command submission task, you can use klist to view the ticket information: You can see that the ticket authentication ticket will be updated HTTP/host-172-16-2-118@HADOOP.COM Submit task example using Livy batch(1) \u00b6 The livy batch mode corresponds to the spark-submit interactive mode. The spark task is submitted and ran by using a compiled jar package or a completed py file. This example calculates the pi value by submitting a jar package locally using yarn client mode Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Find the test jar package spark-examples_2.11-2.1.0.jar in the FI HD client and transfer it to the /opt/ path of the livy server Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results: Submit task example using Livy batch(2) \u00b6 This example calculates the pi value by submitting a py file locally using yarn client mode Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Create a py2.py file and upload it to the /opt/ path of the Livy server. The specific content is as follows\uff1a import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) spark.stop() Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/pi2.py\" }' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results: Submit task example using Livy batch(3) \u00b6 This example uses the yarn cluster mode to submit a jar package under the cluster hdfs path and run it to calculate the pi value. Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Modify the livy.conf file configuration to: Upload the jar package in the /tmp path of the HDFS connected to the FI HD cluster Restart Livy Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results: Submit task example using Livy batch(4) \u00b6 This example uses the yarn cluster mode to submit a jar package under the local path of the cluster and run it to calculate the pi value. Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Because you use the yarn cluster to submit the jar package locally, you do not know in advance which cluster node the worker is in, so put the jar package spark-examples_2.11-2.1.0.jar in the /home path of each cluster node: Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results: Configure Kerberos Spnego on Windows jump machine to log in livy web UI \u00b6 The authentication mechanism of the Windows jump machine (172.16.2.111) accessing Livy web UI is the same as that of the client (172.16.2.119) using curl command to access Livy server and submit spark task. Product documentation -> Application Development Guide -> Security Mode -> Spark2x Development Guide -> Preparing the Environment -> Preparing for the HiveODBC Development Environment -> Windows Environment -> Finish the first 4 steps Configure JCE Download the Java Cryptography Extension (JCE) from the java official website, then decompress it to %JAVA_HOME%/jre/lib/security and replace the corresponding file. Check if the hostname of the livy server is added to the hosts file: Configure Firefox Firefox under windows needs to adjust the following parameters by visiting the about:config page: network.negotiate-auth.trusted-uris is set to trusted addresses network.auth.use-sspi is set to disable sspi authentication protocol Authentication with MIT Kerberos Login to Livy's web ui address http://host-172-16-2-118:8998/ui Submit the task using the previous sample and check it on Livy web ui Check the service ticket generated by MIT Kerberos","title":"0.6.0 <--> 6.5"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#connection-instruction-between-apache-livy-and-fusioninsight","text":"","title":"Connection Instruction between Apache Livy and FusionInsight"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#succeeded-case","text":"Apache Livy 0.6.0 \u2194 FusionInsight HD 6.5 (Spark2x) Apache Livy 0.5.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x)","title":"Succeeded Case"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#deploy-the-externally-verified-livy-service-and-submit-tasks-using-session-and-batch-methods","text":"","title":"Deploy the externally verified livy service and submit tasks using session and batch methods"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#scenario-description","text":"In some complex scenarios, access control is required for users who submit tasks. The livy service supports Kerberos SPNEGO authentication for external access. The following is a specific test scenario Connection with FI HD cluster: 172.16.6.10-12, three-node deployment Apache Livy server side\uff1a 172.16.2.118, Install the FI HD client on this node and complete the download and installation of Livy by referring to the previous chapter. Client: 172.16.2.119, Submit a task request using the curl command on this node. You need to install an FI HD client and check whether curl supports SPNEGO authentication by using the command 'curl -V' Two authenticated users are used in this scenario, user developuser, and user livy User livy is the user who needs to actually submit a spark task request to the FI HD cluster for the livy service User developuser is the user used by the client to submit tasks to the Livy server The entire business process is actually the proxy user developuser submits the spark task to the FI HD cluster in the name of the user livy, but before the task is performed, the user developuser needs to pass the Kerberos authentication of the FI HD cluster. In this way, the Apache Livy server access control is implemented","title":"Scenario Description"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#kerberos-authentication-related-configuration","text":"Log in to the FI HD manager to create the users developuser, livy to be used in the test. And download the user livy authentication information (user.keytab, krb5.conf) Log in to kadmin with the FI HD client, and create a new principal for FI HD's Kerberos authentication to the Livy HTTP service. The principal is \"HTTP/host-172-16-2-118\". host-172-16-2-118 is the hostname of the node where Apache Livy is deployed. When you execute the command \"kadmin \u2013p kadmin/admin\", the initial password is \"Admin@123\", and the new password must be kept in mind after modification. Pass the generated http2.keytab (keytab file name can be customized) authentication file to the /opt path of the livy server and use the \"kinit -kt\" command to check whether the authentication is successful kinit -kt /opt/http2.keytab HTTP/host-172-16-2-118@HADOOP.COM Use the command \"kdestroy\" to clear the cached notes when done To log in to the cluster, click Service Management-> Yarn-> Service Configuration-> Select All Configuration-> Custom. Add the following configuration under the corresponding parameter file as core-site.xml: hadoop.proxyuser.livy.hosts = * hadoop.proxyuser.livy.groups = * Follow the same method above to add the same configuration to the core-site.xml file on both hdfs and hive service:","title":"Kerberos authentication-related configuration"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#client-related-checks","text":"Use \"curl -V\" command to check if client curl command supports Kerberos Spnego Install the corresponding client for FI HD cluster Check that client time and cluster time are less than 5 minutes","title":"Client related checks"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#livy-server-configuration","text":"Install the corresponding client for FI HD cluster Check that client time and cluster time are less than 5 minutes Check the livy.conf file configuration Note\uff1a \"livy.file.local-dir-whitelist=/opt/\". This configuration parameter is to use the livy batch method to submit the task locally, you need to open the whitelist of the local path. launch.kerberos.* related configuration parameters are required for Livy to actually interact with the FI HD cluster auth.kerberos.* related configuration parameters are used for external access control to livy web UI Check the livy-client.conf file configuration Check the livy-env.sh file configuration Check spark-blacklist.conf file configuration Add the following configuration to the log4j.properties file to adjust the log level (optional) log4j.logger.org.eclipse.jetty=DEBUG","title":"Livy server configuration"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#submit-tasks-using-livy-session","text":"The livy session mode corresponds to the spark console interactive mode. The spark task is submitted and ran by using the detailed code Log into the livy server and use bin / livy-server start to start the livy service Open Livy-end livy-root-server.out log to see if livy starts successfully Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket Use the following command to start a pyspark session in livy curl --negotiate -k -v -u developuser : -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/sessions Submit a piece of code for session/0 using the following command curl --negotiate -k -v -u developuser : -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://host-172-16-2-118:8998/sessions/0/statements Use the following command to view the results: curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0/statements | python -m json.tool Use the following command to close the session curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0 -X DELETE Log in to the connected cluster's yarn view the results: Besides, after the client (172.16.2.119) completes the curl command submission task, you can use klist to view the ticket information: You can see that the ticket authentication ticket will be updated HTTP/host-172-16-2-118@HADOOP.COM","title":"Submit tasks using Livy session"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#submit-task-example-using-livy-batch1","text":"The livy batch mode corresponds to the spark-submit interactive mode. The spark task is submitted and ran by using a compiled jar package or a completed py file. This example calculates the pi value by submitting a jar package locally using yarn client mode Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Find the test jar package spark-examples_2.11-2.1.0.jar in the FI HD client and transfer it to the /opt/ path of the livy server Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results:","title":"Submit task example using Livy batch(1)"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#submit-task-example-using-livy-batch2","text":"This example calculates the pi value by submitting a py file locally using yarn client mode Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Create a py2.py file and upload it to the /opt/ path of the Livy server. The specific content is as follows\uff1a import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) spark.stop() Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/pi2.py\" }' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results:","title":"Submit task example using Livy batch(2)"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#submit-task-example-using-livy-batch3","text":"This example uses the yarn cluster mode to submit a jar package under the cluster hdfs path and run it to calculate the pi value. Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Modify the livy.conf file configuration to: Upload the jar package in the /tmp path of the HDFS connected to the FI HD cluster Restart Livy Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results:","title":"Submit task example using Livy batch(3)"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#submit-task-example-using-livy-batch4","text":"This example uses the yarn cluster mode to submit a jar package under the local path of the cluster and run it to calculate the pi value. Log in to the client (172.16.2.119) and use kinit developuser to enter a password to obtain a ticket. Because you use the yarn cluster to submit the jar package locally, you do not know in advance which cluster node the worker is in, so put the jar package spark-examples_2.11-2.1.0.jar in the /home path of each cluster node: Submit the spark task on the client (172.16.2.119) with the following command curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches Open the livy-root-server.out log to see the results: Log in to the yarn to view the results:","title":"Submit task example using Livy batch(4)"},{"location":"Other/EN_Using_Livy0.6.0_with_FusionInsight_HD_6_5_1_update/#configure-kerberos-spnego-on-windows-jump-machine-to-log-in-livy-web-ui","text":"The authentication mechanism of the Windows jump machine (172.16.2.111) accessing Livy web UI is the same as that of the client (172.16.2.119) using curl command to access Livy server and submit spark task. Product documentation -> Application Development Guide -> Security Mode -> Spark2x Development Guide -> Preparing the Environment -> Preparing for the HiveODBC Development Environment -> Windows Environment -> Finish the first 4 steps Configure JCE Download the Java Cryptography Extension (JCE) from the java official website, then decompress it to %JAVA_HOME%/jre/lib/security and replace the corresponding file. Check if the hostname of the livy server is added to the hosts file: Configure Firefox Firefox under windows needs to adjust the following parameters by visiting the about:config page: network.negotiate-auth.trusted-uris is set to trusted addresses network.auth.use-sspi is set to disable sspi authentication protocol Authentication with MIT Kerberos Login to Livy's web ui address http://host-172-16-2-118:8998/ui Submit the task using the previous sample and check it on Livy web ui Check the service ticket generated by MIT Kerberos","title":"Configure Kerberos Spnego on Windows jump machine to log in livy web UI"},{"location":"SQL_Analytics/","text":"SQL Analytics \u00b6","title":"Index"},{"location":"SQL_Analytics/#sql-analytics","text":"","title":"SQL Analytics"}]}