{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u667a\u80fd\u6570\u636e\u751f\u6001\u5730\u56fe \u00b6 FusionInsight\u652f\u6301\u5f00\u6e90\u6807\u51c6\u7684Hadoop\u63a5\u53e3\uff0c\u53ef\u4ee5\u4e0e\u4ee5\u4e0b\u7b2c\u4e09\u65b9\u5de5\u5177\u8fdb\u884c\u5bf9\u63a5 \u7b2c\u4e09\u65b9\u5de5\u5177 FusionInsight \u6d89\u53ca\u9886\u57df \u5de5\u5177\u540d\u79f0 \u7248\u672c C50 C60 C70 C80 6.5 \u6570\u636e\u53ef\u89c6\u5316 FineBI 5.1 Hive IBM Cognos 10.2.2fp4 Hive Oracle BIEE 11g Hive SparkSQL Hive SparkSQL ELK GaussDB 12c Hive SparkSQL Hive SparkSQL ELK GaussDB Hive SparkSQL ELK GaussDB Qlik Sense 3.2.4 Hive SparkSQL Hive SparkSQL QlikView 12 Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL SmartBI 7.2.32464.17374 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Hive SparkSQL \u6570\u636e\u5206\u6790 Alteryx 2018.2.5.48994 HDFS Hive SparkSQL HDFS Hive SparkSQL HDFS Hive SparkSQL GeoMesa 2.3.1 HBase Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark SAS 9.4M3 HDFS Hive Yarn HDFS Hive Yarn HDFS Hive Yarn Splunk 7.2.4 HDFS Hive HDFS \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 Hive SparkSQL \u6570\u636e\u96c6\u6210 Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka 1.9.2 HDFS HBase Hive Spark Kafka Confluent 4.1.0 HDFS Kafka HDFS Kafka Denodo Platform 7.0 Hive Spark2x Hive Spark2x H2O.ai 3.24.0.2 HDFS GuassDB IBM InfoSphere CDC 11.3.3.1 HDFS IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL 11.5.0.2 HDFS Hive Phoenix SparkSQL Kafka GaussDB Informatica 10.0.0 HDFS HBase Hive HDFS HBase Hive HDFS HBase Hive Yarn HDFS Hive 10.2.2 HDFS HBase Hive Informatica PowerCenter 10.2.0 HDFS Hive HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Kafka Manager 1.3.3.21 Kafka Kettle 6.1 HDFS Hive HDFS Hive HDFS Hive Knime 3.6.1 HDFS Hive Spark HDFS Hive OceanSource 1.0 HBase Hive Kafka ElasticSearch Redis GaussDB Oracle GoldenGate 12.2 HDFS HBase Flume Kafka HDFS HBase Flume Kafka 12.3 HDFS HBase Flume Kafka HDFS HBase Flume Kafka Pentaho 7.1 HDFS Hive 8.0 HDFS Hive SharePlex 9.2.1 Kafka Kafka Talend 6.4.1 HDFS HBase Hive 7.0.1 HDFS HBase 7.2.1 HDFS HBase Hive Tibco BW 5.13 GaussDB \u676d\u5dde\u5408\u4f17UTL 5.1 HDFS HBase Hive Kafka \u6570\u636e\u5e93 Apache Druid 0.14.2 HDFS Kafka 0.15.1 HDFS Kafka SAP VORA 2.0 Spark 2.1 Spark \u676d\u5dde\u5408\u4f17UDB 6.1 GaussDB \u96c6\u6210\u5f00\u53d1\u73af\u5883 DBeaver 4.0.8 Hive Phoenix SparkSQL 4.2.1 Hive Phoenix SparkSQL 6.1.4 Hive Phoenix SparkSQL DbVisualizer 10.0.1 Hive Phoenix SparkSQL 10.0.21 Hive Phoenix SparkSQL 9.5.7 Hive Phoenix SparkSQL HUE 4.0.1 HDFS HBase Hive Spark Jupyter Notebook 2.4.4.0 pySpark 2.7.16 Hive Elk Spark2x Hive Elk Spark2x JupyterHub 1.0.0 Spark2x RStudio 3.4.1 SparkR SparkR Squirrel 3.7.1 Hive Phoenix SparkSQL 3.8.0 Hive Phoenix SparkSQL 3.9.1 Hive Phoenix SparkSQL Zeppelin 0.7.2 HBase Hive Spark SparkR 0.7.3 HBase Hive Spark SparkR HBase Hive Spark SparkR 0.8.0 HBase Hive Spark SparkR ELK \u5176\u4ed6 Apache Livy 0.5.0 Spark2x 0.6.0 Spark2x GIS Tools for Hadoop 1.0 Hive MapReduce IBM WAS 8.5.5.9 IBM_JDK Kibana 6.1.3 ElasticSearch Logstash 6.4.2 ElasticSearch NeoKylin 6.9 OS 7.2 OS elasticsearch-head 1.0 ElasticSearch filebeat 6.5.1 ElasticSearch SQL\u5206\u6790 Apache Drill 1.15.0 HDFS Hive HBase Kafka Apache Kylin 1.6.0 HBase Hive 2.1.0 HBase Hive 2.3.1 HBase Hive 2.6.1 HBase Hive Kyligence Analytics Platform 2.2 HBase Hive 2.3 HBase Hive 2.4 HBase Hive 2.5 HBase Hive 3.0 HBase Hive Yarn Presto 0.155 HDFS Hive 0.184 HDFS Hive 0.196 HDFS Hive 0.210 HDFS Hive ElasticSearch HDFS Hive","title":"Home"},{"location":"#_1","text":"FusionInsight\u652f\u6301\u5f00\u6e90\u6807\u51c6\u7684Hadoop\u63a5\u53e3\uff0c\u53ef\u4ee5\u4e0e\u4ee5\u4e0b\u7b2c\u4e09\u65b9\u5de5\u5177\u8fdb\u884c\u5bf9\u63a5 \u7b2c\u4e09\u65b9\u5de5\u5177 FusionInsight \u6d89\u53ca\u9886\u57df \u5de5\u5177\u540d\u79f0 \u7248\u672c C50 C60 C70 C80 6.5 \u6570\u636e\u53ef\u89c6\u5316 FineBI 5.1 Hive IBM Cognos 10.2.2fp4 Hive Oracle BIEE 11g Hive SparkSQL Hive SparkSQL ELK GaussDB 12c Hive SparkSQL Hive SparkSQL ELK GaussDB Hive SparkSQL ELK GaussDB Qlik Sense 3.2.4 Hive SparkSQL Hive SparkSQL QlikView 12 Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL SmartBI 7.2.32464.17374 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Hive SparkSQL \u6570\u636e\u5206\u6790 Alteryx 2018.2.5.48994 HDFS Hive SparkSQL HDFS Hive SparkSQL HDFS Hive SparkSQL GeoMesa 2.3.1 HBase Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark SAS 9.4M3 HDFS Hive Yarn HDFS Hive Yarn HDFS Hive Yarn Splunk 7.2.4 HDFS Hive HDFS \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 Hive SparkSQL \u6570\u636e\u96c6\u6210 Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka 1.9.2 HDFS HBase Hive Spark Kafka Confluent 4.1.0 HDFS Kafka HDFS Kafka Denodo Platform 7.0 Hive Spark2x Hive Spark2x H2O.ai 3.24.0.2 HDFS GuassDB IBM InfoSphere CDC 11.3.3.1 HDFS IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL 11.5.0.2 HDFS Hive Phoenix SparkSQL Kafka GaussDB Informatica 10.0.0 HDFS HBase Hive HDFS HBase Hive HDFS HBase Hive Yarn HDFS Hive 10.2.2 HDFS HBase Hive Informatica PowerCenter 10.2.0 HDFS Hive HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Kafka Manager 1.3.3.21 Kafka Kettle 6.1 HDFS Hive HDFS Hive HDFS Hive Knime 3.6.1 HDFS Hive Spark HDFS Hive OceanSource 1.0 HBase Hive Kafka ElasticSearch Redis GaussDB Oracle GoldenGate 12.2 HDFS HBase Flume Kafka HDFS HBase Flume Kafka 12.3 HDFS HBase Flume Kafka HDFS HBase Flume Kafka Pentaho 7.1 HDFS Hive 8.0 HDFS Hive SharePlex 9.2.1 Kafka Kafka Talend 6.4.1 HDFS HBase Hive 7.0.1 HDFS HBase 7.2.1 HDFS HBase Hive Tibco BW 5.13 GaussDB \u676d\u5dde\u5408\u4f17UTL 5.1 HDFS HBase Hive Kafka \u6570\u636e\u5e93 Apache Druid 0.14.2 HDFS Kafka 0.15.1 HDFS Kafka SAP VORA 2.0 Spark 2.1 Spark \u676d\u5dde\u5408\u4f17UDB 6.1 GaussDB \u96c6\u6210\u5f00\u53d1\u73af\u5883 DBeaver 4.0.8 Hive Phoenix SparkSQL 4.2.1 Hive Phoenix SparkSQL 6.1.4 Hive Phoenix SparkSQL DbVisualizer 10.0.1 Hive Phoenix SparkSQL 10.0.21 Hive Phoenix SparkSQL 9.5.7 Hive Phoenix SparkSQL HUE 4.0.1 HDFS HBase Hive Spark Jupyter Notebook 2.4.4.0 pySpark 2.7.16 Hive Elk Spark2x Hive Elk Spark2x JupyterHub 1.0.0 Spark2x RStudio 3.4.1 SparkR SparkR Squirrel 3.7.1 Hive Phoenix SparkSQL 3.8.0 Hive Phoenix SparkSQL 3.9.1 Hive Phoenix SparkSQL Zeppelin 0.7.2 HBase Hive Spark SparkR 0.7.3 HBase Hive Spark SparkR HBase Hive Spark SparkR 0.8.0 HBase Hive Spark SparkR ELK \u5176\u4ed6 Apache Livy 0.5.0 Spark2x 0.6.0 Spark2x GIS Tools for Hadoop 1.0 Hive MapReduce IBM WAS 8.5.5.9 IBM_JDK Kibana 6.1.3 ElasticSearch Logstash 6.4.2 ElasticSearch NeoKylin 6.9 OS 7.2 OS elasticsearch-head 1.0 ElasticSearch filebeat 6.5.1 ElasticSearch SQL\u5206\u6790 Apache Drill 1.15.0 HDFS Hive HBase Kafka Apache Kylin 1.6.0 HBase Hive 2.1.0 HBase Hive 2.3.1 HBase Hive 2.6.1 HBase Hive Kyligence Analytics Platform 2.2 HBase Hive 2.3 HBase Hive 2.4 HBase Hive 2.5 HBase Hive 3.0 HBase Hive Yarn Presto 0.155 HDFS Hive 0.184 HDFS Hive 0.196 HDFS Hive 0.210 HDFS Hive ElasticSearch HDFS Hive","title":"\u667a\u80fd\u6570\u636e\u751f\u6001\u5730\u56fe"},{"location":"Business_Intelligence/","text":"\u6570\u636e\u53ef\u89c6\u5316 \u00b6 FineBI 5.1 \u2194 6.5 IBM Cognos 10.2.2fp4 \u2194 C60 Oracle BIEE 11g \u2194 C60 11g \u2194 C70 12c \u2194 C60 12c \u2194 C70 12c \u2194 6.5 Qlik Sense 3.2.4 \u2194 C70 3.2.4 \u2194 6.5 QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 12 \u2194 6.5 SmartBI 7.2.32464.17374 \u2194 C70 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80 10.5.0 \u2194 6.5","title":"Home"},{"location":"Business_Intelligence/#_1","text":"FineBI 5.1 \u2194 6.5 IBM Cognos 10.2.2fp4 \u2194 C60 Oracle BIEE 11g \u2194 C60 11g \u2194 C70 12c \u2194 C60 12c \u2194 C70 12c \u2194 6.5 Qlik Sense 3.2.4 \u2194 C70 3.2.4 \u2194 6.5 QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 12 \u2194 6.5 SmartBI 7.2.32464.17374 \u2194 C70 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80 10.5.0 \u2194 6.5","title":"\u6570\u636e\u53ef\u89c6\u5316"},{"location":"Business_Intelligence/FineBI_5.1/","text":"FineBI\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 FineBI 5.1 \u2194 FusionInsight HD 6.5 (Hive) \u5b89\u88c5FineBI \u00b6 \u5b89\u88c5FineBI, \u4ee5\u8def\u5f84 C:\\soft\\fineBI\\FineBI5.1 \u4e3a\u4f8b \u914d\u7f6eJDBC\u63a5\u53e3\u5bf9\u63a5Hive \u00b6 \u5c06\u5bf9\u63a5\u96c6\u7fa4\uff08\u7248\u672c6.5.1\uff09\u7684\u8ba4\u8bc1\u6587\u4ef6\u4e0b\u8f7d\u5230 C:\\651client \u6587\u4ef6\u5939\u4e0b\uff0c\u5305\u62ecuser.keytab\u548ckrb5.conf \u5728FineBI\u7684bin\u76ee\u5f55\u4e0b\u627e\u5230\u914d\u7f6e\u6587\u4ef6finebi.vmoptions \u5e76\u5728\u8be5\u6587\u4ef6\u4e2d\u6dfb\u52a0\u914d\u7f6e\uff1a jaas.conf\u6587\u4ef6\u5185\u5bb9\u4e3a\uff1a \u4e0b\u8f7dFI HD6.5.1\u7684\u5ba2\u6237\u7aef\u5230\u672c\u5730\uff0c\u8865\u5168jar\u5305\uff0c\u627e\u5230Hive\\Beeline\\lib\u8def\u5f84\u53ef\u4ee5\u770b\u5230\u76f8\u5173\u7684jar\u5305 \u627e\u5230\u5e76\u8fdb\u5165FineBI\u76f8\u5173\u4f9d\u8d56\u8def\u5f84\uff0c\u5177\u4f53\u4e3a C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib , \u9700\u8981\u505a\u4ee5\u4e0b\u4e09\u4e2a\u64cd\u4f5c \u627e\u5230jar\u5305fine-bi-engine-third-5.1.jar\uff0c\u53f3\u952e\u4f7f\u7528winRAR\u6253\u5f00 \u8fdb\u5165org/apache\u76ee\u5f55\uff0c\u627e\u5230\u5e76\u5220\u9664zookeeper\u6587\u4ef6\u5939 \u5220\u9664FineBI\u81ea\u5e26\u7684zookeeper-3.4.6.jar \u5c06\u4e0a\u4e00\u6b65\u96c6\u7fa4\u5ba2\u6237\u7aefHive\\Beeline\\lib\u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u5230\u5f53\u524d\u6587\u4ef6\u5939\uff08 C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib \uff09 \u542f\u52a8FineBI\uff0c\u627e\u5230\u7ba1\u7406\u7cfb\u7edf -> \u6570\u636e\u8fde\u63a5 -> \u65b0\u5efa\u6570\u636e\u8fde\u63a5 -> \u66f4\u591a\u6570\u636e\u8fde\u63a5 \u627e\u5230fusioninsight hd \u9009\u4e2d\u70b9\u786e\u5b9a \u53c2\u8003\u4e0b\u56fe\u914d\u7f6e\u8fde\u63a5\u53c2\u6570 URL: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/651client/user.keytab \u9700\u8981\u6ce8\u610f\u7684\u662fuser.principal\u548cuser.keytab\u9700\u8981\u540c\u5b9e\u9645\u60c5\u51b5\u5339\u914d \u70b9\u51fb\u6d4b\u8bd5\u8fde\u63a5\u6d4b\u8bd5 \u70b9\u51fb\u521b\u5efa -> \u6dfb\u52a0\u6dfb\u52a0\u6570\u636e\u5e93\u8868 \u9009\u62e9\u4e00\u5f20\u8868\uff0c\u5982\u679c\u5df2\u7ecf\u9009\u62e9\u914d\u7f6e\u8fc7\u5219\u4e3a\u7070\u8272,\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u6570\u636e\u5217\u8868\u8def\u5f84 \u5728\u6570\u636e\u51c6\u5907->\u5bf9\u5e94\u7684\u6570\u636e\u5217\u8868\u8def\u5f84\u4e2d\u627e\u5230\u4e4b\u524d\u914d\u7f6e\u597d\u7684\u8868test","title":"5.1 <--> 6.5"},{"location":"Business_Intelligence/FineBI_5.1/#finebifusioninsight","text":"","title":"FineBI\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/FineBI_5.1/#_1","text":"FineBI 5.1 \u2194 FusionInsight HD 6.5 (Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/FineBI_5.1/#finebi","text":"\u5b89\u88c5FineBI, \u4ee5\u8def\u5f84 C:\\soft\\fineBI\\FineBI5.1 \u4e3a\u4f8b","title":"\u5b89\u88c5FineBI"},{"location":"Business_Intelligence/FineBI_5.1/#jdbchive","text":"\u5c06\u5bf9\u63a5\u96c6\u7fa4\uff08\u7248\u672c6.5.1\uff09\u7684\u8ba4\u8bc1\u6587\u4ef6\u4e0b\u8f7d\u5230 C:\\651client \u6587\u4ef6\u5939\u4e0b\uff0c\u5305\u62ecuser.keytab\u548ckrb5.conf \u5728FineBI\u7684bin\u76ee\u5f55\u4e0b\u627e\u5230\u914d\u7f6e\u6587\u4ef6finebi.vmoptions \u5e76\u5728\u8be5\u6587\u4ef6\u4e2d\u6dfb\u52a0\u914d\u7f6e\uff1a jaas.conf\u6587\u4ef6\u5185\u5bb9\u4e3a\uff1a \u4e0b\u8f7dFI HD6.5.1\u7684\u5ba2\u6237\u7aef\u5230\u672c\u5730\uff0c\u8865\u5168jar\u5305\uff0c\u627e\u5230Hive\\Beeline\\lib\u8def\u5f84\u53ef\u4ee5\u770b\u5230\u76f8\u5173\u7684jar\u5305 \u627e\u5230\u5e76\u8fdb\u5165FineBI\u76f8\u5173\u4f9d\u8d56\u8def\u5f84\uff0c\u5177\u4f53\u4e3a C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib , \u9700\u8981\u505a\u4ee5\u4e0b\u4e09\u4e2a\u64cd\u4f5c \u627e\u5230jar\u5305fine-bi-engine-third-5.1.jar\uff0c\u53f3\u952e\u4f7f\u7528winRAR\u6253\u5f00 \u8fdb\u5165org/apache\u76ee\u5f55\uff0c\u627e\u5230\u5e76\u5220\u9664zookeeper\u6587\u4ef6\u5939 \u5220\u9664FineBI\u81ea\u5e26\u7684zookeeper-3.4.6.jar \u5c06\u4e0a\u4e00\u6b65\u96c6\u7fa4\u5ba2\u6237\u7aefHive\\Beeline\\lib\u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u5230\u5f53\u524d\u6587\u4ef6\u5939\uff08 C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib \uff09 \u542f\u52a8FineBI\uff0c\u627e\u5230\u7ba1\u7406\u7cfb\u7edf -> \u6570\u636e\u8fde\u63a5 -> \u65b0\u5efa\u6570\u636e\u8fde\u63a5 -> \u66f4\u591a\u6570\u636e\u8fde\u63a5 \u627e\u5230fusioninsight hd \u9009\u4e2d\u70b9\u786e\u5b9a \u53c2\u8003\u4e0b\u56fe\u914d\u7f6e\u8fde\u63a5\u53c2\u6570 URL: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/651client/user.keytab \u9700\u8981\u6ce8\u610f\u7684\u662fuser.principal\u548cuser.keytab\u9700\u8981\u540c\u5b9e\u9645\u60c5\u51b5\u5339\u914d \u70b9\u51fb\u6d4b\u8bd5\u8fde\u63a5\u6d4b\u8bd5 \u70b9\u51fb\u521b\u5efa -> \u6dfb\u52a0\u6dfb\u52a0\u6570\u636e\u5e93\u8868 \u9009\u62e9\u4e00\u5f20\u8868\uff0c\u5982\u679c\u5df2\u7ecf\u9009\u62e9\u914d\u7f6e\u8fc7\u5219\u4e3a\u7070\u8272,\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u6570\u636e\u5217\u8868\u8def\u5f84 \u5728\u6570\u636e\u51c6\u5907->\u5bf9\u5e94\u7684\u6570\u636e\u5217\u8868\u8def\u5f84\u4e2d\u627e\u5230\u4e4b\u524d\u914d\u7f6e\u597d\u7684\u8868test","title":"\u914d\u7f6eJDBC\u63a5\u53e3\u5bf9\u63a5Hive"},{"location":"Business_Intelligence/IBM_Cognos/","text":"IBM Cognos\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM Cognos 10.2.2fp4 \u2194 FusionInsight HD V100R002C60U20 (Hive)","title":"10.2.2fp4 <--> C60"},{"location":"Business_Intelligence/IBM_Cognos/#ibm-cognosfusioninsight","text":"","title":"IBM Cognos\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/IBM_Cognos/#_1","text":"IBM Cognos 10.2.2fp4 \u2194 FusionInsight HD V100R002C60U20 (Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Oracle_BIEE/","text":"Oracle BIEE\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Oracle BIEE 11g \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 11g \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD 6.5 (Hive/SparkSQL/ELK/GaussDB) Linux\u73af\u5883\u5b89\u88c5OBIEE \u00b6 \u5b89\u88c5OS \u00b6 \u5b89\u88c5RedHat6.5\u64cd\u4f5c\u7cfb\u7edf\uff0cdesktop\u7248 \u521b\u5efa\u7528\u6237oracle \u5b89\u88c5jdk8 \u00b6 \u83b7\u53d6jdk8\u5b89\u88c5\u5305\uff0c\u6267\u884c\u5b89\u88c5 \u5b89\u88c5Weblogic \u00b6 \u521b\u5efaoracle home\u76ee\u5f55\uff1a umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ \u4e0a\u4f20weblogic\u5b89\u88c5\u5305\uff0c\u89e3\u538b \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 \u5b89\u88c5BI Server \u00b6 \u4e0a\u4f20OBIEE\u5b89\u88c5\u5305\uff0c\u89e3\u538b chmod 755 bi_platform-12.2.1.2.0_linux64.bin \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 ./bi_platform-12.2.1.2.0_linux64.bin \u8865\u9f50lib\u5305 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel \u53d6\u6d88\u5f53\u524d\u5b89\u88c5\uff0c\u91cd\u65b0\u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f \u5b89\u88c5Oracle Database 12c \u00b6 \u5b89\u88c5\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b89\u88c5\u76ee\u5f55 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle \u4e0b\u8f7dOracle Database 12c\u5b89\u88c5\u5305\uff0c\u89e3\u538b\u5f97\u5230database\u6587\u4ef6\u5939 chmod -R 755 database/ cd database/ su oracle ./runInstaller \u53ea\u5b89\u88c5\u5355\u5b9e\u4f8b\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b9e\u4f8b cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca \u5b57\u7b26\u96c6\u9009\u62e9AL32UTF8\uff0c\u4e0d\u52fe\u9009\u201ccreate as container database\u201d \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi ~/.bash_profile ORACLE_BASE = /Oracle/database ORACLE_HOME = $ORACLE_BASE /product/12.1.0/dbhome_1 ORACLE_SID = orcl ORACLE_TERM = xterm PATH = $PATH : $ORACLE_HOME /bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source ~/.bash_profile \u914d\u7f6e\u76d1\u542c\u7a0b\u5e8f\u548c\u7f51\u7edc\u670d\u52a1\u540d netca Listener\u7aef\u53e3\u8bbe\u4e3a\u9ed8\u8ba4\u503c1521 \u7f51\u7edc\u670d\u52a1\u540d\u914d\u7f6e\u4e3a ORCL \u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f \u4e3b\u673a\u91cd\u542f\u540e\uff0c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus\u754c\u9762\u6267\u884c startup \u4f7f\u7528RCU\u521b\u5efaSchema \u00b6 \u542f\u52a8rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu \u914d\u7f6eBI Server \u00b6 \u6267\u884c\u914d\u7f6e cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh \u5b89\u88c5BI Client \u00b6 \u5728Win7(64 bit)\u7cfb\u7edf\u4e0a\u5b89\u88c5BI Client \u5bf9\u63a5Hive \u00b6 \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u4ece http://web.mit.edu/kerberos/ \u4e0b\u8f7d\u5b89\u88c5kfw-4.1 \u5b89\u88c5\u914d\u7f6eHive ODBC Driver \u4e0b\u8f7d\u5b89\u88c5Hive ODBC Driver\uff08Windows\u7248\u672c\uff09\uff0c \u4e0b\u8f7d\u5730\u5740 \u5728BI\u5ba2\u6237\u7aef\u6240\u5728\u7684Windows\u673a\u5668\u4e0a\u914d\u7f6e\u7cfb\u7edfDSN \u6d4b\u8bd5ODBC\u8fde\u63a5 BI \u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 Client\u7aef\u6253\u5f00Oracle BI \u7ba1\u7406\u5de5\u5177 \u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684DSN\uff0c\u7528\u6237\u540d\u53e3\u4ee4\u4efb\u610f\u8f93\u5165\uff0c\u4f46\u4e0d\u80fd\u4e3a\u7a7a \u7981\u7528BI Server\u9ad8\u901f\u7f13\u5b58 \u00b6 \u767b\u5f55Weblogic\u57df\u7ba1\u7406\u754c\u9762 http://162.1.115.81:9500/em \u914d\u7f6e\u4e2d\u7981\u7528\u9ad8\u901f\u7f13\u5b58 \u4e0a\u4f20RPD\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u5ba2\u6237\u7aef cmd \u5207\u6362\u5230 E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin \u76ee\u5f55 \u6267\u884c\u547d\u4ee4\u4e0a\u4f20RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 mv /etc/krb5.conf /etc/krb5.conf.bak \u5c06FusionInsight\u96c6\u7fa4\u7684krb5.conf\u4e0a\u4f20\u5230/etc\u76ee\u5f55\u4e0b kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eCloudera Hive ODBC Driver yum install -y unixODBC \u4e0b\u8f7dHive ODBC Driver\uff08Linux\u7248\u672c\uff09 \u4e0b\u8f7d\u5730\u5740 \u5b89\u88c5Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u4e0eClient\u7aef\u751f\u6210\u7684RPD\u6587\u4ef6\u7684DSN\u540d\u79f0\u548c\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini \u4fee\u6539odbc\u914d\u7f6e\u6587\u4ef6 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Cloudera Hive DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e \u00b6 \u6253\u5f00BI Analytics\u754c\u9762 http://162.1.115.81:9502/analytics \u521b\u5efa\u5206\u6790 \u9009\u62e9\u5f85\u5206\u6790\u7684\u5217\u62d6\u5230\u53f3\u4fa7\u533a\u57df \u70b9\u51fb\u201c\u7ed3\u679c\u201d\u9875\u7b7e\uff0c\u68c0\u7d22\u6240\u9009\u5217\u6570\u636e \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u4fdd\u5b58\u6309\u94ae\uff0c\u4fdd\u5b58\u67e5\u8be2\u7ed3\u679c \u521b\u5efa\u53ef\u89c6\u5206\u6790\u5668\u9879\u76ee \u6dfb\u52a0\u6570\u636e\u6e90 \u9009\u53d6\u6570\u636e\u663e\u793a\u5f62\u5f0f \u6dfb\u52a0\u8ba1\u7b97 \u5bf9\u63a5Spark SQL \u00b6 \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 Kerberos\u8ba4\u8bc1 Kerberos\u83b7\u53d6\u8ba4\u8bc1\u7968\u636e \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7d\u5b89\u88c5 Simba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6eDSN\uff1a \u6d4b\u8bd5ODBC\u8fde\u63a5\uff1a BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 \u65b0\u5efaobiee-spark.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 Sample Simba Spark DSN \u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u4e0a\u4f20RDP \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 Kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7dSimba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u589e\u52a0Sample Simba Spark DSN\uff0c\u4e0eClient\u7aef\u914d\u7f6e\u76f8\u540c vi /etc/odbc.ini \u4fee\u6539odbcinst.ini\uff0c vi /etc/odbcinist.ini \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Simba Spark DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e \u00b6 \u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e \u5bf9\u63a5LibrA/ELK \u00b6 \u914d\u7f6eLibrA\u4e0eELK\u7684\u65b9\u5f0f\u6ca1\u6709\u533a\u522b\uff0c\u4ee5\u4e0b\u4ee5\u5bf9\u63a5ELK\u4e3a\u4f8b\u8fdb\u884c\u64cd\u4f5c \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eobiee\u5ba2\u6237\u7aef\u7684ODBC\u9a71\u52a8 \u6309\u7167ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684\u6307\u5bfc\u5b89\u88c5\u914d\u7f6eELK\u7684windows\u9a71\u52a8 \u914d\u7f6eDSN\uff0c\u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u4fdd\u5b58ODBC\u8fde\u63a5 BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 \u65b0\u5efaobiee-elk.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 PostgreSQL35W \u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u4e0a\u4f20RDP \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 \u53c2\u8003LibrA/ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684Linux\u4e0b\u914d\u7f6e\u6570\u636e\u6e90\u7ae0\u8282\uff0c\u5b8c\u6210obiee\u8282\u70b9\u4e0b\u7684ODBC\u9a71\u52a8\u7684\u5b89\u88c5 \u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u786e\u4fddODBC\u9a71\u52a8\u5b89\u88c5\u6210\u529f isql -v PostgreSQL35W BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u5728ODBC Data Sources\u90e8\u5206\u589e\u52a0PostgreSQL35W\u7684DSN \u5728\u6587\u4ef6\u672b\u5c3e\u589e\u52a0PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e\u6700\u540e\u4e00\u884cDriverUnicodeType=1\u9700\u8981\u52a0\u4e0a\uff0c\u5426\u5219obiee\u67e5\u8be2\u7684\u65f6\u5019\u4f1a\u62a5\u9519[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e \u00b6 \u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"12c <--> 6.5"},{"location":"Business_Intelligence/Oracle_BIEE/#oracle-bieefusioninsight","text":"","title":"Oracle BIEE\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/Oracle_BIEE/#_1","text":"Oracle BIEE 11g \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 11g \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD 6.5 (Hive/SparkSQL/ELK/GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Oracle_BIEE/#linuxobiee","text":"","title":"Linux\u73af\u5883\u5b89\u88c5OBIEE"},{"location":"Business_Intelligence/Oracle_BIEE/#os","text":"\u5b89\u88c5RedHat6.5\u64cd\u4f5c\u7cfb\u7edf\uff0cdesktop\u7248 \u521b\u5efa\u7528\u6237oracle","title":"\u5b89\u88c5OS"},{"location":"Business_Intelligence/Oracle_BIEE/#jdk8","text":"\u83b7\u53d6jdk8\u5b89\u88c5\u5305\uff0c\u6267\u884c\u5b89\u88c5","title":"\u5b89\u88c5jdk8"},{"location":"Business_Intelligence/Oracle_BIEE/#weblogic","text":"\u521b\u5efaoracle home\u76ee\u5f55\uff1a umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ \u4e0a\u4f20weblogic\u5b89\u88c5\u5305\uff0c\u89e3\u538b \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762","title":"\u5b89\u88c5Weblogic"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server","text":"\u4e0a\u4f20OBIEE\u5b89\u88c5\u5305\uff0c\u89e3\u538b chmod 755 bi_platform-12.2.1.2.0_linux64.bin \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 ./bi_platform-12.2.1.2.0_linux64.bin \u8865\u9f50lib\u5305 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel \u53d6\u6d88\u5f53\u524d\u5b89\u88c5\uff0c\u91cd\u65b0\u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f","title":"\u5b89\u88c5BI Server"},{"location":"Business_Intelligence/Oracle_BIEE/#oracle-database-12c","text":"\u5b89\u88c5\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b89\u88c5\u76ee\u5f55 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle \u4e0b\u8f7dOracle Database 12c\u5b89\u88c5\u5305\uff0c\u89e3\u538b\u5f97\u5230database\u6587\u4ef6\u5939 chmod -R 755 database/ cd database/ su oracle ./runInstaller \u53ea\u5b89\u88c5\u5355\u5b9e\u4f8b\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b9e\u4f8b cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca \u5b57\u7b26\u96c6\u9009\u62e9AL32UTF8\uff0c\u4e0d\u52fe\u9009\u201ccreate as container database\u201d \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi ~/.bash_profile ORACLE_BASE = /Oracle/database ORACLE_HOME = $ORACLE_BASE /product/12.1.0/dbhome_1 ORACLE_SID = orcl ORACLE_TERM = xterm PATH = $PATH : $ORACLE_HOME /bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source ~/.bash_profile \u914d\u7f6e\u76d1\u542c\u7a0b\u5e8f\u548c\u7f51\u7edc\u670d\u52a1\u540d netca Listener\u7aef\u53e3\u8bbe\u4e3a\u9ed8\u8ba4\u503c1521 \u7f51\u7edc\u670d\u52a1\u540d\u914d\u7f6e\u4e3a ORCL \u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f \u4e3b\u673a\u91cd\u542f\u540e\uff0c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus\u754c\u9762\u6267\u884c startup","title":"\u5b89\u88c5Oracle Database 12c"},{"location":"Business_Intelligence/Oracle_BIEE/#rcuschema","text":"\u542f\u52a8rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu","title":"\u4f7f\u7528RCU\u521b\u5efaSchema"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server_1","text":"\u6267\u884c\u914d\u7f6e cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh","title":"\u914d\u7f6eBI Server"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-client","text":"\u5728Win7(64 bit)\u7cfb\u7edf\u4e0a\u5b89\u88c5BI Client","title":"\u5b89\u88c5BI Client"},{"location":"Business_Intelligence/Oracle_BIEE/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn","text":"\u914d\u7f6eKerberos\u8ba4\u8bc1 \u4ece http://web.mit.edu/kerberos/ \u4e0b\u8f7d\u5b89\u88c5kfw-4.1 \u5b89\u88c5\u914d\u7f6eHive ODBC Driver \u4e0b\u8f7d\u5b89\u88c5Hive ODBC Driver\uff08Windows\u7248\u672c\uff09\uff0c \u4e0b\u8f7d\u5730\u5740 \u5728BI\u5ba2\u6237\u7aef\u6240\u5728\u7684Windows\u673a\u5668\u4e0a\u914d\u7f6e\u7cfb\u7edfDSN \u6d4b\u8bd5ODBC\u8fde\u63a5","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-rdp","text":"Client\u7aef\u6253\u5f00Oracle BI \u7ba1\u7406\u5de5\u5177 \u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684DSN\uff0c\u7528\u6237\u540d\u53e3\u4ee4\u4efb\u610f\u8f93\u5165\uff0c\u4f46\u4e0d\u80fd\u4e3a\u7a7a","title":"BI \u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server_2","text":"\u767b\u5f55Weblogic\u57df\u7ba1\u7406\u754c\u9762 http://162.1.115.81:9500/em \u914d\u7f6e\u4e2d\u7981\u7528\u9ad8\u901f\u7f13\u5b58","title":"\u7981\u7528BI Server\u9ad8\u901f\u7f13\u5b58"},{"location":"Business_Intelligence/Oracle_BIEE/#rpd","text":"\u5ba2\u6237\u7aef cmd \u5207\u6362\u5230 E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin \u76ee\u5f55 \u6267\u884c\u547d\u4ee4\u4e0a\u4f20RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi","title":"\u4e0a\u4f20RPD\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_1","text":"\u914d\u7f6eKerberos\u8ba4\u8bc1 mv /etc/krb5.conf /etc/krb5.conf.bak \u5c06FusionInsight\u96c6\u7fa4\u7684krb5.conf\u4e0a\u4f20\u5230/etc\u76ee\u5f55\u4e0b kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eCloudera Hive ODBC Driver yum install -y unixODBC \u4e0b\u8f7dHive ODBC Driver\uff08Linux\u7248\u672c\uff09 \u4e0b\u8f7d\u5730\u5740 \u5b89\u88c5Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u4e0eClient\u7aef\u751f\u6210\u7684RPD\u6587\u4ef6\u7684DSN\u540d\u79f0\u548c\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini \u4fee\u6539odbc\u914d\u7f6e\u6587\u4ef6 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Cloudera Hive DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#hive_1","text":"\u6253\u5f00BI Analytics\u754c\u9762 http://162.1.115.81:9502/analytics \u521b\u5efa\u5206\u6790 \u9009\u62e9\u5f85\u5206\u6790\u7684\u5217\u62d6\u5230\u53f3\u4fa7\u533a\u57df \u70b9\u51fb\u201c\u7ed3\u679c\u201d\u9875\u7b7e\uff0c\u68c0\u7d22\u6240\u9009\u5217\u6570\u636e \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u4fdd\u5b58\u6309\u94ae\uff0c\u4fdd\u5b58\u67e5\u8be2\u7ed3\u679c \u521b\u5efa\u53ef\u89c6\u5206\u6790\u5668\u9879\u76ee \u6dfb\u52a0\u6570\u636e\u6e90 \u9009\u53d6\u6570\u636e\u663e\u793a\u5f62\u5f0f \u6dfb\u52a0\u8ba1\u7b97","title":"\u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e"},{"location":"Business_Intelligence/Oracle_BIEE/#spark-sql","text":"","title":"\u5bf9\u63a5Spark SQL"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_2","text":"Kerberos\u8ba4\u8bc1 Kerberos\u83b7\u53d6\u8ba4\u8bc1\u7968\u636e \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7d\u5b89\u88c5 Simba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6eDSN\uff1a \u6d4b\u8bd5ODBC\u8fde\u63a5\uff1a","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#birdp","text":"\u65b0\u5efaobiee-spark.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 Sample Simba Spark DSN","title":"BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#rdp","text":"\u4e0a\u4f20RDP","title":"\u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_3","text":"Kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7dSimba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u589e\u52a0Sample Simba Spark DSN\uff0c\u4e0eClient\u7aef\u914d\u7f6e\u76f8\u540c vi /etc/odbc.ini \u4fee\u6539odbcinst.ini\uff0c vi /etc/odbcinist.ini \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Simba Spark DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#spark","text":"\u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"\u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e"},{"location":"Business_Intelligence/Oracle_BIEE/#libraelk","text":"\u914d\u7f6eLibrA\u4e0eELK\u7684\u65b9\u5f0f\u6ca1\u6709\u533a\u522b\uff0c\u4ee5\u4e0b\u4ee5\u5bf9\u63a5ELK\u4e3a\u4f8b\u8fdb\u884c\u64cd\u4f5c","title":"\u5bf9\u63a5LibrA/ELK"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_4","text":"\u914d\u7f6eobiee\u5ba2\u6237\u7aef\u7684ODBC\u9a71\u52a8 \u6309\u7167ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684\u6307\u5bfc\u5b89\u88c5\u914d\u7f6eELK\u7684windows\u9a71\u52a8 \u914d\u7f6eDSN\uff0c\u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u4fdd\u5b58ODBC\u8fde\u63a5","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#birdp_1","text":"\u65b0\u5efaobiee-elk.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 PostgreSQL35W","title":"BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#rdp_1","text":"\u4e0a\u4f20RDP","title":"\u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_5","text":"\u53c2\u8003LibrA/ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684Linux\u4e0b\u914d\u7f6e\u6570\u636e\u6e90\u7ae0\u8282\uff0c\u5b8c\u6210obiee\u8282\u70b9\u4e0b\u7684ODBC\u9a71\u52a8\u7684\u5b89\u88c5 \u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u786e\u4fddODBC\u9a71\u52a8\u5b89\u88c5\u6210\u529f isql -v PostgreSQL35W BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u5728ODBC Data Sources\u90e8\u5206\u589e\u52a0PostgreSQL35W\u7684DSN \u5728\u6587\u4ef6\u672b\u5c3e\u589e\u52a0PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e\u6700\u540e\u4e00\u884cDriverUnicodeType=1\u9700\u8981\u52a0\u4e0a\uff0c\u5426\u5219obiee\u67e5\u8be2\u7684\u65f6\u5019\u4f1a\u62a5\u9519[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#spark_1","text":"\u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"\u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e"},{"location":"Business_Intelligence/QlikSense/","text":"Qlik Sense\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Qlik Sense 3.2.4 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Qlik Sense 3.2.4 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL)","title":"3.2.4 <--> 6.5"},{"location":"Business_Intelligence/QlikSense/#qlik-sensefusioninsight","text":"","title":"Qlik Sense\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/QlikSense/#_1","text":"Qlik Sense 3.2.4 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Qlik Sense 3.2.4 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/QlikView/","text":"QlikView\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL) \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\uff0c\u5730\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684**\u521b\u5efa\u7528\u6237**\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201csparkdemo\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230C:\\ProgramData\\MIT\\Kerberos5\u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u547d\u4ee4\u884c\u8fdb\u5165\u5230MIT Kerberos\u5b89\u88c5\u8def\u5f84\uff0c\u627e\u5230\u53ef\u6267\u884c\u6587\u4ef6kinit.exe\uff0c\u4f8b\u5982\u672c\u6587\u8def\u5f84\u4e3a\uff1a C:\\Program Files\\MIT\\Kerberos\\bin \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a kinit -k -t /path_to_userkeytab/user.keytab UserName \u5176\u4e2dpath_to_userkeytab\u4e3a\u5b58\u653e\u7528\u6237keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0cuser.keytab\u4e3a\u7528\u6237\u7684keytab\uff0cUserName\u4e3a\u7528\u6237\u540d\u3002 \u914d\u7f6eHive\u6570\u636e\u6e90 \u00b6 QlikView\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3 \u4e0b\u8f7d\u5b89\u88c5Hive ODBC\u9a71\u52a8 \u00b6 \u4ece\u4ee5\u4e0b\u5730\u5740\u4e0b\u8f7d\u9a71\u52a8\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6e\u7528\u6237DSN \u00b6 \u5728OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668\u9875\u9762\u7684\u7528\u6237DSN\u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb\u6dfb\u52a0\uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Cloudera ODBC Driver for Apache Hive \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u914d\u7f6eHive\u6570\u636e\u6e90\u3002 Data Source Name\uff1a\u4e3a\u81ea\u5b9a\u4e49\u53c2\u6570 Host(s)\uff1a HiveServer\u7684\u4e1a\u52a1ip Port\uff1a Hive Service\u7aef\u53e3\uff0c21066 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a hive Realm\uff1a \u7559\u7a7a \u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\u5219\u8868\u793a\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb OK \u8fde\u63a5Hive\u6570\u636e\u6e90 \u00b6 \u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728**\u8fde\u63a5\u5230\u6570\u636e\u6e90**\u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90hive_odbc\uff0c\u7136\u540e\u70b9\u51fb**\u786e\u5b9a**\uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002 \u914d\u7f6eSpark\u6570\u636e\u6e90 \u00b6 QlivView\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\uff0c\u5bf9\u63a5SparkSQL\u7684thrift\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5b89\u88c5Spark\u7684ODBC\u9a71\u52a8 \u00b6 \u5728Simba\u5b98\u7f51\u4e0b\u8f7dSpark ODBC\u9a71\u52a8\uff0c\u6839\u636e\u7528\u6237\u81ea\u8eab\u64cd\u4f5c\u7cfb\u7edf\u9009\u62e932bit\u621664bit\uff0cData Source\u9009\u62e9Spark SQL\uff0c\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u63d0\u793a\u5b89\u88c5\u5ba2\u6237\u7aef\u3002 \u914d\u7f6e\u7528\u6237DSN \u00b6 \u5728 OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\u7684 \u7528\u6237DSN \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Simba Spark ODBC Driver \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728 Simba Spark ODBC Driver DSN Setup \u9875\u9762\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\u3002 Data Source Name\uff1a \u81ea\u5b9a\u4e49 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a \u7559\u7a7a\uff0c Host(s)\uff1a JDBCServer(\u4e3b)\u7684\u4e1a\u52a1ip\uff0c Port\uff1a SparkThriftServer\u5ba2\u6237\u7aef\u7aef\u53e3\u53f723040\u3002 \u8bbe\u7f6e\u5b8c\u6bd5\u540e\u70b9\u51fb Advanced Options \uff0c\u5728\u5f39\u51fa\u7684 Advanced Options \u9875\u9762\u4e2d\uff0c\u52fe\u9009 Use Native Query \u548c Get Tables With Query \uff0c\u7136\u540e\u70b9\u51fb OK \u56de\u5230 Simba Spark ODBC Driver DSN Setup \uff0c\u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u9000\u51fa\u9875\u9762\uff0c\u5426\u5219\u5c06\u5f39\u51fa\u5931\u8d25\u5bf9\u8bdd\u6846\u3002 \u56de\u5230 Simba Spark ODBC Driver DSN Setup \u9875\u9762\uff0c\u70b9\u51fb OK \uff0c\u56de\u5230 ODBC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u5e76\u9000\u51fa\u914d\u7f6e\u3002 \u8fde\u63a5Spark\u6570\u636e\u6e90 \u00b6 \u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728 \u8fde\u63a5\u5230\u6570\u636e\u6e90 \u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90spark_odbc\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002 FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662f Host(s) \u3001 Port \u3001 Host FQDN \u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u5f55\u5165","title":"12 <--> 6.5"},{"location":"Business_Intelligence/QlikView/#qlikviewfusioninsight","text":"","title":"QlikView\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/QlikView/#_1","text":"QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/QlikView/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\uff0c\u5730\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684**\u521b\u5efa\u7528\u6237**\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201csparkdemo\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230C:\\ProgramData\\MIT\\Kerberos5\u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u547d\u4ee4\u884c\u8fdb\u5165\u5230MIT Kerberos\u5b89\u88c5\u8def\u5f84\uff0c\u627e\u5230\u53ef\u6267\u884c\u6587\u4ef6kinit.exe\uff0c\u4f8b\u5982\u672c\u6587\u8def\u5f84\u4e3a\uff1a C:\\Program Files\\MIT\\Kerberos\\bin \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a kinit -k -t /path_to_userkeytab/user.keytab UserName \u5176\u4e2dpath_to_userkeytab\u4e3a\u5b58\u653e\u7528\u6237keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0cuser.keytab\u4e3a\u7528\u6237\u7684keytab\uff0cUserName\u4e3a\u7528\u6237\u540d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/QlikView/#hive","text":"QlikView\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3","title":"\u914d\u7f6eHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#hive-odbc","text":"\u4ece\u4ee5\u4e0b\u5730\u5740\u4e0b\u8f7d\u9a71\u52a8\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\uff1a \u4e0b\u8f7d\u5730\u5740","title":"\u4e0b\u8f7d\u5b89\u88c5Hive ODBC\u9a71\u52a8"},{"location":"Business_Intelligence/QlikView/#dsn","text":"\u5728OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668\u9875\u9762\u7684\u7528\u6237DSN\u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb\u6dfb\u52a0\uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Cloudera ODBC Driver for Apache Hive \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u914d\u7f6eHive\u6570\u636e\u6e90\u3002 Data Source Name\uff1a\u4e3a\u81ea\u5b9a\u4e49\u53c2\u6570 Host(s)\uff1a HiveServer\u7684\u4e1a\u52a1ip Port\uff1a Hive Service\u7aef\u53e3\uff0c21066 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a hive Realm\uff1a \u7559\u7a7a \u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\u5219\u8868\u793a\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb OK","title":"\u914d\u7f6e\u7528\u6237DSN"},{"location":"Business_Intelligence/QlikView/#hive_1","text":"\u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728**\u8fde\u63a5\u5230\u6570\u636e\u6e90**\u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90hive_odbc\uff0c\u7136\u540e\u70b9\u51fb**\u786e\u5b9a**\uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002","title":"\u8fde\u63a5Hive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#spark","text":"QlivView\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\uff0c\u5bf9\u63a5SparkSQL\u7684thrift\u63a5\u53e3\u3002","title":"\u914d\u7f6eSpark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#sparkodbc","text":"\u5728Simba\u5b98\u7f51\u4e0b\u8f7dSpark ODBC\u9a71\u52a8\uff0c\u6839\u636e\u7528\u6237\u81ea\u8eab\u64cd\u4f5c\u7cfb\u7edf\u9009\u62e932bit\u621664bit\uff0cData Source\u9009\u62e9Spark SQL\uff0c\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u63d0\u793a\u5b89\u88c5\u5ba2\u6237\u7aef\u3002","title":"\u4e0b\u8f7d\u5b89\u88c5Spark\u7684ODBC\u9a71\u52a8"},{"location":"Business_Intelligence/QlikView/#dsn_1","text":"\u5728 OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\u7684 \u7528\u6237DSN \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Simba Spark ODBC Driver \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728 Simba Spark ODBC Driver DSN Setup \u9875\u9762\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\u3002 Data Source Name\uff1a \u81ea\u5b9a\u4e49 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a \u7559\u7a7a\uff0c Host(s)\uff1a JDBCServer(\u4e3b)\u7684\u4e1a\u52a1ip\uff0c Port\uff1a SparkThriftServer\u5ba2\u6237\u7aef\u7aef\u53e3\u53f723040\u3002 \u8bbe\u7f6e\u5b8c\u6bd5\u540e\u70b9\u51fb Advanced Options \uff0c\u5728\u5f39\u51fa\u7684 Advanced Options \u9875\u9762\u4e2d\uff0c\u52fe\u9009 Use Native Query \u548c Get Tables With Query \uff0c\u7136\u540e\u70b9\u51fb OK \u56de\u5230 Simba Spark ODBC Driver DSN Setup \uff0c\u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u9000\u51fa\u9875\u9762\uff0c\u5426\u5219\u5c06\u5f39\u51fa\u5931\u8d25\u5bf9\u8bdd\u6846\u3002 \u56de\u5230 Simba Spark ODBC Driver DSN Setup \u9875\u9762\uff0c\u70b9\u51fb OK \uff0c\u56de\u5230 ODBC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u5e76\u9000\u51fa\u914d\u7f6e\u3002","title":"\u914d\u7f6e\u7528\u6237DSN"},{"location":"Business_Intelligence/QlikView/#spark_1","text":"\u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728 \u8fde\u63a5\u5230\u6570\u636e\u6e90 \u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90spark_odbc\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002","title":"\u8fde\u63a5Spark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662f Host(s) \u3001 Port \u3001 Host FQDN \u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u5f55\u5165","title":"FAQ"},{"location":"Business_Intelligence/SmartBI/","text":"SmartBI\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SmartBI 7.2.32464.17374 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL)","title":"7.2.32464.17374 <--> C70"},{"location":"Business_Intelligence/SmartBI/#smartbifusioninsight","text":"","title":"SmartBI\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/SmartBI/#_1","text":"SmartBI 7.2.32464.17374 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Tableau/","text":"Tableau\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL) \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctableau\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002 \u914d\u7f6eHive\u6570\u636e\u6e90 \u00b6 Tableau\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a \u4e0b\u8f7d\u5730\u5740 \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb\u4e2d\u7684Test\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Tableau\u4f7f\u7528\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201c\u5176\u4ed6\u6570\u636e\u5e93\uff08ODBC\uff09\u201d\uff1b DSN\u9009\u62e9hive_odbc\uff08\u4e0a\u4e00\u6b65\u4e2d\u8bbe\u7f6eODBC\u7684\u540d\u79f0\uff09\uff0c\u70b9\u51fb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\uff0c\u7136\u540e\u767b\u9646\u3002 \u67e5\u8be2\u767e\u4e07\u7ea7\u6570\u636e\u8868\u6570\u636e \u67e5\u8be2\u591a\u8868\u6570\u636e \u914d\u7f6eSpark\u6570\u636e\u6e90 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5spark\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u521b\u5efaDSN\uff08Data Source Name\uff09 \u6253\u5f00 C:\\Program Files\\Simba Spark ODBC Driver\\lib\\DriverConfiguration64.exe \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 Tableau\u4f7f\u7528Spark\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201cSpark SQL\u201d\uff0c\u4f5c\u5982\u4e0b\u914d\u7f6e\uff1a \u5176\u4e2d\u670d\u52a1\u5668\u4e3aJDBCServer(\u4e3b)\u7684\u4e1a\u52a1IP\u3002 \u7aef\u53e3\u4e3aFusionInsight\u4e2dSpark\u670d\u52a1\u914d\u7f6e\uff0c\u5bfc\u51fa\u670d\u52a1\u914d\u7f6e\u6587\u4ef6\uff0c\u5176\u4e2d hive.server2.thrift.port \u5bf9\u5e94\u503c\u3002 \u70b9\u51fb\u201c\u767b\u5f55\u201d\uff0c\u8fdb\u5165tableau\u9875\u9762\uff0c\u9009\u62e9\u67b6\u6784\u548c\u8868\uff0c\u7ed3\u679c\u5982\u4e0b\u3002 \u7528Tableau\u505a\u5b9e\u65f6\u8fde\u63a5\uff0c\u6253\u5f00\u5de5\u4f5c\u7c3f\uff0c\u5bf9\u8be5\u8868\u8fdb\u884c\u56fe\u5f62\u5316\u5206\u6790\u3002 \u6027\u80fd\u6d4b\u8bd5 \u67e5\u8be2\u5305\u542b\u767e\u4e07\u6761\u6570\u636e\u7684\u8868web_sales \u591a\u8868\u5173\u8054\u67e5\u8be2\uff1astore_sales\u548citem\u8868\u505a\u5173\u8054\u67e5\u8be2 \u589e\u52a0customer_address\u8868 \u67e5\u8be2\u7ed3\u679c\uff1a FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662fHost(s)\u3001Port\u3001Host FQDN\u7b49\u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u8f93\u5165 \u7968\u636e24\u5c0f\u65f6\u540e\u8fc7\u671f\uff0c\u65e0\u6cd5\u518d\u8bbf\u95ee\u6570\u636e \u5728windows\u4e0a\u65b0\u589e\u5b9a\u65f6\u4efb\u52a1\uff0c\u5b9a\u65f6\u6267\u884ckinit\u547d\u4ee4\uff0c\u5237\u65b0kerberos\u7968\u636e","title":"10.5.0 <--> 6.5"},{"location":"Business_Intelligence/Tableau/#tableaufusioninsight","text":"","title":"Tableau\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/Tableau/#_1","text":"Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Tableau/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctableau\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/Tableau/#hive","text":"Tableau\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a \u4e0b\u8f7d\u5730\u5740 \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb\u4e2d\u7684Test\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Tableau\u4f7f\u7528\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201c\u5176\u4ed6\u6570\u636e\u5e93\uff08ODBC\uff09\u201d\uff1b DSN\u9009\u62e9hive_odbc\uff08\u4e0a\u4e00\u6b65\u4e2d\u8bbe\u7f6eODBC\u7684\u540d\u79f0\uff09\uff0c\u70b9\u51fb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\uff0c\u7136\u540e\u767b\u9646\u3002 \u67e5\u8be2\u767e\u4e07\u7ea7\u6570\u636e\u8868\u6570\u636e \u67e5\u8be2\u591a\u8868\u6570\u636e","title":"\u914d\u7f6eHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/Tableau/#spark","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5spark\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u521b\u5efaDSN\uff08Data Source Name\uff09 \u6253\u5f00 C:\\Program Files\\Simba Spark ODBC Driver\\lib\\DriverConfiguration64.exe \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 Tableau\u4f7f\u7528Spark\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201cSpark SQL\u201d\uff0c\u4f5c\u5982\u4e0b\u914d\u7f6e\uff1a \u5176\u4e2d\u670d\u52a1\u5668\u4e3aJDBCServer(\u4e3b)\u7684\u4e1a\u52a1IP\u3002 \u7aef\u53e3\u4e3aFusionInsight\u4e2dSpark\u670d\u52a1\u914d\u7f6e\uff0c\u5bfc\u51fa\u670d\u52a1\u914d\u7f6e\u6587\u4ef6\uff0c\u5176\u4e2d hive.server2.thrift.port \u5bf9\u5e94\u503c\u3002 \u70b9\u51fb\u201c\u767b\u5f55\u201d\uff0c\u8fdb\u5165tableau\u9875\u9762\uff0c\u9009\u62e9\u67b6\u6784\u548c\u8868\uff0c\u7ed3\u679c\u5982\u4e0b\u3002 \u7528Tableau\u505a\u5b9e\u65f6\u8fde\u63a5\uff0c\u6253\u5f00\u5de5\u4f5c\u7c3f\uff0c\u5bf9\u8be5\u8868\u8fdb\u884c\u56fe\u5f62\u5316\u5206\u6790\u3002 \u6027\u80fd\u6d4b\u8bd5 \u67e5\u8be2\u5305\u542b\u767e\u4e07\u6761\u6570\u636e\u7684\u8868web_sales \u591a\u8868\u5173\u8054\u67e5\u8be2\uff1astore_sales\u548citem\u8868\u505a\u5173\u8054\u67e5\u8be2 \u589e\u52a0customer_address\u8868 \u67e5\u8be2\u7ed3\u679c\uff1a","title":"\u914d\u7f6eSpark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/Tableau/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662fHost(s)\u3001Port\u3001Host FQDN\u7b49\u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u8f93\u5165 \u7968\u636e24\u5c0f\u65f6\u540e\u8fc7\u671f\uff0c\u65e0\u6cd5\u518d\u8bbf\u95ee\u6570\u636e \u5728windows\u4e0a\u65b0\u589e\u5b9a\u65f6\u4efb\u52a1\uff0c\u5b9a\u65f6\u6267\u884ckinit\u547d\u4ee4\uff0c\u5237\u65b0kerberos\u7968\u636e","title":"FAQ"},{"location":"Data_Analysis/","text":"\u6570\u636e\u5206\u6790 \u00b6 Alteryx 2018.2.5.48994 \u2194 C60 2018.2.5.48994 \u2194 C80 2018.2.5.48994 \u2194 6.5 GeoMesa 2.3.1 \u2194 6.5 Rapidminer Studio 8.2.001 \u2194 C80 SAS 9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80 Splunk 7.2.4 \u2194 C80 7.2.4 \u2194 6.5 \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 C60","title":"Home"},{"location":"Data_Analysis/#_1","text":"Alteryx 2018.2.5.48994 \u2194 C60 2018.2.5.48994 \u2194 C80 2018.2.5.48994 \u2194 6.5 GeoMesa 2.3.1 \u2194 6.5 Rapidminer Studio 8.2.001 \u2194 C80 SAS 9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80 Splunk 7.2.4 \u2194 C80 7.2.4 \u2194 6.5 \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 C60","title":"\u6570\u636e\u5206\u6790"},{"location":"Data_Analysis/Alteryx/","text":"Alteryx\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD 6.5 (HDFS/Hive/SparkSQL) \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528Kerbers\u8ba4\u8bc1\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002 \u914d\u7f6eSpark ODBC \u8fde\u63a5 \u00b6 \u5728\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u914d\u7f6eSpark ODBC\u9a71\u52a8 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a https://www.tableau.com/support/drivers \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Simba Spark ODBC Driver -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aSpark ODBC\u8fde\u63a5\u6210\u529f\u3002 \u5728Alteryx\u4f7f\u7528Spark\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aApache Spark ODBC COnnection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aApache Spark ODBC Write->Driver: \u9ed8\u8ba4 Connection String\uff1aNew database connection\uff0c\u9009\u62e9Spark DSN\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC->Simba Spark Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Spark DSN\uff1aSimba Spark \uff08System\uff09\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dSpark\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a \u914d\u7f6eHive ODBC\u6570\u636e\u6e90 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Hive\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a \u4e0b\u8f7d\u5730\u5740 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Alteryx\u4f7f\u7528Hive\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aHive Connection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aHive ODBC Write->Driver: Hive ODBC Connection String\uff1aNew database connection\uff0c\u9009\u62e9Hive DSN\uff0c\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u5728\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Hive DSN\uff1aSample Cloudera Hive DSN(System)\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dHive\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer\uff1a \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a \u914d\u7f6eHDFS\u6570\u636e\u6e90 \u00b6 HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6MIT Kerberos Ticket\uff0c\u5e76\u5728Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS\u3002 \u5728Alteryx\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Hadoop \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a Server\uff1aWebHDFS Host\uff1a HDFS\u6240\u5728\u670d\u52a1\u5668IP Port: \u914d\u7f6e\u6587\u4ef6\u4e2ddfs.namenode.http.port\u5bf9\u5e94\u7aef\u53e3 User Name & Password\uff1a Kerberos \u8ba4\u8bc1\u7528\u6237\u540d\u53ca\u5bc6\u7801 Kerberos: Kerberos MIT \u70b9\u51fbTest\uff0c\u51fa\u73b0Connection successful \u8868\u660e\u8fde\u63a5\u6210\u529f\u3002 \u5f39\u51fa\u96c6\u7fa4\u4e2d\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5185\u5bb9\uff0c\u76ee\u524d\u652f\u6301Avro\u548cCSV\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u9700\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002 \u9009\u62e9\u76f8\u5e94\u6587\u4ef6\uff0c\u8fde\u63a5\u6210\u529f\uff0cRefresh\u4e4b\u540e\u5de6\u4fa7\u83dc\u5355\u663e\u793a\u6587\u4ef6\u5185\u5bb9\u9884\u89c8\uff1a Join \u64cd\u4f5c\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b: FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 \u6d4b\u8bd5\u8fde\u63a5\u65f6\u51fa\u73b0Default Kerberos ticket is expired Kerberos MIT ticket \u8fc7\u671f\uff0c\u9700\u8981\u91cd\u65b0\u83b7\u5f97\uff0c\u83b7\u53d6\u4e00\u6b21\u6709\u6548\u671f\u4e3a10h.","title":"2018.2.5.48994 <--> 6.5"},{"location":"Data_Analysis/Alteryx/#alteryxfusioninsight","text":"","title":"Alteryx\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/Alteryx/#_1","text":"Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD 6.5 (HDFS/Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/Alteryx/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528Kerbers\u8ba4\u8bc1\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Data_Analysis/Alteryx/#spark-odbc","text":"\u5728\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u914d\u7f6eSpark ODBC\u9a71\u52a8 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a https://www.tableau.com/support/drivers \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Simba Spark ODBC Driver -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aSpark ODBC\u8fde\u63a5\u6210\u529f\u3002 \u5728Alteryx\u4f7f\u7528Spark\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aApache Spark ODBC COnnection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aApache Spark ODBC Write->Driver: \u9ed8\u8ba4 Connection String\uff1aNew database connection\uff0c\u9009\u62e9Spark DSN\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC->Simba Spark Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Spark DSN\uff1aSimba Spark \uff08System\uff09\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dSpark\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a","title":"\u914d\u7f6eSpark ODBC \u8fde\u63a5"},{"location":"Data_Analysis/Alteryx/#hive-odbc","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Hive\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a \u4e0b\u8f7d\u5730\u5740 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Alteryx\u4f7f\u7528Hive\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aHive Connection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aHive ODBC Write->Driver: Hive ODBC Connection String\uff1aNew database connection\uff0c\u9009\u62e9Hive DSN\uff0c\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u5728\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Hive DSN\uff1aSample Cloudera Hive DSN(System)\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dHive\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer\uff1a \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a","title":"\u914d\u7f6eHive ODBC\u6570\u636e\u6e90"},{"location":"Data_Analysis/Alteryx/#hdfs","text":"HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6MIT Kerberos Ticket\uff0c\u5e76\u5728Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS\u3002 \u5728Alteryx\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Hadoop \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a Server\uff1aWebHDFS Host\uff1a HDFS\u6240\u5728\u670d\u52a1\u5668IP Port: \u914d\u7f6e\u6587\u4ef6\u4e2ddfs.namenode.http.port\u5bf9\u5e94\u7aef\u53e3 User Name & Password\uff1a Kerberos \u8ba4\u8bc1\u7528\u6237\u540d\u53ca\u5bc6\u7801 Kerberos: Kerberos MIT \u70b9\u51fbTest\uff0c\u51fa\u73b0Connection successful \u8868\u660e\u8fde\u63a5\u6210\u529f\u3002 \u5f39\u51fa\u96c6\u7fa4\u4e2d\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5185\u5bb9\uff0c\u76ee\u524d\u652f\u6301Avro\u548cCSV\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u9700\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002 \u9009\u62e9\u76f8\u5e94\u6587\u4ef6\uff0c\u8fde\u63a5\u6210\u529f\uff0cRefresh\u4e4b\u540e\u5de6\u4fa7\u83dc\u5355\u663e\u793a\u6587\u4ef6\u5185\u5bb9\u9884\u89c8\uff1a Join \u64cd\u4f5c\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b:","title":"\u914d\u7f6eHDFS\u6570\u636e\u6e90"},{"location":"Data_Analysis/Alteryx/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 \u6d4b\u8bd5\u8fde\u63a5\u65f6\u51fa\u73b0Default Kerberos ticket is expired Kerberos MIT ticket \u8fc7\u671f\uff0c\u9700\u8981\u91cd\u65b0\u83b7\u5f97\uff0c\u83b7\u53d6\u4e00\u6b21\u6709\u6548\u671f\u4e3a10h.","title":"FAQ"},{"location":"Data_Analysis/GeoMesa_2.3.1/","text":"GeoMesa\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 GeoMesa 2.3.1 \u2194 FusionInsight HD 6.5 (HBase) \u7b80\u4ecb \u00b6 GeoMesa\u662fApache\u5f00\u6e90\u5de5\u5177\u5957\u4ef6\uff0c\u53ef\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u5206\u6790\uff0c\u4f7f\u60a8\u53ef\u4ee5\u7ba1\u7406\u548c\u5206\u6790\u5de8\u5927\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u4f8b\u5982IoT\uff0c\u793e\u4ea4\u5a92\u4f53\uff0c\u8ddf\u8e2a\u548c\u79fb\u52a8\u7535\u8bdd\u7b49\u5e94\u7528\u7a0b\u5e8f\u3002 GeoMesa\u5728\u6d41\u884c\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u4e4b\u4e0a\u63d0\u4f9b\u65f6\u7a7a\u6570\u636e\u6301\u4e45\u6027\u6765\u5b9e\u73b0\u5bf9\u70b9\uff0c\u7ebf\u548c\u9762\u6570\u636e\u7684\u5927\u89c4\u6a21\u5b58\u50a8\u3002\u5b83\u5141\u8bb8\u901a\u8fc7\u5145\u5206\u5229\u7528\u5730\u7406\u5c5e\u6027\u6765\u6307\u5b9a\u8ddd\u79bb\u548c\u533a\u57df\u7684\u67e5\u8be2\u6765\u5feb\u901f\u8bbf\u95ee\u6b64\u6570\u636e\u3002\u901a\u8fc7\u8bf8\u5982GeoServer\u4e4b\u7c7b\u7684\u5730\u7406\u4fe1\u606f\u670d\u52a1\u5668\uff0cGeoMesa\u901a\u8fc7\u652f\u6301\u901a\u8fc7\u6807\u51c6OGC\uff08\u5f00\u653e\u5730\u7406\u7a7a\u95f4\u8054\u76df\uff09API\u548c\u534f\u8bae\uff08\u4f8b\u5982WFS\u548cWMS\uff09\u8bbf\u95ee\u5176\u6570\u636e\u5e93\u548c\u6d41\u529f\u80fd\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4e0e\u73b0\u6709\u5730\u56fe\u5ba2\u6237\u7aef\u7684\u96c6\u6210\u3002\u8fd9\u4e9b\u754c\u9762\u8fd8\u4f7fGeoMesa\u53ef\u4ee5\u9a71\u52a8\u5730\u56fe\u7528\u6237\u754c\u9762\u5e76\u63d0\u4f9b\u6570\u636e\u4ee5\u8fdb\u884c\u5206\u6790\uff0c\u4f8b\u5982\u67e5\u8be2\uff0c\u76f4\u65b9\u56fe\uff0c\u70ed\u56fe\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Linux\u64cd\u4f5c\u7cfb\u7edf\uff0cGeoMesa HBase\u5bf9\u63a5FusionInsight HD\u7684HBase\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002\u672c\u6587\u4f7f\u7528\u7684\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 /opt \u76ee\u5f55\u4e0b\u3002 \u5728 /opt \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5b89\u88c5\u90e8\u7f72GeoMesa HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u7684\u8282\u70b9\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4eceGithub\u4e0b\u8f7d\u5df2\u7ecf\u7f16\u8bd1\u597d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4f8b\u5982\uff1a wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-$VERSION/geomesa-hbase_2.11-$VERSION-bin.tar.gz\" \uff0c\u89e3\u538b\u81f3\u76ee\u6807\u76ee\u5f55 /opt \u3002 cd /opt wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-2.3.1/geomesa-hbase_2.11-2.3.1-bin.tar.gz\" tar -xvf geomesa-hbase_2.11-2.3.1-bin.tar.gz \u8bf4\u660e\uff1a \u5982\u679cwget\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install wget\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u672c\u6587\u4f7f\u7528\u7684\u201c$VERSION\u201d\u4e3a 2.3.1\u3002 \u53ef\u4ee5\u9009\u62e9\u4ece https://github.com/locationtech/geomesa/releases \u4e0b\u8f7d\u76f8\u5e94\u7248\u672c\u7684geomesa-hbase_2.11-$VERSION-bin.tar.gz\u518d\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u3002 \u90e8\u7f72GeoMesa Distributed Runtime JAR \u5c06 /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u62f7\u8d1d\u81f3FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\u7684HBase\u7b2c\u4e09\u65b9jar\u5305\u7ba1\u7406\u76ee\u5f55\uff0c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.21:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.22:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.23:/opt/huawei/Bigdata/third_lib/HBase/ \u8bf4\u660e\uff1aFusionInsight\u7684HBase\u7ec4\u4ef6\u7684RegionServer\u8282\u70b9\u7684IP\u5206\u522b\u4e3a172.16.4.21\u3001172.16.4.22\u3001172.16.4.23\u3002 \u767b\u5f55FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\uff0c\u4fee\u6539\u201cgeomesa-hbase-distributed-runtime_2.11-2.3.1.jar\u201d\u7684\u6240\u6709\u8005\u548c\u7ec4\u4e3a omm:wheel \u3002 \u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u8282\u70b9\u4fee\u6539\u793a\u4f8b\u5982\u4e0b\uff1a chown omm:wheel /opt/huawei/Bigdata/third_lib/HBase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u767b\u5f55FusionInsight Manage\u91cd\u542fHBase\u670d\u52a1\u3002 \u914d\u7f6eGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u4ee5\u4fbf\u80fd\u591f\u4f7f\u7528HBase\u547d\u4ee4\u884c\u5de5\u5177 geomesa-hbase \u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export GEOMESA_HBASE_HOME=/opt/geomesa-hbase_2.11-2.3.1 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u6267\u884c source \u547d\u4ee4\u4f7fGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile source /opt/hadoopclient/bigdata_env echo $GEOMESA_HBASE_HOME $HADOOP_HOME $HBASE_HOME $JAVA_TOOL_OPTIONS \u8c03\u8bd5GeoMesa\u6837\u4f8b\u811a\u672c \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u7684\u8282\u70b9\u8c03\u8bd5GeoMesa\u7684\u6837\u4f8b\u811a\u672cgeomesa-tutorials-hbase-quickstart\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002 \u5df2\u5b89\u88c5Maven\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u3002 \u5fc5\u987b\u4e0b\u8f7d\u4e0eGeoMesa\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002 cd /opt git clone https://github.com/geomesa/geomesa-tutorials.git cd geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1 \u8bf4\u660e\uff1a\u5982\u679cgit\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install git\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u4fee\u6539 /opt/geomesa-tutorials/pom.xml \u7684zookeeper\u7248\u672c\u4e3aFusionInsight\u4f7f\u7528\u7684\u76f8\u540c\u7248\u672c\uff0c\u4f8b\u5982 3.5.1 \u3002 \u5c06FusionInsight\u7684HBase\u7ec4\u4ef6\u76f8\u5173\u7684core-site.xml\u3001hdfs-site.xml\u548chbase-site.xml\u62f7\u8d1d\u81f3 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources \u76ee\u5f55\u3002 cp /opt/hadoopclient/HBase/hbase/conf/core-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hdfs-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hbase-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources vi /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u5728 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.geomesa.principal \u548c hbase.geomesa.keytab \u3002 <property> <name>hbase.geomesa.principal</name> <value>developuser</value> </property> <property> <name>hbase.geomesa.keytab</name> <value>/opt/user.keytab</value> </property> \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6784\u5efa\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5fc5\u987b\u5728 /opt/geomesa-tutorials \u76ee\u5f55\u6267\u884c\u201cmvn clean install\u201d\u547d\u4ee4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u201c[ERROR] Could not find the selected project in the reactor: geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart\u201d \u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u8fd4\u56de\u201cFailed to execute goal on project geomesa-tutorials-hbase: Could not resolve dependencies for project org.geomesa.example:geomesa-tutorials-hbase:pom:2.3.1: Could not find artifact org.apache.zookeeper:zookeeper:jar:3.5.1 in locationtech-releases ( https://repo.locationtech.org/content/groups/releases ) \u201d\uff0c\u5219 \u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002 mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u8fd4\u56de\u9519\u8bef\uff0c\u4e5f\u9700\u8981\u64cd\u4f5c\u6b64\u6b65\u9aa4\u3002 \u6784\u5efa\u5de5\u7a0b\u6210\u529f\u5219\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fd0\u884c\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 java -cp /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u5219\u8868\u793a\u6210\u529f\uff1a \u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u67e5\u8be2\u65b0\u5efa\u4e94\u5f20\u8868\u3002 hbase shell list GeoMesa\u6570\u636e\u53ef\u89c6\u5316 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4f7f\u7528GeoMesa HBase\u5de5\u5177\u53d1\u884c\u7248\u7684 geomesa-hbase export \u547d\u4ee4\u663e\u793aGeoMesa\u6837\u4f8b\u811a\u672c\u63d0\u53d6\u7684\u6570\u636e\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 GeoMesa\u6837\u4f8b\u811a\u672c\u5df2\u8c03\u8bd5\u6210\u529f\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMESA.GEOMESA_HBASE \u4e0b\u8f7d geomesa-hbase export \u547d\u4ee4\u4ea7\u751f\u7684 /opt/index.html \u6587\u4ef6\uff0c\u5e76\u7528\u6d4f\u89c8\u5668\u6253\u5f00\u3002 FAQ \u00b6 \u5982\u4f55\u5b89\u88c5Maven \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u65f6\u8fd4\u56de\u201cmvn: command not found\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728 /usr/local \u76ee\u5f55\u4e0b\u5b89\u88c5maven\u3002 cd /usr/local wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz tar -xvf apache-maven-3.6.0-bin.tar.gz \u914d\u7f6eMaven\u7684\u73af\u5883\u53d8\u91cf\u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin:$MAVEN_HOME/bin \u6267\u884c source \u547d\u4ee4\u4f7f\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile \u8fd0\u884c org.geomesa.example.hbase.HBaseQuickStart \u65f6\u8fd4\u56dejava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c java -Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -cp geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u65f6\u8fd4\u56de\u201cjava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0d\u80fd\u4f7f\u7528\u5f00\u6e90\u7684zookeeper.jar\u5305\uff0c\u5fc5\u987b\u4f7f\u7528FusionInsight\u4f7f\u7528\u7684zookeeper\u7684jar\u5305\u3002\u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002\u64cd\u4f5c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8fd0\u884c geomesa-hbase export \u547d\u4ee4\u65f6\u8fd4\u56deCan't process features serialized with an older version \u3010\u95ee\u9898\u63cf\u8ff0\u3011 GeoMesa\u7248\u672c\u7684\u7248\u672c\u4e3a2.3.1\uff0cGeoMesa HBase\u7684\u6837\u4f8b\u811a\u672c\u7684\u7248\u672c\u4e3a2.4.0\u3002\u6267\u884c geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMEA_HBASE \u65f6\u8fd4\u56dejava.lang.IllegalArgumentException: Can't process features serialized with an older version\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u65f6\u786e\u4fdd\u4e0b\u8f7d\u4e0eGeoMesa HBase\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002\u5982\u679c\u4e0d\u662f\u76f8\u540c\u7248\u672c\uff0c\u6267\u884c git checkout \u547d\u4ee4\u5207\u6362\u6837\u4f8b\u811a\u672c\u7248\u672c\u4e0eGeoMesa HBase\u7248\u672c\u4e00\u81f4\u3002 cd /opt/geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1","title":"2.3.1 <--> 6.5"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesafusioninsight","text":"","title":"GeoMesa\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_1","text":"GeoMesa 2.3.1 \u2194 FusionInsight HD 6.5 (HBase)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_2","text":"GeoMesa\u662fApache\u5f00\u6e90\u5de5\u5177\u5957\u4ef6\uff0c\u53ef\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u5206\u6790\uff0c\u4f7f\u60a8\u53ef\u4ee5\u7ba1\u7406\u548c\u5206\u6790\u5de8\u5927\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u4f8b\u5982IoT\uff0c\u793e\u4ea4\u5a92\u4f53\uff0c\u8ddf\u8e2a\u548c\u79fb\u52a8\u7535\u8bdd\u7b49\u5e94\u7528\u7a0b\u5e8f\u3002 GeoMesa\u5728\u6d41\u884c\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u4e4b\u4e0a\u63d0\u4f9b\u65f6\u7a7a\u6570\u636e\u6301\u4e45\u6027\u6765\u5b9e\u73b0\u5bf9\u70b9\uff0c\u7ebf\u548c\u9762\u6570\u636e\u7684\u5927\u89c4\u6a21\u5b58\u50a8\u3002\u5b83\u5141\u8bb8\u901a\u8fc7\u5145\u5206\u5229\u7528\u5730\u7406\u5c5e\u6027\u6765\u6307\u5b9a\u8ddd\u79bb\u548c\u533a\u57df\u7684\u67e5\u8be2\u6765\u5feb\u901f\u8bbf\u95ee\u6b64\u6570\u636e\u3002\u901a\u8fc7\u8bf8\u5982GeoServer\u4e4b\u7c7b\u7684\u5730\u7406\u4fe1\u606f\u670d\u52a1\u5668\uff0cGeoMesa\u901a\u8fc7\u652f\u6301\u901a\u8fc7\u6807\u51c6OGC\uff08\u5f00\u653e\u5730\u7406\u7a7a\u95f4\u8054\u76df\uff09API\u548c\u534f\u8bae\uff08\u4f8b\u5982WFS\u548cWMS\uff09\u8bbf\u95ee\u5176\u6570\u636e\u5e93\u548c\u6d41\u529f\u80fd\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4e0e\u73b0\u6709\u5730\u56fe\u5ba2\u6237\u7aef\u7684\u96c6\u6210\u3002\u8fd9\u4e9b\u754c\u9762\u8fd8\u4f7fGeoMesa\u53ef\u4ee5\u9a71\u52a8\u5730\u56fe\u7528\u6237\u754c\u9762\u5e76\u63d0\u4f9b\u6570\u636e\u4ee5\u8fdb\u884c\u5206\u6790\uff0c\u4f8b\u5982\u67e5\u8be2\uff0c\u76f4\u65b9\u56fe\uff0c\u70ed\u56fe\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Linux\u64cd\u4f5c\u7cfb\u7edf\uff0cGeoMesa HBase\u5bf9\u63a5FusionInsight HD\u7684HBase\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002\u672c\u6587\u4f7f\u7528\u7684\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 /opt \u76ee\u5f55\u4e0b\u3002 \u5728 /opt \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; };","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesa-hbase","text":"","title":"\u5b89\u88c5\u90e8\u7f72GeoMesa HBase"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_4","text":"\u5728\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u7684\u8282\u70b9\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_6","text":"\u4eceGithub\u4e0b\u8f7d\u5df2\u7ecf\u7f16\u8bd1\u597d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4f8b\u5982\uff1a wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-$VERSION/geomesa-hbase_2.11-$VERSION-bin.tar.gz\" \uff0c\u89e3\u538b\u81f3\u76ee\u6807\u76ee\u5f55 /opt \u3002 cd /opt wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-2.3.1/geomesa-hbase_2.11-2.3.1-bin.tar.gz\" tar -xvf geomesa-hbase_2.11-2.3.1-bin.tar.gz \u8bf4\u660e\uff1a \u5982\u679cwget\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install wget\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u672c\u6587\u4f7f\u7528\u7684\u201c$VERSION\u201d\u4e3a 2.3.1\u3002 \u53ef\u4ee5\u9009\u62e9\u4ece https://github.com/locationtech/geomesa/releases \u4e0b\u8f7d\u76f8\u5e94\u7248\u672c\u7684geomesa-hbase_2.11-$VERSION-bin.tar.gz\u518d\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u3002 \u90e8\u7f72GeoMesa Distributed Runtime JAR \u5c06 /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u62f7\u8d1d\u81f3FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\u7684HBase\u7b2c\u4e09\u65b9jar\u5305\u7ba1\u7406\u76ee\u5f55\uff0c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.21:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.22:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.23:/opt/huawei/Bigdata/third_lib/HBase/ \u8bf4\u660e\uff1aFusionInsight\u7684HBase\u7ec4\u4ef6\u7684RegionServer\u8282\u70b9\u7684IP\u5206\u522b\u4e3a172.16.4.21\u3001172.16.4.22\u3001172.16.4.23\u3002 \u767b\u5f55FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\uff0c\u4fee\u6539\u201cgeomesa-hbase-distributed-runtime_2.11-2.3.1.jar\u201d\u7684\u6240\u6709\u8005\u548c\u7ec4\u4e3a omm:wheel \u3002 \u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u8282\u70b9\u4fee\u6539\u793a\u4f8b\u5982\u4e0b\uff1a chown omm:wheel /opt/huawei/Bigdata/third_lib/HBase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u767b\u5f55FusionInsight Manage\u91cd\u542fHBase\u670d\u52a1\u3002 \u914d\u7f6eGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u4ee5\u4fbf\u80fd\u591f\u4f7f\u7528HBase\u547d\u4ee4\u884c\u5de5\u5177 geomesa-hbase \u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export GEOMESA_HBASE_HOME=/opt/geomesa-hbase_2.11-2.3.1 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u6267\u884c source \u547d\u4ee4\u4f7fGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile source /opt/hadoopclient/bigdata_env echo $GEOMESA_HBASE_HOME $HADOOP_HOME $HBASE_HOME $JAVA_TOOL_OPTIONS","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesa","text":"","title":"\u8c03\u8bd5GeoMesa\u6837\u4f8b\u811a\u672c"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_7","text":"\u5728\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u7684\u8282\u70b9\u8c03\u8bd5GeoMesa\u7684\u6837\u4f8b\u811a\u672cgeomesa-tutorials-hbase-quickstart\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_8","text":"\u5df2\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002 \u5df2\u5b89\u88c5Maven\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_9","text":"\u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u3002 \u5fc5\u987b\u4e0b\u8f7d\u4e0eGeoMesa\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002 cd /opt git clone https://github.com/geomesa/geomesa-tutorials.git cd geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1 \u8bf4\u660e\uff1a\u5982\u679cgit\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install git\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u4fee\u6539 /opt/geomesa-tutorials/pom.xml \u7684zookeeper\u7248\u672c\u4e3aFusionInsight\u4f7f\u7528\u7684\u76f8\u540c\u7248\u672c\uff0c\u4f8b\u5982 3.5.1 \u3002 \u5c06FusionInsight\u7684HBase\u7ec4\u4ef6\u76f8\u5173\u7684core-site.xml\u3001hdfs-site.xml\u548chbase-site.xml\u62f7\u8d1d\u81f3 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources \u76ee\u5f55\u3002 cp /opt/hadoopclient/HBase/hbase/conf/core-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hdfs-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hbase-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources vi /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u5728 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.geomesa.principal \u548c hbase.geomesa.keytab \u3002 <property> <name>hbase.geomesa.principal</name> <value>developuser</value> </property> <property> <name>hbase.geomesa.keytab</name> <value>/opt/user.keytab</value> </property> \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6784\u5efa\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5fc5\u987b\u5728 /opt/geomesa-tutorials \u76ee\u5f55\u6267\u884c\u201cmvn clean install\u201d\u547d\u4ee4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u201c[ERROR] Could not find the selected project in the reactor: geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart\u201d \u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u8fd4\u56de\u201cFailed to execute goal on project geomesa-tutorials-hbase: Could not resolve dependencies for project org.geomesa.example:geomesa-tutorials-hbase:pom:2.3.1: Could not find artifact org.apache.zookeeper:zookeeper:jar:3.5.1 in locationtech-releases ( https://repo.locationtech.org/content/groups/releases ) \u201d\uff0c\u5219 \u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002 mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u8fd4\u56de\u9519\u8bef\uff0c\u4e5f\u9700\u8981\u64cd\u4f5c\u6b64\u6b65\u9aa4\u3002 \u6784\u5efa\u5de5\u7a0b\u6210\u529f\u5219\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fd0\u884c\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 java -cp /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u5219\u8868\u793a\u6210\u529f\uff1a \u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u67e5\u8be2\u65b0\u5efa\u4e94\u5f20\u8868\u3002 hbase shell list","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesa_1","text":"","title":"GeoMesa\u6570\u636e\u53ef\u89c6\u5316"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_10","text":"\u4f7f\u7528GeoMesa HBase\u5de5\u5177\u53d1\u884c\u7248\u7684 geomesa-hbase export \u547d\u4ee4\u663e\u793aGeoMesa\u6837\u4f8b\u811a\u672c\u63d0\u53d6\u7684\u6570\u636e\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_11","text":"GeoMesa\u6837\u4f8b\u811a\u672c\u5df2\u8c03\u8bd5\u6210\u529f\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_12","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMESA.GEOMESA_HBASE \u4e0b\u8f7d geomesa-hbase export \u547d\u4ee4\u4ea7\u751f\u7684 /opt/index.html \u6587\u4ef6\uff0c\u5e76\u7528\u6d4f\u89c8\u5668\u6253\u5f00\u3002","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Analysis/GeoMesa_2.3.1/#faq","text":"\u5982\u4f55\u5b89\u88c5Maven \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u65f6\u8fd4\u56de\u201cmvn: command not found\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728 /usr/local \u76ee\u5f55\u4e0b\u5b89\u88c5maven\u3002 cd /usr/local wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz tar -xvf apache-maven-3.6.0-bin.tar.gz \u914d\u7f6eMaven\u7684\u73af\u5883\u53d8\u91cf\u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin:$MAVEN_HOME/bin \u6267\u884c source \u547d\u4ee4\u4f7f\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile \u8fd0\u884c org.geomesa.example.hbase.HBaseQuickStart \u65f6\u8fd4\u56dejava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c java -Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -cp geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u65f6\u8fd4\u56de\u201cjava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0d\u80fd\u4f7f\u7528\u5f00\u6e90\u7684zookeeper.jar\u5305\uff0c\u5fc5\u987b\u4f7f\u7528FusionInsight\u4f7f\u7528\u7684zookeeper\u7684jar\u5305\u3002\u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002\u64cd\u4f5c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8fd0\u884c geomesa-hbase export \u547d\u4ee4\u65f6\u8fd4\u56deCan't process features serialized with an older version \u3010\u95ee\u9898\u63cf\u8ff0\u3011 GeoMesa\u7248\u672c\u7684\u7248\u672c\u4e3a2.3.1\uff0cGeoMesa HBase\u7684\u6837\u4f8b\u811a\u672c\u7684\u7248\u672c\u4e3a2.4.0\u3002\u6267\u884c geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMEA_HBASE \u65f6\u8fd4\u56dejava.lang.IllegalArgumentException: Can't process features serialized with an older version\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u65f6\u786e\u4fdd\u4e0b\u8f7d\u4e0eGeoMesa HBase\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002\u5982\u679c\u4e0d\u662f\u76f8\u540c\u7248\u672c\uff0c\u6267\u884c git checkout \u547d\u4ee4\u5207\u6362\u6837\u4f8b\u811a\u672c\u7248\u672c\u4e0eGeoMesa HBase\u7248\u672c\u4e00\u81f4\u3002 cd /opt/geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1","title":"FAQ"},{"location":"Data_Analysis/RapidMiner/","text":"RapidMiner\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark) \u51c6\u5907\u5de5\u4f5c \u00b6 \u4e0b\u8f7d\u5b89\u88c5RapidMiner Studio, \u5f53\u524d\u6700\u65b0\u7248\u672c\u4e3a8.2.001,\u4e0b\u8f7d\u5730\u5740 https://rapidminer.com/ \u5b89\u88c5\u5b8c\u6210\u540e\u5728\u4e3b\u754c\u9762\u9876\u90e8\u83dc\u5355\u680f\u9009\u62e9 Extensions->Marketplace ,\u641c\u7d22 radoop ,\u5b89\u88c5\u540e\u91cd\u542frapidminer \u4fee\u6539\u672c\u5730host\u6587\u4ef6\uff0c\u8def\u5f84\u4e3aC:\\Windows\\System32\\drivers\\etc\uff0c\u52a0\u5165\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9IP\u4e0e\u4e3b\u673a\u540d\u5bf9\u5e94\u5173\u7cfb\uff0c\u4fdd\u5b58\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002 \u51c6\u5907FusionInsight\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4ee5\u53cajar\u5305 \u5728\u96c6\u7fa4\u7684Manager\u4e2d\uff0c\u9009\u62e9\u670d\u52a1->\u4e0b\u8f7d\u5ba2\u6237\u7aef->\u5b8c\u6574\u5ba2\u6237\u7aef \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cYarn\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u5939\u91cc\uff0c\u4f8b\u5982\u547d\u540d\u4e3aconfig\u3002 \u6253\u5f00 yarn-site.xml ,\u5220\u9664\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e <property> <name>audit.service.name</name> <value>Yarn</value> </property> \u8fdb\u5165Spark\u7ec4\u4ef6\u7684Jar\u5305\u76ee\u5f55\u201c\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars\u201d\uff0c\u5c06\u6240\u6709jar\u5305\u590d\u5236\u51fa\u6765\uff0c\u4fdd\u5b58\u5728\u672c\u673a\u67d0\u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:/jars \u3002 \u96c6\u7fa4\u914d\u7f6e \u00b6 \u914d\u7f6eUDP\u7aef\u53e3\u7ed1\u5b9a \u4e0b\u8f7d\u5b89\u88c5UDP\u7aef\u53e3\u7ed1\u5b9a\u5de5\u5177uredir\uff0c\u4e0b\u8f7d\u5730\u5740 https://github.com/troglobit/uredir \u7f16\u8bd1\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5206\u522b\u4e0a\u4f20\u81f3KDC\u670d\u52a1\u6240\u5728\u7684\u4e3b\u5907\u8282\u70b9(\u53ef\u5728krb5.conf\u6587\u4ef6\u4e2d\u67e5\u770b)\uff0c\u8fdb\u5165uredir\u6267\u884c\u6587\u4ef6\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a,\u5176\u4e2dIP\u4e3a\u6240\u5728\u8282\u70b9IP ./uredir IP:88 IP:21732 \u914d\u7f6eRadoop\u4f9d\u8d56jar\u5305 \u5728Radoop\u6587\u6863\u4e2d\u5fc3\uff0c\u4e0b\u8f7dRadoop\u4f9d\u8d56jar\u5305\uff0c\u4e0b\u8f7d\u5730\u5740 https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,\u4e0b\u8f7d\u4e0e\u5b89\u88c5\u7684RapidMiner\u7248\u672c\u5bf9\u5e94\u7684jar\u5305\u3002 \u5c06jar\u5305\u4e0a\u4f20\u81f3\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u76f8\u540c\u7684\u8def\u5f84\u4e0b\uff0c\u4f8b\u5982/usr/local/lib/radoop/ \u5728\u96c6\u7fa4HiveServer\u6240\u5728\u8282\u70b9\uff0c\u5206\u522b\u4e0a\u4f20Radoop\u7684jar\u5305\u81f3\u4ee5\u4e0b\u8def\u5f84\uff0c\u5e76\u4fee\u6539\u6240\u6709\u8005\u548c\u6267\u884c\u6743\u9650 Hive\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib\"\uff0c Mapreduce\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\uff1a\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib\" cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar * \u5728FusionInsight Manager \u754c\u9762\u6dfb\u52a0Hive\u767d\u540d\u5355\u914d\u7f6e radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name \u9700\u8981\u4ee5 | \u5206\u5272 * \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u91cd\u542fHiveServer \u521b\u5efaRadoop UDF\u51fd\u6570 \u5728\u4e3b\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a #cd /opt/hadoopclient #source bigdata_env #kinit developuser \u8f93\u5165developuser\u7528\u6237\u5bc6\u7801\uff0c\u6267\u884cbeeline\uff0c\u8fdb\u5165Hive Hive\u4e2d\u521b\u5efa\u6570\u636e\u5e93\uff0c\u4f8b\u5982\u521b\u5efa\u6570\u636e\u5e93rapidminer,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a create database rapidminer\uff1b use rapidminer\uff1b DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep'; RapidMiner\u914d\u7f6e \u00b6 \u5728RapidMiner\u4e2d\uff0c\u83dc\u5355\u9009\u62e9Connections->Manage Radoop Connections \u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u9009\u62e9New Connections->Import Hadoop Configuration Files\uff0c\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939\uff0c\u70b9\u51fbImport Configuration \u5bfc\u5165\u6210\u529f\u540e\u70b9\u51fbNext\uff0c\u8fdb\u5165\u8fde\u63a5\u914d\u7f6e\u7a97\u53e3\uff0c\u6839\u636e\u5de6\u4fa7\u83dc\u5355\u680f\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199\uff1a Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory\uff1aSpark\u7ec4\u4ef6\u7684jars\u5305 Client Principal\uff1a Kerberos\u7528\u6237\u540d@HADOOP.com Keytab File: \u4eceManager\u4e0b\u8f7d\u7684keytab\u6587\u4ef6 KDC Address\uff1a \u96c6\u7fa4KDC\u6240\u5728\u670d\u52a1\u5668IP REALM\uff1a HADOOP.COM Kerberos Config File: \u4eceManager\u4e0b\u8f7d\u7684krb5\u914d\u7f6e\u6587\u4ef6 Hadoop\uff1a \u5728\u5de6\u4e0a\u89d2\u641c\u7d22\u6846\u4e2d\u641c\u7d22split\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.input.fileinputformat.split.maxsize\u53c2\u6570 \u641c\u7d22classpath\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.application.classpath\u53c2\u6570 Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive\uff08or libs\uff09Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1a\u6dfb\u52a0spark.driver.extraJavaOptions\u548cspark.executor.extraJavaOptions\u4e24\u4e2a\u53c2\u6570 \u53c2\u6570value\u5728Manager\uff0cServices->Spark2X Configuration->\u6240\u6709\u914d\u7f6e\uff0c\u641c\u7d22extraJavaOptions\uff0c\u9009\u62e9Spark2x->SparkResource2x\u4e2d\u7684\u8fd9\u4e24\u4e2a\u53c2\u6570\u503c\uff0c\u5c06\u5176\u4e2d\u4f7f\u7528\u7684\u6240\u6709\u201c./\u201d\u76f8\u5bf9\u8def\u5f84\u66ff\u6362\u4e3a\u670d\u52a1\u7aefSpark\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4f8b\u5982\u201c/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc\u201d Hive\uff1a Hive Version\uff1aHive Server2 Hive Server Address\uff1aHive \u670d\u52a1\u6240\u5728\u8282\u70b9IP Hive Port\uff1a 21066 Database Name\uff1a \u5728Hive\u4e2d\u521b\u5efa\u7684Radoop Function\u6240\u5728\u7684\u6570\u636e\u5e93\u540d\u79f0 Customer database for UDFs: \u540cDatabase Name \u70b9\u51fbOK->Proced Anyway->Save \u6d4b\u8bd5\u8fde\u63a5 \u00b6 \u70b9\u51fbConfigure,\u5728Global\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eGlobal\u6d4b\u8bd5\u6210\u529f \u5728Hadoop\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHadoop\u6d4b\u8bd5\u6210\u529f \u5728Spark\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eSpark\u6d4b\u8bd5\u6210\u529f \u5728Hive\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHive\u6d4b\u8bd5\u6210\u529f \u5728Manage Radoop Connections \u7a97\u53e3\uff0c\u9009\u4e2d\u6240\u521b\u5efa\u7684\u8fde\u63a5\uff0c\u70b9\u51fbFull test\u8fdb\u884c\u5b8c\u6574\u6d4b\u8bd5\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u5b8c\u6574\u6d4b\u8bd5\u901a\u8fc7 Radoop\u6837\u4f8b\u8fd0\u884c \u00b6 \u5728RapidMiner Studio \u4e3b\u9875\u9762\uff0cHelp->Tutorials->User Hadoop->Rapidminer Radoop \u6839\u636eTutorials\u7684\u6307\u5bfc\u8fd0\u884c\u6837\u4f8b\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a FAQ \u00b6 \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0c\u63d0\u793aICMP port unreachable/Error retrieving Hive object list\u95ee\u9898 \u68c0\u67e5\u96c6\u7fa4\u4e2d\u7aef\u53e3\u7ed1\u5b9a\u7a0b\u5e8f\u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff0c\u7ed1\u5b9a\u7684\u7aef\u53e3\u662f\u5426\u6b63\u786e\u3002RapidMiner\u5728\u6d4b\u8bd5\u65f6\uff0c\u4f1a\u4e0e\u96c6\u7fa4\u768488\u7aef\u53e3\u8fde\u63a5\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight\u5e73\u53f0\u5bf9\u7aef\u53e3\u8fdb\u884c\u4e86\u89c4\u5212\uff0cKerberos\u8ba4\u8bc1\u4f7f\u7528\u7684\u7aef\u53e3\u662f21732\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u63d0\u793aGSS initiate failed \u68c0\u67e5\u672c\u5730host\u6587\u4ef6\u662f\u5426\u6dfb\u52a0\u4e86\u96c6\u7fa4IP\u4e0e\u4e3b\u673a\u540d\u7684\u5bf9\u5e94\u5173\u7cfb\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u5c06\u5404\u79cd\u7248\u672c\u90fd\u6d4b\u8bd5\u4e86\u4e00\u904d\uff0c\u6700\u540e\u63d0\u793aSpark test failed \u68c0\u67e5\u6dfb\u52a0\u7684\u4e24\u4e2aAdvanced Parameters\u662f\u5426\u586b\u5199\u6b63\u786e\uff0c\u5176value\u503c\u4e2d\u7684\u7edd\u5bf9\u8def\u5f84\u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5f53\u96c6\u7fa4\u91cd\u88c5\u540e\u9700\u8981\u4fee\u6539\u8be5\u503c\u3002","title":"8.2.001 <--> C80"},{"location":"Data_Analysis/RapidMiner/#rapidminerfusioninsight","text":"","title":"RapidMiner\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/RapidMiner/#_1","text":"Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/RapidMiner/#_2","text":"\u4e0b\u8f7d\u5b89\u88c5RapidMiner Studio, \u5f53\u524d\u6700\u65b0\u7248\u672c\u4e3a8.2.001,\u4e0b\u8f7d\u5730\u5740 https://rapidminer.com/ \u5b89\u88c5\u5b8c\u6210\u540e\u5728\u4e3b\u754c\u9762\u9876\u90e8\u83dc\u5355\u680f\u9009\u62e9 Extensions->Marketplace ,\u641c\u7d22 radoop ,\u5b89\u88c5\u540e\u91cd\u542frapidminer \u4fee\u6539\u672c\u5730host\u6587\u4ef6\uff0c\u8def\u5f84\u4e3aC:\\Windows\\System32\\drivers\\etc\uff0c\u52a0\u5165\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9IP\u4e0e\u4e3b\u673a\u540d\u5bf9\u5e94\u5173\u7cfb\uff0c\u4fdd\u5b58\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002 \u51c6\u5907FusionInsight\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4ee5\u53cajar\u5305 \u5728\u96c6\u7fa4\u7684Manager\u4e2d\uff0c\u9009\u62e9\u670d\u52a1->\u4e0b\u8f7d\u5ba2\u6237\u7aef->\u5b8c\u6574\u5ba2\u6237\u7aef \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cYarn\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u5939\u91cc\uff0c\u4f8b\u5982\u547d\u540d\u4e3aconfig\u3002 \u6253\u5f00 yarn-site.xml ,\u5220\u9664\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e <property> <name>audit.service.name</name> <value>Yarn</value> </property> \u8fdb\u5165Spark\u7ec4\u4ef6\u7684Jar\u5305\u76ee\u5f55\u201c\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars\u201d\uff0c\u5c06\u6240\u6709jar\u5305\u590d\u5236\u51fa\u6765\uff0c\u4fdd\u5b58\u5728\u672c\u673a\u67d0\u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:/jars \u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Analysis/RapidMiner/#_3","text":"\u914d\u7f6eUDP\u7aef\u53e3\u7ed1\u5b9a \u4e0b\u8f7d\u5b89\u88c5UDP\u7aef\u53e3\u7ed1\u5b9a\u5de5\u5177uredir\uff0c\u4e0b\u8f7d\u5730\u5740 https://github.com/troglobit/uredir \u7f16\u8bd1\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5206\u522b\u4e0a\u4f20\u81f3KDC\u670d\u52a1\u6240\u5728\u7684\u4e3b\u5907\u8282\u70b9(\u53ef\u5728krb5.conf\u6587\u4ef6\u4e2d\u67e5\u770b)\uff0c\u8fdb\u5165uredir\u6267\u884c\u6587\u4ef6\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a,\u5176\u4e2dIP\u4e3a\u6240\u5728\u8282\u70b9IP ./uredir IP:88 IP:21732 \u914d\u7f6eRadoop\u4f9d\u8d56jar\u5305 \u5728Radoop\u6587\u6863\u4e2d\u5fc3\uff0c\u4e0b\u8f7dRadoop\u4f9d\u8d56jar\u5305\uff0c\u4e0b\u8f7d\u5730\u5740 https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,\u4e0b\u8f7d\u4e0e\u5b89\u88c5\u7684RapidMiner\u7248\u672c\u5bf9\u5e94\u7684jar\u5305\u3002 \u5c06jar\u5305\u4e0a\u4f20\u81f3\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u76f8\u540c\u7684\u8def\u5f84\u4e0b\uff0c\u4f8b\u5982/usr/local/lib/radoop/ \u5728\u96c6\u7fa4HiveServer\u6240\u5728\u8282\u70b9\uff0c\u5206\u522b\u4e0a\u4f20Radoop\u7684jar\u5305\u81f3\u4ee5\u4e0b\u8def\u5f84\uff0c\u5e76\u4fee\u6539\u6240\u6709\u8005\u548c\u6267\u884c\u6743\u9650 Hive\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib\"\uff0c Mapreduce\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\uff1a\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib\" cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar * \u5728FusionInsight Manager \u754c\u9762\u6dfb\u52a0Hive\u767d\u540d\u5355\u914d\u7f6e radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name \u9700\u8981\u4ee5 | \u5206\u5272 * \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u91cd\u542fHiveServer \u521b\u5efaRadoop UDF\u51fd\u6570 \u5728\u4e3b\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a #cd /opt/hadoopclient #source bigdata_env #kinit developuser \u8f93\u5165developuser\u7528\u6237\u5bc6\u7801\uff0c\u6267\u884cbeeline\uff0c\u8fdb\u5165Hive Hive\u4e2d\u521b\u5efa\u6570\u636e\u5e93\uff0c\u4f8b\u5982\u521b\u5efa\u6570\u636e\u5e93rapidminer,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a create database rapidminer\uff1b use rapidminer\uff1b DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep';","title":"\u96c6\u7fa4\u914d\u7f6e"},{"location":"Data_Analysis/RapidMiner/#rapidminer","text":"\u5728RapidMiner\u4e2d\uff0c\u83dc\u5355\u9009\u62e9Connections->Manage Radoop Connections \u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u9009\u62e9New Connections->Import Hadoop Configuration Files\uff0c\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939\uff0c\u70b9\u51fbImport Configuration \u5bfc\u5165\u6210\u529f\u540e\u70b9\u51fbNext\uff0c\u8fdb\u5165\u8fde\u63a5\u914d\u7f6e\u7a97\u53e3\uff0c\u6839\u636e\u5de6\u4fa7\u83dc\u5355\u680f\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199\uff1a Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory\uff1aSpark\u7ec4\u4ef6\u7684jars\u5305 Client Principal\uff1a Kerberos\u7528\u6237\u540d@HADOOP.com Keytab File: \u4eceManager\u4e0b\u8f7d\u7684keytab\u6587\u4ef6 KDC Address\uff1a \u96c6\u7fa4KDC\u6240\u5728\u670d\u52a1\u5668IP REALM\uff1a HADOOP.COM Kerberos Config File: \u4eceManager\u4e0b\u8f7d\u7684krb5\u914d\u7f6e\u6587\u4ef6 Hadoop\uff1a \u5728\u5de6\u4e0a\u89d2\u641c\u7d22\u6846\u4e2d\u641c\u7d22split\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.input.fileinputformat.split.maxsize\u53c2\u6570 \u641c\u7d22classpath\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.application.classpath\u53c2\u6570 Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive\uff08or libs\uff09Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1a\u6dfb\u52a0spark.driver.extraJavaOptions\u548cspark.executor.extraJavaOptions\u4e24\u4e2a\u53c2\u6570 \u53c2\u6570value\u5728Manager\uff0cServices->Spark2X Configuration->\u6240\u6709\u914d\u7f6e\uff0c\u641c\u7d22extraJavaOptions\uff0c\u9009\u62e9Spark2x->SparkResource2x\u4e2d\u7684\u8fd9\u4e24\u4e2a\u53c2\u6570\u503c\uff0c\u5c06\u5176\u4e2d\u4f7f\u7528\u7684\u6240\u6709\u201c./\u201d\u76f8\u5bf9\u8def\u5f84\u66ff\u6362\u4e3a\u670d\u52a1\u7aefSpark\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4f8b\u5982\u201c/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc\u201d Hive\uff1a Hive Version\uff1aHive Server2 Hive Server Address\uff1aHive \u670d\u52a1\u6240\u5728\u8282\u70b9IP Hive Port\uff1a 21066 Database Name\uff1a \u5728Hive\u4e2d\u521b\u5efa\u7684Radoop Function\u6240\u5728\u7684\u6570\u636e\u5e93\u540d\u79f0 Customer database for UDFs: \u540cDatabase Name \u70b9\u51fbOK->Proced Anyway->Save","title":"RapidMiner\u914d\u7f6e"},{"location":"Data_Analysis/RapidMiner/#_4","text":"\u70b9\u51fbConfigure,\u5728Global\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eGlobal\u6d4b\u8bd5\u6210\u529f \u5728Hadoop\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHadoop\u6d4b\u8bd5\u6210\u529f \u5728Spark\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eSpark\u6d4b\u8bd5\u6210\u529f \u5728Hive\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHive\u6d4b\u8bd5\u6210\u529f \u5728Manage Radoop Connections \u7a97\u53e3\uff0c\u9009\u4e2d\u6240\u521b\u5efa\u7684\u8fde\u63a5\uff0c\u70b9\u51fbFull test\u8fdb\u884c\u5b8c\u6574\u6d4b\u8bd5\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u5b8c\u6574\u6d4b\u8bd5\u901a\u8fc7","title":"\u6d4b\u8bd5\u8fde\u63a5"},{"location":"Data_Analysis/RapidMiner/#radoop","text":"\u5728RapidMiner Studio \u4e3b\u9875\u9762\uff0cHelp->Tutorials->User Hadoop->Rapidminer Radoop \u6839\u636eTutorials\u7684\u6307\u5bfc\u8fd0\u884c\u6837\u4f8b\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a","title":"Radoop\u6837\u4f8b\u8fd0\u884c"},{"location":"Data_Analysis/RapidMiner/#faq","text":"\u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0c\u63d0\u793aICMP port unreachable/Error retrieving Hive object list\u95ee\u9898 \u68c0\u67e5\u96c6\u7fa4\u4e2d\u7aef\u53e3\u7ed1\u5b9a\u7a0b\u5e8f\u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff0c\u7ed1\u5b9a\u7684\u7aef\u53e3\u662f\u5426\u6b63\u786e\u3002RapidMiner\u5728\u6d4b\u8bd5\u65f6\uff0c\u4f1a\u4e0e\u96c6\u7fa4\u768488\u7aef\u53e3\u8fde\u63a5\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight\u5e73\u53f0\u5bf9\u7aef\u53e3\u8fdb\u884c\u4e86\u89c4\u5212\uff0cKerberos\u8ba4\u8bc1\u4f7f\u7528\u7684\u7aef\u53e3\u662f21732\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u63d0\u793aGSS initiate failed \u68c0\u67e5\u672c\u5730host\u6587\u4ef6\u662f\u5426\u6dfb\u52a0\u4e86\u96c6\u7fa4IP\u4e0e\u4e3b\u673a\u540d\u7684\u5bf9\u5e94\u5173\u7cfb\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u5c06\u5404\u79cd\u7248\u672c\u90fd\u6d4b\u8bd5\u4e86\u4e00\u904d\uff0c\u6700\u540e\u63d0\u793aSpark test failed \u68c0\u67e5\u6dfb\u52a0\u7684\u4e24\u4e2aAdvanced Parameters\u662f\u5426\u586b\u5199\u6b63\u786e\uff0c\u5176value\u503c\u4e2d\u7684\u7edd\u5bf9\u8def\u5f84\u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5f53\u96c6\u7fa4\u91cd\u88c5\u540e\u9700\u8981\u4fee\u6539\u8be5\u503c\u3002","title":"FAQ"},{"location":"Data_Analysis/SAS_9.4M3/","text":"SAS\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SAS 9.4M3 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Yarn) \u652f\u6301\u4ee5\u4e0b\u7ec4\u4ef6\u5bf9\u63a5\uff08SAS Access for Hadoop\u3001SAS HPA\u3001SAS EP\uff09 \u5bf9\u63a5\u6307\u5bfc \u00b6 9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80","title":"9.4M3 <--> C80"},{"location":"Data_Analysis/SAS_9.4M3/#sasfusioninsight","text":"","title":"SAS\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/SAS_9.4M3/#_1","text":"SAS 9.4M3 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Yarn) \u652f\u6301\u4ee5\u4e0b\u7ec4\u4ef6\u5bf9\u63a5\uff08SAS Access for Hadoop\u3001SAS HPA\u3001SAS EP\uff09","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/SAS_9.4M3/#_2","text":"9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80","title":"\u5bf9\u63a5\u6307\u5bfc"},{"location":"Data_Analysis/Splunk/","text":"Splunk\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Splunk 7.2.4 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Splunk 7.2.4 \u2194 FusionInsight HD 6.5 (HDFS) \u5b89\u88c5\u4e0e\u542f\u52a8Splunk,\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6 \u00b6 \u5173\u95ed\u4e3b\u673a\u9632\u706b\u5899 systemctl stop firewalld \u5b89\u88c5Splunk 7.2.4,\u5728\u7f51\u5740 https://www.splunk.com/en_us/download/splunk-enterprise.html \u4e0b\u8f7dLinux\u5e73\u53f0\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf splunk-7.2.4-8a94541dcfac-Linux-x86_64.tgz \u89e3\u538b\u51fasplunk\u76ee\u5f55\u3002 Splunk \u5bf9\u63a5Hadoop\u96c6\u7fa4\u9700\u8981\u4f7f\u7528Splunk Analytics for Hadoop \u7ec4\u4ef6\uff0c\u8be5\u7ec4\u4ef6\u4e0d\u652f\u6301Windows\u7248\u672c\u7684Splunk Enterprise\uff0c\u9700\u4e0b\u8f7dLinux\u7248\u672cSplunk \u542f\u52a8\u548c\u505c\u6b62splunk,\u8fdb\u5165splunk\u76ee\u5f55\u6267\u884c ./bin/splunk start ./bin/splunk stop \u7b2c\u4e00\u6b21\u542f\u52a8\u4f1a\u663e\u793aLicence Agreement\u9875\u9762\uff0c\u8f93\u5165 y ,\u7136\u540e\u8f93\u5165\u7528\u6237\u540d\u548c\u5bc6\u7801 \u542f\u52a8\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b \u5728\u6d4f\u89c8\u5668\u8f93\u5165 http://ip:8080 \uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801\u5373\u53ef\u8fdb\u5165splunk\u9875\u9762\u3002 - \u5728\u96c6\u7fa4\u670d\u52a1\u7aef\uff0c\u83b7\u53d6mapred\u7528\u6237\u7684keytab\u6587\u4ef6\u4ee5\u53ca\u96c6\u7fa4\u7684krb5.conf\u6587,\u4e0a\u4f20\u81f3splunk\u4e3b\u673a\u4e2d,\u4f8b\u5982 /opt/splunk/ \u76ee\u5f55\u4e0b \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f \u00b6 \u8fdb\u5165splunk\u4e3b\u754c\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u8bbe\u7f6e\uff0c\u9009\u62e9\u865a\u62df\u7d22\u5f15 \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u63d0\u4f9b\u7a0b\u5e8f\u5e8f\u5217\uff1ahadoop Java\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfJAVA_HOME\u7684\u503c Hadoop\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfHADOOP_HOME\u7684\u503c \u586b\u5199Hadoop\u96c6\u7fa4\u4fe1\u606f Hadoop\u7248\u672c\uff1aHadoop2.x(Yarn) \u6587\u4ef6\u7cfb\u7edf\uff1ahdfs://hacluster \u52fe\u9009\u542f\u7528\u901a\u8fc7\u8eab\u4efd\u9a8c\u8bc1 \u8d44\u6e90\u7ba1\u7406\u5668\u5730\u5740:resourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip\u6216\u4e3b\u673a\u540d,\u7aef\u53e3\u4e3a26004,\u5728\u96c6\u7fa4manager\u754c\u9762,\u9009\u62e9\u670d\u52a1\u7ba1\u7406->yarn->resourcemanager\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip,\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\uff0c\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u7aef\u53e3 \u8d44\u6e90\u8ba1\u5212\u7a0b\u5e8f\u5730\u5740:\u8282\u70b9\u540cresourcemanager,\u7aef\u53e3\u53ef\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\u67e5\u770b HDFS\u5de5\u4f5c\u76ee\u5f55\uff1a\u81ea\u884c\u5236\u5b9a \u586b\u5199\u5b89\u5168\u8bbe\u7f6e\u4fe1\u606f \u52fe\u9009\u6dfb\u52a0\u5b89\u5168\u96c6\u7fa4\uff0c\u5b89\u5168\u6a21\u5f0f\u9009\u62e9kerberos Kerberos\u670d\u52a1\u5668\u914d\u7f6e\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u8def\u5f84,\u586b\u5199\u8def\u5f84 kerberos\u4e3b\u4f53\u540d\u79f0:mapred/hadoop. hadoop.com@HADOOP.com kerberos\u5bc6\u94a5\u5373\u4e3akeytab\u6587\u4ef6 HDFS\u4e3b\u4f53:hdfa/hadoop. hadoop.com@HADOOP.com MapreReduce\u4e3b\u4f53\u4e3a:mapred/hadoop. hadoop.com@HADOOP.com \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u4e0e\u8282\u70b9\u7ba1\u7406\u5668\u4e3b\u4f53\u53ef\u4e0d\u586b \u5176\u4ed6\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb\u201c\u4fdd\u5b58\u201d\u3002 \u65b0\u5efa\u865a\u62df\u7d22\u5f15 \u00b6 \u5728\u65b0\u5efa\u7d22\u5f15\u754c\u9762\uff0c\u81ea\u5b9a\u4e49\u7d22\u5f15\u540d\u79f0\uff0c\u63d0\u4f9b\u7a0b\u5e8f\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\uff0cHDFS \u4e2d\u6570\u636e\u7684\u8def\u5f84\u6839\u636e\u9700\u8981\u641c\u7d22\u7684\u8def\u5f84\u8fdb\u884c\u586b\u5199\uff0c\u52fe\u9009\u9012\u5f52\u5904\u7406\u76ee\u5f55\uff0c\u70b9\u51fb\u4fdd\u5b58\u3002 \u4f7f\u7528\u641c\u7d22\u7a0b\u5e8f \u00b6 \u5728splunk\u4e3b\u9875\u9762\uff0c\u70b9\u51fb\u6d4f\u89c8\u6570\u636e \u9009\u62e9\u5df2\u521b\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\u548c\u865a\u62df\u7d22\u5f15 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u9009\u62e9\u8981\u641c\u7d22\u7684\u6587\u4ef6 \u5728\u6570\u636e\u9884\u89c8\u4e2d\uff0c\u9009\u62e9\u6570\u636e\u6765\u6e90\u7c7b\u578b\uff0c\u6839\u636e\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u9009\u62e9 \u5728\u4e0a\u4e0b\u6587\u914d\u7f6e\u4e2d\u9009\u62e9\u5e94\u7528\u7a0b\u5e8f\u7684\u4e0a\u4e0b\u6587\uff0c\u70b9\u51fb\u4e0b\u4e00\u6b65 \u70b9\u51fb\u5b8c\u6210 \u70b9\u51fb\u641c\u7d22\u53ef\u4ee5\u8fdb\u5165\u5bf9\u6b64\u6587\u4ef6\u7684\u641c\u7d22\u9875\u9762 \u53ef\u4ee5\u6839\u636e\u67e5\u8be2\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u53ef\u89c6\u5316\u5c55\u793a \u8bfb\u53d6Hive\u8868 \u00b6 \u901a\u8fc7Splunk\u8bfb\u53d6Hive\u8868\uff0c\u9700\u8981\u5728\u63d0\u4f9b\u7a0b\u5e8f\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u914d\u7f6e\uff1a vix.splunk.search.splitter = HiveSplitGenerator vix.splunk.search.splitter.hive.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe \u5728\u865a\u62df\u7d22\u5f15\u4e2d\u914d\u7f6e\u8981\u641c\u7d22\u7684\u8868\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u6570\u636e\u5e93\u540d\u79f0\uff0c\u8868\u540d\uff0c\u8868\u5934\uff0c\u5b57\u6bb5\u7c7b\u578b\uff0c\u6587\u4ef6\u7c7b\u578b\uff0c\u5206\u9694\u7b26\uff0c\u6362\u884c\u7b26 \u76ee\u524d\u4ec5\u80fd\u6b63\u786e\u8bfb\u53d6rcfile\u683c\u5f0f\u7684\u6587\u4ef6 \u7136\u540e\u5728\u865a\u62df\u7d22\u5f15\u5904\u70b9\u51fb \u641c\u7d22 \uff0c\u8fdb\u5165\u641c\u7d22\u9875\u9762\uff0c\u5e76\u5728\u641c\u7d22\u6846\u524d\u9009\u62e9 \u6240\u6709\u65f6\u95f4 \uff0c\u5373\u53ef\u770b\u5230\u8868\u4e2d\u6570\u636e","title":"7.2.4 <--> 6.5"},{"location":"Data_Analysis/Splunk/#splunkfusioninsight-hd","text":"","title":"Splunk\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Analysis/Splunk/#_1","text":"Splunk 7.2.4 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Splunk 7.2.4 \u2194 FusionInsight HD 6.5 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/Splunk/#splunk","text":"\u5173\u95ed\u4e3b\u673a\u9632\u706b\u5899 systemctl stop firewalld \u5b89\u88c5Splunk 7.2.4,\u5728\u7f51\u5740 https://www.splunk.com/en_us/download/splunk-enterprise.html \u4e0b\u8f7dLinux\u5e73\u53f0\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf splunk-7.2.4-8a94541dcfac-Linux-x86_64.tgz \u89e3\u538b\u51fasplunk\u76ee\u5f55\u3002 Splunk \u5bf9\u63a5Hadoop\u96c6\u7fa4\u9700\u8981\u4f7f\u7528Splunk Analytics for Hadoop \u7ec4\u4ef6\uff0c\u8be5\u7ec4\u4ef6\u4e0d\u652f\u6301Windows\u7248\u672c\u7684Splunk Enterprise\uff0c\u9700\u4e0b\u8f7dLinux\u7248\u672cSplunk \u542f\u52a8\u548c\u505c\u6b62splunk,\u8fdb\u5165splunk\u76ee\u5f55\u6267\u884c ./bin/splunk start ./bin/splunk stop \u7b2c\u4e00\u6b21\u542f\u52a8\u4f1a\u663e\u793aLicence Agreement\u9875\u9762\uff0c\u8f93\u5165 y ,\u7136\u540e\u8f93\u5165\u7528\u6237\u540d\u548c\u5bc6\u7801 \u542f\u52a8\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b \u5728\u6d4f\u89c8\u5668\u8f93\u5165 http://ip:8080 \uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801\u5373\u53ef\u8fdb\u5165splunk\u9875\u9762\u3002 - \u5728\u96c6\u7fa4\u670d\u52a1\u7aef\uff0c\u83b7\u53d6mapred\u7528\u6237\u7684keytab\u6587\u4ef6\u4ee5\u53ca\u96c6\u7fa4\u7684krb5.conf\u6587,\u4e0a\u4f20\u81f3splunk\u4e3b\u673a\u4e2d,\u4f8b\u5982 /opt/splunk/ \u76ee\u5f55\u4e0b","title":"\u5b89\u88c5\u4e0e\u542f\u52a8Splunk,\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Analysis/Splunk/#_2","text":"\u8fdb\u5165splunk\u4e3b\u754c\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u8bbe\u7f6e\uff0c\u9009\u62e9\u865a\u62df\u7d22\u5f15 \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u63d0\u4f9b\u7a0b\u5e8f\u5e8f\u5217\uff1ahadoop Java\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfJAVA_HOME\u7684\u503c Hadoop\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfHADOOP_HOME\u7684\u503c \u586b\u5199Hadoop\u96c6\u7fa4\u4fe1\u606f Hadoop\u7248\u672c\uff1aHadoop2.x(Yarn) \u6587\u4ef6\u7cfb\u7edf\uff1ahdfs://hacluster \u52fe\u9009\u542f\u7528\u901a\u8fc7\u8eab\u4efd\u9a8c\u8bc1 \u8d44\u6e90\u7ba1\u7406\u5668\u5730\u5740:resourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip\u6216\u4e3b\u673a\u540d,\u7aef\u53e3\u4e3a26004,\u5728\u96c6\u7fa4manager\u754c\u9762,\u9009\u62e9\u670d\u52a1\u7ba1\u7406->yarn->resourcemanager\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip,\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\uff0c\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u7aef\u53e3 \u8d44\u6e90\u8ba1\u5212\u7a0b\u5e8f\u5730\u5740:\u8282\u70b9\u540cresourcemanager,\u7aef\u53e3\u53ef\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\u67e5\u770b HDFS\u5de5\u4f5c\u76ee\u5f55\uff1a\u81ea\u884c\u5236\u5b9a \u586b\u5199\u5b89\u5168\u8bbe\u7f6e\u4fe1\u606f \u52fe\u9009\u6dfb\u52a0\u5b89\u5168\u96c6\u7fa4\uff0c\u5b89\u5168\u6a21\u5f0f\u9009\u62e9kerberos Kerberos\u670d\u52a1\u5668\u914d\u7f6e\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u8def\u5f84,\u586b\u5199\u8def\u5f84 kerberos\u4e3b\u4f53\u540d\u79f0:mapred/hadoop. hadoop.com@HADOOP.com kerberos\u5bc6\u94a5\u5373\u4e3akeytab\u6587\u4ef6 HDFS\u4e3b\u4f53:hdfa/hadoop. hadoop.com@HADOOP.com MapreReduce\u4e3b\u4f53\u4e3a:mapred/hadoop. hadoop.com@HADOOP.com \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u4e0e\u8282\u70b9\u7ba1\u7406\u5668\u4e3b\u4f53\u53ef\u4e0d\u586b \u5176\u4ed6\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb\u201c\u4fdd\u5b58\u201d\u3002","title":"\u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f"},{"location":"Data_Analysis/Splunk/#_3","text":"\u5728\u65b0\u5efa\u7d22\u5f15\u754c\u9762\uff0c\u81ea\u5b9a\u4e49\u7d22\u5f15\u540d\u79f0\uff0c\u63d0\u4f9b\u7a0b\u5e8f\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\uff0cHDFS \u4e2d\u6570\u636e\u7684\u8def\u5f84\u6839\u636e\u9700\u8981\u641c\u7d22\u7684\u8def\u5f84\u8fdb\u884c\u586b\u5199\uff0c\u52fe\u9009\u9012\u5f52\u5904\u7406\u76ee\u5f55\uff0c\u70b9\u51fb\u4fdd\u5b58\u3002","title":"\u65b0\u5efa\u865a\u62df\u7d22\u5f15"},{"location":"Data_Analysis/Splunk/#_4","text":"\u5728splunk\u4e3b\u9875\u9762\uff0c\u70b9\u51fb\u6d4f\u89c8\u6570\u636e \u9009\u62e9\u5df2\u521b\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\u548c\u865a\u62df\u7d22\u5f15 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u9009\u62e9\u8981\u641c\u7d22\u7684\u6587\u4ef6 \u5728\u6570\u636e\u9884\u89c8\u4e2d\uff0c\u9009\u62e9\u6570\u636e\u6765\u6e90\u7c7b\u578b\uff0c\u6839\u636e\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u9009\u62e9 \u5728\u4e0a\u4e0b\u6587\u914d\u7f6e\u4e2d\u9009\u62e9\u5e94\u7528\u7a0b\u5e8f\u7684\u4e0a\u4e0b\u6587\uff0c\u70b9\u51fb\u4e0b\u4e00\u6b65 \u70b9\u51fb\u5b8c\u6210 \u70b9\u51fb\u641c\u7d22\u53ef\u4ee5\u8fdb\u5165\u5bf9\u6b64\u6587\u4ef6\u7684\u641c\u7d22\u9875\u9762 \u53ef\u4ee5\u6839\u636e\u67e5\u8be2\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u53ef\u89c6\u5316\u5c55\u793a","title":"\u4f7f\u7528\u641c\u7d22\u7a0b\u5e8f"},{"location":"Data_Analysis/Splunk/#hive","text":"\u901a\u8fc7Splunk\u8bfb\u53d6Hive\u8868\uff0c\u9700\u8981\u5728\u63d0\u4f9b\u7a0b\u5e8f\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u914d\u7f6e\uff1a vix.splunk.search.splitter = HiveSplitGenerator vix.splunk.search.splitter.hive.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe \u5728\u865a\u62df\u7d22\u5f15\u4e2d\u914d\u7f6e\u8981\u641c\u7d22\u7684\u8868\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u6570\u636e\u5e93\u540d\u79f0\uff0c\u8868\u540d\uff0c\u8868\u5934\uff0c\u5b57\u6bb5\u7c7b\u578b\uff0c\u6587\u4ef6\u7c7b\u578b\uff0c\u5206\u9694\u7b26\uff0c\u6362\u884c\u7b26 \u76ee\u524d\u4ec5\u80fd\u6b63\u786e\u8bfb\u53d6rcfile\u683c\u5f0f\u7684\u6587\u4ef6 \u7136\u540e\u5728\u865a\u62df\u7d22\u5f15\u5904\u70b9\u51fb \u641c\u7d22 \uff0c\u8fdb\u5165\u641c\u7d22\u9875\u9762\uff0c\u5e76\u5728\u641c\u7d22\u6846\u524d\u9009\u62e9 \u6240\u6709\u65f6\u95f4 \uff0c\u5373\u53ef\u770b\u5230\u8868\u4e2d\u6570\u636e","title":"\u8bfb\u53d6Hive\u8868"},{"location":"Data_Analysis/\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0/","text":"\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL)","title":"7.1 <--> C60"},{"location":"Data_Analysis/\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0/#fusioninsight","text":"","title":"\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0/#_1","text":"\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/","text":"\u6570\u636e\u96c6\u6210 \u00b6 Apache NiFi 1.7.1 \u2194 C80 1.9.2 \u2194 6.5 Confluent 4.1.0 \u2194 C80 4.1.0 \u2194 6.5 Denodo Platform 7.0 \u2194 C80 7.0 \u2194 6.5 H2O.ai 3.24.0.2 \u2194 6.5 IBM InfoSphere CDC 11.3.3.1 \u2194 C50 IBM InfoSphere DataStage 11.3.1.0 \u2194 C50 11.5.0.2 \u2194 C60 Informatica PowerCenter 10.2.0 \u2194 C80 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 C80 Informatica 10.0.0 \u2194 C50 10.0.0 \u2194 C60 10.0.0 \u2194 C80 10.0.0 \u2194 C70 10.2.2 \u2194 6.5 Kafka Manager 1.3.3.21 \u2194 C80 Kettle 6.1 \u2194 C60 6.1 \u2194 C70 6.1 \u2194 C80 Knime 3.6.1 \u2194 C80 3.6.1 \u2194 6.5 OceanSource 1.0 \u2194 C80 Oracle GoldenGate 12.2 \u2194 C60 12.2 \u2194 6.5 12.3 \u2194 C70 12.3 \u2194 C80 Pentaho 7.1 \u2194 C70 8.0 \u2194 C60 SharePlex 9.2.1 \u2194 C80 9.2.1 \u2194 6.5 Talend 6.4.1 \u2194 C80 7.0.1 \u2194 C80 7.2.1 \u2194 6.5 Tibco BW 5.13 \u2194 6.5 \u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 C50","title":"Home"},{"location":"Data_Integration/#_1","text":"Apache NiFi 1.7.1 \u2194 C80 1.9.2 \u2194 6.5 Confluent 4.1.0 \u2194 C80 4.1.0 \u2194 6.5 Denodo Platform 7.0 \u2194 C80 7.0 \u2194 6.5 H2O.ai 3.24.0.2 \u2194 6.5 IBM InfoSphere CDC 11.3.3.1 \u2194 C50 IBM InfoSphere DataStage 11.3.1.0 \u2194 C50 11.5.0.2 \u2194 C60 Informatica PowerCenter 10.2.0 \u2194 C80 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 C80 Informatica 10.0.0 \u2194 C50 10.0.0 \u2194 C60 10.0.0 \u2194 C80 10.0.0 \u2194 C70 10.2.2 \u2194 6.5 Kafka Manager 1.3.3.21 \u2194 C80 Kettle 6.1 \u2194 C60 6.1 \u2194 C70 6.1 \u2194 C80 Knime 3.6.1 \u2194 C80 3.6.1 \u2194 6.5 OceanSource 1.0 \u2194 C80 Oracle GoldenGate 12.2 \u2194 C60 12.2 \u2194 6.5 12.3 \u2194 C70 12.3 \u2194 C80 Pentaho 7.1 \u2194 C70 8.0 \u2194 C60 SharePlex 9.2.1 \u2194 C80 9.2.1 \u2194 6.5 Talend 6.4.1 \u2194 C80 7.0.1 \u2194 C80 7.2.1 \u2194 6.5 Tibco BW 5.13 \u2194 6.5 \u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 C50","title":"\u6570\u636e\u96c6\u6210"},{"location":"Data_Integration/Apache_NiFi/","text":"Apache NiFi\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka) Apache NiFi 1.9.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive/Spark/Kafka) \u5b89\u88c5Apache NiFi \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Apache NiFi 1.7.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.7.1-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.7.1\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /usr/nifi/nifi-1.7.1 \u6267\u884c vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Nifi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1) \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210 NiFi\u8fde\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e PutHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3a\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/nifitest/HDFS \u8def\u5f84\u4e0b \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c ListHDFS & FetchHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ListHDFS\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest \u5904\u7406\u5668RouteOnAttribute\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u56fe\u6807\u589e\u52a0\u4e00\u6761\u914d\u7f6e\uff0c Property \u914d\u7f6e\u4e3a requiredfilenames \uff0c Value \u914d\u7f6e\u4e3a ${filename:matches('sanguo.*')} \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} - \u5904\u7406\u5668RouteOnAttribute\u548c\u4e0a\u3001\u4e0b\u5904\u7406\u5668FetchHDFS\u7684\u8fde\u63a5\u914d\u7f6e\u5206\u522b\u5bf9\u5e94\u4e3a requiredfilenames \u548c unmatched \uff0c\u5982\u56fe\uff1a \u4e24\u4e2a\u5904\u7406\u5668FetchHDFS\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService \u4e0a\u3001\u4e0b\u5904\u7406\u5668PutFile\u914d\u7f6e\u5206\u522b\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff0c\u6267\u884c\u547d\u4ee4 hdfs dfs -ls /tmp/nifitest \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf /tmp/nifitest \u67e5\u770b\u6587\u4ef6 \u6d4b\u8bd5\u540e \u767b\u5f55FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS/matchedFiles \u548c /home/dataset/HDFS/unmatchedFiles \u5206\u522b\u67e5\u770b\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true - \u6267\u884c\u547d\u4ee4 cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868t2: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c: PutHiveQL \u5355\u884c\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt \u6570\u636e\u6587\u4ef6 iris_add.txt \u7684\u5185\u5bb9\u5982\u4e0b: \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" \u5904\u7406\u5668SplitText\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668ExtractText\u914d\u7f6e\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b: \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris_add.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a NiFi\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u66f4\u6362\u8def\u5f84 /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210 PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a GetHbase \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5 \u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210 \u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c NiFi\u8fde\u63a5Kafka\u666e\u901a\u6a21\u5f0f \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21005\u7aef\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e GetHTTP & PutKafka \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 2: SASL_PLAINTEXT 3: wikipedia21005 6: Guarantee Replicated Delivery \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41: \u767b\u9646FI HD kafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21007\u7aef\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e nifi\u4e3b\u673aip: 172.16.2.119, FI HD\u4e09\u8282\u70b9ip: 172.16.6.10-12 \u8ba4\u8bc1\u76f8\u5173\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728nifi\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u547d\u4ee4bin/nifi.sh stop\u505c\u6b62nifi \u5728FI HD\u7684kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982/opt/hadoopclient/Kafka/kafka/libs/kafka-clients-0.11.0.1.jar \u5c06nifi\u4e3b\u673a\u4e0b /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-kafka-0-11-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u8def\u5f84\u4e2d\u539f\u6765\u7684kafka client jar\u5305kafka-clients-0.11.0.1.jar \u4f7f\u7528\u91cd\u547d\u540d\u547d\u4ee4\u547d\u540d\u4e3a kafka-clients-0.11.0.1.jar.org \u5e76\u4e14\u628a\u4e0a\u4e00\u6b65\u5728 FI HD kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u7684kafka-clients-0.11.0.1.jar\u590d\u5236\u5230\u6b64\u8def\u5f84\u4e0b\uff1a \u6ce8\u610f\uff1a\u534e\u4e3akafka\u5ba2\u6237\u7aef\u4e2d\u7684kafka-clients-0.11.0.1.jar\u66f4\u5927 \u767b\u9646nifi\u4e3b\u673a\uff0c\u5148\u8bd5\u7528 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u8fd0\u884c\u7684\u73af\u5883\u53d8\u91cf\uff0c\u7136\u540e\u518d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7djava\u8fd0\u884c\u7684jvm\u53c2\u6570: export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf\" \u5176\u4e2d/etc/krb5.conf\u4e3a\u5bf9\u5e94\u5bf9\u63a5\u96c6\u7fa4\u7684\u8ba4\u8bc1krb5.conf\u6587\u4ef6 \u5b8c\u6210\u4e0a\u8ff0\u6b65\u9aa4\u540e\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5jvm\u53c2\u6570\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u547d\u4ee4bin/nifi.sh start\u542f\u52a8nifi: PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Guarantee Replicated Delivery \u8fd0\u884c\u6574\u4e2a\u5de5\u4f5c\u6d41\uff1a \u53bb\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\u68c0\u67e5\uff1a ConsumeKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668ConsumeKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Demo \u9a71\u52a8\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41 \u4f7f\u7528FI HD\u6837\u4f8b\u4ee3\u7801\u4f7f\u7528producer\u4f20\u6570\u636e\uff1a \u767b\u9646druid\u4e3b\u673a/opt/nifikafka21007\u8def\u5f84\u68c0\u67e5\u7ed3\u679c","title":"1.9.2 <--> 6.5"},{"location":"Data_Integration/Apache_NiFi/#apache-nififusioninsight","text":"","title":"Apache NiFi\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Apache_NiFi/#_1","text":"Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka) Apache NiFi 1.9.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive/Spark/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#apache-nifi","text":"","title":"\u5b89\u88c5Apache NiFi"},{"location":"Data_Integration/Apache_NiFi/#_2","text":"\u5b89\u88c5Apache NiFi 1.7.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_4","text":"\u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.7.1-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.7.1\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /usr/nifi/nifi-1.7.1 \u6267\u884c vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikerberos","text":"","title":"NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/Apache_NiFi/#_5","text":"NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Nifi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1)","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_7","text":"\u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihdfs","text":"","title":"NiFi\u8fde\u63a5HDFS"},{"location":"Data_Integration/Apache_NiFi/#_8","text":"NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_9","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#puthdfs","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3a\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv","title":"PutHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#gethdfs","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/nifitest/HDFS \u8def\u5f84\u4e0b \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c","title":"GetHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#listhdfs-fetchhdfs","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ListHDFS\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest \u5904\u7406\u5668RouteOnAttribute\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u56fe\u6807\u589e\u52a0\u4e00\u6761\u914d\u7f6e\uff0c Property \u914d\u7f6e\u4e3a requiredfilenames \uff0c Value \u914d\u7f6e\u4e3a ${filename:matches('sanguo.*')} \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} - \u5904\u7406\u5668RouteOnAttribute\u548c\u4e0a\u3001\u4e0b\u5904\u7406\u5668FetchHDFS\u7684\u8fde\u63a5\u914d\u7f6e\u5206\u522b\u5bf9\u5e94\u4e3a requiredfilenames \u548c unmatched \uff0c\u5982\u56fe\uff1a \u4e24\u4e2a\u5904\u7406\u5668FetchHDFS\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService \u4e0a\u3001\u4e0b\u5904\u7406\u5668PutFile\u914d\u7f6e\u5206\u522b\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff0c\u6267\u884c\u547d\u4ee4 hdfs dfs -ls /tmp/nifitest \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf /tmp/nifitest \u67e5\u770b\u6587\u4ef6 \u6d4b\u8bd5\u540e \u767b\u5f55FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS/matchedFiles \u548c /home/dataset/HDFS/unmatchedFiles \u5206\u522b\u67e5\u770b\u7ed3\u679c\uff1a","title":"ListHDFS &amp; FetchHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihive","text":"","title":"NiFi\u8fde\u63a5Hive"},{"location":"Data_Integration/Apache_NiFi/#_10","text":"NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_11","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#hiveconnectionpool","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true - \u6267\u884c\u547d\u4ee4 cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar","title":"HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#selecthiveql-hive","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868t2: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a","title":"SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthiveql","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c:","title":"PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthiveql_1","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt \u6570\u636e\u6587\u4ef6 iris_add.txt \u7684\u5185\u5bb9\u5982\u4e0b: \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" \u5904\u7406\u5668SplitText\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668ExtractText\u914d\u7f6e\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b: \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris_add.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"PutHiveQL \u5355\u884c\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihbase","text":"","title":"NiFi\u8fde\u63a5HBase"},{"location":"Data_Integration/Apache_NiFi/#_12","text":"NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_13","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#hbase_1_1_2_clientservice","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u66f4\u6362\u8def\u5f84 /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210","title":"HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthbasejson-hbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a","title":"PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868"},{"location":"Data_Integration/Apache_NiFi/#gethbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a","title":"GetHbase \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifispark","text":"","title":"NiFi\u8fde\u63a5Spark"},{"location":"Data_Integration/Apache_NiFi/#_14","text":"NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_15","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#livysessioncontroller","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210","title":"\u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#spark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#pyspark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#sparkr","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikafka","text":"","title":"NiFi\u8fde\u63a5Kafka\u666e\u901a\u6a21\u5f0f"},{"location":"Data_Integration/Apache_NiFi/#_16","text":"NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21005\u7aef\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_17","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#gethttp-putkafka","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning","title":"GetHTTP &amp; PutKafka \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#publishkafka_0_11","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 2: SASL_PLAINTEXT 3: wikipedia21005 6: Guarantee Replicated Delivery \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41: \u767b\u9646FI HD kafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a","title":"PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_0_11","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikafka_1","text":"","title":"NiFi\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Data_Integration/Apache_NiFi/#_18","text":"NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21007\u7aef\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_19","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e nifi\u4e3b\u673aip: 172.16.2.119, FI HD\u4e09\u8282\u70b9ip: 172.16.6.10-12","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_20","text":"\u5728nifi\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u547d\u4ee4bin/nifi.sh stop\u505c\u6b62nifi \u5728FI HD\u7684kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982/opt/hadoopclient/Kafka/kafka/libs/kafka-clients-0.11.0.1.jar \u5c06nifi\u4e3b\u673a\u4e0b /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-kafka-0-11-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u8def\u5f84\u4e2d\u539f\u6765\u7684kafka client jar\u5305kafka-clients-0.11.0.1.jar \u4f7f\u7528\u91cd\u547d\u540d\u547d\u4ee4\u547d\u540d\u4e3a kafka-clients-0.11.0.1.jar.org \u5e76\u4e14\u628a\u4e0a\u4e00\u6b65\u5728 FI HD kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u7684kafka-clients-0.11.0.1.jar\u590d\u5236\u5230\u6b64\u8def\u5f84\u4e0b\uff1a \u6ce8\u610f\uff1a\u534e\u4e3akafka\u5ba2\u6237\u7aef\u4e2d\u7684kafka-clients-0.11.0.1.jar\u66f4\u5927 \u767b\u9646nifi\u4e3b\u673a\uff0c\u5148\u8bd5\u7528 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u8fd0\u884c\u7684\u73af\u5883\u53d8\u91cf\uff0c\u7136\u540e\u518d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7djava\u8fd0\u884c\u7684jvm\u53c2\u6570: export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf\" \u5176\u4e2d/etc/krb5.conf\u4e3a\u5bf9\u5e94\u5bf9\u63a5\u96c6\u7fa4\u7684\u8ba4\u8bc1krb5.conf\u6587\u4ef6 \u5b8c\u6210\u4e0a\u8ff0\u6b65\u9aa4\u540e\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5jvm\u53c2\u6570\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u547d\u4ee4bin/nifi.sh start\u542f\u52a8nifi:","title":"\u8ba4\u8bc1\u76f8\u5173\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#publishkafka_0_12","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Guarantee Replicated Delivery \u8fd0\u884c\u6574\u4e2a\u5de5\u4f5c\u6d41\uff1a \u53bb\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\u68c0\u67e5\uff1a","title":"PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_0_12","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668ConsumeKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Demo \u9a71\u52a8\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41 \u4f7f\u7528FI HD\u6837\u4f8b\u4ee3\u7801\u4f7f\u7528producer\u4f20\u6570\u636e\uff1a \u767b\u9646druid\u4e3b\u673a/opt/nifikafka21007\u8def\u5f84\u68c0\u67e5\u7ed3\u679c","title":"ConsumeKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/","text":"Confluent\u5bf9\u63a5FusionInsight \u00b6 \u4f7f\u7528\u573a\u666f \u00b6 Confluent 4.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) Confluent 4.1.0 \u2194 FusionInsight HD 6.5 (HDFS/Kafka) \u5b89\u88c5Confluent \u00b6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55Confluent\u5b98\u65b9\u7f51\u7ad9\u4e0b\u8f7d\u9875\u9762\uff1a https://www.confluent.io/previous-versions/?_ga=2.102961223.611794173.1561088831-1783953529.1561088831 \u627e\u5230\u76f8\u5e94\u7684\u7248\u672c\u4e0b\u8f7d \u5c06\u4e0b\u8f7d\u7684\u5f00\u6e90\u538b\u7f29\u5305\u4f7f\u7528WinSCP\u5de5\u5177\u4e0a\u4f20\u81f3linux\u4e3b\u673a\uff0c\u4f7f\u7528 tar -xvf confluent-oss-4.1.0-2.11.tar \u89e3\u538b \u589e\u52a0confluent\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \uff0c\u589e\u52a0confluent bin\u76ee\u5f55\u5230PATH\u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u5b8c\u6210\u540e\u4f7f\u7528 source ~/.bashrc \u4f7f\u4e4b\u751f\u6548 \u914d\u7f6eConfluent \u00b6 \u8bf4\u660e\uff1aConfluent\u542f\u52a8\u65f6\u4f1a\u8d77\u81ea\u5df1\u7684zookeeper\u548ckafka\u670d\u52a1\uff0c\u8fd9\u91cc\u4e0d\u505a\u4fee\u6539\u3002\u9700\u8981\u66f4\u6539\u7684\u662fconnect\uff0c schema-registry\u4ee5\u53caksql\u670d\u52a1\u914d\u7f6e\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u670d\u52a1\u76f4\u63a5\u5bf9\u63a5FusionInsight HD\u5b89\u5168\u6a21\u5f0f\u7684zookeeper\u548ckafka\u670d\u52a1 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728confluent\u5b89\u88c5\u76ee\u5f55\\share\\java\u4e0b\u65b0\u5efa\u8def\u5f84\uff0c\u540d\u4e3ahuawei \u5230FusionInsight 6.5.1\u7684kafka\u5ba2\u6237\u7aef\u4e0b\u83b7\u53d6 kafka-clients-1.1.0.jar\uff0c \u6ce8\u610f\u5373\u4f7f\u5bf9\u63a5FI HD 2.8\u7248\u7248\u672c\u4e5f\u9700\u8981\u5bf9\u5e94\u4e8eFI HD 6.5.1\u7684 kafka-clients-1.1.0.jar \u5305\uff0c\u5426\u5219\u4f1a\u62a5kafka\u7248\u672c\u5339\u914d\u76f8\u5173\u95ee\u9898 \u5c06kafka-clients-1.1.0.jar\u62f7\u8d1d\u81f3\u7b2c\u4e00\u6b65\u65b0\u5efa\u7684huawei\u8def\u5f84\u4e0b \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0/bin\u4e0b\u627e\u5230connect-distributed\u6587\u4ef6\uff0c\u8fdb\u884c\u5982\u4e0b\u7f16\u8f91\uff1a \u5728\u9002\u5f53\u4f4d\u7f6e\u6dfb\u52a0KAFKA_OPTS\u7684\u542f\u52a8JVM\u53c2\u6570 \u5177\u4f53\u5185\u5bb9\u4e3a\uff1a export KAFKA_OPTS=\"-Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf -Dkerberos.domain.name=hadoop.hadoop.com\" \u5176\u4e2d-Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u7684krb5.conf\u6587\u4ef6\uff0c\u53ef\u5728\u96c6\u7fa4 anager\u9875\u9762\u4e0a\u83b7\u53d6 \u53e6\u5916\u8fd8\u53ef\u4ee5\u6dfb\u52a0 -Dsun.security.krb5.debug=true \u6253\u5f00kerberos\u8ba4\u8bc1\u65e5\u5fd7\u5f00\u5173\uff0c\u8fdb\u884c\u9519\u8bef\u5b9a\u4f4d\u3001\u6392\u67e5 \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\uff0c\u4e0e\u4e0a\u4e00\u6b65\u7c7b\u4f3c\uff1a \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka producer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka consumer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/bin/ksql-run-class\u6587\u4ef6\u5982\u4e0b\uff0c\u76ee\u7684\u662f\u5728\u8d77ksql-server\u670d\u52a1\u7684\u65f6\u5019\u80fd\u591f\u5728classpath\u91cc\u52a0\u8f7d\u5230\u4e4b\u524d\u5bfc\u5165\u7684\u534e\u4e3akafka jar\u5305\uff1a \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/ksql/ksql-server.properties\u914d\u7f6e\u6587\u4ef6 #bootstrap.servers=localhost:9092 security.protocol = SASL_PLAINTEXT bootstrap.servers=172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 kerberos.domain.name = hadoop.hadoop.com listeners=http://localhost:8088 ksql.server.ui.enabled=true sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4f7f\u7528\u547d\u4ee4confluent start\u542f\u52a8 \u4f7f\u7528Confluent KSQL\u670d\u52a1\u67e5\u8be2FI HD\u96c6\u7fa4Kafka\u7684Topic \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210 FI HD Kafka\u6837\u4f8b\u4ee3\u7801\u8c03\u8bd5\uff0c\u5177\u4f53\u53c2\u8003 https://support-it.huawei.com/solution-fid-gw/#/Intelligent_Data_Developer_Platform \u4e0b\u8f7d\uff0c\u8c03\u8bd5kafka\u6837\u4f8b\u4ee3\u7801 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8KSQL CLI LOG_DIR=/opt/confluent/confluent-4.1.0/ksql_logs /opt/confluent/confluent-4.1.0/bin/ksql http://localhost:8088 \u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2aSTREAM CREATE STREAM TEST_01 (id BIGINT) \\ WITH (KAFKA_TOPIC='testtopic_01', VALUE_FORMAT='DELIMITED', KEY = 'id'); \u914d\u7f6eKAFKA\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684producer\u4ee3\u7801,\u4f7f\u752821007\u7aef\u53e3\u7684\u5b89\u5168\u6a21\u5f0f\uff1a \u540c\u65f6\u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u8be2\uff0c\u68c0\u67e5\u7ed3\u679c\uff1a select ID from TEST_01; \u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u4fe1\u606f \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\uff1a \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-sink.properties\u6587\u4ef6\uff1a \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0\u4e0b\u65b0\u5efa\u7a7a\u6587\u4ef6test.txt \u4f7f\u7528confluent start\u547d\u4ee4\u542f\u52a8confluent \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u6dfb\u52a0file-source, file-sink\uff0c \u5b8c\u6210\u540e\u67e5\u770bconnector\u72b6\u6001 confluent load file-source confluent load file-sink confluent status connectors \u5728test.txt\u7a7a\u6587\u4ef6\u4e2d\u8f93\u5165\u4fe1\u606f\uff0c\u4fdd\u5b58\uff0c\u5728\u751f\u6210\u7684test.sink.txt\u6587\u4ef6\u4e2d\u67e5\u770b\u4fe1\u606f\u540c\u6b65\u60c5\u51b5 \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-console-consumer.sh --topic connect-test --bootstrap-server 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --consumer.config config/consumer.properties -from-beginning \u67e5\u770b\u540c\u6b65kafka\u7684topic \u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u5230FusionInsight HDFS \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55\u5bf9\u63a5FusionInsight HD\u96c6\u7fa4\uff0c\u67e5\u770bhdfs\u914d\u7f6e\uff1a \u5b89\u88c5kafka-connect-hdfs connector, \u53c2\u8003\uff1a https://docs.confluent.io/current/connect/managing/install.html \u5728Confluent Hub\u4e0a\u4e0b\u8f7d\u7248\u672c\u5339\u914d\u7684plugin\u5305\uff1a \u5c06\u4e0b\u8f7d\u597d\u7684confluentinc-kafka-connect-hdfs-4.1.0.zip\u538b\u7f29\u5305\u672c\u5730\u89e3\u538b\uff0c\u5e76\u4f7f\u7528WinSCP\u4e0a\u4f20\u5230/opt/confluent/confluent-4.1.0/share/java\u8def\u5f84\u4e0b \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\u540c\u6b65\u7684topic\u4e3atest_hdfs: \u5728/opt/confluent/confluent-4.1.0\u8def\u5f84\u4e0b\u65b0\u5efatest_hdfs.txt\u7684\u7a7a\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a\u67e5\u770b/tmp\u8def\u5f84\u662f\u5426\u5b58\u5728,\u6ca1\u6709\u9700\u8981\u521b\u5efa \u91cd\u542fconfluent, \u542f\u52a8\u4e4b\u540e\u68c0\u67e5 file-source \u662f\u5426\u4e3a\u8ddf\u65b0\u540e\u7684\u914d\u7f6e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u4f7f\u7528 confluent unload file-source \u5378\u8f7d\u91cd\u65b0 confluent load file-source \u52a0\u8f7d \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dhdfs-sink confluent load hdfs-sink -d /opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u6253\u5f00/opt/confluent/confluent-4.1.0/test_hdfs.txt\u6587\u4ef6,\u8f93\u5165\u4ee5\u4e0b\u4fe1\u606f \u5230\u5bf9\u63a5\u96c6\u7fa4kafka\u7aef\u6d88\u8d39\u7528\u4e8e\u4f20\u8f93\u7684topic test_hdfs \u767b\u5f55\u5230\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a/tmp\u8def\u5f84\u4e0b\u67e5\u770b\u7ed3\u679c","title":"4.1.0 <--> 6.5"},{"location":"Data_Integration/Confluent_4_1_0/#confluentfusioninsight","text":"","title":"Confluent\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Confluent_4_1_0/#_1","text":"Confluent 4.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) Confluent 4.1.0 \u2194 FusionInsight HD 6.5 (HDFS/Kafka)","title":"\u4f7f\u7528\u573a\u666f"},{"location":"Data_Integration/Confluent_4_1_0/#confluent","text":"","title":"\u5b89\u88c5Confluent"},{"location":"Data_Integration/Confluent_4_1_0/#_2","text":"\u767b\u5f55Confluent\u5b98\u65b9\u7f51\u7ad9\u4e0b\u8f7d\u9875\u9762\uff1a https://www.confluent.io/previous-versions/?_ga=2.102961223.611794173.1561088831-1783953529.1561088831 \u627e\u5230\u76f8\u5e94\u7684\u7248\u672c\u4e0b\u8f7d \u5c06\u4e0b\u8f7d\u7684\u5f00\u6e90\u538b\u7f29\u5305\u4f7f\u7528WinSCP\u5de5\u5177\u4e0a\u4f20\u81f3linux\u4e3b\u673a\uff0c\u4f7f\u7528 tar -xvf confluent-oss-4.1.0-2.11.tar \u89e3\u538b \u589e\u52a0confluent\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \uff0c\u589e\u52a0confluent bin\u76ee\u5f55\u5230PATH\u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u5b8c\u6210\u540e\u4f7f\u7528 source ~/.bashrc \u4f7f\u4e4b\u751f\u6548","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent_1","text":"\u8bf4\u660e\uff1aConfluent\u542f\u52a8\u65f6\u4f1a\u8d77\u81ea\u5df1\u7684zookeeper\u548ckafka\u670d\u52a1\uff0c\u8fd9\u91cc\u4e0d\u505a\u4fee\u6539\u3002\u9700\u8981\u66f4\u6539\u7684\u662fconnect\uff0c schema-registry\u4ee5\u53caksql\u670d\u52a1\u914d\u7f6e\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u670d\u52a1\u76f4\u63a5\u5bf9\u63a5FusionInsight HD\u5b89\u5168\u6a21\u5f0f\u7684zookeeper\u548ckafka\u670d\u52a1","title":"\u914d\u7f6eConfluent"},{"location":"Data_Integration/Confluent_4_1_0/#_3","text":"\u5728confluent\u5b89\u88c5\u76ee\u5f55\\share\\java\u4e0b\u65b0\u5efa\u8def\u5f84\uff0c\u540d\u4e3ahuawei \u5230FusionInsight 6.5.1\u7684kafka\u5ba2\u6237\u7aef\u4e0b\u83b7\u53d6 kafka-clients-1.1.0.jar\uff0c \u6ce8\u610f\u5373\u4f7f\u5bf9\u63a5FI HD 2.8\u7248\u7248\u672c\u4e5f\u9700\u8981\u5bf9\u5e94\u4e8eFI HD 6.5.1\u7684 kafka-clients-1.1.0.jar \u5305\uff0c\u5426\u5219\u4f1a\u62a5kafka\u7248\u672c\u5339\u914d\u76f8\u5173\u95ee\u9898 \u5c06kafka-clients-1.1.0.jar\u62f7\u8d1d\u81f3\u7b2c\u4e00\u6b65\u65b0\u5efa\u7684huawei\u8def\u5f84\u4e0b \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0/bin\u4e0b\u627e\u5230connect-distributed\u6587\u4ef6\uff0c\u8fdb\u884c\u5982\u4e0b\u7f16\u8f91\uff1a \u5728\u9002\u5f53\u4f4d\u7f6e\u6dfb\u52a0KAFKA_OPTS\u7684\u542f\u52a8JVM\u53c2\u6570 \u5177\u4f53\u5185\u5bb9\u4e3a\uff1a export KAFKA_OPTS=\"-Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf -Dkerberos.domain.name=hadoop.hadoop.com\" \u5176\u4e2d-Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u7684krb5.conf\u6587\u4ef6\uff0c\u53ef\u5728\u96c6\u7fa4 anager\u9875\u9762\u4e0a\u83b7\u53d6 \u53e6\u5916\u8fd8\u53ef\u4ee5\u6dfb\u52a0 -Dsun.security.krb5.debug=true \u6253\u5f00kerberos\u8ba4\u8bc1\u65e5\u5fd7\u5f00\u5173\uff0c\u8fdb\u884c\u9519\u8bef\u5b9a\u4f4d\u3001\u6392\u67e5 \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\uff0c\u4e0e\u4e0a\u4e00\u6b65\u7c7b\u4f3c\uff1a \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka producer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka consumer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/bin/ksql-run-class\u6587\u4ef6\u5982\u4e0b\uff0c\u76ee\u7684\u662f\u5728\u8d77ksql-server\u670d\u52a1\u7684\u65f6\u5019\u80fd\u591f\u5728classpath\u91cc\u52a0\u8f7d\u5230\u4e4b\u524d\u5bfc\u5165\u7684\u534e\u4e3akafka jar\u5305\uff1a \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/ksql/ksql-server.properties\u914d\u7f6e\u6587\u4ef6 #bootstrap.servers=localhost:9092 security.protocol = SASL_PLAINTEXT bootstrap.servers=172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 kerberos.domain.name = hadoop.hadoop.com listeners=http://localhost:8088 ksql.server.ui.enabled=true sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4f7f\u7528\u547d\u4ee4confluent start\u542f\u52a8","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent-ksqlfi-hdkafkatopic","text":"","title":"\u4f7f\u7528Confluent KSQL\u670d\u52a1\u67e5\u8be2FI HD\u96c6\u7fa4Kafka\u7684Topic"},{"location":"Data_Integration/Confluent_4_1_0/#_4","text":"\u5b8c\u6210 FI HD Kafka\u6837\u4f8b\u4ee3\u7801\u8c03\u8bd5\uff0c\u5177\u4f53\u53c2\u8003 https://support-it.huawei.com/solution-fid-gw/#/Intelligent_Data_Developer_Platform \u4e0b\u8f7d\uff0c\u8c03\u8bd5kafka\u6837\u4f8b\u4ee3\u7801","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Confluent_4_1_0/#_5","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8KSQL CLI LOG_DIR=/opt/confluent/confluent-4.1.0/ksql_logs /opt/confluent/confluent-4.1.0/bin/ksql http://localhost:8088 \u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2aSTREAM CREATE STREAM TEST_01 (id BIGINT) \\ WITH (KAFKA_TOPIC='testtopic_01', VALUE_FORMAT='DELIMITED', KEY = 'id'); \u914d\u7f6eKAFKA\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684producer\u4ee3\u7801,\u4f7f\u752821007\u7aef\u53e3\u7684\u5b89\u5168\u6a21\u5f0f\uff1a \u540c\u65f6\u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u8be2\uff0c\u68c0\u67e5\u7ed3\u679c\uff1a select ID from TEST_01;","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent-connect","text":"","title":"\u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Confluent_4_1_0/#_6","text":"\u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Confluent_4_1_0/#_7","text":"\u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\uff1a \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-sink.properties\u6587\u4ef6\uff1a \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0\u4e0b\u65b0\u5efa\u7a7a\u6587\u4ef6test.txt \u4f7f\u7528confluent start\u547d\u4ee4\u542f\u52a8confluent \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u6dfb\u52a0file-source, file-sink\uff0c \u5b8c\u6210\u540e\u67e5\u770bconnector\u72b6\u6001 confluent load file-source confluent load file-sink confluent status connectors \u5728test.txt\u7a7a\u6587\u4ef6\u4e2d\u8f93\u5165\u4fe1\u606f\uff0c\u4fdd\u5b58\uff0c\u5728\u751f\u6210\u7684test.sink.txt\u6587\u4ef6\u4e2d\u67e5\u770b\u4fe1\u606f\u540c\u6b65\u60c5\u51b5 \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-console-consumer.sh --topic connect-test --bootstrap-server 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --consumer.config config/consumer.properties -from-beginning \u67e5\u770b\u540c\u6b65kafka\u7684topic","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent-connectfusioninsight-hdfs","text":"","title":"\u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u5230FusionInsight HDFS"},{"location":"Data_Integration/Confluent_4_1_0/#_8","text":"\u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Confluent_4_1_0/#_9","text":"\u767b\u5f55\u5bf9\u63a5FusionInsight HD\u96c6\u7fa4\uff0c\u67e5\u770bhdfs\u914d\u7f6e\uff1a \u5b89\u88c5kafka-connect-hdfs connector, \u53c2\u8003\uff1a https://docs.confluent.io/current/connect/managing/install.html \u5728Confluent Hub\u4e0a\u4e0b\u8f7d\u7248\u672c\u5339\u914d\u7684plugin\u5305\uff1a \u5c06\u4e0b\u8f7d\u597d\u7684confluentinc-kafka-connect-hdfs-4.1.0.zip\u538b\u7f29\u5305\u672c\u5730\u89e3\u538b\uff0c\u5e76\u4f7f\u7528WinSCP\u4e0a\u4f20\u5230/opt/confluent/confluent-4.1.0/share/java\u8def\u5f84\u4e0b \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\u540c\u6b65\u7684topic\u4e3atest_hdfs: \u5728/opt/confluent/confluent-4.1.0\u8def\u5f84\u4e0b\u65b0\u5efatest_hdfs.txt\u7684\u7a7a\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a\u67e5\u770b/tmp\u8def\u5f84\u662f\u5426\u5b58\u5728,\u6ca1\u6709\u9700\u8981\u521b\u5efa \u91cd\u542fconfluent, \u542f\u52a8\u4e4b\u540e\u68c0\u67e5 file-source \u662f\u5426\u4e3a\u8ddf\u65b0\u540e\u7684\u914d\u7f6e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u4f7f\u7528 confluent unload file-source \u5378\u8f7d\u91cd\u65b0 confluent load file-source \u52a0\u8f7d \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dhdfs-sink confluent load hdfs-sink -d /opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u6253\u5f00/opt/confluent/confluent-4.1.0/test_hdfs.txt\u6587\u4ef6,\u8f93\u5165\u4ee5\u4e0b\u4fe1\u606f \u5230\u5bf9\u63a5\u96c6\u7fa4kafka\u7aef\u6d88\u8d39\u7528\u4e8e\u4f20\u8f93\u7684topic test_hdfs \u767b\u5f55\u5230\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a/tmp\u8def\u5f84\u4e0b\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Denodo/","text":"Denodo\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Denodo Platform 7.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/Spark2x) Denodo Platform 7.0 \u2194 FusionInsight HD 6.5 (Hive/Spark2x) \u51c6\u5907\u5de5\u4f5c \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Denodo Platform 7.0 Denodo\u662f\u4e00\u4e2a\u6570\u636e\u865a\u62df\u5316\u7cfb\u7edf\uff0c\u5141\u8bb8\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u5e76\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7edf\u4e00\u7684\u8bbf\u95ee\u63a5\u53e3\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u6e90\u5b9e\u65f6\u5730\u8bbf\u95ee\u548c\u96c6\u6210\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u4ece\u6570\u636e\u6e90\u590d\u5236\u6216\u79fb\u52a8\u6570\u636e\u3002\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u5728\u865a\u62df\u5c42\u4e2d\u5b9a\u4e49\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u72ec\u7acb\u4e8e\u5b58\u50a8\u6570\u636e\u7684\u7269\u7406\u6e90\u3002 \u4ece https://community.denodo.com/express/download \u4e0b\u8f7dDenodo Platform 7.0\u7684\u201cDenodo Express Installer\u201d\u548c\u201cDenodo Express License\u201d\u3002\u4e0b\u8f7d\u9009\u62e9\u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672c\u662fWindows 64 bits\u3002 \u4e0b\u8f7d\u5b8c\u6210\u540e\u5b89\u88c5\u4e8e\u672c\u5730 C:\\Denodo\\ \u3002 FusionInsight HD\u76f8\u5173\u914d\u7f6e\uff08\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff09 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88Hive\u548cSpark2x\u7684\u6240\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06user.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55FusionInsight Manager \u4e3b\u673a->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u4e0b\u8f7dFusionInsight HD\u5ba2\u6237\u7aef\u5230\u672c\u5730\u3002 \u5bf9\u63a5Hive\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Hive\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive\\ \uff0c\u5982\u679chive\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5bf9\u63a5Spark2x\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u5982\u679cspark2x\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5982\u679c\u662fFusionInsight HD 6.5.X\u7248\u672c\uff0c\u8fd8\u9700\u8981\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \u3002\u5982\u679c\u662fFusionInsight HD V100R002C80SPC100\u7248\u672c\uff0c\u5219\u4e0d\u9700\u8981\u3002 \u51c6\u5907\u6570\u636e Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); INSERT INTO student VALUES (5,'Vina',2); INSERT INTO student VALUES (6,'Manson',3); INSERT INTO student VALUES (7,'Summy',1); INSERT INTO student VALUES (8,'Peter',2); INSERT INTO student VALUES (9,'Wendy',3); INSERT INTO student VALUES (10,'Andy',1); INSERT INTO student VALUES (11,'Miki',2); INSERT INTO student VALUES (12,'Aurora',3); INSERT INTO student VALUES (13,'Carina',1); INSERT INTO student VALUES (14,'Hely',1); INSERT INTO student VALUES (15,'Tracy',2); \u521b\u5efa\u4e0estudent.class_id\u76f8\u5173\u7684\u6570\u636e\u5b58\u653e\u4e8eexcel\u8868\u4e2d\u3002\u4f8b\u5982\u521b\u5efa C:\\developuser\\Class.xlsx \uff0csheet\u547d\u540d\u4e3a\u201cClass\u201d\uff0c\u5305\u542b\u4e24\u5217\uff0c\u5206\u522b\u662fid\u548cname\uff0cid\u5217\u7684\u53d6\u503c\u5fc5\u987b\u5b58\u5728\u4e8estudent.class_id\u4e2d\u3002 JDBC\u8fde\u63a5\u9700\u8981\u67e5\u8be2Zookeeper\uff0cZookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8\u5e76\u914d\u7f6eDenodo \u00b6 \u70b9\u51fb \u5f00\u59cb->Denodo Platform->Denodo Platform 7.0 \u542f\u52a8Denodo Platform Control Center\u3002 \u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server\u3002 \u70b9\u51fb Virtual DataPort->Configure \u3002 \u70b9\u51fb JVM Options \u3002 Virtual DataPort Server\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \uff0c\u4e24\u4e2aOptions\u4e4b\u95f4\u7528\u7a7a\u683c\u9694\u5f00\u3002\u70b9\u51fb Ok \u3002 \u70b9\u51fb Virtual DataPort \u8fd4\u56de\u4e3b\u754c\u9762\uff0c\u70b9\u51fb Start \u542f\u52a8Virtual DataPort Server\u3002 \u542f\u52a8Virtual DataPort Administration Tool\u3002 Virtual DataPort Server\u542f\u52a8\u6210\u529f\u540e\u72b6\u6001\u663e\u793a\u4e3aRunning\uff0c\u70b9\u51fb LAUNCH \u542f\u52a8Virtual DataPort Administration Tool\u3002 \u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Connect \u767b\u5f55\u3002 \u6210\u529f\u767b\u5f55Virtual DataPort Administration Tool\u3002 \u5bf9\u63a5Hive\u6216\u8005Spark2x \u00b6 \u521b\u5efaJDBC\u8fde\u63a5\u7684Data source \u00b6 \u53f3\u952e admin->Big Data \u9009\u62e9 New->Data source->JDBC \u3002 \u914d\u7f6e\u8fde\u63a5\u4fe1\u606f\uff1a Name\uff1a\u81ea\u547d\u540d\u7684\u65b0\u5efa\u7684Data Source\u540d\u79f0\u3002 Driver class path\uff1aHive\u6216\u8005Spark2x\u7684Jar\u5305\u6240\u5728\u7684\u4f4d\u7f6e\u3002\u5177\u4f53\u914d\u7f6e\u8def\u5f84\u53c2\u8003\u672c\u6587 \u51c6\u5907\u5de5\u4f5c->\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5 \u7ae0\u8282\u3002 Database URI\uff1aHive\u6216\u8005Spark2x\u8fde\u63a5\u7684URL\u3002 Authentication\uff1a\u9009\u62e9Kerberos\u8ba4\u8bc1\u3002 \u5bf9\u63a5Hive\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: hive_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u5bf9\u63a5Spark2x\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: spark2x_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u70b9\u51fb Test connection \uff0c\u8fd4\u56de JDBC connection tested successfully \u3002\u5982\u679c\u8fd4\u56de\u5931\u8d25\uff0c\u53ef\u5728 C:\\Denodo\\DenodoPlatform7.0\\logs\\vdp\\vdp.log \u67e5\u770b\u8be6\u7ec6\u7684\u5931\u8d25\u65e5\u5fd7\u3002\u70b9\u51fb Ok \u5173\u95ed\u6210\u529f\u63d0\u793a\u3002 \u70b9\u51fb Save \u4fdd\u5b58hive_ds\u3002 \u4fdd\u5b58\u6210\u529f\u540e\uff0c\u5de6\u8fb9\u663e\u793a\u7684 admin->Big Data->hive_ds \u5373\u4e3a\u65b0\u589e\u7684Data Source\u3002 \u521b\u5efaHive\u6570\u636e\u6e90 \u00b6 \u4e3a\u4e86\u66f4\u597d\u89c2\u5bdf\uff0c\u53f3\u952eBig Data\u6587\u4ef6\u5939 New->Folder \u65b0\u5efa\u4e09\u4e2a\u6587\u4ef6\u5939\u5206\u522b\u5b58\u5728Data source(01_data source)\u3001base views(02_base views)\u3001\u96c6\u6210\u6570\u636e(03_reports)\uff0c\u5e76\u628a\u5df2\u521b\u5efa\u7684data sources\u79fb\u5165\u6587\u4ef6\u593901_data source\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684Data Source hive_ds \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Create base view \u3002\u9009\u62e9employee\u8868 default->Tables->student \uff0c\u518d\u70b9\u51fb Create selected \u3002 View name\u547d\u540d\u4e3a student \uff0c\u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View student \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56de\u7684student\u8868\u7684\u6570\u636e\u3002 \u521b\u5efaExcel\u8868\u6570\u636e\u6e90 \u00b6 \u53f3\u952e\u6587\u4ef6\u593901_data source\uff0c\u9009\u62e9 New->Data source->Excel \u3002 \u9009\u62e9\u5bfc\u5165\u5df2\u51c6\u5907\u597d\u5b58\u653e\u4e8e C:\\developuser\\ \u7684 Class.xlsx \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u5177\u4f53\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Name: class Type of file: \u6839\u636e\u51c6\u5907\u7684Excel\u8868\u7684\u7248\u672c\u9009\u62e9 File location: \u4e0b\u62c9\u6846\u9009\u62e9Local\u540e\uff0c\u518d\u70b9\u51fbConfigure\u9009\u62e9C:\\developuser\\Class.xlsx Worksheets: \u8f93\u5165\u51c6\u5907\u6570\u636e\u5bf9\u5e94\u7684Sheet\u540d\u79f0Class Start cell: \u51c6\u5907\u6570\u636e\u5f00\u59cb\u7684\u5355\u5143\u683c End cell: \u51c6\u5907\u6570\u636e\u7ed3\u675f\u7684\u5355\u5143\u683c Has headers: \u52fe\u9009 Stream tuples: \u52fe\u9009 \u9009\u62e9Data source class \uff0c\u70b9\u51fb Create base view \u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View class \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deExcel\u7684Class\u7684\u6570\u636e\u3002 \u5c06View class \u548c student \u79fb\u5165\u6587\u4ef6\u593902_base views\u3002 \u7ec4\u5408Hive\u548cExcel\u7684\u6570\u636e \u00b6 \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New->Join \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06View name\u8bbe\u7f6e\u4e3a\u201cstudent_class\u201d\uff0c\u5c06student\u7684class_id\u548cclass\u7684id\u5220\u9664\u3002 \u91cd\u547d\u540dclass\u7684name\u4e3aclass_name\u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u9009\u62e9 student_class \uff0c\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deHive\u548cExcel\u7ec4\u5408\u540e\u7684\u6570\u636e\u3002 \u4f7f\u7528DbVisualizer\u67e5\u770bDenodo Views\u7684\u6570\u636e \u00b6 \u4ece https://www.dbvis.com/download/ \u4e0b\u8f7dDbVisualizer\u5e76\u5b89\u88c5\u4e8e\u672c\u5730\u3002 \u6253\u5f00DbVisualizer\uff0c\u9009\u62e9 Tools->Driver Manager \u3002 \u9009\u62e9 Drive->Create Driver \u3002 \u8f93\u5165\u4ee5\u4e0b\u914d\u7f6e\u4fe1\u606f\u540e\u5173\u95ed\u8be5\u754c\u9762\uff1a Name: Denodo 7.0 URL Format: jdbc:vdb://host:port/database Drive Class: \u70b9\u51fb\u6587\u4ef6\u5939\u5bfc\u5165Denodo\u81ea\u5e26\u7684JDBC jar\u5305\uff0c\u4f8b\u5982C:\\Denodo\\DenodoPlatform7.0\\tools\\client-drivers\\jdbc\\denodo-vdp-jdbcdriver.jar\uff0c\u518d\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9com.denodo.vdp.jdbc.Driver \u8fd4\u56de\u4e3b\u754c\u9762\u540e\uff0c\u9009\u62e9 Database->Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u9009\u62e9 Denodo 7.0 \uff0c\u70b9\u51fb Next \u3002 \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb Finish \u3002 \u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff1a Database URL: jdbc:vdb://localhost:9999/admin Database Userid: admin Database Password: admin \u8fde\u63a5Denodo\u9ed8\u8ba4\u7684admin\u6570\u636e\u5e93\u6210\u529f\u3002 \u53cc\u51fb VIEW->student_class \uff0c\u9009\u62e9 Open Object \u3002 \u70b9\u51fb Data \u67e5\u8be2\u8fd4\u56de\u6570\u636e\u6b63\u786e\u3002 \u767b\u5f55RESTful Web service\u67e5\u770bAssociations \u00b6 \u521b\u5efastudent\u548cclass\u7684Association \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New Association \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u5e76\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06\u201cAssociation name\u201d\u8bbe\u7f6e\u4e3a student_class \uff0c\u201cEnd point 'student'\u201d\u4e3a Principal \u4e14\u201cRole name\u201d\u4e3a class \uff0c\u201cEnd point 'class'\u201d\u4e3a Dependent \u4e14\u201cRole name\u201d\u4e3a belongs_to_student \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u4fdd\u5b58\u540e\u53ef\u4ee5\u5728\u6587\u4ef6\u593903_reports\u4e0b\u9762\u770b\u5230Association student_class\u3002 \u767b\u5f55Denodo\u7684RESTful Web service\u67e5\u770bAssociation student_class \u767b\u5f55 http://localhost:9090/denodo-restfulws/admin/ \uff0c\u7528\u6237\u540d\u4e3a admin \uff0c\u5bc6\u7801\u4e3a admin \u3002 \u70b9\u51fb class \uff0c\u8fd4\u56de\u8be5view\u7684\u76f8\u5173\u4fe1\u606f\u3002 \u70b9\u51fb belongs_to_student \uff0c\u8fd4\u56de\u5c5e\u4e8e\u8be5class\u7684\u6240\u6709student\u3002 \u767b\u5f55Data Catalog\u67e5\u770bViews \u00b6 \u5728\u4e3b\u754c\u9762\u70b9\u51fb Denodo Platform Control Center->Virtual DataPort->Start \u542f\u52a8Data Catalog\u670d\u52a1\u3002 \u72b6\u6001\u663e\u793a\u4e3aRunning\uff0cData Catalog\u670d\u52a1\u542f\u52a8\u6210\u529f\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:9090/denodo-data-catalog \uff0c\u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Sign In \u767b\u5f55\u3002 \u9009\u62e9 Browser->DB/Folders \u3002 \u9009\u62e9 admin->Big Data->02_base views->student->Query \u67e5\u8be2\u89c6\u56festudent\u7684\u6570\u636e\u3002 \u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u8f93\u51fa\u5217\u3002 \u8f93\u5165Name= id \uff0cExpression= id \uff0c\u70b9\u51fb Save \u3002 id\u5217\u6210\u529f\u4fdd\u5b58\u5728Output columns\u3002 \u91c7\u7528\u540c\u6837\u7684\u64cd\u4f5c\uff0c\u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u5176\u4ed6\u8f93\u51fa\u5217\uff0c\u4f8b\u5982name\u3002 \u70b9\u51fb Run \u67e5\u8be2\u6210\u529f\u8fd4\u56destudent\u8868\u7684id\u3001name\u3001\u4e24\u5217\u7684\u503c\u3002 FAQ \u00b6 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: KrbException: Cannot locate default realm \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06developuser\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u8bc1krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u540e\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spar2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.lang.SecurityException: class \"com.ctc.wstx.io.SystemId\"'s signer information does not match signer information of other classes in the same package \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u5c06 woodstox-core-5.0.3.jar \u5305\u62f7\u8d1d\u81f3 Drive class path \u5bf9\u5e94\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spark2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.sql.SQLException: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 uri from ZooKeeper \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5219\u521b\u5efa\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u914d\u7f6eVirtual DataPort Server\u7684JVM Options\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \u3002\u5982\u679c\u6ca1\u6709\uff0c\u5219\u5148\u505c\u6b62Virtual DataPort Server\uff0c\u518d\u5728Virtual DataPort Server\u7684JVM Options\u4e2d\u65b0\u589ejaas.conf\u7684\u914d\u7f6e\u3002\u8be6\u7ec6\u64cd\u4f5c\u53ef\u53c2\u8003\u672c\u6587\u7684 \u542f\u52a8\u5e76\u914d\u7f6eDenodo->\u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server \u7ae0\u8282\u3002 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: Clock skew too great (37) - PREAUTH_FAILED \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u5ba2\u6237\u7aef\u673a\u5668\uff08\u672c\u5730\uff09\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u7684\u65f6\u95f4\u5dee\u662f\u5426\u5c0f\u4e8e5\u5206\u949f\u3002\u5982\u679c\u4e0d\u662f\uff0c\u5efa\u8bae\u4fee\u6539\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4fdd\u6301\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002","title":"7.0 <--> 6.5"},{"location":"Data_Integration/Denodo/#denodofusioninsight","text":"","title":"Denodo\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Denodo/#_1","text":"Denodo Platform 7.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/Spark2x) Denodo Platform 7.0 \u2194 FusionInsight HD 6.5 (Hive/Spark2x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Denodo/#_2","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Denodo Platform 7.0 Denodo\u662f\u4e00\u4e2a\u6570\u636e\u865a\u62df\u5316\u7cfb\u7edf\uff0c\u5141\u8bb8\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u5e76\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7edf\u4e00\u7684\u8bbf\u95ee\u63a5\u53e3\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u6e90\u5b9e\u65f6\u5730\u8bbf\u95ee\u548c\u96c6\u6210\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u4ece\u6570\u636e\u6e90\u590d\u5236\u6216\u79fb\u52a8\u6570\u636e\u3002\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u5728\u865a\u62df\u5c42\u4e2d\u5b9a\u4e49\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u72ec\u7acb\u4e8e\u5b58\u50a8\u6570\u636e\u7684\u7269\u7406\u6e90\u3002 \u4ece https://community.denodo.com/express/download \u4e0b\u8f7dDenodo Platform 7.0\u7684\u201cDenodo Express Installer\u201d\u548c\u201cDenodo Express License\u201d\u3002\u4e0b\u8f7d\u9009\u62e9\u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672c\u662fWindows 64 bits\u3002 \u4e0b\u8f7d\u5b8c\u6210\u540e\u5b89\u88c5\u4e8e\u672c\u5730 C:\\Denodo\\ \u3002 FusionInsight HD\u76f8\u5173\u914d\u7f6e\uff08\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff09 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88Hive\u548cSpark2x\u7684\u6240\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06user.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55FusionInsight Manager \u4e3b\u673a->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u4e0b\u8f7dFusionInsight HD\u5ba2\u6237\u7aef\u5230\u672c\u5730\u3002 \u5bf9\u63a5Hive\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Hive\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive\\ \uff0c\u5982\u679chive\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5bf9\u63a5Spark2x\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u5982\u679cspark2x\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u5982\u679c\u662fFusionInsight HD 6.5.X\u7248\u672c\uff0c\u8fd8\u9700\u8981\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \u3002\u5982\u679c\u662fFusionInsight HD V100R002C80SPC100\u7248\u672c\uff0c\u5219\u4e0d\u9700\u8981\u3002 \u51c6\u5907\u6570\u636e Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); INSERT INTO student VALUES (5,'Vina',2); INSERT INTO student VALUES (6,'Manson',3); INSERT INTO student VALUES (7,'Summy',1); INSERT INTO student VALUES (8,'Peter',2); INSERT INTO student VALUES (9,'Wendy',3); INSERT INTO student VALUES (10,'Andy',1); INSERT INTO student VALUES (11,'Miki',2); INSERT INTO student VALUES (12,'Aurora',3); INSERT INTO student VALUES (13,'Carina',1); INSERT INTO student VALUES (14,'Hely',1); INSERT INTO student VALUES (15,'Tracy',2); \u521b\u5efa\u4e0estudent.class_id\u76f8\u5173\u7684\u6570\u636e\u5b58\u653e\u4e8eexcel\u8868\u4e2d\u3002\u4f8b\u5982\u521b\u5efa C:\\developuser\\Class.xlsx \uff0csheet\u547d\u540d\u4e3a\u201cClass\u201d\uff0c\u5305\u542b\u4e24\u5217\uff0c\u5206\u522b\u662fid\u548cname\uff0cid\u5217\u7684\u53d6\u503c\u5fc5\u987b\u5b58\u5728\u4e8estudent.class_id\u4e2d\u3002 JDBC\u8fde\u63a5\u9700\u8981\u67e5\u8be2Zookeeper\uff0cZookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; };","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Denodo/#denodo","text":"\u70b9\u51fb \u5f00\u59cb->Denodo Platform->Denodo Platform 7.0 \u542f\u52a8Denodo Platform Control Center\u3002 \u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server\u3002 \u70b9\u51fb Virtual DataPort->Configure \u3002 \u70b9\u51fb JVM Options \u3002 Virtual DataPort Server\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \uff0c\u4e24\u4e2aOptions\u4e4b\u95f4\u7528\u7a7a\u683c\u9694\u5f00\u3002\u70b9\u51fb Ok \u3002 \u70b9\u51fb Virtual DataPort \u8fd4\u56de\u4e3b\u754c\u9762\uff0c\u70b9\u51fb Start \u542f\u52a8Virtual DataPort Server\u3002 \u542f\u52a8Virtual DataPort Administration Tool\u3002 Virtual DataPort Server\u542f\u52a8\u6210\u529f\u540e\u72b6\u6001\u663e\u793a\u4e3aRunning\uff0c\u70b9\u51fb LAUNCH \u542f\u52a8Virtual DataPort Administration Tool\u3002 \u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Connect \u767b\u5f55\u3002 \u6210\u529f\u767b\u5f55Virtual DataPort Administration Tool\u3002","title":"\u542f\u52a8\u5e76\u914d\u7f6eDenodo"},{"location":"Data_Integration/Denodo/#hivespark2x","text":"","title":"\u5bf9\u63a5Hive\u6216\u8005Spark2x"},{"location":"Data_Integration/Denodo/#jdbcdata-source","text":"\u53f3\u952e admin->Big Data \u9009\u62e9 New->Data source->JDBC \u3002 \u914d\u7f6e\u8fde\u63a5\u4fe1\u606f\uff1a Name\uff1a\u81ea\u547d\u540d\u7684\u65b0\u5efa\u7684Data Source\u540d\u79f0\u3002 Driver class path\uff1aHive\u6216\u8005Spark2x\u7684Jar\u5305\u6240\u5728\u7684\u4f4d\u7f6e\u3002\u5177\u4f53\u914d\u7f6e\u8def\u5f84\u53c2\u8003\u672c\u6587 \u51c6\u5907\u5de5\u4f5c->\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5 \u7ae0\u8282\u3002 Database URI\uff1aHive\u6216\u8005Spark2x\u8fde\u63a5\u7684URL\u3002 Authentication\uff1a\u9009\u62e9Kerberos\u8ba4\u8bc1\u3002 \u5bf9\u63a5Hive\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: hive_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u5bf9\u63a5Spark2x\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: spark2x_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u70b9\u51fb Test connection \uff0c\u8fd4\u56de JDBC connection tested successfully \u3002\u5982\u679c\u8fd4\u56de\u5931\u8d25\uff0c\u53ef\u5728 C:\\Denodo\\DenodoPlatform7.0\\logs\\vdp\\vdp.log \u67e5\u770b\u8be6\u7ec6\u7684\u5931\u8d25\u65e5\u5fd7\u3002\u70b9\u51fb Ok \u5173\u95ed\u6210\u529f\u63d0\u793a\u3002 \u70b9\u51fb Save \u4fdd\u5b58hive_ds\u3002 \u4fdd\u5b58\u6210\u529f\u540e\uff0c\u5de6\u8fb9\u663e\u793a\u7684 admin->Big Data->hive_ds \u5373\u4e3a\u65b0\u589e\u7684Data Source\u3002","title":"\u521b\u5efaJDBC\u8fde\u63a5\u7684Data source"},{"location":"Data_Integration/Denodo/#hive","text":"\u4e3a\u4e86\u66f4\u597d\u89c2\u5bdf\uff0c\u53f3\u952eBig Data\u6587\u4ef6\u5939 New->Folder \u65b0\u5efa\u4e09\u4e2a\u6587\u4ef6\u5939\u5206\u522b\u5b58\u5728Data source(01_data source)\u3001base views(02_base views)\u3001\u96c6\u6210\u6570\u636e(03_reports)\uff0c\u5e76\u628a\u5df2\u521b\u5efa\u7684data sources\u79fb\u5165\u6587\u4ef6\u593901_data source\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684Data Source hive_ds \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Create base view \u3002\u9009\u62e9employee\u8868 default->Tables->student \uff0c\u518d\u70b9\u51fb Create selected \u3002 View name\u547d\u540d\u4e3a student \uff0c\u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View student \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56de\u7684student\u8868\u7684\u6570\u636e\u3002","title":"\u521b\u5efaHive\u6570\u636e\u6e90"},{"location":"Data_Integration/Denodo/#excel","text":"\u53f3\u952e\u6587\u4ef6\u593901_data source\uff0c\u9009\u62e9 New->Data source->Excel \u3002 \u9009\u62e9\u5bfc\u5165\u5df2\u51c6\u5907\u597d\u5b58\u653e\u4e8e C:\\developuser\\ \u7684 Class.xlsx \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u5177\u4f53\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Name: class Type of file: \u6839\u636e\u51c6\u5907\u7684Excel\u8868\u7684\u7248\u672c\u9009\u62e9 File location: \u4e0b\u62c9\u6846\u9009\u62e9Local\u540e\uff0c\u518d\u70b9\u51fbConfigure\u9009\u62e9C:\\developuser\\Class.xlsx Worksheets: \u8f93\u5165\u51c6\u5907\u6570\u636e\u5bf9\u5e94\u7684Sheet\u540d\u79f0Class Start cell: \u51c6\u5907\u6570\u636e\u5f00\u59cb\u7684\u5355\u5143\u683c End cell: \u51c6\u5907\u6570\u636e\u7ed3\u675f\u7684\u5355\u5143\u683c Has headers: \u52fe\u9009 Stream tuples: \u52fe\u9009 \u9009\u62e9Data source class \uff0c\u70b9\u51fb Create base view \u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View class \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deExcel\u7684Class\u7684\u6570\u636e\u3002 \u5c06View class \u548c student \u79fb\u5165\u6587\u4ef6\u593902_base views\u3002","title":"\u521b\u5efaExcel\u8868\u6570\u636e\u6e90"},{"location":"Data_Integration/Denodo/#hiveexcel","text":"\u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New->Join \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06View name\u8bbe\u7f6e\u4e3a\u201cstudent_class\u201d\uff0c\u5c06student\u7684class_id\u548cclass\u7684id\u5220\u9664\u3002 \u91cd\u547d\u540dclass\u7684name\u4e3aclass_name\u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u9009\u62e9 student_class \uff0c\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deHive\u548cExcel\u7ec4\u5408\u540e\u7684\u6570\u636e\u3002","title":"\u7ec4\u5408Hive\u548cExcel\u7684\u6570\u636e"},{"location":"Data_Integration/Denodo/#dbvisualizerdenodo-views","text":"\u4ece https://www.dbvis.com/download/ \u4e0b\u8f7dDbVisualizer\u5e76\u5b89\u88c5\u4e8e\u672c\u5730\u3002 \u6253\u5f00DbVisualizer\uff0c\u9009\u62e9 Tools->Driver Manager \u3002 \u9009\u62e9 Drive->Create Driver \u3002 \u8f93\u5165\u4ee5\u4e0b\u914d\u7f6e\u4fe1\u606f\u540e\u5173\u95ed\u8be5\u754c\u9762\uff1a Name: Denodo 7.0 URL Format: jdbc:vdb://host:port/database Drive Class: \u70b9\u51fb\u6587\u4ef6\u5939\u5bfc\u5165Denodo\u81ea\u5e26\u7684JDBC jar\u5305\uff0c\u4f8b\u5982C:\\Denodo\\DenodoPlatform7.0\\tools\\client-drivers\\jdbc\\denodo-vdp-jdbcdriver.jar\uff0c\u518d\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9com.denodo.vdp.jdbc.Driver \u8fd4\u56de\u4e3b\u754c\u9762\u540e\uff0c\u9009\u62e9 Database->Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u9009\u62e9 Denodo 7.0 \uff0c\u70b9\u51fb Next \u3002 \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb Finish \u3002 \u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff1a Database URL: jdbc:vdb://localhost:9999/admin Database Userid: admin Database Password: admin \u8fde\u63a5Denodo\u9ed8\u8ba4\u7684admin\u6570\u636e\u5e93\u6210\u529f\u3002 \u53cc\u51fb VIEW->student_class \uff0c\u9009\u62e9 Open Object \u3002 \u70b9\u51fb Data \u67e5\u8be2\u8fd4\u56de\u6570\u636e\u6b63\u786e\u3002","title":"\u4f7f\u7528DbVisualizer\u67e5\u770bDenodo Views\u7684\u6570\u636e"},{"location":"Data_Integration/Denodo/#restful-web-serviceassociations","text":"\u521b\u5efastudent\u548cclass\u7684Association \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New Association \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u5e76\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06\u201cAssociation name\u201d\u8bbe\u7f6e\u4e3a student_class \uff0c\u201cEnd point 'student'\u201d\u4e3a Principal \u4e14\u201cRole name\u201d\u4e3a class \uff0c\u201cEnd point 'class'\u201d\u4e3a Dependent \u4e14\u201cRole name\u201d\u4e3a belongs_to_student \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u4fdd\u5b58\u540e\u53ef\u4ee5\u5728\u6587\u4ef6\u593903_reports\u4e0b\u9762\u770b\u5230Association student_class\u3002 \u767b\u5f55Denodo\u7684RESTful Web service\u67e5\u770bAssociation student_class \u767b\u5f55 http://localhost:9090/denodo-restfulws/admin/ \uff0c\u7528\u6237\u540d\u4e3a admin \uff0c\u5bc6\u7801\u4e3a admin \u3002 \u70b9\u51fb class \uff0c\u8fd4\u56de\u8be5view\u7684\u76f8\u5173\u4fe1\u606f\u3002 \u70b9\u51fb belongs_to_student \uff0c\u8fd4\u56de\u5c5e\u4e8e\u8be5class\u7684\u6240\u6709student\u3002","title":"\u767b\u5f55RESTful Web service\u67e5\u770bAssociations"},{"location":"Data_Integration/Denodo/#data-catalogviews","text":"\u5728\u4e3b\u754c\u9762\u70b9\u51fb Denodo Platform Control Center->Virtual DataPort->Start \u542f\u52a8Data Catalog\u670d\u52a1\u3002 \u72b6\u6001\u663e\u793a\u4e3aRunning\uff0cData Catalog\u670d\u52a1\u542f\u52a8\u6210\u529f\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:9090/denodo-data-catalog \uff0c\u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Sign In \u767b\u5f55\u3002 \u9009\u62e9 Browser->DB/Folders \u3002 \u9009\u62e9 admin->Big Data->02_base views->student->Query \u67e5\u8be2\u89c6\u56festudent\u7684\u6570\u636e\u3002 \u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u8f93\u51fa\u5217\u3002 \u8f93\u5165Name= id \uff0cExpression= id \uff0c\u70b9\u51fb Save \u3002 id\u5217\u6210\u529f\u4fdd\u5b58\u5728Output columns\u3002 \u91c7\u7528\u540c\u6837\u7684\u64cd\u4f5c\uff0c\u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u5176\u4ed6\u8f93\u51fa\u5217\uff0c\u4f8b\u5982name\u3002 \u70b9\u51fb Run \u67e5\u8be2\u6210\u529f\u8fd4\u56destudent\u8868\u7684id\u3001name\u3001\u4e24\u5217\u7684\u503c\u3002","title":"\u767b\u5f55Data Catalog\u67e5\u770bViews"},{"location":"Data_Integration/Denodo/#faq","text":"\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: KrbException: Cannot locate default realm \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06developuser\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u8bc1krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u540e\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spar2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.lang.SecurityException: class \"com.ctc.wstx.io.SystemId\"'s signer information does not match signer information of other classes in the same package \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u5c06 woodstox-core-5.0.3.jar \u5305\u62f7\u8d1d\u81f3 Drive class path \u5bf9\u5e94\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u5c06 ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars\\woodstox-core-5.0.3.jar \u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\spark2x\\ \uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u5bf9\u63a5Spark2x\u65f6\uff0c\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: java.sql.SQLException: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 uri from ZooKeeper \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5219\u521b\u5efa\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u914d\u7f6eVirtual DataPort Server\u7684JVM Options\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \u3002\u5982\u679c\u6ca1\u6709\uff0c\u5219\u5148\u505c\u6b62Virtual DataPort Server\uff0c\u518d\u5728Virtual DataPort Server\u7684JVM Options\u4e2d\u65b0\u589ejaas.conf\u7684\u914d\u7f6e\u3002\u8be6\u7ec6\u64cd\u4f5c\u53ef\u53c2\u8003\u672c\u6587\u7684 \u542f\u52a8\u5e76\u914d\u7f6eDenodo->\u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server \u7ae0\u8282\u3002 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: Clock skew too great (37) - PREAUTH_FAILED \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u5ba2\u6237\u7aef\u673a\u5668\uff08\u672c\u5730\uff09\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u7684\u65f6\u95f4\u5dee\u662f\u5426\u5c0f\u4e8e5\u5206\u949f\u3002\u5982\u679c\u4e0d\u662f\uff0c\u5efa\u8bae\u4fee\u6539\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4fdd\u6301\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002","title":"FAQ"},{"location":"Data_Integration/H2O.ai/","text":"H2O.ai \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 H2O.ai 3.24.0.2 \u2194 FusionInsight HD 6.5 (HDFS/GuassDB) \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7dH2O.ai\u5b89\u88c5\u5305 \u4e0b\u8f7d\u5730\u5740\u4e3a http://h2o-release.s3.amazonaws.com/h2o/rel-yates/2/h2o-3.24.0.2-cdh6.0.zip \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55h2o-3.24.0.2-cdh6.0 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient H2o\u4f7f\u7528 \u00b6 \u542f\u52a8H2O cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 > -nodes \u6307\u5b9aH2o\u96c6\u7fa4\u4e2d\u8282\u70b9\u6570\u91cf > -mapperXmx \u6307\u5b9aH2O\u96c6\u7fa4\u4f7f\u7528\u5185\u5b58\u5927\u5c0f > -network \u6307\u5b9aH2Oweb\u754c\u9762\u8bbf\u95ee\u7684IP\u5730\u5740\u8303\u56f4 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165http://172.16.4.21:54321\uff0c\u5373\u53ef\u8bbf\u95eeH2O \u8fde\u63a5HDFS \u00b6 \u5728H2O\u7684web\u754c\u9762\u4e0a\uff0c\u4f7f\u7528 Import Files \uff0c\u586b\u5165HDFS\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u70b9\u51fb import \u5373\u53ef \u5728\u4e0b\u9762\u53ef\u4ee5\u770b\u5230\u6267\u884c\u7ed3\u679c * \u53ef\u4ee5\u5bf9\u6587\u4ef6\u8fdb\u884c\u4e00\u4e9b\u8f6c\u6362\uff0c\u9884\u5904\u7406 \u8fde\u63a5GaussDB \u00b6 \u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u4e0a\u4f20\u81f3\u8282\u70b9\uff0c\u4f8b\u5982 /opt/h2o-3.24.0.2-cdh6.0 \u76ee\u5f55\u4e0b \u8fde\u63a5GaussDB \u9700\u8981\u52a0\u8f7dJDBC\u9a71\u52a8\u5305\uff0c\u9700\u5728\u542f\u52a8H2O\u96c6\u7fa4\u65f6\u6307\u5b9a,\u4f7f\u7528\u4ee5\u4e0b\u8bed\u53e5\u542f\u52a8H2O\u96c6\u7fa4 cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -libjars gsjdbc4.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 * \u5728H2O\u7684web\u754c\u9762\uff0c\u4f7f\u7528 import SQL Table \uff0c\u586b\u5165\u4ee5\u4e0b\u4fe1\u606f,\u70b9\u51fb import \u70b9\u51fb view Data \uff0c\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"3.24.0.2 <--> 6.5"},{"location":"Data_Integration/H2O.ai/#h2oai-fusioninsight","text":"","title":"H2O.ai \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/H2O.ai/#_1","text":"H2O.ai 3.24.0.2 \u2194 FusionInsight HD 6.5 (HDFS/GuassDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/H2O.ai/#_2","text":"\u4e0b\u8f7dH2O.ai\u5b89\u88c5\u5305 \u4e0b\u8f7d\u5730\u5740\u4e3a http://h2o-release.s3.amazonaws.com/h2o/rel-yates/2/h2o-3.24.0.2-cdh6.0.zip \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55h2o-3.24.0.2-cdh6.0 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/H2O.ai/#h2o","text":"\u542f\u52a8H2O cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 > -nodes \u6307\u5b9aH2o\u96c6\u7fa4\u4e2d\u8282\u70b9\u6570\u91cf > -mapperXmx \u6307\u5b9aH2O\u96c6\u7fa4\u4f7f\u7528\u5185\u5b58\u5927\u5c0f > -network \u6307\u5b9aH2Oweb\u754c\u9762\u8bbf\u95ee\u7684IP\u5730\u5740\u8303\u56f4 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165http://172.16.4.21:54321\uff0c\u5373\u53ef\u8bbf\u95eeH2O","title":"H2o\u4f7f\u7528"},{"location":"Data_Integration/H2O.ai/#hdfs","text":"\u5728H2O\u7684web\u754c\u9762\u4e0a\uff0c\u4f7f\u7528 Import Files \uff0c\u586b\u5165HDFS\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u70b9\u51fb import \u5373\u53ef \u5728\u4e0b\u9762\u53ef\u4ee5\u770b\u5230\u6267\u884c\u7ed3\u679c * \u53ef\u4ee5\u5bf9\u6587\u4ef6\u8fdb\u884c\u4e00\u4e9b\u8f6c\u6362\uff0c\u9884\u5904\u7406","title":"\u8fde\u63a5HDFS"},{"location":"Data_Integration/H2O.ai/#gaussdb","text":"\u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u4e0a\u4f20\u81f3\u8282\u70b9\uff0c\u4f8b\u5982 /opt/h2o-3.24.0.2-cdh6.0 \u76ee\u5f55\u4e0b \u8fde\u63a5GaussDB \u9700\u8981\u52a0\u8f7dJDBC\u9a71\u52a8\u5305\uff0c\u9700\u5728\u542f\u52a8H2O\u96c6\u7fa4\u65f6\u6307\u5b9a,\u4f7f\u7528\u4ee5\u4e0b\u8bed\u53e5\u542f\u52a8H2O\u96c6\u7fa4 cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -libjars gsjdbc4.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 * \u5728H2O\u7684web\u754c\u9762\uff0c\u4f7f\u7528 import SQL Table \uff0c\u586b\u5165\u4ee5\u4e0b\u4fe1\u606f,\u70b9\u51fb import \u70b9\u51fb view Data \uff0c\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"\u8fde\u63a5GaussDB"},{"location":"Data_Integration/IBM_InfoSphere_CDC/","text":"IBM InfoSphere CDC\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM InfoSphere CDC 11.3.3.1 \u2194 FusionInsight HD V100R002C50 (HDFS)","title":"11.3.3.1 <--> C50"},{"location":"Data_Integration/IBM_InfoSphere_CDC/#ibm-infosphere-cdcfusioninsight","text":"","title":"IBM InfoSphere CDC\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/IBM_InfoSphere_CDC/#_1","text":"IBM InfoSphere CDC 11.3.3.1 \u2194 FusionInsight HD V100R002C50 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/","text":"IBM InfoSphere DataStage\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 (HDFS/Hive/SparkSQL) IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Phoenix/SparkSQL/Kafka/GaussDB) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210IBM InfoSphere DataStage 11.5.0.2\u7684\u5b89\u88c5\u90e8\u7f72\uff08\u672c\u6587\u90e8\u7f72\u5728Centos7.2\u4e0a\uff09 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD V100R002C60U20 \u51c6\u5907\u5de5\u4f5c \u00b6 \u914d\u7f6e\u57df\u540d\u89e3\u6790 \u00b6 \u4f7f\u7528 vi /etc/hosts \u547d\u4ee4\u4fee\u6539DataStage Server\u548cClient\u7684hosts\u6587\u4ef6\uff0c\u6dfb\u52a0FI\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f\uff0c\u5982\uff1a 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u5728FI\u7ba1\u7406\u754c\u9762\u521b\u5efaDataStage\u5bf9\u63a5\u7528\u6237\uff0c\u5e76\u8d4b\u4e88\u8be5\u7528\u6237\u6240\u9700\u6743\u9650\uff0c\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u4e0b\u8f7d\u7684tar\u6587\u4ef6\uff0c\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u7684keytab\u6587\u4ef6\u3002 \u4ee5root\u767b\u5f55DataStage Server\u8282\u70b9\uff0c\u5c06FI\u96c6\u7fa4\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u5230 /etc \u76ee\u5f55\u3002 \u5c06\u7528\u6237\u7684user.keytab\u6587\u4ef6\u4e0a\u4f20\u5230DataStage Server\u8282\u70b9\u7684\u4efb\u610f\u76ee\u5f55\uff0c\u5982 /home/dsadm \u3002 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u00b6 \u53c2\u8003FI\u4ea7\u54c1\u6587\u6863\uff0c\u5728FI\u670d\u52a1\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230DataStageServer\uff0c\u5b89\u88c5\u81f3\u81ea\u5b9a\u4e49\u76ee\u5f55\uff0c\u5982 /opt/ficlient \u3002 \u5bf9\u63a5HDFS \u00b6 \u5bfc\u5165FI\u96c6\u7fa4\u7684SSL\u8bc1\u4e66 \u00b6 \u6d4f\u89c8\u5668\u5bfc\u51faFI\u96c6\u7fa4\u7684\u6839\u8bc1\u4e66 \u6d4f\u89c8\u5668\u6253\u5f00FI\u7ba1\u7406\u754c\u9762\uff0c\u67e5\u770b\u8bc1\u4e66\uff0c\u70b9\u51fb\u201c\u8bc1\u4e66\u8def\u5f84\u201d\u9875\u7b7e\uff0c\u9009\u62e9\u6839\u8def\u5f84\uff0c\u67e5\u770b\u6839\u8bc1\u4e66\uff0c\u5728\u201c\u8be6\u7ec6\u4fe1\u606f\u201d\u9875\u7b7e\u4e0b\uff0c\u70b9\u51fb\u201c\u590d\u5236\u5230\u6587\u4ef6\u201d\uff0c\u5bfc\u51fa\u4e3acer\u683c\u5f0f \u8bc1\u4e66\u5bfc\u5165DataStage\u7684keystore\u6587\u4ef6 \u5c06\u5bfc\u51fa\u7684FI\u6839\u8bc1\u4e66fi-root-ca.cer\u4e0a\u4f20\u5230DataStage\u670d\u52a1\u7aef\uff0c\u5982 /home/dsadm \u8def\u5f84\u4e0b\uff0c\u5c06\u8bc1\u4e66\u5bfc\u5165\u5230keystore\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003\uff1a /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks \u751f\u6210\u5e76\u4fdd\u5b58\u52a0\u5bc6\u540e\u7684keystore\u5bc6\u7801 \u4f7f\u7528 vi /home/dsadm/authenticate.properties \u547d\u4ee4\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6\uff0c\u4fdd\u5b58\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5bc6\u6587\uff1a password={iisenc}SvtJ2f/uNTrvbuh26XDzag== \u6267\u884c chown dsadm:dstage /home/dsadm/ authenticate.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u7684\u5c5e\u4e3b \u5bfc\u51fatruststore\u73af\u5883\u53d8\u91cf \u4f7f\u7528 vi /opt/IBM/InformationServer/Server/DSEngine/dsenv \u7f16\u8f91DSEngine\u7684\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6700\u540e\u6dfb\u52a0 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties \u91cd\u542fDSEngine\uff0c\u53c2\u8003\u547d\u4ee4 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfs2sf \u6dfb\u52a0File_Connector\u7ec4\u4ef6\u548cSequential File\u7ec4\u4ef6\uff0c\u4ee5\u53caFile_Connector\u5230Sequential File\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u7f16\u8bd1\uff0c\u8fd0\u884c \u5728\u83dc\u5355 Tools -> Run Director \u4e2d\u6253\u5f00Director\u5ba2\u6237\u7aef\uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7 \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u5199\u5165HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfswrite \u6dfb\u52a0Row Generator\u7ec4\u4ef6\u548cFile Connector\u7ec4\u4ef6\uff0c\u4ee5\u53caRow Generator\u5230File Connector\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u5199\u5165\u6570\u636e \u5bf9\u63a5Hive \u00b6 \u4f7f\u7528Hive Connector \u00b6 \u8bf4\u660e\uff1aHive Connector\u5b98\u65b9\u8ba4\u8bc1\u8fc7\u7684Hive JDBC Driver\u53ea\u6709DataDirect Hive Driver(IShive.jar)\uff0c\u7528DataStage 11.5.0.2\u4e2d\u81ea\u5e26\u7684IShive.jar\u8fde\u63a5FusionInsight\u7684hive\u65f6\uff0c\u4f1a\u6709thrift protocol\u62a5\u9519\uff0c\u9700\u8981\u54a8\u8be2IBM\u6280\u672f\u652f\u6301\u63d0\u4f9b\u7684\u6700\u65b0\u7684IShive.jar \u8bbe\u7f6eJDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u5728$DSHOME\u8def\u5f84\u4e0b\u521b\u5efaisjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0DataDirect Hive Driver (IShive.jar)\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.ibm.isf.jdbc.hive.HiveDriver\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u5728isjdbc.config\u4e2d\u6dfb\u52a0\u5982\u4e0b\u4fe1\u606f: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver \u914d\u7f6eKerberos\u8ba4\u8bc1\u4fe1\u606f\uff1a \u5728IShive.jar\u6240\u5728\u76ee\u5f55\u4e0b\u521b\u5efaJDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\u5982\u4e0b\u8fdb\u884c\u914d\u7f6e\uff1a jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; \u5176\u4e2dJDBC_DRIVER_test_keytab\u4e3a\u4e0a\u4e00\u6b65\u6307\u5b9a\u7684\u9274\u6743\u4fe1\u606f \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u6570\u636e\u5199\u5165Hive\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff0c\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62\u201911\u201d \u67e5\u770bHive\u8868\u6570\u636e\uff1a Hive Connector\u5411Hive\u8868\u5199\u6570\u636e\u4f7f\u7528Insert\u8bed\u53e5\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u5c06\u6570\u636e\u76f4\u63a5\u5199\u5165HDFS\u6587\u4ef6\u3002 \u4f7f\u7528JDBC Connector \u00b6 \u5982\u679c\u8981\u4f7f\u7528FusionInsight\u7684Hive JDBC\u9a71\u52a8\uff0c \u7528isjdbc.config\u6587\u4ef6CLASSPATH\u4e2d\u6dfb\u52a0jdbc\u9a71\u52a8\u548c\u4f9d\u8d56\u5305\u7684\u65b9\u5f0f\uff0c\u5728\u8fd0\u884c\u4f5c\u4e1a\u65f6\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff0c\u6b64\u65f6\u9700\u8981\u7528\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u52a0\u8f7d \u800c\u4e14\u53ea\u80fd\u7528JDBC Connector\uff0c\u4e0d\u80fd\u7528Hive Connector\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 Hive jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eHive\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Hive/Beeline/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u8fd9\u4e9bjar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm vi $DSHOME/dsenv \u6587\u4ef6\u6700\u540e\u6dfb\u52a0\u76f8\u5173\u7684jar\u5305\uff08\u5177\u4f53\u8def\u5f84\u6839\u636e\u5b9e\u9645\u73af\u5883\u8c03\u6574\uff09 export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u5176\u4e2dURL\u4e3a\uff1a jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c \u6570\u636e\u5199\u5165Hive\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5199\u51655\u6761\u6570\u636e\uff0c\u7528\u65f61\u201949\u201d \u6570\u636e\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u5199\u5165\u6570\u636e hive\u8868\u6570\u636e\u589e\u91cf100 \u589e\u91cf\u6570\u636e\u5b9a\u671f\u81ea\u52a8\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6 \u00b6 \u589e\u91cf\u6570\u636e\u53ef\u4ee5\u65b0\u589eHDFS\u6587\u4ef6\u7684\u65b9\u5f0f\u5bfc\u5165hive\uff0c\u5982\u679c\u8981\u5b9a\u671f\u81ea\u52a8\u5316\u6267\u884c\uff0c\u5bfc\u5165\u7684\u6587\u4ef6\u540d\u4e2d\u9700\u8981\u5305\u542b\u53ef\u53d8\u53c2\u6570\u8fdb\u884c\u8bbe\u7f6e\u548c\u533a\u5206\uff0c\u7136\u540e\u4ee5\u547d\u4ee4\u6216\u811a\u672c\u65b9\u5f0f\u8fd0\u884c\u4f5c\u4e1a\uff0c\u7ed9\u8be5\u53c2\u6570\u8d4b\u503c\u3002 \u521b\u5efa\u4f5c\u4e1a \u8bbe\u7f6e\u4f5c\u4e1a\u53c2\u6570 \u70b9\u51fb\u201cjob properties\u201d\u6309\u94ae\uff0c\u8bbe\u7f6e\u53c2\u6570\u5982\u4e0b \u4fee\u6539\u914d\u7f6e File Connector\u914d\u7f6e\u5bfc\u51fa\u6587\u4ef6\u7684\u540d\u79f0\uff0c\u4ee5\u201c#\u201d\u5f15\u7528\u8bbe\u7f6e\u7684\u53c2\u6570 dsjob\u547d\u4ee4\u8fd0\u884c\u4f5c\u4e1a \u4fdd\u5b58\u7f16\u8bd1\u4f5c\u4e1a\uff0c\u5728DataStage Server\u4e0a\u6267\u884cdsjob -run\u547d\u4ee4\uff0c\u683c\u5f0f\u4e3a\uff1a dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME \u547d\u4ee4\u53c2\u8003: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append \u67e5\u770bHDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u6570\u636e\u589e\u91cf\u4e3a200\u6761 \u5bf9\u63a5SparkSQL \u00b6 \u4e0e\u4f7f\u7528FI Hive JDBC\u9a71\u52a8\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u7528SparkSQL JDBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u540c\u6837\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 SparkSQL jdbc\u4e0d\u652f\u6301insert into\u8bed\u53e5\uff0c\u53ea\u80fd\u7528\u6765\u8bfbhive\u6570\u636e\uff0c\u4e0d\u80fd\u63d2\u5165\u6570\u636e\u5230hive\u8868\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 SparkSQL jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eSpark\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Spark/spark/lib/ \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caspark\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08SparkSQL jdbc\u8fde\u63a5hive\u65f6\u9700\u8981\u8bfb\u53d6hive-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6Hive\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c \u5bf9\u63a5Phoenix \u00b6 \u4f7f\u7528Phoenix\u4ee5JDBC\u65b9\u5f0f\u8bbf\u95eeHBase\u8868\uff0c\u4e5f\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 Phoenix\u76f8\u5173\u7684jar\u5305\u4f4d\u4e8eHBase\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/HBase/hbase/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caHBase\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08phoenix\u8fde\u63a5\u65f6\u9700\u8981\u8bfb\u53d6hbase-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u521b\u5efajaas\u914d\u7f6e\u6587\u4ef6 \u00b6 Phoenix\u8fde\u63a5\u9700\u8981\u67e5\u8be2zookeeper \uff0czookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6 su - admin vi /home/dsadm/jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u8bfb\u53d6Phoenix\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab \u914d\u7f6eJVM options\u4e3a -Djava.security.auth.login.config=/home/dsadm/jaas.conf \u7f16\u8bd1\u8fd0\u884c \u5199\u5165Phoenix\u8868\u6570\u636e \u00b6 Phoenix\u63d2\u5165\u8bed\u53e5\u662fupsert into\uff0c\u4e0d\u652f\u6301Insert into \u8bed\u53e5\uff0c\u6240\u4ee5\u4e0d\u80fd\u7528JDBC Connector\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210SQL\u8bed\u53e5\uff0c\u9700\u8981\u81ea\u5df1\u586b\u5199\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff1a main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5bf9\u63a5Fiber \u00b6 \u5bf9\u63a5Fiber\u9700\u8981\u5148\u5b89\u88c5FI\u5ba2\u6237\u7aef \u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0Fiber jdbc driver\u53ca\u4f9d\u8d56\u5305\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver \u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\u5982\u4e0b\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver \u4fee\u6539Fiber\u914d\u7f6e\u6587\u4ef6 \u00b6 DataStage\u4f7f\u7528IBM jdk\uff0c\u9700\u8981\u65b0\u5efaFiber\u914d\u7f6e\u6587\u4ef6\u7ed9DataStage\u4f7f\u7528 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml \u4fee\u6539fiber_ibm.xml\u4e2dphoenix,hive,spark\u5404driver\u7684\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570\uff1a java.security.auth.login.config \u4fee\u6539\u4e3a /home/dsadm/jaas.conf zookeeper.kinit \u4fee\u6539\u4e3a /opt/IBM/InformationServer/jdk/jre/bin/kinit \u6587\u4ef6/home/dsadm/jaas.conf\u7684\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u5176\u5b83\u914d\u7f6e\u9879\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863Fiber\u5ba2\u6237\u7aef\u914d\u7f6e\u6307\u5bfc\u4fee\u6539\u3002 \u4f7f\u7528Hive Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Hive Driver\u5199\u5165\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Spark Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Phoenix Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u76ee\u524d\u672a\u80fd\u8bfb\u53d6\u5230\u6570\u636e\uff0c\u201dThe connector could not determine the value for the fetch size.\u201d\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d \u4f7f\u7528Phoenix Driver\u5199\u5165\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u5199\u5165\u6570\u636e0\u884c\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d \u5bf9\u63a5Kafka \u00b6 \u8bf4\u660e\uff1akafka Connector\u4e0d\u652f\u6301\u53d1\u9001\u6216\u8005\u6d88\u8d39integer, float, double, numeric, decimal\u7b49\u6570\u503c\u7c7b\u578b\u7684\u5b57\u6bb5\uff0c\u9700\u8981\u8f6c\u6362\u6210char, varchar, longvarchar\u7b49\u7c7b\u578b\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff1a main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL). \u5b89\u88c5kafka\u5ba2\u6237\u7aef \u00b6 kafka Connector\u9700\u8981\u914d\u7f6eKafka client Classpath\uff0c\u53ef\u4ee5\u5728DataStage\u8282\u70b9\u5b89\u88c5kafka\u5ba2\u6237\u7aef\u6765\u83b7\u53d6kafka-client jar\u5305\u3002\u5b89\u88c5\u6b65\u9aa4\u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u3002 Kafka Client Classpath \u9700\u8981\u63d0\u4f9bkafka-client, log4j, slf4j-api \u4e09\u4e2ajar\u5305\u7684\u8def\u5f84\uff0c\u5982\uff1a /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar \u53d1\u9001\u6d88\u606f\u5230kafka \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e RowGenerator \u751f\u6210\u6570\u636e transformer\u6570\u636e\u7c7b\u578b\u8f6c\u6362\uff1a Kafka\u914d\u7f6e\uff1a \u7f16\u8bd1\u8fd0\u884c \u8bfb\u53d6Kafka\u6d88\u606f \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u5bf9\u63a5MPPDB \u00b6 \u83b7\u53d6MPPDB JDBC Driver \u00b6 \u4eceMPPDB\u53d1\u5e03\u5305\u4e2d\u83b7\u53d6\uff0c\u5305\u540d\u4e3aGauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz \u89e3\u538b\u540e\u5f97\u5230gsjdbc4.jar\uff0c\u4e0a\u4f20\u5230DataStage Server \u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0MPPDB Driver \u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver; \u8bfb\u53d6MPPDB\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u6570\u636e\u5199\u5165MPPDB\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u67e5\u770bMPPDB\u8868\u6570\u636e\uff1a","title":"11.5.0.2 <--> C60"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#ibm-infosphere-datastagefusioninsight","text":"","title":"IBM InfoSphere DataStage\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_1","text":"IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 (HDFS/Hive/SparkSQL) IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Phoenix/SparkSQL/Kafka/GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_2","text":"\u5df2\u5b8c\u6210IBM InfoSphere DataStage 11.5.0.2\u7684\u5b89\u88c5\u90e8\u7f72\uff08\u672c\u6587\u90e8\u7f72\u5728Centos7.2\u4e0a\uff09 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD V100R002C60U20","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_3","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_4","text":"\u4f7f\u7528 vi /etc/hosts \u547d\u4ee4\u4fee\u6539DataStage Server\u548cClient\u7684hosts\u6587\u4ef6\uff0c\u6dfb\u52a0FI\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f\uff0c\u5982\uff1a 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3","title":"\u914d\u7f6e\u57df\u540d\u89e3\u6790"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kerberos","text":"\u5728FI\u7ba1\u7406\u754c\u9762\u521b\u5efaDataStage\u5bf9\u63a5\u7528\u6237\uff0c\u5e76\u8d4b\u4e88\u8be5\u7528\u6237\u6240\u9700\u6743\u9650\uff0c\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u4e0b\u8f7d\u7684tar\u6587\u4ef6\uff0c\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u7684keytab\u6587\u4ef6\u3002 \u4ee5root\u767b\u5f55DataStage Server\u8282\u70b9\uff0c\u5c06FI\u96c6\u7fa4\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u5230 /etc \u76ee\u5f55\u3002 \u5c06\u7528\u6237\u7684user.keytab\u6587\u4ef6\u4e0a\u4f20\u5230DataStage Server\u8282\u70b9\u7684\u4efb\u610f\u76ee\u5f55\uff0c\u5982 /home/dsadm \u3002","title":"\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fusioninsight","text":"\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863\uff0c\u5728FI\u670d\u52a1\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230DataStageServer\uff0c\u5b89\u88c5\u81f3\u81ea\u5b9a\u4e49\u76ee\u5f55\uff0c\u5982 /opt/ficlient \u3002","title":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fissl","text":"\u6d4f\u89c8\u5668\u5bfc\u51faFI\u96c6\u7fa4\u7684\u6839\u8bc1\u4e66 \u6d4f\u89c8\u5668\u6253\u5f00FI\u7ba1\u7406\u754c\u9762\uff0c\u67e5\u770b\u8bc1\u4e66\uff0c\u70b9\u51fb\u201c\u8bc1\u4e66\u8def\u5f84\u201d\u9875\u7b7e\uff0c\u9009\u62e9\u6839\u8def\u5f84\uff0c\u67e5\u770b\u6839\u8bc1\u4e66\uff0c\u5728\u201c\u8be6\u7ec6\u4fe1\u606f\u201d\u9875\u7b7e\u4e0b\uff0c\u70b9\u51fb\u201c\u590d\u5236\u5230\u6587\u4ef6\u201d\uff0c\u5bfc\u51fa\u4e3acer\u683c\u5f0f \u8bc1\u4e66\u5bfc\u5165DataStage\u7684keystore\u6587\u4ef6 \u5c06\u5bfc\u51fa\u7684FI\u6839\u8bc1\u4e66fi-root-ca.cer\u4e0a\u4f20\u5230DataStage\u670d\u52a1\u7aef\uff0c\u5982 /home/dsadm \u8def\u5f84\u4e0b\uff0c\u5c06\u8bc1\u4e66\u5bfc\u5165\u5230keystore\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003\uff1a /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks \u751f\u6210\u5e76\u4fdd\u5b58\u52a0\u5bc6\u540e\u7684keystore\u5bc6\u7801 \u4f7f\u7528 vi /home/dsadm/authenticate.properties \u547d\u4ee4\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6\uff0c\u4fdd\u5b58\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5bc6\u6587\uff1a password={iisenc}SvtJ2f/uNTrvbuh26XDzag== \u6267\u884c chown dsadm:dstage /home/dsadm/ authenticate.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u7684\u5c5e\u4e3b \u5bfc\u51fatruststore\u73af\u5883\u53d8\u91cf \u4f7f\u7528 vi /opt/IBM/InformationServer/Server/DSEngine/dsenv \u7f16\u8f91DSEngine\u7684\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6700\u540e\u6dfb\u52a0 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties \u91cd\u542fDSEngine\uff0c\u53c2\u8003\u547d\u4ee4 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u5bfc\u5165FI\u96c6\u7fa4\u7684SSL\u8bc1\u4e66"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs_1","text":"\u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfs2sf \u6dfb\u52a0File_Connector\u7ec4\u4ef6\u548cSequential File\u7ec4\u4ef6\uff0c\u4ee5\u53caFile_Connector\u5230Sequential File\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u7f16\u8bd1\uff0c\u8fd0\u884c \u5728\u83dc\u5355 Tools -> Run Director \u4e2d\u6253\u5f00Director\u5ba2\u6237\u7aef\uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7 \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs_2","text":"\u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfswrite \u6dfb\u52a0Row Generator\u7ec4\u4ef6\u548cFile Connector\u7ec4\u4ef6\uff0c\u4ee5\u53caRow Generator\u5230File Connector\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u5199\u5165\u6570\u636e","title":"\u5199\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-connector","text":"\u8bf4\u660e\uff1aHive Connector\u5b98\u65b9\u8ba4\u8bc1\u8fc7\u7684Hive JDBC Driver\u53ea\u6709DataDirect Hive Driver(IShive.jar)\uff0c\u7528DataStage 11.5.0.2\u4e2d\u81ea\u5e26\u7684IShive.jar\u8fde\u63a5FusionInsight\u7684hive\u65f6\uff0c\u4f1a\u6709thrift protocol\u62a5\u9519\uff0c\u9700\u8981\u54a8\u8be2IBM\u6280\u672f\u652f\u6301\u63d0\u4f9b\u7684\u6700\u65b0\u7684IShive.jar","title":"\u4f7f\u7528Hive Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver","text":"\u5728$DSHOME\u8def\u5f84\u4e0b\u521b\u5efaisjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0DataDirect Hive Driver (IShive.jar)\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.ibm.isf.jdbc.hive.HiveDriver\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u5728isjdbc.config\u4e2d\u6dfb\u52a0\u5982\u4e0b\u4fe1\u606f: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver \u914d\u7f6eKerberos\u8ba4\u8bc1\u4fe1\u606f\uff1a \u5728IShive.jar\u6240\u5728\u76ee\u5f55\u4e0b\u521b\u5efaJDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"\u8bbe\u7f6eJDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\u5982\u4e0b\u8fdb\u884c\u914d\u7f6e\uff1a jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; \u5176\u4e2dJDBC_DRIVER_test_keytab\u4e3a\u4e0a\u4e00\u6b65\u6307\u5b9a\u7684\u9274\u6743\u4fe1\u606f \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff0c\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62\u201911\u201d \u67e5\u770bHive\u8868\u6570\u636e\uff1a Hive Connector\u5411Hive\u8868\u5199\u6570\u636e\u4f7f\u7528Insert\u8bed\u53e5\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u5c06\u6570\u636e\u76f4\u63a5\u5199\u5165HDFS\u6587\u4ef6\u3002","title":"\u6570\u636e\u5199\u5165Hive\u8868"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-connector","text":"\u5982\u679c\u8981\u4f7f\u7528FusionInsight\u7684Hive JDBC\u9a71\u52a8\uff0c \u7528isjdbc.config\u6587\u4ef6CLASSPATH\u4e2d\u6dfb\u52a0jdbc\u9a71\u52a8\u548c\u4f9d\u8d56\u5305\u7684\u65b9\u5f0f\uff0c\u5728\u8fd0\u884c\u4f5c\u4e1a\u65f6\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff0c\u6b64\u65f6\u9700\u8981\u7528\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u52a0\u8f7d \u800c\u4e14\u53ea\u80fd\u7528JDBC Connector\uff0c\u4e0d\u80fd\u7528Hive Connector\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519","title":"\u4f7f\u7528JDBC Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath","text":"Hive jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eHive\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Hive/Beeline/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u8fd9\u4e9bjar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm vi $DSHOME/dsenv \u6587\u4ef6\u6700\u540e\u6dfb\u52a0\u76f8\u5173\u7684jar\u5305\uff08\u5177\u4f53\u8def\u5f84\u6839\u636e\u5b9e\u9645\u73af\u5883\u8c03\u6574\uff09 export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_3","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u5176\u4e2dURL\u4e3a\uff1a jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_4","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5199\u51655\u6761\u6570\u636e\uff0c\u7528\u65f61\u201949\u201d","title":"\u6570\u636e\u5199\u5165Hive\u8868"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hivehdfs","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u5199\u5165\u6570\u636e hive\u8868\u6570\u636e\u589e\u91cf100","title":"\u6570\u636e\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hivehdfs_1","text":"\u589e\u91cf\u6570\u636e\u53ef\u4ee5\u65b0\u589eHDFS\u6587\u4ef6\u7684\u65b9\u5f0f\u5bfc\u5165hive\uff0c\u5982\u679c\u8981\u5b9a\u671f\u81ea\u52a8\u5316\u6267\u884c\uff0c\u5bfc\u5165\u7684\u6587\u4ef6\u540d\u4e2d\u9700\u8981\u5305\u542b\u53ef\u53d8\u53c2\u6570\u8fdb\u884c\u8bbe\u7f6e\u548c\u533a\u5206\uff0c\u7136\u540e\u4ee5\u547d\u4ee4\u6216\u811a\u672c\u65b9\u5f0f\u8fd0\u884c\u4f5c\u4e1a\uff0c\u7ed9\u8be5\u53c2\u6570\u8d4b\u503c\u3002 \u521b\u5efa\u4f5c\u4e1a \u8bbe\u7f6e\u4f5c\u4e1a\u53c2\u6570 \u70b9\u51fb\u201cjob properties\u201d\u6309\u94ae\uff0c\u8bbe\u7f6e\u53c2\u6570\u5982\u4e0b \u4fee\u6539\u914d\u7f6e File Connector\u914d\u7f6e\u5bfc\u51fa\u6587\u4ef6\u7684\u540d\u79f0\uff0c\u4ee5\u201c#\u201d\u5f15\u7528\u8bbe\u7f6e\u7684\u53c2\u6570 dsjob\u547d\u4ee4\u8fd0\u884c\u4f5c\u4e1a \u4fdd\u5b58\u7f16\u8bd1\u4f5c\u4e1a\uff0c\u5728DataStage Server\u4e0a\u6267\u884cdsjob -run\u547d\u4ee4\uff0c\u683c\u5f0f\u4e3a\uff1a dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME \u547d\u4ee4\u53c2\u8003: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append \u67e5\u770bHDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u6570\u636e\u589e\u91cf\u4e3a200\u6761","title":"\u589e\u91cf\u6570\u636e\u5b9a\u671f\u81ea\u52a8\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#sparksql","text":"\u4e0e\u4f7f\u7528FI Hive JDBC\u9a71\u52a8\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u7528SparkSQL JDBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u540c\u6837\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 SparkSQL jdbc\u4e0d\u652f\u6301insert into\u8bed\u53e5\uff0c\u53ea\u80fd\u7528\u6765\u8bfbhive\u6570\u636e\uff0c\u4e0d\u80fd\u63d2\u5165\u6570\u636e\u5230hive\u8868\u3002","title":"\u5bf9\u63a5SparkSQL"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath_1","text":"SparkSQL jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eSpark\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Spark/spark/lib/ \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caspark\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08SparkSQL jdbc\u8fde\u63a5hive\u65f6\u9700\u8981\u8bfb\u53d6hive-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_5","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Hive\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix","text":"\u4f7f\u7528Phoenix\u4ee5JDBC\u65b9\u5f0f\u8bbf\u95eeHBase\u8868\uff0c\u4e5f\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002","title":"\u5bf9\u63a5Phoenix"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath_2","text":"Phoenix\u76f8\u5173\u7684jar\u5305\u4f4d\u4e8eHBase\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/HBase/hbase/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caHBase\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08phoenix\u8fde\u63a5\u65f6\u9700\u8981\u8bfb\u53d6hbase-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jaas","text":"Phoenix\u8fde\u63a5\u9700\u8981\u67e5\u8be2zookeeper \uff0czookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6 su - admin vi /home/dsadm/jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"\u521b\u5efajaas\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab \u914d\u7f6eJVM options\u4e3a -Djava.security.auth.login.config=/home/dsadm/jaas.conf \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Phoenix\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix_2","text":"Phoenix\u63d2\u5165\u8bed\u53e5\u662fupsert into\uff0c\u4e0d\u652f\u6301Insert into \u8bed\u53e5\uff0c\u6240\u4ee5\u4e0d\u80fd\u7528JDBC Connector\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210SQL\u8bed\u53e5\uff0c\u9700\u8981\u81ea\u5df1\u586b\u5199\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff1a main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c","title":"\u5199\u5165Phoenix\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fiber","text":"\u5bf9\u63a5Fiber\u9700\u8981\u5148\u5b89\u88c5FI\u5ba2\u6237\u7aef","title":"\u5bf9\u63a5Fiber"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver_1","text":"\u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0Fiber jdbc driver\u53ca\u4f9d\u8d56\u5305\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver \u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\u5982\u4e0b\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver","title":"\u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fiber_1","text":"DataStage\u4f7f\u7528IBM jdk\uff0c\u9700\u8981\u65b0\u5efaFiber\u914d\u7f6e\u6587\u4ef6\u7ed9DataStage\u4f7f\u7528 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml \u4fee\u6539fiber_ibm.xml\u4e2dphoenix,hive,spark\u5404driver\u7684\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570\uff1a java.security.auth.login.config \u4fee\u6539\u4e3a /home/dsadm/jaas.conf zookeeper.kinit \u4fee\u6539\u4e3a /opt/IBM/InformationServer/jdk/jre/bin/kinit \u6587\u4ef6/home/dsadm/jaas.conf\u7684\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u5176\u5b83\u914d\u7f6e\u9879\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863Fiber\u5ba2\u6237\u7aef\u914d\u7f6e\u6307\u5bfc\u4fee\u6539\u3002","title":"\u4fee\u6539Fiber\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Hive Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-driver_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Hive Driver\u5199\u5165\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#spark-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Spark Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u76ee\u524d\u672a\u80fd\u8bfb\u53d6\u5230\u6570\u636e\uff0c\u201dThe connector could not determine the value for the fetch size.\u201d\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d","title":"\u4f7f\u7528Phoenix Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix-driver_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u5199\u5165\u6570\u636e0\u884c\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d","title":"\u4f7f\u7528Phoenix Driver\u5199\u5165\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka","text":"\u8bf4\u660e\uff1akafka Connector\u4e0d\u652f\u6301\u53d1\u9001\u6216\u8005\u6d88\u8d39integer, float, double, numeric, decimal\u7b49\u6570\u503c\u7c7b\u578b\u7684\u5b57\u6bb5\uff0c\u9700\u8981\u8f6c\u6362\u6210char, varchar, longvarchar\u7b49\u7c7b\u578b\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff1a main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL).","title":"\u5bf9\u63a5Kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_1","text":"kafka Connector\u9700\u8981\u914d\u7f6eKafka client Classpath\uff0c\u53ef\u4ee5\u5728DataStage\u8282\u70b9\u5b89\u88c5kafka\u5ba2\u6237\u7aef\u6765\u83b7\u53d6kafka-client jar\u5305\u3002\u5b89\u88c5\u6b65\u9aa4\u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u3002 Kafka Client Classpath \u9700\u8981\u63d0\u4f9bkafka-client, log4j, slf4j-api \u4e09\u4e2ajar\u5305\u7684\u8def\u5f84\uff0c\u5982\uff1a /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar","title":"\u5b89\u88c5kafka\u5ba2\u6237\u7aef"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e RowGenerator \u751f\u6210\u6570\u636e transformer\u6570\u636e\u7c7b\u578b\u8f6c\u6362\uff1a Kafka\u914d\u7f6e\uff1a \u7f16\u8bd1\u8fd0\u884c","title":"\u53d1\u9001\u6d88\u606f\u5230kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_3","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6Kafka\u6d88\u606f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb","text":"","title":"\u5bf9\u63a5MPPDB"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb-jdbc-driver","text":"\u4eceMPPDB\u53d1\u5e03\u5305\u4e2d\u83b7\u53d6\uff0c\u5305\u540d\u4e3aGauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz \u89e3\u538b\u540e\u5f97\u5230gsjdbc4.jar\uff0c\u4e0a\u4f20\u5230DataStage Server","title":"\u83b7\u53d6MPPDB JDBC Driver"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver_2","text":"\u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0MPPDB Driver \u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;","title":"\u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6MPPDB\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u67e5\u770bMPPDB\u8868\u6570\u636e\uff1a","title":"\u6570\u636e\u5199\u5165MPPDB\u8868"},{"location":"Data_Integration/Informatica_BDM_10.2.2/","text":"Informatica BDM\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica 10.0.0 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica 10.2.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive) \u6ce8\uff1a\u4ee5\u4e0a\u5bf9\u63a5\u6d4b\u8bd5Informatica BDM\u91c7\u7528\u7684\u662fNative Engine\u3002Informatica 10.2.2\u5bf9\u63a5FusionInsight HD 6.5 HBase\u7ec4\u4ef6\u65f6\uff0c\u9700\u8981Zookeeper\u7ec4\u4ef6\u914d\u7f6eenforce.auth.enabled=false\uff0c\u5426\u5219\u5bf9\u63a5\u5931\u8d25\u3002 \u7b80\u4ecb \u00b6 Informatica\u7528\u4e8e\u7ba1\u7406\u5927\u6570\u636e\u5de5\u7a0b\u7684\u5de5\u5177\u4e3b\u8981\u6709Informatica Administrator\u3001Infoormatica Analyst\u548cInformatica Developer\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0Linux\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica 10.2.2\u670d\u52a1\u7aef\uff08Informatica Administrator\uff09\u5e76\u4f7f\u7528Oracle\u6570\u636e\u5e93\u7ba1\u7406\u57df\u6570\u636e\u3001\u8fde\u63a5\u6570\u636e\u7b49\uff0c\u5728Window\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica\u5ba2\u6237\u7aefBig Data Developuser\uff08Informatica Developer\u5176\u4e2d\u4e00\u79cd\u5de5\u5177\uff09\u3002Informatica\u670d\u52a1\u7aef\u4e0eFusionInsight HD\u7684HDFS\u548cHive\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u3001HDFS\u3001Hive\u3001HBase\u4e4b\u95f4\u6570\u636e\u4e92\u4f20\u3002 \u672c\u6587\u6863\u7684\u63cf\u8ff0\u4f7f\u7528\u7684Informatic Server\u5b89\u88c5\u8282\u70b9\u7684IP\u4e3a172.16.6.120\uff0c\u4e3b\u673a\u540d\u4e3a172-16-6-120\u3002\u5bf9\u63a5\u7684FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u5206\u522b\u662f172.16.4.21/172.16.4.22/172.16.4.23. \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\ecotesting\\hadoopConfig \u76ee\u5f55\u4e0b\uff0c\u5e76\u538b\u7f29\u4e3a hadoopConfig.zip \u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002 \u5b89\u88c5Infomatica\u670d\u52a1\u7aef \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728Linux\u4e0a\u5b89\u88c5Infomatica Server\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5b89\u88c5\u8282\u70b9\u4e0a\u5df2\u5b89\u88c5\u597dOracle\u6570\u636e\u5e93\u3002\u672c\u6307\u5bfc\u6587\u6863\u5b89\u88c5\u7248\u672c\u4e3a Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production \uff0c\u975e\u5bb9\u5668\u6570\u636e\u5e93\uff0c\u5b89\u88c5\u7528\u6237\u540d\u4e3a oracle \u5e76\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \uff0c\u6570\u636e\u5e93SID\u4e3a orcl \u3002 \u5df2\u83b7\u53d6Informatica\u670d\u52a1\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_server_linux-x64.tar\uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u5df2\u83b7\u53d6Informatica\u7684License\uff0c\u4f8b\u5982\uff1a infa1022.key \uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5\u670d\u52a1\u7aef \u00b6 \u5b89\u88c5Informatica\u670d\u52a1\u7aef\u9700\u8981\u8fde\u63a5Domain\u548cModel_Repository_Service\u7684\u6570\u636e\u5e93\u7528\u6237\u3002\u767b\u5f55oracle\u6570\u636e\u5e93\uff0c\u521b\u5efa\u4e24\u4e2a\u7528\u6237\uff0c\u5206\u522b\u547d\u540d\u4e3a domain_user \u548c mdl_user \u3002 su - oracle sqlplus / as sysdba SQL> create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512m; SQL> create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp; SQL> create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp; SQL> grant dba to domain_user,mdl_user; SQL> exit \u521b\u5efa\u5b89\u88c5\u7528\u6237 infa \u5e76\u5f52\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \u3002 su - root useradd -g oinstall -d /home/infa infa echo \"Huawei@123\" | passwd --stdin infa \u4f7f\u7528 root \u7528\u6237\u89e3\u538binformatica_1022_server_linux-x64.tar\u81f3 /opt/informatica \uff0c\u8bbe\u7f6e\u62e5\u6709\u8005\u4e3ainfa\u7528\u6237\u5e76\u8d4b\u4e88755\u7684\u64cd\u4f5c\u6743\u9650\u3002 su - root mkdir -p /opt/informatica tar -xvf /opt/informatica_1022_server_linux-x64.tar -C /opt/informatica chown -R infa:oinstall /opt/informatica chmod -R 755 /opt/informatica \u4fee\u6539 infa \u7528\u6237\u7684\u73af\u5883\u53d8\u91cf\u3002 su - infa vi ~/.bash_profile source ~/.bash_profile \u6dfb\u52a0\u73af\u5883\u53d8\u91cf\u5982\u4e0b\u6240\u793a\uff1a PATH=$PATH:/u01/app/oracle/product/12.2.1/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.2.1/db_1 export ORACLE_SID=orcl export NLS_LANG=AMERICAN_AMERICA.AL32UTF8 export INFA_CODEPAGENAME=\"UTF-8\" export PATH=/opt/informatica/10.2.2/server/bin:$PATH export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/informatica/10.2.2/server/bin \u8bf4\u660e\uff1a12.2.1\u4e3aoracle\u7248\u672c\u53f7\uff0c10.2.2\u4e3aInformatica\u7684\u7248\u672c\u53f7\u3002 \u4f7f\u7528infa\u7528\u6237\u767b\u5f55\u542f\u52a8\u56fe\u5f62\u5316\u7ec8\u7aef\u5f00\u59cb\u5b89\u88c5\u3002 su - root export display=:0.0 xhost + su - infa export display=:0.0 cd /opt/informatica ./install.sh \u8bf4\u660e\uff1a\u6267\u884c xhost + \u547d\u4ee4\u65f6\uff0c\u786e\u8ba4\u8fd4\u56de\u201caccess control disabled, clients can connect from any host\u201d\uff0c\u624d\u80fd\u7ee7\u7eed\u6267\u884c\u540e\u9762\u7684\u547d\u4ee4\u3002 \u8f93\u5165 y \u9009\u62e9\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall and configure Informatica Big Data suite products.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 3 \u9009\u62e9\u201cRun the installer.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u540c\u610f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall Informatica domain services.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\u4e0d\u6fc0\u6d3bKerberos\u8ba4\u8bc1\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165License\u7684\u8def\u5f84\uff0c\u4f8b\u5982\uff1a /opt/infa1022.key \uff0c\u6309 Enter \u952e\uff0c\u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff08\u786e\u4fdd\u548c\u73af\u5883\u53d8\u91cf~/.bash_profile\u914d\u7f6e\u7684\u8def\u5f84\u4e00\u81f4\uff09\uff0c\u4f8b\u5982\uff1a /opt/informatica/10.2.2 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6309 Enter \u952e\u5f00\u59cb\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u8fdb\u5ea6100%\u540e\uff0c\u6b65\u9aa45\u7684\u9009\u62e9\u521b\u5efaDomain\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa46\u8f93\u5165Domain\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa47\u8f93\u5165\u81ea\u5b9a\u4e49\u5bc6\u7801 Huawei@123 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48\u8f93\u5165Domain\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48B\u8f93\u5165Model Repository Service\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa49\u8f93\u5165Data Integration Service\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u9009\u62e9\u4e0d\u521b\u5efaCCO\u8fde\u63a5\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6309 Enter \u952e\u7ed3\u675f\u5b89\u88c5\u3002 \u786e\u8ba4\u9632\u706b\u5899\u662f\u5173\u95ed\u72b6\u6001\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u6253\u5f00 http://172-16-6-120:6008 \u767b\u5f55Informatica Administrator\uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u767b\u5f55 \u3002 \u767b\u5f55\u6210\u529f\u3002 \u914d\u7f6eInformatica Server \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 krb5.conf \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5c06 krb5.conf \u590d\u5236\u5230 $INFA_HOME/java/jre/lib/security \u548c $INFA_HOME/services/shared/security \u3002\u547d\u4ee4\u6267\u884c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a chown infa:oinstall /opt/krb5.conf cp /opt/krb5.conf /opt/informatica/10.2.2/services/shared/security/ cp /opt/krb5.conf /opt/informatica/10.2.2/java/jre/lib/security/ \u914d\u7f6eData_Integration_Service \u00b6 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9->Data_Integration_Service \u3002\u70b9\u51fb\u201c\u6267\u884c\u9009\u9879\u201d\u7684\u7f16\u8f91\u6309\u94ae\uff0c\u8bbe\u7f6e\u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u4e3a developuser\\@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u4e3a /opt/user.keytab \uff0c\u70b9\u89e3 \u786e\u5b9a \uff0c\u9009\u62e9 \u662f\uff0c\u4fdd\u5b58\u66f4\u6539 \u3002 \u70b9\u51fb\u201cData_Integration_Service\u201d\u7684\u5e94\u7528\u670d\u52a1\u6309\u94ae \uff0c\u70b9\u51fb \u786e\u5b9a \u8ba9\u4fee\u6539\u751f\u6548\u3002 \u521b\u5efa\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1 \u00b6 \u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 user.keytab \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u767b\u5f55 http://172-16-6-120:6008 \uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a Metadata_Service \uff0c\u201c\u4f4d\u7f6e\u201d\u9ed8\u8ba4\u4e3a Domain_172-16-6-120 \uff0c\u201c\u8bb8\u53ef\u8bc1\u201d\u9009\u62e9\u6709\u6548\u7684License\uff0c\u201c\u8282\u70b9\u201d\u9009\u62e9 node01_172-16-6-120 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u8f93\u5165 developuser\\@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u8f93\u5165 /opt/user.keytab \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u70b9\u51fb\u201cMetadata_Service\u201d\u53f3\u4e0a\u89d2\u7684 \u6309\u94ae\u542f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u521b\u5efaInformatica\u7fa4\u96c6 \u00b6 \u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120->ClusterConfigurations\u201d\uff0c\u9009\u62e9 \u65b0->\u7fa4\u96c6\u914d\u7f6e \u3002 \u201c\u7fa4\u96c6\u914d\u7f6e\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a FusionInsightHD \uff0c\u201c\u5206\u53d1\u7c7b\u578b\u201d\u9009\u62e9 Cloudera \uff0c\u201c\u5bfc\u5165\u7fa4\u96c6\u914d\u7f6e\u7684\u65b9\u6cd5\u201d\u9009\u62e9 \u4ece\u5b58\u6863\u6587\u4ef6\u4e2d\u5bfc\u5165 \uff0c\u4e0a\u8f7d\u914d\u7f6e\u5b58\u6863\u6587\u4ef6\u9009\u62e9 C:\\ecotesting\\hadoopConfig.zip \uff0c\u52fe\u9009 \u521b\u5efa\u8fde\u63a5 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u786e\u8ba4\u5b89\u88c5\u4fe1\u606f\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5b89\u88c5\u5b8c\u6210\u3002 \u4fee\u6539\u8fde\u63a5\u201cHIVE_fusionginsighthd\u201d\u4ee5\u4e0b\u5171\u540c\u5c5e\u6027\u3002 \u5143\u6570\u636e\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM \u6570\u636e\u8bbf\u95ee\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM HDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\uff1a/user/hive/warehouse Hive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\uff1adefault \u8bf4\u660e\uff1a\u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u914d\u7f6e\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u548c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u3002 \u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002\u5426\u5219Mapping\u65e5\u5fd7\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u201cClass org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider not found\u201d\u7684\u9519\u8bef\u4e14\u5199\u5165\u5931\u8d25\u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002 \u521b\u5efaOracle\u8fde\u63a5 \u00b6 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u8fde\u63a5 \u3002 \u9009\u62e9 Oracle \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u201c\u540d\u79f0\u201d\u548c\u201cID\u201d\u81ea\u5b9a\u4e49\u4e3a ORACLE \uff0c\u201c\u7528\u6237\u540d\u201d\u548c\u201c\u5bc6\u7801\u201d\u90fd\u8f93\u5165 mdl_user \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5143\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u8f93\u5165 jdbc:informatica:oracle://172-16-6-120:1521;SID=orcl \uff0c\u201c\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u4e3a orcl \u548c \u201c\u4ee3\u7801\u9875\u201d\u9009\u62e9 UTF-8 encoding of Unicode \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \u8fd4\u56de \u201c\u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u201d\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u91cd\u542fInformatica Server\u3002\u5173\u95edInformatica Server\u540e\uff0c\u9700\u8981\u6267\u884c ps -ef | grep informatica \u68c0\u67e5\u6240\u6709\u7684informatica\u8fdb\u7a0b\u90fd\u5173\u95ed\u540e\u518d\u542f\u52a8Informatica Server\u3002 su - infa cd /opt/informatica/10.2.2/tomcat/bin ./infaservice.sh shutdown ps -ef | grep informatica ./infaservice.sh startup \u8bf4\u660e:\u5982\u679c\u65b0\u5efaORACLE\u8fde\u63a5\u540e\u672a\u91cd\u542f\u8fc7Informatica Server\uff0c\u5728\u5ba2\u6237\u7aefBig Data Developer\u8fd0\u884cORACLE\u5173\u7cfb\u578b\u6570\u636e\u5bf9\u8c61\u65f6\uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a [LDTMCMN_0029] \u7531\u4e8e\u4ee5\u4e0b\u9519\u8bef\uff0cLDTM \u65e0\u6cd5\u5b8c\u6210\u8bf7\u6c42: com.informatica.sdk.dtm.ExecutionException: [EdtmExec_00007] CMN_1022 Database driver error... CMN_1022 [Database driver event...Error occurred loading library [libclntsh.so.11.1: cannot open shared object file: No such file or directory]Database driver event...Error occurred loading library [libpmora8.so]] \u589e\u52a0\u7528\u6237Administrator\u6743\u9650 \u00b6 \u5bfc\u822a\u81f3 \u5b89\u5168->\u7528\u6237 \uff0c\u9009\u4e2d \u7528\u6237->Native->Administrator \uff0c\u70b9\u51fb \u6982\u89c8->\u7f16\u8f91->\u7ec4 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u5c06 Operator \u6dfb\u52a0\u81f3\u201c\u5206\u914d\u7684\u7ec4\u201d \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u7528\u6237\u4e0d\u5c5e\u4e8eOperator\u7ec4\uff0c\u4f7f\u7528\u8be5\u7528\u6237\u6267\u884cmapping\u4ece\u8fde\u63a5\u4e2d\u83b7\u53d6\u6570\u636e\u65f6\uff0c\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u4e8e\u201c\u6ca1\u6709\u9488\u5bf9\u8fde\u63a5 [ORACLE] (\u5728\u57df [Domain_172-16-6-120] \u4e2d)\u7684\u6267\u884c\u6743\u9650\u201d\u7684\u9519\u8bef\u3002 \u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728Windows\u4e0a\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5df2\u83b7\u53d6Informatica\u5ba2\u6237\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_client_winem-64t.zip\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5\u5ba2\u6237\u7aef \u00b6 \u89e3\u538binformatica_1022_client_winem-64t.zip\u540e\uff0c\u53cc\u51fb install.bat \u3002\u9009\u62e9 \u5b89\u88c5Informatica Developer\u7248\u672c10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff0c\u9ed8\u8ba4\u5b89\u88c5\u8def\u5f84\u4e3a C:\\Informatica\\10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u5b89\u88c5 \u540e\u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 Informatica BDM\u5bf9\u63a5FusionInsight HD \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Informatica BDM\u5bf9\u63a5FusionInsight HD\u7684HDFS\u548cHive\u3002\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u4e0eHDFS\u548cHive\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS\u4e0eHive\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS/Hive\u4e0e\u672c\u5730\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20\u4e0b\u8f7d\u6570\u636e\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b89\u88c5Informatica\u670d\u52a1\u7aef\u548cBig Data Developer\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06FusionInsight\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230Informatica Server\u5b89\u88c5\u8282\u70b9\u7684 /etc/hosts \u6587\u4ef6\u4e2d\u3002 \u5b89\u88c5Infomatica\u670d\u52a1\u7aef\u8282\u70b9\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u51c6\u5907\u6570\u636e\u3002 \u672c\u5730 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5728 /tmp \u76ee\u5f55\u4e0b\u521b\u5efa\u6587\u4ef6 user_local_to_hdfs.csv \uff0c\u64cd\u4f5c\u547d\u4ee4\u5982\u4e0b\u3002\u5e76\u4e14\u5c06 user_local_to_hdfs.csv \u62f7\u8d1d\u81f3\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u7684window\u7cfb\u7edf\uff0c\u4f8b\u5982 C:\\ \u76ee\u5f55\u4e0b\u3002 su - infa cd /tmp vi user_local_to_hdfs.csv user_local_to_hdfs.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 10,Andy-in-local 11,Benny-in-local 12,Tom-in-local HDFS\u6587\u4ef6\u7cfb\u7edf \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp \u76ee\u5f55\u521b\u5efa\u4e24\u4e2a\u6587\u4ef6\u5206\u522b\u547d\u540d\u4e3a user_hdfs_to_oracle.csv \u548c user_hdfs_to_hive.csv \u3002 cd /opt vi user_hdfs_to_oracle.csv vi user_hdfs_to_hive.csv hdfs dfs -put user_hdfs_to_* /tmp user_hdfs_to_oracle.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 20,Andy-in-hdfs 21,Benny-in-hdfs 22,Tom-in-hdfs user_hdfs_to_hive.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 30,Andy-in-hdfs 31,Benny-in-hdfs 32,Tom-in-hdfs Hive\u6570\u636e\u5e93 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_hive_in \u548c user_hive_out \u3002 \u521b\u5efauser_hive_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_in(id INT, name STRING); \u521b\u5efauser_hive_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_out(id INT, name STRING); INSERT INTO user_hive_out VALUES (40,'Andy-in-hive'); INSERT INTO user_hive_out VALUES (41,'Benny-in-hive'); INSERT INTO user_hive_out VALUES (42,'Tom-in-hive'); Oracle\u6570\u636e\u5e93 \u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_oracle_in \u548c user_oracle_out \u3002 su - oracle sqlplus mdl_user/mdl_user \u521b\u5efauser_oracle_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_in(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); \u521b\u5efauser_oracle_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_out(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); INSERT INTO user_oracle_out VALUES (50,'Andy-in-oracle'); INSERT INTO user_oracle_out VALUES (51,'Benny-in-oracle'); INSERT INTO user_oracle_out VALUES (52,'Tom-in-oracle'); HBase \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a USER_HBASE_IN \u548c USER_HBASE_OUT \u3002 \u521b\u5efa\u547d\u540d\u7a7a\u95f4INFA\uff1a hbase shell create_namespace 'INFA' \u521b\u5efaUSER_HBASE_IN\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_IN',{NAME=>'cf1'},{NAME=>'cf2'} \u521b\u5efaUSER_HBASE_OUT\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_OUT',{NAME=>'cf1'},{NAME=>'cf2'} put 'INFA.USER_HBASE_OUT', '001','cf1:id','60' put 'INFA.USER_HBASE_OUT', '001','cf1:name','Andy-in-HBase' put 'INFA.USER_HBASE_OUT', '002','cf1:id','61' put 'INFA.USER_HBASE_OUT', '002','cf1:name','Benny-in-HBase' put 'INFA.USER_HBASE_OUT', '003','cf1:id','62' put 'INFA.USER_HBASE_OUT', '003','cf1:name','Tom-in-HBase' \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5efa\u7acb\u9879\u76ee \u00b6 \u6253\u5f00 Big Data Developer \uff0c\u70b9\u51fb \u6587\u4ef6->\u8fde\u63a5\u5230\u5b58\u50a8\u5e93 \u3002 \u70b9\u51fb \u914d\u7f6e\u57df \uff0c \u70b9\u51fb \u6dfb\u52a0 \uff0c\u201c\u57df\u540d\u201d\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0\u4e3a Domain_172-16-6-120 \uff0c\u201c\u4e3b\u673a\u540d\u201d\u8f93\u5165Informatica Server\u5b89\u88c5\u8282\u70b9\u5bf9\u5e94\u7684\u4e3b\u673a\u540d 172-16-6-120 \uff0c\u201c\u7aef\u53e3\u53f7\u201d\u8f93\u5165\u5b89\u88c5Informatica Server\u65f6\u6307\u5b9a\u7684\u7aef\u53e3 6005 \uff0c\u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u201c\u8fde\u63a5\u6210\u529f\u201d\u5219\u8868\u793a\u4e3b\u673a\u540d\u4e3a172-16-6-120\u7684\u57df\u53ef\u7528\u3002\u70b9\u51fb \u786e\u5b9a \u5e76 \u5b8c\u6210 \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 Domain_172-16-6-120.Model_Repository_Service \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u8f93\u5165\u201c\u7528\u6237\u540d\u201d\u4e3a Administrator \uff0c\u201c\u5bc6\u7801\u201d\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728\u201c\u5bf9\u8c61\u6d4f\u89c8\u5668\u201d\u4e0b\u663e\u793a\u8fde\u63a5\u6210\u529f\u7684\u5b58\u50a8\u5e93 Model_Repository_Service \uff08Administrator\uff09 \u3002 \u53f3\u952e Model_Repository_Service \uff08Administrator\uff09 \u9009\u62e9 \u65b0\u5efa->\u9879\u76ee \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0 fi_project \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 \u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \u00b6 \u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Oracle \u00b6 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 ORACLE \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 USER_ORACLE_IN \u548c USER_ORACLE_OUT \u3002\u70b9\u51fb \u5b8c\u6210 \u3002 \u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Hive \u00b6 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HIVE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 user_hive_in \u548c user_hive_out \u3002\u70b9\u51fb \u5b8c\u6210 \u3002 \u521b\u5efaHBase\u6570\u636e\u5bf9\u8c61 \u00b6 \u767b\u5f55FusionInsight Manager\u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 HBase\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN \uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HBASE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u9009\u5b9a\u8d44\u6e90\u201d\u7684 \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9\u8868 INFA.USER_HBASE_IN \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 cf1 \uff0c\u9009\u62e9 \u6dfb\u52a0\u5217 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u6dfb\u52a0\u4e24\u5217\u540d\u79f0\u5206\u522b\u4e3a id \u548c name \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 \u5305\u542b\u884cID \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 \u540c\u6837\u5730\u65b0\u589eHBase\u8868 INFA.USER_HBASE_OUT \u3002 Informatica BDM\u5bf9\u63a5FusionInsight HDFS \u00b6 HDFS from Local \u00b6 \u5c06\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_to_hdfs.csv \u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_local.csv\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u9009\u62e9 \u4ece\u73b0\u6709\u5e73\u9762\u6587\u4ef6\u521b\u5efa \u5e76\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 C:\\user_local_to_hdfs.csv \uff0c\u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u683c\u5f0f\u201d\u9009\u62e9 \u5e26\u5206\u9694\u7b26 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5206\u9694\u7b26\u201d\u9009\u62e9 \u9017\u53f7 \uff0c\u52fe\u9009 \u5bfc\u5165\u7b2c\u4e00\u884c\u4e2d\u7684\u5217\u540d\u79f0 \uff0c\u5176\u4f59\u4fdd\u6301\u9ed8\u8ba4\u9009\u9879\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u9009\u62e9\u6570\u636e\u5bf9\u8c61 hdfs_from_local \uff0c\u8bbe\u7f6e \u9ad8\u7ea7 \u5c5e\u6027\u540e\u4fdd\u5b58\u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\u3002\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_local_to_hdfs.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\u3002\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_local.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u7c7b\u4f3c\u5730\u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002\u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_local.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_local.csv\u201d\u3002 HDFS from Oracle \u00b6 \u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_oracle.csv\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u3002 hdfs_from_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_oracle.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_hdfs_from_oracle\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_oracle.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_oracle.csv\u201d\u3002 HDFS to Oracle \u00b6 \u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_oracle.csv\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u3002 hdfs_to_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_oracle.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_oracle\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN; HDFS to Hive \u00b6 \u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_hive.csv\u6570\u636e\u4e0a\u4f20\u81f3Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u3002 hdfs_to_hive**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_hive.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_hive_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_hive_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_hive\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in; Informatica BDM\u5bf9\u63a5FusionInsight Hive \u00b6 Hive to Local \u00b6 \u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u3002 hive_to_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hive.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 cat /tmp/user_local_from_hive.csv Hive to Oracle \u00b6 \u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN; Hive from Oracle \u00b6 \u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in; Hive to HDFS \u00b6 \u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/user_hdfs_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u3002 hive_to_hdfs**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_hive.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_hdfs_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_hdfs_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_hdfs\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_hive.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_hive.csv\u201d\u3002 cat /tmp/user_hdfs_from_hive.csv Informatica BDM\u5bf9\u63a5FusionInsight HBase \u00b6 HBase to Local \u00b6 \u5c06FusionInsight HD\u7684HBase\u8868USER_HBASE_OUT\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u3002 hbase_from_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hbase.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_OUT_Read \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u8bfb\u53d6 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_OUT \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_OUT_Read \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201cUSER_HBASE_OUT_Read\u201d\u548c\u201c\u5199\u5165_hbase_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 cat /tmp/user_local_from_hbase.csv HBase from Oracle \u00b6 \u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HBase\u8868USER_HBASE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN_Write \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u5199\u5165 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_IN \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_IN_Write \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201cUSER_HBASE_IN_Write\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 INFA.USER_HBASE_IN \u6570\u636e\u3002 hbase shell scan 'INFA.USER_HBASE_IN' FAQ \u00b6 \u6267\u884c./infaservice.sh startup\u542f\u52a8Infomatica Server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5728\u6267\u884c./infaservice.sh startup\u542f\u52a8infomatica server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u6216\u8005\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/logs/catalina.out\u8fd4\u56de\u9519\u8befjava.io.FileNotFoundException: null/isp/config/nodemeta.xml (No such file or directory)\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a$INFA_HOME/isp/config/nodemeta.xml\u7684\u6240\u6709\u8005\u548c\u6240\u5c5e\u7ec4\u4e0d\u6b63\u786e\u3002infa\u7528\u6237\u65e0\u6cd5\u83b7\u53d6\u5230nodemeta.xml\u3002 root\u7528\u6237\u767b\u5f55\u5e76\u5207\u6362\u81f3 $INFA_HOME/isp/config/ \uff0c\u6267\u884c chown -R infa:oinstall nodemeta.xml \u4fee\u6539nodemeta.xml\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002\u5efa\u8bae\u6267\u884c chown -R infa:oinstall /opt/informatica/10.2.2/ \u4fee\u6539Informatica Server\u6240\u6709\u6587\u4ef6\u7684\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002 \u5411Hive\u6570\u636e\u5e93\u5199\u5165\u6570\u636e\u5931\u8d25 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fd0\u884cmapping\u5411Hive\u6570\u636e\u5e93\u7684\u67d0\u4e00\u5f20\u8868\u5199\u5165\u6570\u636e\uff0c\u67e5\u8be2\u76ee\u6807\u8868\u65f6\uff0c\u6ca1\u6709\u6570\u636e\u5199\u5165\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5982\u679c\u5411Hive\u8868\u5199\u5165\u6570\u636e\uff0c\u786e\u8ba4\u4ee5\u4e0b\u4e24\u70b9\u662f\u5426\u5df2\u914d\u7f6e\uff1a \u914d\u7f6eHIVE\u8fde\u63a5\u4ee5\u4e0b\u4e24\u4e2a\u5c5e\u6027\u7684\u503c\uff1a\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u8bbe\u7f6e\u4e3a /user/hive/warehouse \uff0c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u8bbe\u7f6e\u4e3a default \u3002 \u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002 Big Data Developer\u6dfb\u52a0Hbase\u6570\u636e\u5bf9\u8c61\u65f6\u8fd4\u56deKeeperErrorCode = ConnectionLoss for /hbase \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u65b0\u5efaHBase\u6570\u636e\u5bf9\u8c61\uff0c\u70b9\u51fb \u6dfb\u52a0 \u83b7\u53d6HBase\u8868\u65f6\uff0c\u8fd4\u56de\u9519\u8befSDK_APP_COM_20000\u3002 Java.lang.RuntionException:org.apache.hadoop.hbase.ZookeeperConnectionException: Can\u2019t connet to Zookeeper KeeperErrorCode = ConnectionLoss for /hbase FusionInsight HD\u7684zookeeper\u65e5\u5fd7\uff0c\u4f8b\u5982\uff1a /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-euleros-hd01.log \uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a ERROR | NIOWorkerThread-41 | Authentication failed as scheme is not valid: ['ip,'172.16.6.120], expected scheme zookeeper.enforce.auth.scheme=sasl \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u5982\u4f55\u67e5\u770bmapping\u7684\u65e5\u5fd7 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5982\u679cmapping\u8fd0\u884c\u5b8c\u6210\u4e4b\u540e\uff0c\u6ca1\u6709\u5199\u5165\u6570\u636e\u6216\u8005mapping\u8fd0\u884c\u5931\u8d25\u7b49\uff0c\u5982\u4f55\u83b7\u53d6mapping\u8fd0\u884c\u7684\u8be6\u7ec6\u65e5\u5fd7\uff1f \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 Mapping\u8fd0\u884c\u7684\u65e5\u5fd7\u5b58\u653e\u4e8eInformatica Server\u5b89\u88c5\u8282\u70b9\u7684 $INFA_HOME/logs/node01_172-16-6-120/services/DataIntegrationService/disLogs/ms \u76ee\u5f55\u4e0b\u3002","title":"10.2.2 <--> 6.5"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_1","text":"Informatica 10.0.0 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica 10.2.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive) \u6ce8\uff1a\u4ee5\u4e0a\u5bf9\u63a5\u6d4b\u8bd5Informatica BDM\u91c7\u7528\u7684\u662fNative Engine\u3002Informatica 10.2.2\u5bf9\u63a5FusionInsight HD 6.5 HBase\u7ec4\u4ef6\u65f6\uff0c\u9700\u8981Zookeeper\u7ec4\u4ef6\u914d\u7f6eenforce.auth.enabled=false\uff0c\u5426\u5219\u5bf9\u63a5\u5931\u8d25\u3002","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_2","text":"Informatica\u7528\u4e8e\u7ba1\u7406\u5927\u6570\u636e\u5de5\u7a0b\u7684\u5de5\u5177\u4e3b\u8981\u6709Informatica Administrator\u3001Infoormatica Analyst\u548cInformatica Developer\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0Linux\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica 10.2.2\u670d\u52a1\u7aef\uff08Informatica Administrator\uff09\u5e76\u4f7f\u7528Oracle\u6570\u636e\u5e93\u7ba1\u7406\u57df\u6570\u636e\u3001\u8fde\u63a5\u6570\u636e\u7b49\uff0c\u5728Window\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica\u5ba2\u6237\u7aefBig Data Developuser\uff08Informatica Developer\u5176\u4e2d\u4e00\u79cd\u5de5\u5177\uff09\u3002Informatica\u670d\u52a1\u7aef\u4e0eFusionInsight HD\u7684HDFS\u548cHive\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u3001HDFS\u3001Hive\u3001HBase\u4e4b\u95f4\u6570\u636e\u4e92\u4f20\u3002 \u672c\u6587\u6863\u7684\u63cf\u8ff0\u4f7f\u7528\u7684Informatic Server\u5b89\u88c5\u8282\u70b9\u7684IP\u4e3a172.16.6.120\uff0c\u4e3b\u673a\u540d\u4e3a172-16-6-120\u3002\u5bf9\u63a5\u7684FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u5206\u522b\u662f172.16.4.21/172.16.4.22/172.16.4.23.","title":"\u7b80\u4ecb"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\ecotesting\\hadoopConfig \u76ee\u5f55\u4e0b\uff0c\u5e76\u538b\u7f29\u4e3a hadoopConfig.zip \u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#infomatica","text":"","title":"\u5b89\u88c5Infomatica\u670d\u52a1\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_4","text":"\u5728Linux\u4e0a\u5b89\u88c5Infomatica Server\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5b89\u88c5\u8282\u70b9\u4e0a\u5df2\u5b89\u88c5\u597dOracle\u6570\u636e\u5e93\u3002\u672c\u6307\u5bfc\u6587\u6863\u5b89\u88c5\u7248\u672c\u4e3a Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production \uff0c\u975e\u5bb9\u5668\u6570\u636e\u5e93\uff0c\u5b89\u88c5\u7528\u6237\u540d\u4e3a oracle \u5e76\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \uff0c\u6570\u636e\u5e93SID\u4e3a orcl \u3002 \u5df2\u83b7\u53d6Informatica\u670d\u52a1\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_server_linux-x64.tar\uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u5df2\u83b7\u53d6Informatica\u7684License\uff0c\u4f8b\u5982\uff1a infa1022.key \uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_7","text":"\u5b89\u88c5Informatica\u670d\u52a1\u7aef\u9700\u8981\u8fde\u63a5Domain\u548cModel_Repository_Service\u7684\u6570\u636e\u5e93\u7528\u6237\u3002\u767b\u5f55oracle\u6570\u636e\u5e93\uff0c\u521b\u5efa\u4e24\u4e2a\u7528\u6237\uff0c\u5206\u522b\u547d\u540d\u4e3a domain_user \u548c mdl_user \u3002 su - oracle sqlplus / as sysdba SQL> create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512m; SQL> create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp; SQL> create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp; SQL> grant dba to domain_user,mdl_user; SQL> exit \u521b\u5efa\u5b89\u88c5\u7528\u6237 infa \u5e76\u5f52\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \u3002 su - root useradd -g oinstall -d /home/infa infa echo \"Huawei@123\" | passwd --stdin infa \u4f7f\u7528 root \u7528\u6237\u89e3\u538binformatica_1022_server_linux-x64.tar\u81f3 /opt/informatica \uff0c\u8bbe\u7f6e\u62e5\u6709\u8005\u4e3ainfa\u7528\u6237\u5e76\u8d4b\u4e88755\u7684\u64cd\u4f5c\u6743\u9650\u3002 su - root mkdir -p /opt/informatica tar -xvf /opt/informatica_1022_server_linux-x64.tar -C /opt/informatica chown -R infa:oinstall /opt/informatica chmod -R 755 /opt/informatica \u4fee\u6539 infa \u7528\u6237\u7684\u73af\u5883\u53d8\u91cf\u3002 su - infa vi ~/.bash_profile source ~/.bash_profile \u6dfb\u52a0\u73af\u5883\u53d8\u91cf\u5982\u4e0b\u6240\u793a\uff1a PATH=$PATH:/u01/app/oracle/product/12.2.1/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.2.1/db_1 export ORACLE_SID=orcl export NLS_LANG=AMERICAN_AMERICA.AL32UTF8 export INFA_CODEPAGENAME=\"UTF-8\" export PATH=/opt/informatica/10.2.2/server/bin:$PATH export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/informatica/10.2.2/server/bin \u8bf4\u660e\uff1a12.2.1\u4e3aoracle\u7248\u672c\u53f7\uff0c10.2.2\u4e3aInformatica\u7684\u7248\u672c\u53f7\u3002 \u4f7f\u7528infa\u7528\u6237\u767b\u5f55\u542f\u52a8\u56fe\u5f62\u5316\u7ec8\u7aef\u5f00\u59cb\u5b89\u88c5\u3002 su - root export display=:0.0 xhost + su - infa export display=:0.0 cd /opt/informatica ./install.sh \u8bf4\u660e\uff1a\u6267\u884c xhost + \u547d\u4ee4\u65f6\uff0c\u786e\u8ba4\u8fd4\u56de\u201caccess control disabled, clients can connect from any host\u201d\uff0c\u624d\u80fd\u7ee7\u7eed\u6267\u884c\u540e\u9762\u7684\u547d\u4ee4\u3002 \u8f93\u5165 y \u9009\u62e9\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall and configure Informatica Big Data suite products.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 3 \u9009\u62e9\u201cRun the installer.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u540c\u610f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall Informatica domain services.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\u4e0d\u6fc0\u6d3bKerberos\u8ba4\u8bc1\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165License\u7684\u8def\u5f84\uff0c\u4f8b\u5982\uff1a /opt/infa1022.key \uff0c\u6309 Enter \u952e\uff0c\u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff08\u786e\u4fdd\u548c\u73af\u5883\u53d8\u91cf~/.bash_profile\u914d\u7f6e\u7684\u8def\u5f84\u4e00\u81f4\uff09\uff0c\u4f8b\u5982\uff1a /opt/informatica/10.2.2 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6309 Enter \u952e\u5f00\u59cb\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u8fdb\u5ea6100%\u540e\uff0c\u6b65\u9aa45\u7684\u9009\u62e9\u521b\u5efaDomain\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa46\u8f93\u5165Domain\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa47\u8f93\u5165\u81ea\u5b9a\u4e49\u5bc6\u7801 Huawei@123 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48\u8f93\u5165Domain\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48B\u8f93\u5165Model Repository Service\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa49\u8f93\u5165Data Integration Service\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u9009\u62e9\u4e0d\u521b\u5efaCCO\u8fde\u63a5\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6309 Enter \u952e\u7ed3\u675f\u5b89\u88c5\u3002 \u786e\u8ba4\u9632\u706b\u5899\u662f\u5173\u95ed\u72b6\u6001\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u6253\u5f00 http://172-16-6-120:6008 \u767b\u5f55Informatica Administrator\uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u767b\u5f55 \u3002 \u767b\u5f55\u6210\u529f\u3002","title":"\u5b89\u88c5\u670d\u52a1\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-server","text":"","title":"\u914d\u7f6eInformatica Server"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#kerberos","text":"\u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 krb5.conf \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5c06 krb5.conf \u590d\u5236\u5230 $INFA_HOME/java/jre/lib/security \u548c $INFA_HOME/services/shared/security \u3002\u547d\u4ee4\u6267\u884c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a chown infa:oinstall /opt/krb5.conf cp /opt/krb5.conf /opt/informatica/10.2.2/services/shared/security/ cp /opt/krb5.conf /opt/informatica/10.2.2/java/jre/lib/security/","title":"\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#data_integration_service","text":"\u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9->Data_Integration_Service \u3002\u70b9\u51fb\u201c\u6267\u884c\u9009\u9879\u201d\u7684\u7f16\u8f91\u6309\u94ae\uff0c\u8bbe\u7f6e\u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u4e3a developuser\\@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u4e3a /opt/user.keytab \uff0c\u70b9\u89e3 \u786e\u5b9a \uff0c\u9009\u62e9 \u662f\uff0c\u4fdd\u5b58\u66f4\u6539 \u3002 \u70b9\u51fb\u201cData_Integration_Service\u201d\u7684\u5e94\u7528\u670d\u52a1\u6309\u94ae \uff0c\u70b9\u51fb \u786e\u5b9a \u8ba9\u4fee\u6539\u751f\u6548\u3002","title":"\u914d\u7f6eData_Integration_Service"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_8","text":"\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 user.keytab \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u767b\u5f55 http://172-16-6-120:6008 \uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a Metadata_Service \uff0c\u201c\u4f4d\u7f6e\u201d\u9ed8\u8ba4\u4e3a Domain_172-16-6-120 \uff0c\u201c\u8bb8\u53ef\u8bc1\u201d\u9009\u62e9\u6709\u6548\u7684License\uff0c\u201c\u8282\u70b9\u201d\u9009\u62e9 node01_172-16-6-120 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u8f93\u5165 developuser\\@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u8f93\u5165 /opt/user.keytab \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u70b9\u51fb\u201cMetadata_Service\u201d\u53f3\u4e0a\u89d2\u7684 \u6309\u94ae\u542f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002","title":"\u521b\u5efa\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica","text":"\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120->ClusterConfigurations\u201d\uff0c\u9009\u62e9 \u65b0->\u7fa4\u96c6\u914d\u7f6e \u3002 \u201c\u7fa4\u96c6\u914d\u7f6e\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a FusionInsightHD \uff0c\u201c\u5206\u53d1\u7c7b\u578b\u201d\u9009\u62e9 Cloudera \uff0c\u201c\u5bfc\u5165\u7fa4\u96c6\u914d\u7f6e\u7684\u65b9\u6cd5\u201d\u9009\u62e9 \u4ece\u5b58\u6863\u6587\u4ef6\u4e2d\u5bfc\u5165 \uff0c\u4e0a\u8f7d\u914d\u7f6e\u5b58\u6863\u6587\u4ef6\u9009\u62e9 C:\\ecotesting\\hadoopConfig.zip \uff0c\u52fe\u9009 \u521b\u5efa\u8fde\u63a5 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u786e\u8ba4\u5b89\u88c5\u4fe1\u606f\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5b89\u88c5\u5b8c\u6210\u3002 \u4fee\u6539\u8fde\u63a5\u201cHIVE_fusionginsighthd\u201d\u4ee5\u4e0b\u5171\u540c\u5c5e\u6027\u3002 \u5143\u6570\u636e\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM \u6570\u636e\u8bbf\u95ee\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM HDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\uff1a/user/hive/warehouse Hive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\uff1adefault \u8bf4\u660e\uff1a\u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u914d\u7f6e\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u548c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u3002 \u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002\u5426\u5219Mapping\u65e5\u5fd7\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u201cClass org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider not found\u201d\u7684\u9519\u8bef\u4e14\u5199\u5165\u5931\u8d25\u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002","title":"\u521b\u5efaInformatica\u7fa4\u96c6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#oracle","text":"\u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u8fde\u63a5 \u3002 \u9009\u62e9 Oracle \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u201c\u540d\u79f0\u201d\u548c\u201cID\u201d\u81ea\u5b9a\u4e49\u4e3a ORACLE \uff0c\u201c\u7528\u6237\u540d\u201d\u548c\u201c\u5bc6\u7801\u201d\u90fd\u8f93\u5165 mdl_user \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5143\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u8f93\u5165 jdbc:informatica:oracle://172-16-6-120:1521;SID=orcl \uff0c\u201c\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u4e3a orcl \u548c \u201c\u4ee3\u7801\u9875\u201d\u9009\u62e9 UTF-8 encoding of Unicode \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \u8fd4\u56de \u201c\u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u201d\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u91cd\u542fInformatica Server\u3002\u5173\u95edInformatica Server\u540e\uff0c\u9700\u8981\u6267\u884c ps -ef | grep informatica \u68c0\u67e5\u6240\u6709\u7684informatica\u8fdb\u7a0b\u90fd\u5173\u95ed\u540e\u518d\u542f\u52a8Informatica Server\u3002 su - infa cd /opt/informatica/10.2.2/tomcat/bin ./infaservice.sh shutdown ps -ef | grep informatica ./infaservice.sh startup \u8bf4\u660e:\u5982\u679c\u65b0\u5efaORACLE\u8fde\u63a5\u540e\u672a\u91cd\u542f\u8fc7Informatica Server\uff0c\u5728\u5ba2\u6237\u7aefBig Data Developer\u8fd0\u884cORACLE\u5173\u7cfb\u578b\u6570\u636e\u5bf9\u8c61\u65f6\uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a [LDTMCMN_0029] \u7531\u4e8e\u4ee5\u4e0b\u9519\u8bef\uff0cLDTM \u65e0\u6cd5\u5b8c\u6210\u8bf7\u6c42: com.informatica.sdk.dtm.ExecutionException: [EdtmExec_00007] CMN_1022 Database driver error... CMN_1022 [Database driver event...Error occurred loading library [libclntsh.so.11.1: cannot open shared object file: No such file or directory]Database driver event...Error occurred loading library [libpmora8.so]]","title":"\u521b\u5efaOracle\u8fde\u63a5"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#administrator","text":"\u5bfc\u822a\u81f3 \u5b89\u5168->\u7528\u6237 \uff0c\u9009\u4e2d \u7528\u6237->Native->Administrator \uff0c\u70b9\u51fb \u6982\u89c8->\u7f16\u8f91->\u7ec4 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u5c06 Operator \u6dfb\u52a0\u81f3\u201c\u5206\u914d\u7684\u7ec4\u201d \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u7528\u6237\u4e0d\u5c5e\u4e8eOperator\u7ec4\uff0c\u4f7f\u7528\u8be5\u7528\u6237\u6267\u884cmapping\u4ece\u8fde\u63a5\u4e2d\u83b7\u53d6\u6570\u636e\u65f6\uff0c\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u4e8e\u201c\u6ca1\u6709\u9488\u5bf9\u8fde\u63a5 [ORACLE] (\u5728\u57df [Domain_172-16-6-120] \u4e2d)\u7684\u6267\u884c\u6743\u9650\u201d\u7684\u9519\u8bef\u3002","title":"\u589e\u52a0\u7528\u6237Administrator\u6743\u9650"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#big-data-developer","text":"","title":"\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_9","text":"\u5728Windows\u4e0a\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_10","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5df2\u83b7\u53d6Informatica\u5ba2\u6237\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_client_winem-64t.zip\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_11","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_12","text":"\u89e3\u538binformatica_1022_client_winem-64t.zip\u540e\uff0c\u53cc\u51fb install.bat \u3002\u9009\u62e9 \u5b89\u88c5Informatica Developer\u7248\u672c10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff0c\u9ed8\u8ba4\u5b89\u88c5\u8def\u5f84\u4e3a C:\\Informatica\\10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u5b89\u88c5 \u540e\u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\u3002 \u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u5b89\u88c5\u5ba2\u6237\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hd","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_13","text":"Informatica BDM\u5bf9\u63a5FusionInsight HD\u7684HDFS\u548cHive\u3002\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u4e0eHDFS\u548cHive\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS\u4e0eHive\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS/Hive\u4e0e\u672c\u5730\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20\u4e0b\u8f7d\u6570\u636e\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_14","text":"\u5df2\u5b89\u88c5Informatica\u670d\u52a1\u7aef\u548cBig Data Developer\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06FusionInsight\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230Informatica Server\u5b89\u88c5\u8282\u70b9\u7684 /etc/hosts \u6587\u4ef6\u4e2d\u3002 \u5b89\u88c5Infomatica\u670d\u52a1\u7aef\u8282\u70b9\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u51c6\u5907\u6570\u636e\u3002 \u672c\u5730 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5728 /tmp \u76ee\u5f55\u4e0b\u521b\u5efa\u6587\u4ef6 user_local_to_hdfs.csv \uff0c\u64cd\u4f5c\u547d\u4ee4\u5982\u4e0b\u3002\u5e76\u4e14\u5c06 user_local_to_hdfs.csv \u62f7\u8d1d\u81f3\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u7684window\u7cfb\u7edf\uff0c\u4f8b\u5982 C:\\ \u76ee\u5f55\u4e0b\u3002 su - infa cd /tmp vi user_local_to_hdfs.csv user_local_to_hdfs.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 10,Andy-in-local 11,Benny-in-local 12,Tom-in-local HDFS\u6587\u4ef6\u7cfb\u7edf \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp \u76ee\u5f55\u521b\u5efa\u4e24\u4e2a\u6587\u4ef6\u5206\u522b\u547d\u540d\u4e3a user_hdfs_to_oracle.csv \u548c user_hdfs_to_hive.csv \u3002 cd /opt vi user_hdfs_to_oracle.csv vi user_hdfs_to_hive.csv hdfs dfs -put user_hdfs_to_* /tmp user_hdfs_to_oracle.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 20,Andy-in-hdfs 21,Benny-in-hdfs 22,Tom-in-hdfs user_hdfs_to_hive.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 30,Andy-in-hdfs 31,Benny-in-hdfs 32,Tom-in-hdfs Hive\u6570\u636e\u5e93 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_hive_in \u548c user_hive_out \u3002 \u521b\u5efauser_hive_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_in(id INT, name STRING); \u521b\u5efauser_hive_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_out(id INT, name STRING); INSERT INTO user_hive_out VALUES (40,'Andy-in-hive'); INSERT INTO user_hive_out VALUES (41,'Benny-in-hive'); INSERT INTO user_hive_out VALUES (42,'Tom-in-hive'); Oracle\u6570\u636e\u5e93 \u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_oracle_in \u548c user_oracle_out \u3002 su - oracle sqlplus mdl_user/mdl_user \u521b\u5efauser_oracle_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_in(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); \u521b\u5efauser_oracle_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_out(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); INSERT INTO user_oracle_out VALUES (50,'Andy-in-oracle'); INSERT INTO user_oracle_out VALUES (51,'Benny-in-oracle'); INSERT INTO user_oracle_out VALUES (52,'Tom-in-oracle'); HBase \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a USER_HBASE_IN \u548c USER_HBASE_OUT \u3002 \u521b\u5efa\u547d\u540d\u7a7a\u95f4INFA\uff1a hbase shell create_namespace 'INFA' \u521b\u5efaUSER_HBASE_IN\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_IN',{NAME=>'cf1'},{NAME=>'cf2'} \u521b\u5efaUSER_HBASE_OUT\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_OUT',{NAME=>'cf1'},{NAME=>'cf2'} put 'INFA.USER_HBASE_OUT', '001','cf1:id','60' put 'INFA.USER_HBASE_OUT', '001','cf1:name','Andy-in-HBase' put 'INFA.USER_HBASE_OUT', '002','cf1:id','61' put 'INFA.USER_HBASE_OUT', '002','cf1:name','Benny-in-HBase' put 'INFA.USER_HBASE_OUT', '003','cf1:id','62' put 'INFA.USER_HBASE_OUT', '003','cf1:name','Tom-in-HBase'","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_15","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_16","text":"\u6253\u5f00 Big Data Developer \uff0c\u70b9\u51fb \u6587\u4ef6->\u8fde\u63a5\u5230\u5b58\u50a8\u5e93 \u3002 \u70b9\u51fb \u914d\u7f6e\u57df \uff0c \u70b9\u51fb \u6dfb\u52a0 \uff0c\u201c\u57df\u540d\u201d\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0\u4e3a Domain_172-16-6-120 \uff0c\u201c\u4e3b\u673a\u540d\u201d\u8f93\u5165Informatica Server\u5b89\u88c5\u8282\u70b9\u5bf9\u5e94\u7684\u4e3b\u673a\u540d 172-16-6-120 \uff0c\u201c\u7aef\u53e3\u53f7\u201d\u8f93\u5165\u5b89\u88c5Informatica Server\u65f6\u6307\u5b9a\u7684\u7aef\u53e3 6005 \uff0c\u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u201c\u8fde\u63a5\u6210\u529f\u201d\u5219\u8868\u793a\u4e3b\u673a\u540d\u4e3a172-16-6-120\u7684\u57df\u53ef\u7528\u3002\u70b9\u51fb \u786e\u5b9a \u5e76 \u5b8c\u6210 \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 Domain_172-16-6-120.Model_Repository_Service \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u8f93\u5165\u201c\u7528\u6237\u540d\u201d\u4e3a Administrator \uff0c\u201c\u5bc6\u7801\u201d\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728\u201c\u5bf9\u8c61\u6d4f\u89c8\u5668\u201d\u4e0b\u663e\u793a\u8fde\u63a5\u6210\u529f\u7684\u5b58\u50a8\u5e93 Model_Repository_Service \uff08Administrator\uff09 \u3002 \u53f3\u952e Model_Repository_Service \uff08Administrator\uff09 \u9009\u62e9 \u65b0\u5efa->\u9879\u76ee \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0 fi_project \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u5efa\u7acb\u9879\u76ee"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_17","text":"","title":"\u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#-oracle","text":"\u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 ORACLE \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 USER_ORACLE_IN \u548c USER_ORACLE_OUT \u3002\u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#-hive","text":"\u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HIVE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 user_hive_in \u548c user_hive_out \u3002\u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Hive"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hbase","text":"\u767b\u5f55FusionInsight Manager\u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 HBase\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN \uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HBASE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u9009\u5b9a\u8d44\u6e90\u201d\u7684 \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9\u8868 INFA.USER_HBASE_IN \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 cf1 \uff0c\u9009\u62e9 \u6dfb\u52a0\u5217 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u6dfb\u52a0\u4e24\u5217\u540d\u79f0\u5206\u522b\u4e3a id \u548c name \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 \u5305\u542b\u884cID \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 \u540c\u6837\u5730\u65b0\u589eHBase\u8868 INFA.USER_HBASE_OUT \u3002","title":"\u521b\u5efaHBase\u6570\u636e\u5bf9\u8c61"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hdfs","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight HDFS"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-from-local","text":"\u5c06\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_to_hdfs.csv \u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_local.csv\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u9009\u62e9 \u4ece\u73b0\u6709\u5e73\u9762\u6587\u4ef6\u521b\u5efa \u5e76\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 C:\\user_local_to_hdfs.csv \uff0c\u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u683c\u5f0f\u201d\u9009\u62e9 \u5e26\u5206\u9694\u7b26 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5206\u9694\u7b26\u201d\u9009\u62e9 \u9017\u53f7 \uff0c\u52fe\u9009 \u5bfc\u5165\u7b2c\u4e00\u884c\u4e2d\u7684\u5217\u540d\u79f0 \uff0c\u5176\u4f59\u4fdd\u6301\u9ed8\u8ba4\u9009\u9879\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u9009\u62e9\u6570\u636e\u5bf9\u8c61 hdfs_from_local \uff0c\u8bbe\u7f6e \u9ad8\u7ea7 \u5c5e\u6027\u540e\u4fdd\u5b58\u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\u3002\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_local_to_hdfs.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\u3002\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_local.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u7c7b\u4f3c\u5730\u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002\u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_local.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_local.csv\u201d\u3002","title":"HDFS from Local"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-from-oracle","text":"\u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_oracle.csv\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u3002 hdfs_from_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_oracle.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_hdfs_from_oracle\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_oracle.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_oracle.csv\u201d\u3002","title":"HDFS from Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-to-oracle","text":"\u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_oracle.csv\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u3002 hdfs_to_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_oracle.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_oracle\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN;","title":"HDFS to Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-to-hive","text":"\u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_hive.csv\u6570\u636e\u4e0a\u4f20\u81f3Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u3002 hdfs_to_hive**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_hive.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_hive_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_hive_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_hive\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in;","title":"HDFS to Hive"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hive","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight Hive"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-to-local","text":"\u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u3002 hive_to_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hive.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 cat /tmp/user_local_from_hive.csv","title":"Hive to Local"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-to-oracle","text":"\u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN;","title":"Hive to Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-from-oracle","text":"\u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in;","title":"Hive from Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-to-hdfs","text":"\u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/user_hdfs_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u3002 hive_to_hdfs**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_hive.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_hdfs_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_hdfs_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_hdfs\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_hive.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_hive.csv\u201d\u3002 cat /tmp/user_hdfs_from_hive.csv","title":"Hive to HDFS"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hbase","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight HBase"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hbase-to-local","text":"\u5c06FusionInsight HD\u7684HBase\u8868USER_HBASE_OUT\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u3002 hbase_from_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hbase.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_OUT_Read \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u8bfb\u53d6 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_OUT \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_OUT_Read \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201cUSER_HBASE_OUT_Read\u201d\u548c\u201c\u5199\u5165_hbase_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 cat /tmp/user_local_from_hbase.csv","title":"HBase to Local"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hbase-from-oracle","text":"\u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HBase\u8868USER_HBASE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN_Write \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u5199\u5165 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_IN \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_IN_Write \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201cUSER_HBASE_IN_Write\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 INFA.USER_HBASE_IN \u6570\u636e\u3002 hbase shell scan 'INFA.USER_HBASE_IN'","title":"HBase from Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#faq","text":"\u6267\u884c./infaservice.sh startup\u542f\u52a8Infomatica Server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5728\u6267\u884c./infaservice.sh startup\u542f\u52a8infomatica server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u6216\u8005\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/logs/catalina.out\u8fd4\u56de\u9519\u8befjava.io.FileNotFoundException: null/isp/config/nodemeta.xml (No such file or directory)\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a$INFA_HOME/isp/config/nodemeta.xml\u7684\u6240\u6709\u8005\u548c\u6240\u5c5e\u7ec4\u4e0d\u6b63\u786e\u3002infa\u7528\u6237\u65e0\u6cd5\u83b7\u53d6\u5230nodemeta.xml\u3002 root\u7528\u6237\u767b\u5f55\u5e76\u5207\u6362\u81f3 $INFA_HOME/isp/config/ \uff0c\u6267\u884c chown -R infa:oinstall nodemeta.xml \u4fee\u6539nodemeta.xml\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002\u5efa\u8bae\u6267\u884c chown -R infa:oinstall /opt/informatica/10.2.2/ \u4fee\u6539Informatica Server\u6240\u6709\u6587\u4ef6\u7684\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002 \u5411Hive\u6570\u636e\u5e93\u5199\u5165\u6570\u636e\u5931\u8d25 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fd0\u884cmapping\u5411Hive\u6570\u636e\u5e93\u7684\u67d0\u4e00\u5f20\u8868\u5199\u5165\u6570\u636e\uff0c\u67e5\u8be2\u76ee\u6807\u8868\u65f6\uff0c\u6ca1\u6709\u6570\u636e\u5199\u5165\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5982\u679c\u5411Hive\u8868\u5199\u5165\u6570\u636e\uff0c\u786e\u8ba4\u4ee5\u4e0b\u4e24\u70b9\u662f\u5426\u5df2\u914d\u7f6e\uff1a \u914d\u7f6eHIVE\u8fde\u63a5\u4ee5\u4e0b\u4e24\u4e2a\u5c5e\u6027\u7684\u503c\uff1a\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u8bbe\u7f6e\u4e3a /user/hive/warehouse \uff0c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u8bbe\u7f6e\u4e3a default \u3002 \u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002 Big Data Developer\u6dfb\u52a0Hbase\u6570\u636e\u5bf9\u8c61\u65f6\u8fd4\u56deKeeperErrorCode = ConnectionLoss for /hbase \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u65b0\u5efaHBase\u6570\u636e\u5bf9\u8c61\uff0c\u70b9\u51fb \u6dfb\u52a0 \u83b7\u53d6HBase\u8868\u65f6\uff0c\u8fd4\u56de\u9519\u8befSDK_APP_COM_20000\u3002 Java.lang.RuntionException:org.apache.hadoop.hbase.ZookeeperConnectionException: Can\u2019t connet to Zookeeper KeeperErrorCode = ConnectionLoss for /hbase FusionInsight HD\u7684zookeeper\u65e5\u5fd7\uff0c\u4f8b\u5982\uff1a /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-euleros-hd01.log \uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a ERROR | NIOWorkerThread-41 | Authentication failed as scheme is not valid: ['ip,'172.16.6.120], expected scheme zookeeper.enforce.auth.scheme=sasl \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u5982\u4f55\u67e5\u770bmapping\u7684\u65e5\u5fd7 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5982\u679cmapping\u8fd0\u884c\u5b8c\u6210\u4e4b\u540e\uff0c\u6ca1\u6709\u5199\u5165\u6570\u636e\u6216\u8005mapping\u8fd0\u884c\u5931\u8d25\u7b49\uff0c\u5982\u4f55\u83b7\u53d6mapping\u8fd0\u884c\u7684\u8be6\u7ec6\u65e5\u5fd7\uff1f \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 Mapping\u8fd0\u884c\u7684\u65e5\u5fd7\u5b58\u653e\u4e8eInformatica Server\u5b89\u88c5\u8282\u70b9\u7684 $INFA_HOME/logs/node01_172-16-6-120/services/DataIntegrationService/disLogs/ms \u76ee\u5f55\u4e0b\u3002","title":"FAQ"},{"location":"Data_Integration/Informatica_BDM_PushDown/","text":"Informatica BDM\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica 10.0.0 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Yarn)","title":"10.0.0 <--> C70"},{"location":"Data_Integration/Informatica_BDM_PushDown/#informatica-bdmfusioninsight","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Informatica_BDM_PushDown/#_1","text":"Informatica 10.0.0 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Yarn)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PWX_CDC/","text":"Informatica PowerExchange CDC\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) \u73af\u5883\u4fe1\u606f \u00b6 Informatica PowerExchange CDC 10.2.0 Linux & Windows\u7248\u672c Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka\u5ba2\u6237\u7aef \u90e8\u7f72\u65b9\u6848 \u00b6 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72oracle\u6570\u636e\u5e93\uff0c\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u540c\u65f6\u90e8\u7f72Informatica PWX CDC\uff0c\u5e76\u542f\u7528listener\u548clogger\u8fdb\u884c\u65e5\u5fd7\u76d1\u542c\uff0c\u518d\u5b89\u88c5PWX Publisher,\u5c06\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e\u4f20\u9001\u5230kafka\u7684topic\u4e2d\u3002 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u5b89\u88c5FusionInsight HD Kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39PWX Publisher\u4f20\u9001\u8fc7\u6765\u7684\u6570\u636e (\u53ef\u9009)\u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5PWX CDC\uff0c\u542f\u7528listener\uff0c\u542f\u52a8navigator\u56fe\u5f62\u5316\u754c\u9762\uff0c\u67e5\u770bPWX\u6355\u83b7\u5230\u7684\u6570\u636e. \u6570\u636e\u5e93\u914d\u7f6e \u00b6 >\u6b64\u90e8\u5206\u914d\u7f6e\u8bf7\u53c2\u8003Informatica PowerExchange CDC\u6307\u5bfc\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html \u5207\u6362\u81f3oracle\u7528\u6237,\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >\u5efa\u8bae\u5728\u4e24\u6b21SHUTDOWN\u64cd\u4f5c\u4e4b\u524d\u5907\u4efd\u6570\u636e\u5e93. \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; \u62f7\u8d1dOracle Catalog \u81f3\u5f52\u6863\u65e5\u5fd7\u4e2d EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u521b\u5efa\u666e\u901a\u7528\u6237C##PWX,\u8d4b\u4e88\u521b\u5efa\u8868\u7684\u6743\u9650\uff0c\u8fde\u63a5\u81f3\u6570\u636e\u5e93 \u521b\u5efa\u6d4b\u8bd5\u8868,\u5411\u8868\u4e2d\u63d2\u5165\u4e00\u4e9b\u6570\u636e. Informatica PWX CDC & PWX Publisher \u5b89\u88c5\u914d\u7f6e \u00b6 \u5728Linux\u4e0a\u5b89\u88c5Informatica PWX CDC \u00b6 \u83b7\u53d6\u5b89\u88c5\u5305 pwx1020_linux_em64t.tar . \u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8bbe\u7f6e\u5b89\u88c5\u8def\u5f84\u5373\u53ef,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/PowerExchange/10.2.0 . \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u6253\u5f00\u914d\u7f6e\u6587\u4ef6 vi ~/.bash_profile \u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * \u6267\u884c source ~/.bash_profile ,source\u73af\u5883\u53d8\u91cf * \u6267\u884c dtlinfo ,\u68c0\u67e5\u5b89\u88c5\u4ee5\u53ca\u914d\u7f6e\u662f\u5426\u6210\u529f \u914d\u7f6edbmover.cfg\u4e0epwxccl.cfg\u6587\u4ef6 \u00b6 \u4fee\u6539PWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6\u5982\u4e0b nodeln \u4e3a\u81ea\u5b9a\u4e49\u7684\u76d1\u542c\u8282\u70b9\u540d ORACLEID\u4e2d\u7684\u7b2c\u4e8c\u4e2aORCL\uff0c\u4e3a\u88ab\u76d1\u542c\u7684\u6570\u636e\u5e93\u540d\u79f0\uff0c\u6b64\u5904\u4e3a\u9ed8\u8ba4\u7684ORCL CAPT_PATH\u6307\u5b9a\u4e86CDC\u7684\u63a7\u5236\u6587\u4ef6\u8def\u5f84\uff0c\u9700\u63d0\u524d\u521b\u5efa\u597d\u76f8\u5e94\u76ee\u5f55 \u6307\u5b9aSVCNODE\u548cCMDNODE\u540d\u79f0 \u4fee\u6539pwxccl.cfg\u6587\u4ef6\u5982\u4e0b CONDENSENAME\u9700\u8981\u548cdbmover.cfg\u6587\u4ef6\u4e2dSVCNODE\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 DBID \u4e3a\u6570\u636e\u5e93\u540d\u79f0 CAPTURE_NODE \u4e3a\u8fdb\u884c\u6355\u83b7\u8282\u70b9\u540d\u79f0 CAPTURE_NODE_UID \u4e3a\u767b\u5f55\u6570\u636e\u5e93\u7684\u7528\u6237\u540d CAPTURE_NODE_PWD \u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 \u542f\u52a8listener\u4ee5\u53calogger PWX CDC \u6355\u83b7ORACLE\u65e5\u5fd7\u6570\u636e \u00b6 ### \u5728Windows\u4e0a\u5b89\u88c5Informatica PWX CDC Windows\u4e0a\u5b89\u88c5Informatica PWX CDC\u4e3b\u8981\u662f\u53ef\u4ee5\u4f7f\u7528Navigator\u754c\u9762,\u67e5\u770b\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e. \u83b7\u53d6\u5b89\u88c5\u5305\u4e4b\u540e\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5,\u4fee\u6539\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH,\u6dfb\u52a0PWX\u5b89\u88c5\u76ee\u5f55. * \u6dfb\u52a0\u73af\u5883\u53d8\u91cfPWX_CONFIG,\u8bbe\u7f6e\u4e3aPWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6 * \u914d\u7f6edbmover.cfg\u6587\u4ef6 - \u914d\u7f6elistener\u540d\u79f0,\u6dfb\u52a0\u670d\u52a1\u7aeflistener\u914d\u7f6e\u4fe1\u606f - \u6307\u5b9a\u76d1\u542c\u6570\u636e\u5e93\u540d\u79f0 - \u8bbe\u7f6e\u63a7\u5236\u6587\u4ef6\u8def\u5f84 * \u542f\u52a8listener * \u4ece\u5f00\u59cb\u83dc\u5355\u680f\u542f\u52a8Navigator * \u5728\u83dc\u5355\u680f\u8d44\u6e90->\u6570\u636e\u6355\u83b7->\u6ce8\u518c\u7ec4\uff0c\u53f3\u952e\u65b0\u5efa\u6ce8\u518c\u7ec4\uff0c\u586b\u5199\u4fe1\u606f\u5982\u4e0b - \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 - \u4f4d\u7f6e\uff1aLinux\u670d\u52a1\u7aef\u76d1\u542c\u8282\u70b9\u540d\u79f0 - \u7c7b\u578b\uff1aORACLE - \u7528\u6237ID\u548c\u5bc6\u7801\uff1a\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 - \u96c6\u5408\u6807\u5fd7\u7b26\uff1a\u6570\u636e\u5e93\u540dORCL \u70b9\u51fb\u4e0b\u4e00\u6b65 \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u67b6\u6784\uff1aschema\u540d\u79f0\uff0c\u5373\u7528\u6237\u540d \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4f1a\u770b\u5230\u521a\u624d\u521b\u5efa\u7684test\u8868\uff0c\u53cc\u51fb\u8868\u540d\uff0c\u88ab\u9009\u5165\u53f3\u4fa7\uff0c\u9009\u62e9\u6240\u6709\u5217 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4fee\u6539\u72b6\u6001\u4e3a \u6d3b\u52a8 \uff0c\u52fe\u9009 \u7acb\u5373\u6267\u884cDDL ,\u70b9\u51fb\u5b8c\u6210 \u5728\u63d0\u53d6\u7ec4\uff0c\u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684orcl12,\u8fdb\u5165\u63d0\u53d6\u7ec4\u754c\u9762\uff0c\u53f3\u952e\uff0c\u6dfb\u52a0\u63d0\u53d6\u81ea\u5b9a\u4e49\uff0c\u586b\u5199\u6620\u5c04\u540d\u79f0\u4ee5\u53ca\u8868\u540d\u79f0 * \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u521b\u5efa\u7684\u6ce8\u518c \u70b9\u51fb\u6dfb\u52a0\uff0c\u5b8c\u6210 \u70b9\u51fb\u83dc\u5355\u680f\u56fe\u8868\uff0c\u6267\u884c\u884c\u6d4b\u8bd5,\u53ef\u770b\u5230\u6355\u83b7\u5230\u7684\u6570\u636e\u5e93\u65e5\u5fd7\u8bb0\u5f55 \u4f7f\u7528PWX CDC publisher\u5bf9\u63a5Kafka \u00b6 ### \u4fee\u6539kafka\u914d\u7f6e\u6587\u4ef6 * \u4fee\u6539producer.properties\u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * \u4fee\u6539jaas.conf\u6587\u4ef6\u5982\u4e0b ![](assets/Informatica_PWX_CDC/14cae.png) \u521b\u5efa\u4e00\u4e2akafka topic, pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### \u5b89\u88c5\u914d\u7f6eInformatica PWX Publisher * \u83b7\u53d6\u5b89\u88c5\u5305 pwxcdcpub120_linux_x64.tar.gz ,\u4ee5root\u7528\u6237\u8eab\u4efd\u89e3\u538b\u81f3\u5b89\u88c5\u76ee\u5f55\u5373\u53ef \u4ee5root\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u5728~/.bash_profile\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0csource\u73af\u5883\u53d8\u91cf,\u8fdb\u884ckerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser \u5c06\u5b89\u88c5\u76ee\u5f55samples\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\u590d\u5236\u5230instanceA/config\u76ee\u5f55\u4e0b\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u5185\u5bb9 > \u914d\u7f6ePWX Publisher\u53ef\u53c2\u8003Informatica \u5b98\u65b9\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html cdcPublisherAvro.cfg\u6587\u4ef6\u914d\u7f6e\u5982\u4e0b - cdcPublisherCommon.cfg\u6587\u4ef6\u4e2d\u6307\u5b9a\u7aef\u53e3 - cdcPublisherKafka.cfg\u6587\u4ef6\u4e2d\u6307\u5b9akafka topic\u540d\u79f0\u4ee5\u53caproperties\u6587\u4ef6\u8def\u5f84 - cdcPowerExchange.cfg\u6587\u4ef6\u4e2d\u914d\u7f6e\u5982\u4e0b * Extract.pwxCapiConnectionName\u4e3a\u5728dbmover.cfg\u4e2dCAPI_CONNECTION\u914d\u7f6e\u7684name * Extract.pwxExtractionMapSchemaName \u4e3apwx \u6355\u83b7\u6620\u5c04\u4e2d\u7684schema\u540d\u79f0\uff0c\u901a\u5e38\u683c\u5f0f\u4e3a unninstance \u6216\u8005 dnninstance \uff0c\u8fd9\u91cc\u4e3a u8orcl * Extract.pwxNodeLocation \u914d\u7f6e\u4e3apwx\u8282\u70b9\u540d\u79f0 * Extract.pwxNodeUserId\uff0cExtract.pwxNodePwd\u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 * Extract.pwxXmapUserId\u4e3a\u8bbf\u95eepwx\u63d0\u53d6\u6620\u5c04\u7684\u7528\u6237\u540d\u5bc6\u7801 * \u4fee\u6539\u5b89\u88c5\u8def\u5f84bin\u76ee\u5f55\u4e0b\u7684PwxCDCPublisher.sh\u542f\u52a8\u811a\u672c\u6587\u4ef6,\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u52a0\u5165\u4e00\u884c RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" * \u542f\u52a8pwx CDC Publisher,\u5728bin\u76ee\u5f55\u4e0b\u6267\u884c sh PwxCDCPublisher.sh \u542f\u52a8kafka consumer\uff0c\u67e5\u770b\u6d88\u8d39\u5230\u7684\u6570\u636e \u00b6 \u5728FusionInsight HD Kafka \u5ba2\u6237\u7aef,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cinsert\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cupdate\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cdelete\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b Q&A \u00b6 1.\u82e5\u542f\u52a8pwxccl\u62a5\u9519\u5982\u4e0b A:\u68c0\u67e5\u5728oracle\u4e2d\u662f\u5426\u6267\u884c\u8fc7 exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u82e5\u6267\u884c\u6210\u529f\uff0c\u4ecd\u7136\u62a5\u9519\uff0c\u7ed9C##PWX\u7528\u6237\u8d4b\u4e88sysdba\u6743\u9650 grant sysdba to C##PWX","title":"10.2.0 <--> C80"},{"location":"Data_Integration/Informatica_PWX_CDC/#informatica-powerexchange-cdcfusioninsight","text":"","title":"Informatica PowerExchange CDC\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Informatica_PWX_CDC/#_1","text":"Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PWX_CDC/#_2","text":"Informatica PowerExchange CDC 10.2.0 Linux & Windows\u7248\u672c Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka\u5ba2\u6237\u7aef","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Informatica_PWX_CDC/#_3","text":"\u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72oracle\u6570\u636e\u5e93\uff0c\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u540c\u65f6\u90e8\u7f72Informatica PWX CDC\uff0c\u5e76\u542f\u7528listener\u548clogger\u8fdb\u884c\u65e5\u5fd7\u76d1\u542c\uff0c\u518d\u5b89\u88c5PWX Publisher,\u5c06\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e\u4f20\u9001\u5230kafka\u7684topic\u4e2d\u3002 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u5b89\u88c5FusionInsight HD Kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39PWX Publisher\u4f20\u9001\u8fc7\u6765\u7684\u6570\u636e (\u53ef\u9009)\u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5PWX CDC\uff0c\u542f\u7528listener\uff0c\u542f\u52a8navigator\u56fe\u5f62\u5316\u754c\u9762\uff0c\u67e5\u770bPWX\u6355\u83b7\u5230\u7684\u6570\u636e.","title":"\u90e8\u7f72\u65b9\u6848"},{"location":"Data_Integration/Informatica_PWX_CDC/#_4","text":">\u6b64\u90e8\u5206\u914d\u7f6e\u8bf7\u53c2\u8003Informatica PowerExchange CDC\u6307\u5bfc\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html \u5207\u6362\u81f3oracle\u7528\u6237,\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >\u5efa\u8bae\u5728\u4e24\u6b21SHUTDOWN\u64cd\u4f5c\u4e4b\u524d\u5907\u4efd\u6570\u636e\u5e93. \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; \u62f7\u8d1dOracle Catalog \u81f3\u5f52\u6863\u65e5\u5fd7\u4e2d EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u521b\u5efa\u666e\u901a\u7528\u6237C##PWX,\u8d4b\u4e88\u521b\u5efa\u8868\u7684\u6743\u9650\uff0c\u8fde\u63a5\u81f3\u6570\u636e\u5e93 \u521b\u5efa\u6d4b\u8bd5\u8868,\u5411\u8868\u4e2d\u63d2\u5165\u4e00\u4e9b\u6570\u636e.","title":"\u6570\u636e\u5e93\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PWX_CDC/#informatica-pwx-cdc-pwx-publisher","text":"","title":"Informatica PWX CDC &amp; PWX Publisher \u5b89\u88c5\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PWX_CDC/#linuxinformatica-pwx-cdc","text":"\u83b7\u53d6\u5b89\u88c5\u5305 pwx1020_linux_em64t.tar . \u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8bbe\u7f6e\u5b89\u88c5\u8def\u5f84\u5373\u53ef,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/PowerExchange/10.2.0 .","title":"\u5728Linux\u4e0a\u5b89\u88c5Informatica PWX CDC"},{"location":"Data_Integration/Informatica_PWX_CDC/#_5","text":"\u6253\u5f00\u914d\u7f6e\u6587\u4ef6 vi ~/.bash_profile \u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * \u6267\u884c source ~/.bash_profile ,source\u73af\u5883\u53d8\u91cf * \u6267\u884c dtlinfo ,\u68c0\u67e5\u5b89\u88c5\u4ee5\u53ca\u914d\u7f6e\u662f\u5426\u6210\u529f","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/Informatica_PWX_CDC/#dbmovercfgpwxcclcfg","text":"\u4fee\u6539PWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6\u5982\u4e0b nodeln \u4e3a\u81ea\u5b9a\u4e49\u7684\u76d1\u542c\u8282\u70b9\u540d ORACLEID\u4e2d\u7684\u7b2c\u4e8c\u4e2aORCL\uff0c\u4e3a\u88ab\u76d1\u542c\u7684\u6570\u636e\u5e93\u540d\u79f0\uff0c\u6b64\u5904\u4e3a\u9ed8\u8ba4\u7684ORCL CAPT_PATH\u6307\u5b9a\u4e86CDC\u7684\u63a7\u5236\u6587\u4ef6\u8def\u5f84\uff0c\u9700\u63d0\u524d\u521b\u5efa\u597d\u76f8\u5e94\u76ee\u5f55 \u6307\u5b9aSVCNODE\u548cCMDNODE\u540d\u79f0 \u4fee\u6539pwxccl.cfg\u6587\u4ef6\u5982\u4e0b CONDENSENAME\u9700\u8981\u548cdbmover.cfg\u6587\u4ef6\u4e2dSVCNODE\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 DBID \u4e3a\u6570\u636e\u5e93\u540d\u79f0 CAPTURE_NODE \u4e3a\u8fdb\u884c\u6355\u83b7\u8282\u70b9\u540d\u79f0 CAPTURE_NODE_UID \u4e3a\u767b\u5f55\u6570\u636e\u5e93\u7684\u7528\u6237\u540d CAPTURE_NODE_PWD \u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 \u542f\u52a8listener\u4ee5\u53calogger","title":"\u914d\u7f6edbmover.cfg\u4e0epwxccl.cfg\u6587\u4ef6"},{"location":"Data_Integration/Informatica_PWX_CDC/#pwx-cdc-oracle","text":"### \u5728Windows\u4e0a\u5b89\u88c5Informatica PWX CDC Windows\u4e0a\u5b89\u88c5Informatica PWX CDC\u4e3b\u8981\u662f\u53ef\u4ee5\u4f7f\u7528Navigator\u754c\u9762,\u67e5\u770b\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e. \u83b7\u53d6\u5b89\u88c5\u5305\u4e4b\u540e\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5,\u4fee\u6539\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH,\u6dfb\u52a0PWX\u5b89\u88c5\u76ee\u5f55. * \u6dfb\u52a0\u73af\u5883\u53d8\u91cfPWX_CONFIG,\u8bbe\u7f6e\u4e3aPWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6 * \u914d\u7f6edbmover.cfg\u6587\u4ef6 - \u914d\u7f6elistener\u540d\u79f0,\u6dfb\u52a0\u670d\u52a1\u7aeflistener\u914d\u7f6e\u4fe1\u606f - \u6307\u5b9a\u76d1\u542c\u6570\u636e\u5e93\u540d\u79f0 - \u8bbe\u7f6e\u63a7\u5236\u6587\u4ef6\u8def\u5f84 * \u542f\u52a8listener * \u4ece\u5f00\u59cb\u83dc\u5355\u680f\u542f\u52a8Navigator * \u5728\u83dc\u5355\u680f\u8d44\u6e90->\u6570\u636e\u6355\u83b7->\u6ce8\u518c\u7ec4\uff0c\u53f3\u952e\u65b0\u5efa\u6ce8\u518c\u7ec4\uff0c\u586b\u5199\u4fe1\u606f\u5982\u4e0b - \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 - \u4f4d\u7f6e\uff1aLinux\u670d\u52a1\u7aef\u76d1\u542c\u8282\u70b9\u540d\u79f0 - \u7c7b\u578b\uff1aORACLE - \u7528\u6237ID\u548c\u5bc6\u7801\uff1a\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 - \u96c6\u5408\u6807\u5fd7\u7b26\uff1a\u6570\u636e\u5e93\u540dORCL \u70b9\u51fb\u4e0b\u4e00\u6b65 \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u67b6\u6784\uff1aschema\u540d\u79f0\uff0c\u5373\u7528\u6237\u540d \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4f1a\u770b\u5230\u521a\u624d\u521b\u5efa\u7684test\u8868\uff0c\u53cc\u51fb\u8868\u540d\uff0c\u88ab\u9009\u5165\u53f3\u4fa7\uff0c\u9009\u62e9\u6240\u6709\u5217 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4fee\u6539\u72b6\u6001\u4e3a \u6d3b\u52a8 \uff0c\u52fe\u9009 \u7acb\u5373\u6267\u884cDDL ,\u70b9\u51fb\u5b8c\u6210 \u5728\u63d0\u53d6\u7ec4\uff0c\u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684orcl12,\u8fdb\u5165\u63d0\u53d6\u7ec4\u754c\u9762\uff0c\u53f3\u952e\uff0c\u6dfb\u52a0\u63d0\u53d6\u81ea\u5b9a\u4e49\uff0c\u586b\u5199\u6620\u5c04\u540d\u79f0\u4ee5\u53ca\u8868\u540d\u79f0 * \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u521b\u5efa\u7684\u6ce8\u518c \u70b9\u51fb\u6dfb\u52a0\uff0c\u5b8c\u6210 \u70b9\u51fb\u83dc\u5355\u680f\u56fe\u8868\uff0c\u6267\u884c\u884c\u6d4b\u8bd5,\u53ef\u770b\u5230\u6355\u83b7\u5230\u7684\u6570\u636e\u5e93\u65e5\u5fd7\u8bb0\u5f55","title":"PWX CDC \u6355\u83b7ORACLE\u65e5\u5fd7\u6570\u636e"},{"location":"Data_Integration/Informatica_PWX_CDC/#pwx-cdc-publisherkafka","text":"### \u4fee\u6539kafka\u914d\u7f6e\u6587\u4ef6 * \u4fee\u6539producer.properties\u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * \u4fee\u6539jaas.conf\u6587\u4ef6\u5982\u4e0b ![](assets/Informatica_PWX_CDC/14cae.png) \u521b\u5efa\u4e00\u4e2akafka topic, pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### \u5b89\u88c5\u914d\u7f6eInformatica PWX Publisher * \u83b7\u53d6\u5b89\u88c5\u5305 pwxcdcpub120_linux_x64.tar.gz ,\u4ee5root\u7528\u6237\u8eab\u4efd\u89e3\u538b\u81f3\u5b89\u88c5\u76ee\u5f55\u5373\u53ef \u4ee5root\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u5728~/.bash_profile\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0csource\u73af\u5883\u53d8\u91cf,\u8fdb\u884ckerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser \u5c06\u5b89\u88c5\u76ee\u5f55samples\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\u590d\u5236\u5230instanceA/config\u76ee\u5f55\u4e0b\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u5185\u5bb9 > \u914d\u7f6ePWX Publisher\u53ef\u53c2\u8003Informatica \u5b98\u65b9\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html cdcPublisherAvro.cfg\u6587\u4ef6\u914d\u7f6e\u5982\u4e0b - cdcPublisherCommon.cfg\u6587\u4ef6\u4e2d\u6307\u5b9a\u7aef\u53e3 - cdcPublisherKafka.cfg\u6587\u4ef6\u4e2d\u6307\u5b9akafka topic\u540d\u79f0\u4ee5\u53caproperties\u6587\u4ef6\u8def\u5f84 - cdcPowerExchange.cfg\u6587\u4ef6\u4e2d\u914d\u7f6e\u5982\u4e0b * Extract.pwxCapiConnectionName\u4e3a\u5728dbmover.cfg\u4e2dCAPI_CONNECTION\u914d\u7f6e\u7684name * Extract.pwxExtractionMapSchemaName \u4e3apwx \u6355\u83b7\u6620\u5c04\u4e2d\u7684schema\u540d\u79f0\uff0c\u901a\u5e38\u683c\u5f0f\u4e3a unninstance \u6216\u8005 dnninstance \uff0c\u8fd9\u91cc\u4e3a u8orcl * Extract.pwxNodeLocation \u914d\u7f6e\u4e3apwx\u8282\u70b9\u540d\u79f0 * Extract.pwxNodeUserId\uff0cExtract.pwxNodePwd\u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 * Extract.pwxXmapUserId\u4e3a\u8bbf\u95eepwx\u63d0\u53d6\u6620\u5c04\u7684\u7528\u6237\u540d\u5bc6\u7801 * \u4fee\u6539\u5b89\u88c5\u8def\u5f84bin\u76ee\u5f55\u4e0b\u7684PwxCDCPublisher.sh\u542f\u52a8\u811a\u672c\u6587\u4ef6,\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u52a0\u5165\u4e00\u884c RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" * \u542f\u52a8pwx CDC Publisher,\u5728bin\u76ee\u5f55\u4e0b\u6267\u884c sh PwxCDCPublisher.sh","title":"\u4f7f\u7528PWX CDC publisher\u5bf9\u63a5Kafka"},{"location":"Data_Integration/Informatica_PWX_CDC/#kafka-consumer","text":"\u5728FusionInsight HD Kafka \u5ba2\u6237\u7aef,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cinsert\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cupdate\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cdelete\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b","title":"\u542f\u52a8kafka consumer\uff0c\u67e5\u770b\u6d88\u8d39\u5230\u7684\u6570\u636e"},{"location":"Data_Integration/Informatica_PWX_CDC/#qa","text":"1.\u82e5\u542f\u52a8pwxccl\u62a5\u9519\u5982\u4e0b A:\u68c0\u67e5\u5728oracle\u4e2d\u662f\u5426\u6267\u884c\u8fc7 exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u82e5\u6267\u884c\u6210\u529f\uff0c\u4ecd\u7136\u62a5\u9519\uff0c\u7ed9C##PWX\u7528\u6237\u8d4b\u4e88sysdba\u6743\u9650 grant sysdba to C##PWX","title":"Q&amp;A"},{"location":"Data_Integration/Informatica_PowerCenter/","text":"Informatica PowerCenter\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive) \u73af\u5883\u4fe1\u606f \u00b6 Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD \u5ba2\u6237\u7aef \u90e8\u7f72\u65b9\u6848 \u00b6 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72Informatica Server\uff0c\u5e76\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5Informatica PowerCenter Client \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u00b6 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88HDFS,Hive\u6240\u6709\u6743\u9650\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.confh\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/ \u76ee\u5f55\u4e0b \u5728Linux\u4e0a\u5b89\u88c5Oracle database \u4ee5\u53ca Informatica Server \u00b6 \u521b\u5efaoracle \u7528\u6237\uff0c\u5b89\u88c5oracle \u6570\u636e\u5e93 \u521b\u5efainfa\u7528\u6237\uff0c\u4f7f\u7528 sqlplus / as sysdba \u767b\u5f55\u81f3oracle\u6570\u636e\u5e93\u4e2d\uff0c\u6267\u884c\u4ee5\u4e0bsql\u8bed\u53e5 create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; \u83b7\u53d6Informatica Server\u5b89\u88c5\u5305\u5e76\u4e0a\u4f20\u81f3\u8282\u70b9,\u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u5b89\u88c5,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /home/infa/Informatica/10.2.0 . \u5b89\u88c5\u5b8c\u6210\u540e,Informatica Server\u4f1a\u81ea\u884c\u542f\u52a8\uff0c\u5728\u6d4f\u89c8\u5668\u8f93\u5165ip:6008\u7aef\u53e3\uff0c\u6253\u5f00Administrator \u7ba1\u7406\u754c\u9762\uff0c\u8f93\u5165\u5b89\u88c5\u65f6\u8bbe\u7f6e\u7684\u7528\u6237\u540d\u5bc6\u7801\u8fdb\u884c\u767b\u5f55\u3002 Informatica Server\u914d\u7f6e \u00b6 \u521b\u5efaPowerCenter \u5b58\u50a8\u5e93 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter \u5b58\u50a8\u5e93 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u6570\u636e\u5e93\u4fe1\u606f\uff0c\u5b8c\u6210 - \u70b9\u51fb\u53f3\u4e0a\u89d2\u6309\u94ae\u542f\u7528\u5b58\u50a8\u5e93\uff0c\u5e76\u4e3a\u5b58\u50a8\u5e93\u521b\u5efa\u5185\u5bb9 \u5728\u5b58\u50a8\u5e93\u5c5e\u6027\u4e2d\uff0c\u4fee\u6539\u64cd\u4f5c\u7c7b\u578b\u4e3a\u666e\u901a\uff0c\u5e76\u91cd\u542f\u670d\u52a1 \u521b\u5efaPowerCenter \u6570\u636e\u96c6\u6210\u670d\u52a1 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter\u96c6\u6210\u670d\u52a1 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u5b58\u50a8\u5e93\u4fe1\u606f\uff0c\u70b9\u51fb\u5b8c\u6210\uff0c\u5e76\u542f\u7528\u670d\u52a1 \u5728infa server\u521b\u5efadevelopuser \u5728\u5b89\u5168\u9875\u7b7e\u4e0b\uff0c\u521b\u5efa\u4e00\u4e2a\u7528\u6237\uff0c\u540d\u4e3adevelopuser\uff0c\u4e0eHadoop\u96c6\u7fa4\u7528\u6237\u4fdd\u6301\u4e00\u81f4 \u4fee\u6539\u7528\u6237\u7684\u4f18\u5148\u7ea7\u4ee5\u53ca\u7528\u6237\u7ec4 \u5728infa Server \u8fdb\u884cHadoop\u914d\u7f6e \u5c06 /opt \u76ee\u5f55\u4e0b\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u81f3 /etc \u76ee\u5f55\u4e0b\u4ee5\u53cainformatica\u5b89\u88c5\u76ee\u5f55 ${INFA_HOME}java/jre/lib/security/ \u4e0b\uff0c\u5e76\u8d4b\u4e88infa\u7528\u6237\u6539\u6587\u4ef6\u7684\u8bfb\u53d6\u6743\u9650. \u4ee5infa\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\uff0c\u4f8b\u5982 /opt/pwx-hadoop/conf \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u4ee5\u4e0b\u914d\u7f6e\u6587\u4ef6,\u653e\u81f3 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650\u81f3775 - \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884cKerberos\u8ba4\u8bc1,\u5e76\u6307\u5b9acache\u6587\u4ef6,infa\u7528\u6237\u9700\u8981\u5bf9\u6307\u5b9a\u7684\u8def\u5f84\u6709\u8bfb\u5199\u6743\u9650 source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - \u4fee\u6539 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\u7684 core-site.xml \u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - \u5728Administrator \u7ba1\u7406\u754c\u9762\uff0c\u4e3a\u96c6\u6210\u670d\u52a1\u521b\u5efa\u81ea\u5b9a\u4e49\u53c2\u6570\uff0c\u5e76\u91cd\u542f\u96c6\u6210\u670d\u52a1 \u5220\u9664 /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ \u76ee\u5f55\u4e0bhive\u76f8\u5173\u7684jar\u5305\uff0c\u5e76\u5c06 /opt/hadoopclient/Hive/Beeline/lib \u4e0bhive\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u81f3\u8be5\u76ee\u5f55\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650 rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* PowerCenter Client\u914d\u7f6e \u00b6 PowerCenter Repository Manager\u914d\u7f6e \u00b6 \u83b7\u53d6PowerCenter Client\u5b89\u88c5\u5305\uff0c\u5b89\u88c5\u65f6\u9009\u53d6PowerCenter Client,\u542f\u52a8PowerCenter Repository Manager\uff0c\u9009\u62e9\u83dc\u5355\u680f\u4ed3\u5e93->\u914d\u7f6e\u57df\uff0c\u914d\u7f6e\u5b8c\u6210\u53ef\u4ee5\u770b\u5230\u4e4b\u524d\u521b\u5efa\u7684\u5b58\u50a8\u5e93 \u53cc\u51fb\u5b58\u50a8\u5e93\uff0c\u8f93\u5165\u5bc6\u7801\uff0c\u8fde\u63a5 \u9009\u62e9\u83dc\u5355\u680f\u6587\u4ef6\u5939,\u521b\u5efa\u6587\u4ef6\u5939 PowerCenter Designer\u914d\u7f6e \u00b6 \u6253\u5f00PowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762 - \u70b9\u51fb\u83dc\u5355\u680fSources->import from databases\uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u521b\u5efasitDSN\uff0c\u586b\u5199\u6570\u636e\u5e93\u76f8\u5173\u4fe1\u606f\uff0c\u6d4b\u8bd5\u8fde\u63a5 - \u9009\u62e9\u521a\u624d\u521b\u5efa\u7684\u6570\u636e\u6e90\uff0c\u586b\u5165\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801\uff0c\u8fde\u63a5\uff0c\u53ef\u4ee5\u770b\u5230\u6570\u636e\u5e93\u4e2d\u7684\u8868 - \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868 - \u53cc\u51fb\u8868\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File \u5728mapping\u8bbe\u7f6e\u9875\u9762\uff0c\u521b\u5efa\u65b0\u7684mapping\uff0c\u62d6\u5165source\u548ctarget\u8868\uff0c\u5e76\u8fde\u7ebf \u6253\u5f00PowerCenter Workflow Manager \u00b6 \u5728\u83dc\u5355\u680f\u9009\u62e9task,\u65b0\u5efa\u4e00\u4e2atask,\u547d\u540d\u5e76\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684map \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165\u521a\u624d\u65b0\u5efa\u7684task\uff0c\u5e76\u8fde\u7ebf \u5728\u83dc\u5355\u680fconnection\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2aapplication connection,\u9009\u62e9Hadoop HDFS Connection \u5177\u4f53\u4fe1\u606f\u586b\u5199\u5982\u4e0b HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser \u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684task\uff0c\u5728mapping\u9009\u9879\u5361\uff0c\u70b9\u51fbtarget\uff0c\u8bbe\u7f6e\u5199\u5165\u7c7b\u578b\u4e3a HDFS Flat Write \uff0c\u5e76\u9009\u62e9\u8fde\u63a5\u4e3a\u521a\u624d\u521b\u5efa\u7684connection\uff0c\u5e76\u5728properties\u4e2d\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u4fdd\u5b58\u5f53\u524dworkflow\uff0c\u53f3\u952e\uff0c\u542f\u52a8workflow \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u60c5\u51b5 \u5728HDFS\u4e2d\u53ef\u4ee5\u770b\u5230\u5bfc\u5165\u7684\u6570\u636e \u5728task\u914d\u7f6e\u4e2d\u52fe\u9009\u5199\u5165Hive\u8868\uff0c\u586b\u5165\u4e4b\u524d\u521b\u5efa\u7684\u8868\u540d\uff0c\u8fd0\u884cworkflow \u5728Hive\u4e2d\u53ef\u4ee5\u770b\u5230\u8868\u4e2d\u7684\u6570\u636e","title":"10.2.0 <--> 6.5"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-powercenterfusioninsight-hd","text":"","title":"Informatica PowerCenter\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Integration/Informatica_PowerCenter/#_1","text":"Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PowerCenter/#_2","text":"Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD \u5ba2\u6237\u7aef","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Informatica_PowerCenter/#_3","text":"\u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72Informatica Server\uff0c\u5e76\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5Informatica PowerCenter Client","title":"\u90e8\u7f72\u65b9\u6848"},{"location":"Data_Integration/Informatica_PowerCenter/#_4","text":"","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Informatica_PowerCenter/#fusioninsight-hd","text":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88HDFS,Hive\u6240\u6709\u6743\u9650\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.confh\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/ \u76ee\u5f55\u4e0b","title":"\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef"},{"location":"Data_Integration/Informatica_PowerCenter/#linuxoracle-database-informatica-server","text":"\u521b\u5efaoracle \u7528\u6237\uff0c\u5b89\u88c5oracle \u6570\u636e\u5e93 \u521b\u5efainfa\u7528\u6237\uff0c\u4f7f\u7528 sqlplus / as sysdba \u767b\u5f55\u81f3oracle\u6570\u636e\u5e93\u4e2d\uff0c\u6267\u884c\u4ee5\u4e0bsql\u8bed\u53e5 create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512 m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp ; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp ; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp ; grant dba to domain_user , pwc_user , mdl_user ; \u83b7\u53d6Informatica Server\u5b89\u88c5\u5305\u5e76\u4e0a\u4f20\u81f3\u8282\u70b9,\u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u5b89\u88c5,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /home/infa/Informatica/10.2.0 . \u5b89\u88c5\u5b8c\u6210\u540e,Informatica Server\u4f1a\u81ea\u884c\u542f\u52a8\uff0c\u5728\u6d4f\u89c8\u5668\u8f93\u5165ip:6008\u7aef\u53e3\uff0c\u6253\u5f00Administrator \u7ba1\u7406\u754c\u9762\uff0c\u8f93\u5165\u5b89\u88c5\u65f6\u8bbe\u7f6e\u7684\u7528\u6237\u540d\u5bc6\u7801\u8fdb\u884c\u767b\u5f55\u3002","title":"\u5728Linux\u4e0a\u5b89\u88c5Oracle database \u4ee5\u53ca Informatica Server"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-server","text":"\u521b\u5efaPowerCenter \u5b58\u50a8\u5e93 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter \u5b58\u50a8\u5e93 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u6570\u636e\u5e93\u4fe1\u606f\uff0c\u5b8c\u6210 - \u70b9\u51fb\u53f3\u4e0a\u89d2\u6309\u94ae\u542f\u7528\u5b58\u50a8\u5e93\uff0c\u5e76\u4e3a\u5b58\u50a8\u5e93\u521b\u5efa\u5185\u5bb9 \u5728\u5b58\u50a8\u5e93\u5c5e\u6027\u4e2d\uff0c\u4fee\u6539\u64cd\u4f5c\u7c7b\u578b\u4e3a\u666e\u901a\uff0c\u5e76\u91cd\u542f\u670d\u52a1 \u521b\u5efaPowerCenter \u6570\u636e\u96c6\u6210\u670d\u52a1 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter\u96c6\u6210\u670d\u52a1 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u5b58\u50a8\u5e93\u4fe1\u606f\uff0c\u70b9\u51fb\u5b8c\u6210\uff0c\u5e76\u542f\u7528\u670d\u52a1 \u5728infa server\u521b\u5efadevelopuser \u5728\u5b89\u5168\u9875\u7b7e\u4e0b\uff0c\u521b\u5efa\u4e00\u4e2a\u7528\u6237\uff0c\u540d\u4e3adevelopuser\uff0c\u4e0eHadoop\u96c6\u7fa4\u7528\u6237\u4fdd\u6301\u4e00\u81f4 \u4fee\u6539\u7528\u6237\u7684\u4f18\u5148\u7ea7\u4ee5\u53ca\u7528\u6237\u7ec4 \u5728infa Server \u8fdb\u884cHadoop\u914d\u7f6e \u5c06 /opt \u76ee\u5f55\u4e0b\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u81f3 /etc \u76ee\u5f55\u4e0b\u4ee5\u53cainformatica\u5b89\u88c5\u76ee\u5f55 ${INFA_HOME}java/jre/lib/security/ \u4e0b\uff0c\u5e76\u8d4b\u4e88infa\u7528\u6237\u6539\u6587\u4ef6\u7684\u8bfb\u53d6\u6743\u9650. \u4ee5infa\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\uff0c\u4f8b\u5982 /opt/pwx-hadoop/conf \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u4ee5\u4e0b\u914d\u7f6e\u6587\u4ef6,\u653e\u81f3 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650\u81f3775 - \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884cKerberos\u8ba4\u8bc1,\u5e76\u6307\u5b9acache\u6587\u4ef6,infa\u7528\u6237\u9700\u8981\u5bf9\u6307\u5b9a\u7684\u8def\u5f84\u6709\u8bfb\u5199\u6743\u9650 source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser - \u4fee\u6539 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\u7684 core-site.xml \u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> - \u5728Administrator \u7ba1\u7406\u754c\u9762\uff0c\u4e3a\u96c6\u6210\u670d\u52a1\u521b\u5efa\u81ea\u5b9a\u4e49\u53c2\u6570\uff0c\u5e76\u91cd\u542f\u96c6\u6210\u670d\u52a1 \u5220\u9664 /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ \u76ee\u5f55\u4e0bhive\u76f8\u5173\u7684jar\u5305\uff0c\u5e76\u5c06 /opt/hadoopclient/Hive/Beeline/lib \u4e0bhive\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u81f3\u8be5\u76ee\u5f55\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650 rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive*","title":"Informatica Server\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-client","text":"","title":"PowerCenter Client\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-repository-manager","text":"\u83b7\u53d6PowerCenter Client\u5b89\u88c5\u5305\uff0c\u5b89\u88c5\u65f6\u9009\u53d6PowerCenter Client,\u542f\u52a8PowerCenter Repository Manager\uff0c\u9009\u62e9\u83dc\u5355\u680f\u4ed3\u5e93->\u914d\u7f6e\u57df\uff0c\u914d\u7f6e\u5b8c\u6210\u53ef\u4ee5\u770b\u5230\u4e4b\u524d\u521b\u5efa\u7684\u5b58\u50a8\u5e93 \u53cc\u51fb\u5b58\u50a8\u5e93\uff0c\u8f93\u5165\u5bc6\u7801\uff0c\u8fde\u63a5 \u9009\u62e9\u83dc\u5355\u680f\u6587\u4ef6\u5939,\u521b\u5efa\u6587\u4ef6\u5939","title":"PowerCenter Repository Manager\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-designer","text":"\u6253\u5f00PowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762 - \u70b9\u51fb\u83dc\u5355\u680fSources->import from databases\uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u521b\u5efasitDSN\uff0c\u586b\u5199\u6570\u636e\u5e93\u76f8\u5173\u4fe1\u606f\uff0c\u6d4b\u8bd5\u8fde\u63a5 - \u9009\u62e9\u521a\u624d\u521b\u5efa\u7684\u6570\u636e\u6e90\uff0c\u586b\u5165\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801\uff0c\u8fde\u63a5\uff0c\u53ef\u4ee5\u770b\u5230\u6570\u636e\u5e93\u4e2d\u7684\u8868 - \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868 - \u53cc\u51fb\u8868\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File \u5728mapping\u8bbe\u7f6e\u9875\u9762\uff0c\u521b\u5efa\u65b0\u7684mapping\uff0c\u62d6\u5165source\u548ctarget\u8868\uff0c\u5e76\u8fde\u7ebf","title":"PowerCenter Designer\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-workflow-manager","text":"\u5728\u83dc\u5355\u680f\u9009\u62e9task,\u65b0\u5efa\u4e00\u4e2atask,\u547d\u540d\u5e76\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684map \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165\u521a\u624d\u65b0\u5efa\u7684task\uff0c\u5e76\u8fde\u7ebf \u5728\u83dc\u5355\u680fconnection\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2aapplication connection,\u9009\u62e9Hadoop HDFS Connection \u5177\u4f53\u4fe1\u606f\u586b\u5199\u5982\u4e0b HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser \u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684task\uff0c\u5728mapping\u9009\u9879\u5361\uff0c\u70b9\u51fbtarget\uff0c\u8bbe\u7f6e\u5199\u5165\u7c7b\u578b\u4e3a HDFS Flat Write \uff0c\u5e76\u9009\u62e9\u8fde\u63a5\u4e3a\u521a\u624d\u521b\u5efa\u7684connection\uff0c\u5e76\u5728properties\u4e2d\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u4fdd\u5b58\u5f53\u524dworkflow\uff0c\u53f3\u952e\uff0c\u542f\u52a8workflow \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u60c5\u51b5 \u5728HDFS\u4e2d\u53ef\u4ee5\u770b\u5230\u5bfc\u5165\u7684\u6570\u636e \u5728task\u914d\u7f6e\u4e2d\u52fe\u9009\u5199\u5165Hive\u8868\uff0c\u586b\u5165\u4e4b\u524d\u521b\u5efa\u7684\u8868\u540d\uff0c\u8fd0\u884cworkflow \u5728Hive\u4e2d\u53ef\u4ee5\u770b\u5230\u8868\u4e2d\u7684\u6570\u636e","title":"\u6253\u5f00PowerCenter Workflow Manager"},{"location":"Data_Integration/Kafka_Manager/","text":"Kafka Manager\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kafka Manager 1.3.3.21 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5JDK1.8\u53ca\u4ee5\u4e0a\u7248\u672c \u4e0b\u8f7dkafka-manager\u6e90\u7801 \u4e0b\u8f7d\u5730\u5740\u4e3a https://github.com/yahoo/kafka-manager \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55kafka-manager yum\u5b89\u88c5sbt yum install sbt \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u83b7\u53d6kafka\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6,\u767b\u5f55FusionInsight\u96c6\u7fa4\u8282\u70b9,\u5c06/opt/huawei/Bigdata/om-server_V100R002C80SPC200/apache-tomcat-8.5.28/conf/security/kafka.keytab\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u5e76\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef\u8282\u70b9/opt/hadoopclient/\u76ee\u5f55\u4e0b \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237kafkauser\uff0c\u5e76\u9009\u62e9kafka\u548ckafkaadmin\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.conf\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/hadoopclient/ \u76ee\u5f55\u4e0b kafka-manager\u7f16\u8bd1\u53ca\u914d\u7f6e \u00b6 \u4fee\u6539\u6e90\u7801 \u8fdb\u5165\u5b89\u88c5\u76ee\u5f55/kafka-manager/app/kafka/manager/jmx\uff0c\u4fee\u6539KafkaJMX.scala\u6587\u4ef6\u4e2d\u5199\u6b7b\u7684jmx\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u5c06'jmxrmi'\u4fee\u6539\u4e3a'kafka',\u5982\u4e0b\u56fe\uff1a \u7f16\u8bd1kafka-manager,\u83b7\u53d6\u538b\u7f29\u5305 cd /opt/kafka-manager ./sbt clean dist cp /opt/kafka-manager/target/universal/kafka-manager-1.3.3.21.zip /opt cd /opt uzip kafka-manager-1.3.3.21.zip cd /opt/kafka-manager-1.3.3.21 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6conf/application.conf,'kafka-manager.zkhosts'\u4fee\u6539\u4e3azookeeper\u96c6\u7fa4\u8282\u70b9IP:\u7aef\u53e3,FI\u96c6\u7fa4\u9ed8\u8ba4\u7aef\u53e3\u4e3a24002 kafka-manager.zkhosts=\"172.21.3.115:24002\" \u65b0\u5efaconf/jaas.conf\u6587\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; \u5c06kafka-manager\u7684lib\u5e93\u4e2dzookeeper\u7684jar\u5305\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e2dzookeeper\u7684jar\u5305,\u5e76\u91cd\u547d\u540d cp /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar /opt/kafka-manager-1.3.3.21/lib cd /opt/kafka-manager-1.3.3.21/lib rm org.apache.zookeeper.zookeeper-3.4.10.jar mv zookeeper-3.5.1.jar org.apache.zookeeper.zookeeper-3.4.10.jar kafka-manager\u4f7f\u7528 \u00b6 \u542f\u52a8kafka-manager cd /opt/kafka-manager-1.3.3.21 nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Djava.security.auth.login.config=conf/jaas.conf -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com & > \u53ef\u901a\u8fc7-Dhttp.port=port\u6307\u5b9a\u8bbf\u95ee\u7aef\u53e3,\u9ed8\u8ba4\u4e3a9000 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165172.21.3.48:9000\uff0c\u5373\u53ef\u8bbf\u95eekafka-manager \u70b9\u51fbcluster->Add Cluster\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199 Cluster Name:\u81ea\u5b9a\u4e49 Cluster Zookeeper Hosts:ZooKeeper\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f,\u53ef\u5199\u591a\u4e2a\u6216\u8005\u4e00\u4e2a\u8282\u70b9,\u4e00\u5b9a\u8981\u52a0\u4e0akafka\u540e\u7f00 Kafka Version:\u5f53\u524dFI\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684\u662f0.11.0.1\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u76840.11.0.2\u5373\u53ef \u52fe\u9009Enable JMX\u590d\u9009\u6846 \u5c06\u4ee5\u4e0b\u51e0\u4e2asize\u5927\u5c0f\u8bbe\u7f6e\u4e3a\u5927\u4e8e\u7b49\u4e8e2 \u5176\u4ed6\u8bbe\u7f6e\u53ef\u4ee5\u4fdd\u6301\u9ed8\u8ba4\u6216\u8005\u6839\u636e\u9700\u8981\u4fee\u6539,\u70b9\u51fbSave\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u521b\u5efa\u6210\u529f \u70b9\u51fbGo to cluster view\uff0c\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f \u5728Brokers\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u7684brokers\u60c5\u51b5 \u5728Topic->Create\u83dc\u5355\u680f\u53ef\u4ee5\u521b\u5efa\u65b0\u7684topic \u5728Topic->List\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u6240\u6709\u7684topic","title":"1.3.3.21 <--> C80"},{"location":"Data_Integration/Kafka_Manager/#kafka-managerfusioninsight","text":"","title":"Kafka Manager\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kafka_Manager/#_1","text":"Kafka Manager 1.3.3.21 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kafka_Manager/#_2","text":"\u5b89\u88c5JDK1.8\u53ca\u4ee5\u4e0a\u7248\u672c \u4e0b\u8f7dkafka-manager\u6e90\u7801 \u4e0b\u8f7d\u5730\u5740\u4e3a https://github.com/yahoo/kafka-manager \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55kafka-manager yum\u5b89\u88c5sbt yum install sbt \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u83b7\u53d6kafka\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6,\u767b\u5f55FusionInsight\u96c6\u7fa4\u8282\u70b9,\u5c06/opt/huawei/Bigdata/om-server_V100R002C80SPC200/apache-tomcat-8.5.28/conf/security/kafka.keytab\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u5e76\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef\u8282\u70b9/opt/hadoopclient/\u76ee\u5f55\u4e0b \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237kafkauser\uff0c\u5e76\u9009\u62e9kafka\u548ckafkaadmin\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.conf\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/hadoopclient/ \u76ee\u5f55\u4e0b","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kafka_Manager/#kafka-manager","text":"\u4fee\u6539\u6e90\u7801 \u8fdb\u5165\u5b89\u88c5\u76ee\u5f55/kafka-manager/app/kafka/manager/jmx\uff0c\u4fee\u6539KafkaJMX.scala\u6587\u4ef6\u4e2d\u5199\u6b7b\u7684jmx\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u5c06'jmxrmi'\u4fee\u6539\u4e3a'kafka',\u5982\u4e0b\u56fe\uff1a \u7f16\u8bd1kafka-manager,\u83b7\u53d6\u538b\u7f29\u5305 cd /opt/kafka-manager ./sbt clean dist cp /opt/kafka-manager/target/universal/kafka-manager-1.3.3.21.zip /opt cd /opt uzip kafka-manager-1.3.3.21.zip cd /opt/kafka-manager-1.3.3.21 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6conf/application.conf,'kafka-manager.zkhosts'\u4fee\u6539\u4e3azookeeper\u96c6\u7fa4\u8282\u70b9IP:\u7aef\u53e3,FI\u96c6\u7fa4\u9ed8\u8ba4\u7aef\u53e3\u4e3a24002 kafka-manager.zkhosts=\"172.21.3.115:24002\" \u65b0\u5efaconf/jaas.conf\u6587\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; \u5c06kafka-manager\u7684lib\u5e93\u4e2dzookeeper\u7684jar\u5305\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e2dzookeeper\u7684jar\u5305,\u5e76\u91cd\u547d\u540d cp /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar /opt/kafka-manager-1.3.3.21/lib cd /opt/kafka-manager-1.3.3.21/lib rm org.apache.zookeeper.zookeeper-3.4.10.jar mv zookeeper-3.5.1.jar org.apache.zookeeper.zookeeper-3.4.10.jar","title":"kafka-manager\u7f16\u8bd1\u53ca\u914d\u7f6e"},{"location":"Data_Integration/Kafka_Manager/#kafka-manager_1","text":"\u542f\u52a8kafka-manager cd /opt/kafka-manager-1.3.3.21 nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Djava.security.auth.login.config=conf/jaas.conf -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com & > \u53ef\u901a\u8fc7-Dhttp.port=port\u6307\u5b9a\u8bbf\u95ee\u7aef\u53e3,\u9ed8\u8ba4\u4e3a9000 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165172.21.3.48:9000\uff0c\u5373\u53ef\u8bbf\u95eekafka-manager \u70b9\u51fbcluster->Add Cluster\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199 Cluster Name:\u81ea\u5b9a\u4e49 Cluster Zookeeper Hosts:ZooKeeper\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f,\u53ef\u5199\u591a\u4e2a\u6216\u8005\u4e00\u4e2a\u8282\u70b9,\u4e00\u5b9a\u8981\u52a0\u4e0akafka\u540e\u7f00 Kafka Version:\u5f53\u524dFI\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684\u662f0.11.0.1\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u76840.11.0.2\u5373\u53ef \u52fe\u9009Enable JMX\u590d\u9009\u6846 \u5c06\u4ee5\u4e0b\u51e0\u4e2asize\u5927\u5c0f\u8bbe\u7f6e\u4e3a\u5927\u4e8e\u7b49\u4e8e2 \u5176\u4ed6\u8bbe\u7f6e\u53ef\u4ee5\u4fdd\u6301\u9ed8\u8ba4\u6216\u8005\u6839\u636e\u9700\u8981\u4fee\u6539,\u70b9\u51fbSave\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u521b\u5efa\u6210\u529f \u70b9\u51fbGo to cluster view\uff0c\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f \u5728Brokers\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u7684brokers\u60c5\u51b5 \u5728Topic->Create\u83dc\u5355\u680f\u53ef\u4ee5\u521b\u5efa\u65b0\u7684topic \u5728Topic->List\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u6240\u6709\u7684topic","title":"kafka-manager\u4f7f\u7528"},{"location":"Data_Integration/Kettle_6.1/","text":"Kettle\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C70U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C80U10 (HDFS/Hive) \u73af\u5883\u51c6\u5907 \u00b6 Linux\u5e73\u53f0 \u00b6 \u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5CentOS6.5 Desktop \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kettle \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Windows\u5e73\u53f0 \u00b6 \u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u590d\u5236 C:\\Windows \u76ee\u5f55\u4e0b\uff0c\u91cd\u547d\u540d\u4e3akrb5.ini \u6dfb\u52a0\u7cfb\u7edf\u73af\u5883\u53d8\u91cfKRB5_CONFIG\uff08\u53ef\u9009\u6b65\u9aa4\uff09 KRB5_CONFIG=C:\\Windows \u914d\u7f6e\u5e76\u542f\u52a8Kettle \u00b6 \u4ece\u4ee5\u4e0b\u5730\u5740 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ \u4e0b\u8f7dKettle6.1\u7248\u672c \u89e3\u538b\u5f97\u5230data-integration\u76ee\u5f55 \u66ff\u6362pentaho-big-data-plugin\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u7528\u89e3\u538b\u76ee\u5f55\u4e0b Hive/jdbc-examples/conf/core-site.xml \u6587\u4ef6 \u66ff\u6362 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23 \u76ee\u5f55\u4e0b\u7684core-site.xml\u6587\u4ef6 \u66ff\u6362Hive\u76f8\u5173jar\u5305 \u5c06 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib \u4e0b\u7684hive\u76f8\u5173\u7684jar\u5305 \u66ff\u6362\u6210Hive\u5ba2\u6237\u7aef\u4e0bjdbc-examples/lib\u76ee\u5f55\u4e0b\u7684\u4ee5\u4e0bjar\u5305 \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 Kerberos\u8ba4\u8bc1\uff08\u53ef\u9009\u6b65\u9aa4\uff09 \u5728\u5bf9\u63a5Hive\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\uff0c\u6216\u8005\u5728jdbc URL\u4e2d\u6307\u5b9aprincipal\u548ckeytab\u6587\u4ef6\u8fdb\u884c\u8ba4\u8bc1\uff08\u5bf9\u63a5HDFS\u65f6\uff0c\u53ea\u80fd\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff09 \u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff0c\u9700\u8981\u5728\u542f\u52a8kettle\u524d\u5148\u5b8c\u6210\u8ba4\u8bc1\u3002 \u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1akettle\u53ea\u5728\u542f\u52a8\u65f6\u8bfb\u53d6\u4e00\u6b21\u7968\u636e\uff0c\u800c\u4e0d\u662f\u8fde\u63a5\u65f6\u5b9e\u65f6\u8bfb\u53d6\u5f53\u524d\u7968\u636e\u4fe1\u606f\uff0c\u6240\u4ee5\u5f53kettle\u542f\u52a8\u65f6\u83b7\u53d6\u7684\u7968\u636e\u8fc7\u671f\u4ee5\u540e\uff0c\u8fde\u63a5Hive\u4f1a\u5931\u8d25\uff0c\u5fc5\u987b\u91cd\u542fkettle\u3002 \u542f\u52a8kettle Linux\u5e73\u53f0 VNC\u767b\u5f55CentOS\u684c\u9762\uff0c\u6253\u5f00Terminal cd /opt/data-integration/ ./spoon.sh Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat \u5bf9\u63a5Hive \u00b6 \u521b\u5efaHive\u8fde\u63a5 \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u5982\u679c\u8981\u5728\u8fde\u63a5Hive\u65f6\u4f7f\u7528keytab\u6587\u4ef6\u8ba4\u8bc1\uff0c\u589e\u52a0user.principal\u548cuser.keytab\u4e24\u4e2a\u53c2\u6570\uff1a \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0cHadoop\u7248\u672c\u9009\u7528HDP2.3 \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5 \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u4ee5hive -> postgresql\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3ahive2postgres.ktr \u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c \u8f93\u51fa -> \u8868\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u4fee\u6539postgresql\u8868\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u51fa \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5\u4e2d \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6\u76ee\u6807\u8868\u914d\u7f6e \u5982\u4e0b\uff08\u9700\u8981\u5148\u5728postgresql\u6570\u636e\u5e93\u521b\u5efa\u76ee\u6807\u8868\uff09 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a postgresql\u6570\u636e\u5e93\u67e5\u770b\uff1a \u5199\u5165Hive\u6570\u636e \u00b6 \u4ee5oracle -> hive\u4e3a\u4f8b \u6dfb\u52a0Oracle JDBC Driver \u4ece http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html \u4e0b\u8f7d\u5bf9\u5e94\u7248\u672c\u7684jdbc Driver\uff0c\u653e\u5230 data-integration/lib \u76ee\u5f55\u4e0b\uff0c\u91cd\u542fkettle \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3aoracle2hive.ktr \u521b\u5efaOracle\u8fde\u63a5 \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS kettle_export ( id int , name string ); \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u4fee\u6539\u6b65\u9aa4\u914d\u7f6e Oracle\u8868\u8f93\u5165\u914d\u7f6e Hive\u8868\u8f93\u51fa\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516513\u6761\u6570\u636e\uff0c\u7528\u65f64min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6 \u5bf9\u63a5HDFS \u00b6 \u521b\u5efaHadoop Cluster \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199NameNode\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f25000\uff0c\u5982\u679cNaneNode\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f26004\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP\u3002 \u70b9\u51fb \u6d4b\u8bd5 kettle6.1\u4e0d\u652f\u6301HDFS NameNode\u548cYarn ResourceManager\u7684HA\u914d\u7f6e \u5bfc\u5165HDFS\u6587\u4ef6 \u00b6 \u4ee5postgresql -> HDFS\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3apostgres2hdfs.ktr \u53c2\u8003\u524d\u9762\u7ae0\u8282\u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string , description string , total_emp int , salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"[,]\" ) STORED AS TEXTFILE ; \u5982\u679c\u6570\u636e\u4e2d\u542b\u6709\u201d,\u201d\uff0c\u5217\u5206\u9694\u7b26\u4e0d\u53ef\u4ee5\u4f7f\u7528\u9ed8\u8ba4\u7684\u201d,\u201d\uff0c\u672c\u6837\u4f8b\u4f7f\u7528\u591a\u5b57\u8282\u5206\u9694\u7b26\u201d[,]\u201d \u4fee\u6539postgresql\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u8868 \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9\u5230hive\u8868\u5bf9\u5e94\u7684hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u5206\u9694\u7b26\u8bbe\u7f6e\u4e0e\u524d\u9762\u521b\u5efa\u7684Hive\u8868\u76f8\u540c\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u5bfc\u5165\u7684HDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 kettle\u4f1a\u81ea\u52a8\u626b\u63cf\u6587\u4ef6\u4e2d\u7684\u5b57\u6bb5\u7c7b\u578b\u548c\u957f\u5ea6 \u53ef\u4ee5\u624b\u52a8\u4fee\u6539\u5b57\u6bb5\u540d\u79f0\u548c\u957f\u5ea6 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"6.1 <--> C80"},{"location":"Data_Integration/Kettle_6.1/#kettlefusioninsight","text":"","title":"Kettle\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kettle_6.1/#_1","text":"Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C70U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C80U10 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kettle_6.1/#_2","text":"","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kettle_6.1/#linux","text":"\u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5CentOS6.5 Desktop \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kettle \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/","title":"Linux\u5e73\u53f0"},{"location":"Data_Integration/Kettle_6.1/#windows","text":"\u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u590d\u5236 C:\\Windows \u76ee\u5f55\u4e0b\uff0c\u91cd\u547d\u540d\u4e3akrb5.ini \u6dfb\u52a0\u7cfb\u7edf\u73af\u5883\u53d8\u91cfKRB5_CONFIG\uff08\u53ef\u9009\u6b65\u9aa4\uff09 KRB5_CONFIG=C:\\Windows","title":"Windows\u5e73\u53f0"},{"location":"Data_Integration/Kettle_6.1/#kettle","text":"\u4ece\u4ee5\u4e0b\u5730\u5740 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ \u4e0b\u8f7dKettle6.1\u7248\u672c \u89e3\u538b\u5f97\u5230data-integration\u76ee\u5f55 \u66ff\u6362pentaho-big-data-plugin\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u7528\u89e3\u538b\u76ee\u5f55\u4e0b Hive/jdbc-examples/conf/core-site.xml \u6587\u4ef6 \u66ff\u6362 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23 \u76ee\u5f55\u4e0b\u7684core-site.xml\u6587\u4ef6 \u66ff\u6362Hive\u76f8\u5173jar\u5305 \u5c06 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib \u4e0b\u7684hive\u76f8\u5173\u7684jar\u5305 \u66ff\u6362\u6210Hive\u5ba2\u6237\u7aef\u4e0bjdbc-examples/lib\u76ee\u5f55\u4e0b\u7684\u4ee5\u4e0bjar\u5305 \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 Kerberos\u8ba4\u8bc1\uff08\u53ef\u9009\u6b65\u9aa4\uff09 \u5728\u5bf9\u63a5Hive\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\uff0c\u6216\u8005\u5728jdbc URL\u4e2d\u6307\u5b9aprincipal\u548ckeytab\u6587\u4ef6\u8fdb\u884c\u8ba4\u8bc1\uff08\u5bf9\u63a5HDFS\u65f6\uff0c\u53ea\u80fd\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff09 \u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff0c\u9700\u8981\u5728\u542f\u52a8kettle\u524d\u5148\u5b8c\u6210\u8ba4\u8bc1\u3002 \u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1akettle\u53ea\u5728\u542f\u52a8\u65f6\u8bfb\u53d6\u4e00\u6b21\u7968\u636e\uff0c\u800c\u4e0d\u662f\u8fde\u63a5\u65f6\u5b9e\u65f6\u8bfb\u53d6\u5f53\u524d\u7968\u636e\u4fe1\u606f\uff0c\u6240\u4ee5\u5f53kettle\u542f\u52a8\u65f6\u83b7\u53d6\u7684\u7968\u636e\u8fc7\u671f\u4ee5\u540e\uff0c\u8fde\u63a5Hive\u4f1a\u5931\u8d25\uff0c\u5fc5\u987b\u91cd\u542fkettle\u3002 \u542f\u52a8kettle Linux\u5e73\u53f0 VNC\u767b\u5f55CentOS\u684c\u9762\uff0c\u6253\u5f00Terminal cd /opt/data-integration/ ./spoon.sh Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat","title":"\u914d\u7f6e\u5e76\u542f\u52a8Kettle"},{"location":"Data_Integration/Kettle_6.1/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/Kettle_6.1/#hive_1","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u5982\u679c\u8981\u5728\u8fde\u63a5Hive\u65f6\u4f7f\u7528keytab\u6587\u4ef6\u8ba4\u8bc1\uff0c\u589e\u52a0user.principal\u548cuser.keytab\u4e24\u4e2a\u53c2\u6570\uff1a \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0cHadoop\u7248\u672c\u9009\u7528HDP2.3 \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5","title":"\u521b\u5efaHive\u8fde\u63a5"},{"location":"Data_Integration/Kettle_6.1/#hive_2","text":"\u4ee5hive -> postgresql\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3ahive2postgres.ktr \u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c \u8f93\u51fa -> \u8868\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u4fee\u6539postgresql\u8868\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u51fa \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5\u4e2d \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6\u76ee\u6807\u8868\u914d\u7f6e \u5982\u4e0b\uff08\u9700\u8981\u5148\u5728postgresql\u6570\u636e\u5e93\u521b\u5efa\u76ee\u6807\u8868\uff09 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a postgresql\u6570\u636e\u5e93\u67e5\u770b\uff1a","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_6.1/#hive_3","text":"\u4ee5oracle -> hive\u4e3a\u4f8b \u6dfb\u52a0Oracle JDBC Driver \u4ece http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html \u4e0b\u8f7d\u5bf9\u5e94\u7248\u672c\u7684jdbc Driver\uff0c\u653e\u5230 data-integration/lib \u76ee\u5f55\u4e0b\uff0c\u91cd\u542fkettle \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3aoracle2hive.ktr \u521b\u5efaOracle\u8fde\u63a5 \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS kettle_export ( id int , name string ); \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u4fee\u6539\u6b65\u9aa4\u914d\u7f6e Oracle\u8868\u8f93\u5165\u914d\u7f6e Hive\u8868\u8f93\u51fa\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516513\u6761\u6570\u636e\uff0c\u7528\u65f64min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6","title":"\u5199\u5165Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_6.1/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/Kettle_6.1/#hadoop-cluster","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199NameNode\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f25000\uff0c\u5982\u679cNaneNode\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f26004\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP\u3002 \u70b9\u51fb \u6d4b\u8bd5 kettle6.1\u4e0d\u652f\u6301HDFS NameNode\u548cYarn ResourceManager\u7684HA\u914d\u7f6e","title":"\u521b\u5efaHadoop Cluster"},{"location":"Data_Integration/Kettle_6.1/#hdfs_1","text":"\u4ee5postgresql -> HDFS\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3apostgres2hdfs.ktr \u53c2\u8003\u524d\u9762\u7ae0\u8282\u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string , description string , total_emp int , salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"[,]\" ) STORED AS TEXTFILE ; \u5982\u679c\u6570\u636e\u4e2d\u542b\u6709\u201d,\u201d\uff0c\u5217\u5206\u9694\u7b26\u4e0d\u53ef\u4ee5\u4f7f\u7528\u9ed8\u8ba4\u7684\u201d,\u201d\uff0c\u672c\u6837\u4f8b\u4f7f\u7528\u591a\u5b57\u8282\u5206\u9694\u7b26\u201d[,]\u201d \u4fee\u6539postgresql\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u8868 \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9\u5230hive\u8868\u5bf9\u5e94\u7684hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u5206\u9694\u7b26\u8bbe\u7f6e\u4e0e\u524d\u9762\u521b\u5efa\u7684Hive\u8868\u76f8\u540c\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u5bfc\u5165\u7684HDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u8868\u6570\u636e\uff1a","title":"\u5bfc\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_6.1/#hdfs_2","text":"\u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 kettle\u4f1a\u81ea\u52a8\u626b\u63cf\u6587\u4ef6\u4e2d\u7684\u5b57\u6bb5\u7c7b\u578b\u548c\u957f\u5ea6 \u53ef\u4ee5\u624b\u52a8\u4fee\u6539\u5b57\u6bb5\u540d\u79f0\u548c\u957f\u5ea6 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/","text":"Kettle\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kettle 8.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Kettle 8.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) \u8bf4\u660e\uff1aKettle 8.1.0\u4ec5\u9650POC\u4f7f\u7528 Windows\u5e73\u53f0 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u96c6\u7fa4\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e\uff0c\u4f8b\u5982\u7528\u6237\u4e3a developuser ; \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u540e\u7f00\u540d\u4fee\u6539\u4e3aini,\u5c06krb5.ini\u6587\u4ef6\u590d\u5236\u5230 C:\\Windows \u76ee\u5f55\u4e0b. \u914d\u7f6e\u5e76\u542f\u52a8Kettle \u00b6 \u8f6f\u4ef6\u83b7\u53d6 \u6253\u5f00\u4ee5\u4e0b\u5730\u5740[ https://github.com/pentaho/pentaho-kettle/tree/8.0 ] ( https://github.com/pentaho/pentaho-kettle/tree/8.0 ), \u9009\u62e9DownloadZip\u4e0b\u8f7dKettle8.0\u7248\u672c \u89e3\u538b\u5f97\u5230pdi-ce-8.0.0.0-28; \u83b7\u53d6FusionInsight\u7684\u9002\u914d\u5305\u6587\u4ef6 pentaho-hadoop-shims-hdp26-8.1.0.0-SNAPSHOT.jar \u548c pentaho-hadoop-shims-hdp26-hbase-comparators-8.1.0.0-SNAPSHOT.jar ,\u66ff\u6362\u76ee\u5f55 \\data-integration\\plugins\\pentaho-big-data-plugin\\hadoop-configurations\\hdp26\\ \u4e0b\u7684\u539f\u6709\u6587\u4ef6; \u66ff\u6362hdp26\\lib\u76ee\u5f55\u4e0bHive\u76f8\u5173\u7684jar\u5305\u4ee5\u53cahdp26\\lib\\client\u76ee\u5f55\u4e0bhdfs\u76f8\u5173\u7684jar\u5305 \u5e76\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u5305 \u83b7\u53d6FusionInsightHD\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cHbase\u7b49\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230Fi28\u9002\u914d\u5305\u7684\u6587\u4ef6\u5939\u91cc; \u4fee\u6539core-site.xml\u6587\u4ef6\u4e2d\u4ee5\u4e0b\u5b57\u6bb5\uff1a <name>fs.defaultFS</name> <value>hdfs://hacluster</value> \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6\u53ca\u914d\u7f6e \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 \u4fee\u6539Fi28\u9002\u914d\u5305\u4e2d config.properties \u6587\u4ef6: pentaho.authentication.default.kerberos.keytabLocation=C:/kerberos/user.keytab pentaho.authentication.default.kerberos.conf=C:/kerberos/krb5.conf pentaho.authentication.default.kerberos.principal=developuser@HADOOP.COM \u542f\u52a8kettle Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat,\u8fdb\u5165\u754c\u9762\u540e\uff0c\u5728\u4e0a\u65b9\u83dc\u5355\u680f\u9009\u62e9\u5de5\u5177->Hadoop Distribution,\u9009\u62e9 HortonWorks HDP 2.6.x \u5bf9\u63a5Hive \u00b6 \u521b\u5efaHive\u8fde\u63a5 \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u4e3a\u8fde\u63a5\u547d\u540d\uff0c\u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u70b9\u51fb\u6d4b\u8bd5\uff0c\u663e\u793a\u4ee5\u4e0b\u7a97\u53e3\uff0c\u8868\u660e\u6d4b\u8bd5\u6210\u529f \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5 \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4; \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u9009\u62e9 \u662f ,\u8be5\u8868\u7684\u5b57\u6bb5\u5c06\u4f1a\u5305\u542b\u5728SQL\u8bed\u53e5\u4e2d\uff0c \u53ef\u4ee5\u70b9\u51fb \u9884\u89c8 \uff0c\u5e76\u9009\u62e9\u884c\u6570\uff0c\u9884\u89c8Hive\u8868\u4e2d\u7684\u6570\u636e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u8f93\u51fa\u6587\u4ef6\u540d\u79f0\uff0c\u6269\u5c55\u540d\uff1a \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u8f93\u51fa\u65f6\u5c5e\u6027 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u6587\u4ef6\u5b57\u6bb5\u5185\u5bb9\uff0c\u53ef\u4ee5\u70b9\u51fb \u6700\u5c0f\u5bbd\u5ea6 ,\u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u8bbe\u7f6e\u3002 \u8fd0\u884c\u8f6c\u6362\uff0c\u5728\u4e3b\u754c\u9762\u70b9\u51fb\u5de5\u5177\u680f\u5de6\u4fa7\u7684\u4e09\u89d2\u5f62\u8fd0\u884c\u6309\u94ae \u6267\u884c\u7ed3\u679c\uff1a \u5199\u5165Hive\u6570\u636e \u00b6 \u4ee5\u672c\u5730\u6587\u672c\u6587\u4ef6 -> hive\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahive_out.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4\uff0c\u5c06\u6587\u672c\u6587\u4ef6\u8f93\u5165\u548c\u8868\u8f93\u51fa\u4e24\u4e2a\u6b65\u9aa4\u62d6\u5165\u5de5\u4f5c\u533a\uff0c\u8fde\u63a5\u4e24\u4e2a\u6b65\u9aa4; \u53cc\u51fb\u6587\u672c\u6587\u4ef6\u8f93\u5165\uff0c\u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u672c\u5730\u6587\u4ef6\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u6587\u4ef6\u88ab\u6dfb\u52a0\u81f3\u4e0b\u65b9\u9009\u4e2d\u7684\u6587\u4ef6; \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u9650\u5b9a\u7b26\u3001\u7f16\u7801\u7b49\u7b49 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u5b57\u6bb5\u540e\uff0c\u53ef\u4ee5\u70b9\u51fb Minimal Width \u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb\u8868\u8f93\u51fa\uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u8bbe\u7f6e\u76ee\u6807\u8868\uff0c\u8be5\u8868\u9700\u8981\u5df2\u7ecf\u5728Hive\u4e2d\u521b\u5efa\u597d\uff0c\u5e76\u4e14\u5b57\u6bb5\u4e0e\u672c\u5730\u6587\u4ef6\u4fdd\u6301\u4e00\u81f4; \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6\u4e4b\u540e\u518d\u8f7d\u5165Hive\u8868 \u5bf9\u63a5HDFS \u00b6 \u521b\u5efaHadoop Cluster \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199hacluster; JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f21066,\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; ZooKeeper\u7684Hostname \u586b\u5199ZooKeeper\u7684\u4e3b\u8282\u70b9IP\uff0c\u7aef\u53e3\u53f7\u662f24002\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; Oozie\u7684URL\u586b\u5199oozie WebUI\u7684\u5730\u5740. \u70b9\u51fb \u6d4b\u8bd5 \u5bfc\u5165HDFS\u6587\u4ef6 \u00b6 \u4ee5\u672c\u5730\u6587\u4ef6 -> HDFS\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u8bbe\u7f6e\u5206\u9694\u7b26\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5\uff0c\u5e76\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770bHDFS\u6587\u4ef6 \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6 \u5bf9\u63a5HBase \u00b6 \u5bfc\u5165HBASE\u6587\u4ef6 \u00b6 \u4ee5\u672c\u5730\u6587\u4ef6 -> HBase\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> HBase Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e\uff0c\u6ce8\u610f\u5728\u96c6\u7fa4HBase\u4e2d\u8981\u6709\u548c\u5bfc\u5165\u7684\u8868\u76f8\u540c\u7684\u7a7a\u8868\uff0c\u6307\u660e\u5b57\u6bb5\u548c\u5217\u7c07. \u4fee\u6539 HBase Output \u914d\u7f6e \u53cc\u51fb HBase Output \u6b65\u9aa4\uff0c\u5728 Configure connection \u9875\u7b7e\u4e0b\uff0c\u9009\u62e9\u5df2\u7ecf\u914d\u7f6e\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u70b9\u51fb Get table name \uff0c\u83b7\u53d6\u8981\u8f93\u51fa\u7684\u8868,\u70b9\u51fb Get mapping for specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping. \u82e5\u8be5\u8868\u6ca1\u6709\u521b\u5efamapping,\u5728 Create/Edit Mappings \u9875\u7b7e\u521b\u5efamapping,\u6307\u5b9a\u5404\u9879\u5c5e\u6027 \u70b9\u51fb \u786e\u5b9a \uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae,\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u96c6\u7fa4\u4e2d\u7684HBase\u6587\u4ef6 \u6267\u884c hbase shell count 'customer' \u8bfb\u53d6HBASE\u6587\u4ef6 \u00b6 \u4ee5HBase -> \u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahbase.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> HBase Input \u548c \u8f93\u51fa -> \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 HBase Input\u914d\u7f6e \u53cc\u51fb HBase Input \u6b65\u9aa4\uff0c\u5728 Configure query \u9875\u7b7e\uff0c\u9009\u62e9\u5df2\u7ecf\u8fde\u63a5\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u82e5\u65e0\u5df2\u7ecf\u8fde\u63a5\u7684\u96c6\u7fa4\uff0c\u70b9\u51fb new ,\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Hadoop\u96c6\u7fa4\u914d\u7f6e\uff0c\u914d\u7f6e\u8fde\u63a5\u96c6\u7fa4; \u5728 Create/Edit Mappings \u9875\u7b7e\uff0c\u70b9\u51fb Get table names ,\u83b7\u53d6\u96c6\u7fa4\u4e2d\u7684Hbase\u8868\uff0c\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u5728 Mapping name \u4e0b\u62c9\u9009\u62e9\u4e0e\u8be5\u8868\u5173\u8054\u7684map\uff0c\u82e5\u6ca1\u6709\uff0c\u81ea\u5b9a\u4e49\u4e00\u4e2amap\u7684\u540d\u5b57\uff0c\u586b\u5199\u5b57\u6bb5\u548c\u5217\u7c07\uff0c\u5e76\u6307\u5b9a\u5b57\u6bb5\u662f\u5426\u4e3akey\uff0c\u5b57\u6bb5\u7c7b\u578b. \u56de\u5230 Configure query \u9875\u7b7e,\u70b9\u51fb Get mapped table names ,\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u70b9\u51fb Get mappings for the specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping\uff0c\u70b9\u51fb\u53f3\u4e0b\u89d2 Get Key/Feilds Info \uff0c\u83b7\u53d6\u5bf9\u5e94\u7684\u8868\u7684\u4fe1\u606f. \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\uff0c\u586b\u5199\u6587\u4ef6\u540d\u548c\u6269\u5c55\u540d; \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 ,\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6(\u53ef\u9009) \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684\u6587\u4ef6 Linux\u5e73\u53f0 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5RedHat 6.5 \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u8282\u70b9IP host1 \u8282\u70b9IP host2 \u8282\u70b9IP host3 \u82e5\u662f\u684c\u9762\u7248\u64cd\u4f5c\u7cfb\u7edf\uff0cKettle\u5bf9\u63a5\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Windows\u7cfb\u7edf\u4e0b\u7684\u5bf9\u63a5\u65b9\u5f0f. \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\uff0c\u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e0b\uff0c\u914d\u7f6e\u597dKettle\u4e0eFi\u96c6\u7fa4\u7684\u8fde\u63a5\uff0c\u6d4b\u8bd5\u8fde\u901a\u6027,\u5c06Kettle\u7684 data-integration \u76ee\u5f55\u4ee5\u53ca\u5176\u4e0b\u6240\u6709\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u7684 opt \u76ee\u5f55\u4e0b. \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Hive\u5bf9\u63a5 \u00b6 \u5bfc\u51faHive\u8868 \u00b6 \u4ee5Hive->\u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2a\u8f6c\u6362\uff0c\u5728\u5de5\u4f5c\u533a\u4e2d\u653e\u5165 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4fdd\u5b58\u4e3ahive.ktr; \u70b9\u51fb \u8868\u8f93\u5165 \uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u4e2d\u5173\u4e8eHive\u8fde\u63a5\u7684\u914d\u7f6e\uff0c\u53ea\u9700\u4fee\u6539\u8fde\u63a5\u9009\u9879\u4e2d user.keytab \u6587\u4ef6\u6240\u5728\u8def\u5f84\uff0c\u4fee\u6539\u4e3a /etc/user.keytab \u5c06hive.ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c \u6839\u636eKettle\u7248\u672c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt/data-integration/ \u5bf9\u4e8eKettle-8.0\u7248\u672c,\u6267\u884c\u4ee5\u4e0b\u811a\u672c\u6e05\u9664cache\uff08\u53c2\u89c1FAQ1\uff09 sed -i \"s/^org.pentaho\\.clean\\.karaf\\.cache=false/org\\.pentaho\\.clean\\.karaf\\.cache=true/g\" /opt/data-integration/system/karaf/etc/custom.properties \u53ef\u5c06\u5176\u4fdd\u5b58\u4e3a\u811a\u672c\u6587\u4ef6\uff0c\u6bcf\u6b21\u6267\u884c\u547d\u4ee4\u524d\u5148\u6267\u884c\u8be5\u811a\u672c \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./kitchen.sh -file=hive.ktr \u5bf9\u4e8eKettle-8.1\u7248\u672c,\u624b\u52a8\u5220\u9664 /data-integration/system/karaf/caches/pan/data-1 \u76ee\u5f55\u4e0b\u7684cache\u6587\u4ef6 \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./pan.sh -file=hive.ktr * \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b \u4e0a\u4f20\u6587\u4ef6\u81f3Hive \u00b6 \u540cWindows\u64cd\u4f5c\u7cfb\u7edf\u4e0b\u521b\u5efaktr\u6587\u4ef6\u64cd\u4f5c\uff0c\u5728\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u65f6\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5728Hive\u8fde\u63a5\u9009\u9879\u914d\u7f6e\u4fee\u6539\u4e2d user.keytab \u6587\u4ef6\u7684\u8def\u5f84\u4e3a /etc/user.keytab \u5373\u53ef\uff0c\u5c06ktr\u6587\u4ef6\u7f6e\u4e8eLinux\u7cfb\u7edf\u4e2d data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6267\u884c\u547d\u4ee4\u540c\u4e0a\u5c0f\u8282\u4e2d\u64cd\u4f5c\u3002 HDFS & HBase\u6587\u4ef6\u8f93\u51fa \u00b6 \u5c06\u4e0a\u9762\u7ae0\u8282\u521b\u5efa\u7684ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6839\u636eKettle\u7248\u672c\u6267\u884c\u547d\u4ee4(\u540chive)\u5373\u53ef \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b \u4e0a\u4f20\u6587\u4ef6\u81f3HDFS & HBase \u00b6 \u540c \u4e0a\u4f20\u6587\u4ef6\u81f3Hive \u64cd\u4f5c\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u8def\u5f84\u5373\u53ef\u3002 FAQ \u00b6 1.\u5728Linux\u7cfb\u7edf\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u8f6c\u6362\u6216\u8005\u4efb\u52a1\uff0cKettle\u90fd\u4f1a\u751f\u6210\u4e00\u4e9bCache\u6587\u4ef6\uff0c\u5728\u6267\u884c\u4e0b\u4e00\u6b21\u8f6c\u6362/\u4efb\u52a1\u4e4b\u524d\uff0c\u9700\u8981\u6e05\u9664\u8fd9\u4e9bCache\uff0c\u5426\u5728HDFS Hive \u548cHBase\u8fdb\u884c\u8fde\u63a5\u4f20\u8f93\u65f6\u4f1a\u51fa\u9519","title":"Kettle 8.x"},{"location":"Data_Integration/Kettle_8.x/#kettlefusioninsight","text":"","title":"Kettle\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kettle_8.x/#_1","text":"Kettle 8.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Kettle 8.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) \u8bf4\u660e\uff1aKettle 8.1.0\u4ec5\u9650POC\u4f7f\u7528","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kettle_8.x/#windows","text":"","title":"Windows\u5e73\u53f0"},{"location":"Data_Integration/Kettle_8.x/#_2","text":"\u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u96c6\u7fa4\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e\uff0c\u4f8b\u5982\u7528\u6237\u4e3a developuser ; \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u540e\u7f00\u540d\u4fee\u6539\u4e3aini,\u5c06krb5.ini\u6587\u4ef6\u590d\u5236\u5230 C:\\Windows \u76ee\u5f55\u4e0b.","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kettle_8.x/#kettle","text":"\u8f6f\u4ef6\u83b7\u53d6 \u6253\u5f00\u4ee5\u4e0b\u5730\u5740[ https://github.com/pentaho/pentaho-kettle/tree/8.0 ] ( https://github.com/pentaho/pentaho-kettle/tree/8.0 ), \u9009\u62e9DownloadZip\u4e0b\u8f7dKettle8.0\u7248\u672c \u89e3\u538b\u5f97\u5230pdi-ce-8.0.0.0-28; \u83b7\u53d6FusionInsight\u7684\u9002\u914d\u5305\u6587\u4ef6 pentaho-hadoop-shims-hdp26-8.1.0.0-SNAPSHOT.jar \u548c pentaho-hadoop-shims-hdp26-hbase-comparators-8.1.0.0-SNAPSHOT.jar ,\u66ff\u6362\u76ee\u5f55 \\data-integration\\plugins\\pentaho-big-data-plugin\\hadoop-configurations\\hdp26\\ \u4e0b\u7684\u539f\u6709\u6587\u4ef6; \u66ff\u6362hdp26\\lib\u76ee\u5f55\u4e0bHive\u76f8\u5173\u7684jar\u5305\u4ee5\u53cahdp26\\lib\\client\u76ee\u5f55\u4e0bhdfs\u76f8\u5173\u7684jar\u5305 \u5e76\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u5305 \u83b7\u53d6FusionInsightHD\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cHbase\u7b49\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230Fi28\u9002\u914d\u5305\u7684\u6587\u4ef6\u5939\u91cc; \u4fee\u6539core-site.xml\u6587\u4ef6\u4e2d\u4ee5\u4e0b\u5b57\u6bb5\uff1a <name>fs.defaultFS</name> <value>hdfs://hacluster</value> \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6\u53ca\u914d\u7f6e \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 \u4fee\u6539Fi28\u9002\u914d\u5305\u4e2d config.properties \u6587\u4ef6: pentaho.authentication.default.kerberos.keytabLocation=C:/kerberos/user.keytab pentaho.authentication.default.kerberos.conf=C:/kerberos/krb5.conf pentaho.authentication.default.kerberos.principal=developuser@HADOOP.COM \u542f\u52a8kettle Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat,\u8fdb\u5165\u754c\u9762\u540e\uff0c\u5728\u4e0a\u65b9\u83dc\u5355\u680f\u9009\u62e9\u5de5\u5177->Hadoop Distribution,\u9009\u62e9 HortonWorks HDP 2.6.x","title":"\u914d\u7f6e\u5e76\u542f\u52a8Kettle"},{"location":"Data_Integration/Kettle_8.x/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/Kettle_8.x/#hive_1","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u4e3a\u8fde\u63a5\u547d\u540d\uff0c\u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u70b9\u51fb\u6d4b\u8bd5\uff0c\u663e\u793a\u4ee5\u4e0b\u7a97\u53e3\uff0c\u8868\u660e\u6d4b\u8bd5\u6210\u529f \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5","title":"\u521b\u5efaHive\u8fde\u63a5"},{"location":"Data_Integration/Kettle_8.x/#hive_2","text":"\u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4; \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u9009\u62e9 \u662f ,\u8be5\u8868\u7684\u5b57\u6bb5\u5c06\u4f1a\u5305\u542b\u5728SQL\u8bed\u53e5\u4e2d\uff0c \u53ef\u4ee5\u70b9\u51fb \u9884\u89c8 \uff0c\u5e76\u9009\u62e9\u884c\u6570\uff0c\u9884\u89c8Hive\u8868\u4e2d\u7684\u6570\u636e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u8f93\u51fa\u6587\u4ef6\u540d\u79f0\uff0c\u6269\u5c55\u540d\uff1a \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u8f93\u51fa\u65f6\u5c5e\u6027 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u6587\u4ef6\u5b57\u6bb5\u5185\u5bb9\uff0c\u53ef\u4ee5\u70b9\u51fb \u6700\u5c0f\u5bbd\u5ea6 ,\u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u8bbe\u7f6e\u3002 \u8fd0\u884c\u8f6c\u6362\uff0c\u5728\u4e3b\u754c\u9762\u70b9\u51fb\u5de5\u5177\u680f\u5de6\u4fa7\u7684\u4e09\u89d2\u5f62\u8fd0\u884c\u6309\u94ae \u6267\u884c\u7ed3\u679c\uff1a","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_8.x/#hive_3","text":"\u4ee5\u672c\u5730\u6587\u672c\u6587\u4ef6 -> hive\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahive_out.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4\uff0c\u5c06\u6587\u672c\u6587\u4ef6\u8f93\u5165\u548c\u8868\u8f93\u51fa\u4e24\u4e2a\u6b65\u9aa4\u62d6\u5165\u5de5\u4f5c\u533a\uff0c\u8fde\u63a5\u4e24\u4e2a\u6b65\u9aa4; \u53cc\u51fb\u6587\u672c\u6587\u4ef6\u8f93\u5165\uff0c\u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u672c\u5730\u6587\u4ef6\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u6587\u4ef6\u88ab\u6dfb\u52a0\u81f3\u4e0b\u65b9\u9009\u4e2d\u7684\u6587\u4ef6; \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u9650\u5b9a\u7b26\u3001\u7f16\u7801\u7b49\u7b49 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u5b57\u6bb5\u540e\uff0c\u53ef\u4ee5\u70b9\u51fb Minimal Width \u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb\u8868\u8f93\u51fa\uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u8bbe\u7f6e\u76ee\u6807\u8868\uff0c\u8be5\u8868\u9700\u8981\u5df2\u7ecf\u5728Hive\u4e2d\u521b\u5efa\u597d\uff0c\u5e76\u4e14\u5b57\u6bb5\u4e0e\u672c\u5730\u6587\u4ef6\u4fdd\u6301\u4e00\u81f4; \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6\u4e4b\u540e\u518d\u8f7d\u5165Hive\u8868","title":"\u5199\u5165Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_8.x/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/Kettle_8.x/#hadoop-cluster","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199hacluster; JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f21066,\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; ZooKeeper\u7684Hostname \u586b\u5199ZooKeeper\u7684\u4e3b\u8282\u70b9IP\uff0c\u7aef\u53e3\u53f7\u662f24002\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; Oozie\u7684URL\u586b\u5199oozie WebUI\u7684\u5730\u5740. \u70b9\u51fb \u6d4b\u8bd5","title":"\u521b\u5efaHadoop Cluster"},{"location":"Data_Integration/Kettle_8.x/#hdfs_1","text":"\u4ee5\u672c\u5730\u6587\u4ef6 -> HDFS\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u8bbe\u7f6e\u5206\u9694\u7b26\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5\uff0c\u5e76\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770bHDFS\u6587\u4ef6","title":"\u5bfc\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#hdfs_2","text":"\u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#hbase","text":"","title":"\u5bf9\u63a5HBase"},{"location":"Data_Integration/Kettle_8.x/#hbase_1","text":"\u4ee5\u672c\u5730\u6587\u4ef6 -> HBase\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> HBase Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e\uff0c\u6ce8\u610f\u5728\u96c6\u7fa4HBase\u4e2d\u8981\u6709\u548c\u5bfc\u5165\u7684\u8868\u76f8\u540c\u7684\u7a7a\u8868\uff0c\u6307\u660e\u5b57\u6bb5\u548c\u5217\u7c07. \u4fee\u6539 HBase Output \u914d\u7f6e \u53cc\u51fb HBase Output \u6b65\u9aa4\uff0c\u5728 Configure connection \u9875\u7b7e\u4e0b\uff0c\u9009\u62e9\u5df2\u7ecf\u914d\u7f6e\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u70b9\u51fb Get table name \uff0c\u83b7\u53d6\u8981\u8f93\u51fa\u7684\u8868,\u70b9\u51fb Get mapping for specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping. \u82e5\u8be5\u8868\u6ca1\u6709\u521b\u5efamapping,\u5728 Create/Edit Mappings \u9875\u7b7e\u521b\u5efamapping,\u6307\u5b9a\u5404\u9879\u5c5e\u6027 \u70b9\u51fb \u786e\u5b9a \uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae,\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u96c6\u7fa4\u4e2d\u7684HBase\u6587\u4ef6 \u6267\u884c hbase shell count 'customer'","title":"\u5bfc\u5165HBASE\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#hbase_2","text":"\u4ee5HBase -> \u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahbase.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> HBase Input \u548c \u8f93\u51fa -> \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 HBase Input\u914d\u7f6e \u53cc\u51fb HBase Input \u6b65\u9aa4\uff0c\u5728 Configure query \u9875\u7b7e\uff0c\u9009\u62e9\u5df2\u7ecf\u8fde\u63a5\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u82e5\u65e0\u5df2\u7ecf\u8fde\u63a5\u7684\u96c6\u7fa4\uff0c\u70b9\u51fb new ,\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Hadoop\u96c6\u7fa4\u914d\u7f6e\uff0c\u914d\u7f6e\u8fde\u63a5\u96c6\u7fa4; \u5728 Create/Edit Mappings \u9875\u7b7e\uff0c\u70b9\u51fb Get table names ,\u83b7\u53d6\u96c6\u7fa4\u4e2d\u7684Hbase\u8868\uff0c\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u5728 Mapping name \u4e0b\u62c9\u9009\u62e9\u4e0e\u8be5\u8868\u5173\u8054\u7684map\uff0c\u82e5\u6ca1\u6709\uff0c\u81ea\u5b9a\u4e49\u4e00\u4e2amap\u7684\u540d\u5b57\uff0c\u586b\u5199\u5b57\u6bb5\u548c\u5217\u7c07\uff0c\u5e76\u6307\u5b9a\u5b57\u6bb5\u662f\u5426\u4e3akey\uff0c\u5b57\u6bb5\u7c7b\u578b. \u56de\u5230 Configure query \u9875\u7b7e,\u70b9\u51fb Get mapped table names ,\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u70b9\u51fb Get mappings for the specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping\uff0c\u70b9\u51fb\u53f3\u4e0b\u89d2 Get Key/Feilds Info \uff0c\u83b7\u53d6\u5bf9\u5e94\u7684\u8868\u7684\u4fe1\u606f. \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\uff0c\u586b\u5199\u6587\u4ef6\u540d\u548c\u6269\u5c55\u540d; \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 ,\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6(\u53ef\u9009) \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684\u6587\u4ef6","title":"\u8bfb\u53d6HBASE\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#linux","text":"","title":"Linux\u5e73\u53f0"},{"location":"Data_Integration/Kettle_8.x/#_3","text":"\u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5RedHat 6.5 \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u8282\u70b9IP host1 \u8282\u70b9IP host2 \u8282\u70b9IP host3 \u82e5\u662f\u684c\u9762\u7248\u64cd\u4f5c\u7cfb\u7edf\uff0cKettle\u5bf9\u63a5\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Windows\u7cfb\u7edf\u4e0b\u7684\u5bf9\u63a5\u65b9\u5f0f. \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\uff0c\u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e0b\uff0c\u914d\u7f6e\u597dKettle\u4e0eFi\u96c6\u7fa4\u7684\u8fde\u63a5\uff0c\u6d4b\u8bd5\u8fde\u901a\u6027,\u5c06Kettle\u7684 data-integration \u76ee\u5f55\u4ee5\u53ca\u5176\u4e0b\u6240\u6709\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u7684 opt \u76ee\u5f55\u4e0b. \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kettle_8.x/#hive_4","text":"","title":"Hive\u5bf9\u63a5"},{"location":"Data_Integration/Kettle_8.x/#hive_5","text":"\u4ee5Hive->\u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2a\u8f6c\u6362\uff0c\u5728\u5de5\u4f5c\u533a\u4e2d\u653e\u5165 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4fdd\u5b58\u4e3ahive.ktr; \u70b9\u51fb \u8868\u8f93\u5165 \uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u4e2d\u5173\u4e8eHive\u8fde\u63a5\u7684\u914d\u7f6e\uff0c\u53ea\u9700\u4fee\u6539\u8fde\u63a5\u9009\u9879\u4e2d user.keytab \u6587\u4ef6\u6240\u5728\u8def\u5f84\uff0c\u4fee\u6539\u4e3a /etc/user.keytab \u5c06hive.ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c \u6839\u636eKettle\u7248\u672c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt/data-integration/ \u5bf9\u4e8eKettle-8.0\u7248\u672c,\u6267\u884c\u4ee5\u4e0b\u811a\u672c\u6e05\u9664cache\uff08\u53c2\u89c1FAQ1\uff09 sed -i \"s/^org.pentaho\\.clean\\.karaf\\.cache=false/org\\.pentaho\\.clean\\.karaf\\.cache=true/g\" /opt/data-integration/system/karaf/etc/custom.properties \u53ef\u5c06\u5176\u4fdd\u5b58\u4e3a\u811a\u672c\u6587\u4ef6\uff0c\u6bcf\u6b21\u6267\u884c\u547d\u4ee4\u524d\u5148\u6267\u884c\u8be5\u811a\u672c \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./kitchen.sh -file=hive.ktr \u5bf9\u4e8eKettle-8.1\u7248\u672c,\u624b\u52a8\u5220\u9664 /data-integration/system/karaf/caches/pan/data-1 \u76ee\u5f55\u4e0b\u7684cache\u6587\u4ef6 \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./pan.sh -file=hive.ktr * \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b","title":"\u5bfc\u51faHive\u8868"},{"location":"Data_Integration/Kettle_8.x/#hive_6","text":"\u540cWindows\u64cd\u4f5c\u7cfb\u7edf\u4e0b\u521b\u5efaktr\u6587\u4ef6\u64cd\u4f5c\uff0c\u5728\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u65f6\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5728Hive\u8fde\u63a5\u9009\u9879\u914d\u7f6e\u4fee\u6539\u4e2d user.keytab \u6587\u4ef6\u7684\u8def\u5f84\u4e3a /etc/user.keytab \u5373\u53ef\uff0c\u5c06ktr\u6587\u4ef6\u7f6e\u4e8eLinux\u7cfb\u7edf\u4e2d data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6267\u884c\u547d\u4ee4\u540c\u4e0a\u5c0f\u8282\u4e2d\u64cd\u4f5c\u3002","title":"\u4e0a\u4f20\u6587\u4ef6\u81f3Hive"},{"location":"Data_Integration/Kettle_8.x/#hdfs-hbase","text":"\u5c06\u4e0a\u9762\u7ae0\u8282\u521b\u5efa\u7684ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6839\u636eKettle\u7248\u672c\u6267\u884c\u547d\u4ee4(\u540chive)\u5373\u53ef \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b","title":"HDFS &amp; HBase\u6587\u4ef6\u8f93\u51fa"},{"location":"Data_Integration/Kettle_8.x/#hdfs-hbase_1","text":"\u540c \u4e0a\u4f20\u6587\u4ef6\u81f3Hive \u64cd\u4f5c\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u8def\u5f84\u5373\u53ef\u3002","title":"\u4e0a\u4f20\u6587\u4ef6\u81f3HDFS &amp; HBase"},{"location":"Data_Integration/Kettle_8.x/#faq","text":"1.\u5728Linux\u7cfb\u7edf\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u8f6c\u6362\u6216\u8005\u4efb\u52a1\uff0cKettle\u90fd\u4f1a\u751f\u6210\u4e00\u4e9bCache\u6587\u4ef6\uff0c\u5728\u6267\u884c\u4e0b\u4e00\u6b21\u8f6c\u6362/\u4efb\u52a1\u4e4b\u524d\uff0c\u9700\u8981\u6e05\u9664\u8fd9\u4e9bCache\uff0c\u5426\u5728HDFS Hive \u548cHBase\u8fdb\u884c\u8fde\u63a5\u4f20\u8f93\u65f6\u4f1a\u51fa\u9519","title":"FAQ"},{"location":"Data_Integration/Knime/","text":"Knime \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Knime 3.6.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Spark) Knime 3.6.1 \u2194 FusionInsight HD 6.5 (HDFS/Hive) \u73af\u5883\u51c6\u5907\u4ee5\u53caKnime\u4e0b\u8f7d \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u4e0b\u8f7dKnime \u00b6 \u5728Knime\u5b98\u7f51 https://www.knime.com/downloads/download-knime \u9009\u62e9\u5408\u9002\u7684\u5b89\u88c5\u5305\u8fdb\u884c\u4e0b\u8f7d. \u4e0b\u8f7dKnime extension \u00b6 \u5728\u83dc\u5355\u680f File->Install Knime extensions \u641c\u7d22 big data ,\u5728\u7ed3\u679c\u4e2d\u9009\u62e9 KNIME Big data Extensions ,\u7136\u540e next accept licence ,\u70b9\u51fb finish \u5f00\u59cb\u5b89\u88c5. \u5728\u53f3\u4e0b\u89d2\u53ef\u4ee5\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6 \u5b89\u88c5\u5b8c\u6210\u540e\u91cd\u542fKnime \u914d\u7f6eKnime \u00b6 \u83b7\u53d6\u96c6\u7fa4\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6\uff0c\u4fdd\u5b58\u5728\u672c\u5730. \u5728Knime\u7684\u5b89\u88c5\u76ee\u5f55\u4e2d\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u201cknime.ini\u201d,\u5728\u672b\u5c3e\u6dfb\u52a0\u4e00\u884c `Djava.security.krb5.conf=path\\to\\krb5.conf` \u53cc\u51fb Knime.exe \uff0c\u542f\u52a8Knime \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Hadoop \uff0c\u5728 Hadoop Configuration \u4e2d\u586b\u5165\u672c\u5730\u4fdd\u5b58\u7684HDFS\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Kerberos \uff0c\u586b\u5165kerberos\u8ba4\u8bc1\u7528\u6237\u540d\u548c\u672c\u5730keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5e76\u9009\u62e9 Enable Kerberos Logging ,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 Knime\u8fde\u63a5HDFS \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5efa\u7acbHDFS\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 * \u5728Node Repository\u4e2d\u641c\u7d22 HDFS \u5c06 HDFS Connection \u8282\u70b9\u62d6\u5165\u5de5\u4f5c\u533a \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Host: NameNode\u4e3b\u8282\u70b9 Port: 25000 Authentication: Kerberos \u70b9\u51fb Test connection ,\u663e\u793a\u5982\u4e0b\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f - \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 Download \u8282\u70b9\uff0c\u5c06\u5176\u4e0e HDFS Connection \u76f8\u8fde \u53cc\u51fb Download \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4eceHDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u7684\u6587\u4ef6\u4ee5\u53ca\u6587\u4ef6\u7684\u672c\u5730\u4fdd\u5b58\u8def\u5f84 \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u67e5\u770b\u672c\u5730\u6587\u4ef6 \u4e0a\u4f20\u6587\u4ef6\u81f3HDFS \u00b6 \u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u653e\u5728\u672c\u5730\u7684\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u4f8b\u5982 C:\\KnimeData \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 List Files , String to URI \u4ee5\u53ca Upload \u8282\u70b9\uff0c\u5c06\u5176\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u53cc\u51fb List Files \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4e0a\u4f20\u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53cc\u51fb Upload \u8282\u70b9\uff0c\u9009\u62e9\u5728HDFS\u4e2d\u6587\u4ef6\u4fdd\u5b58\u7684\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770bHDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u6240\u4e0a\u4f20\u7684\u6587\u4ef6 Knime\u8fde\u63a5Hive \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5efa\u7acbHive\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Hive Connector \u8282\u70b9 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Hostname: HIve\u4e3b\u8282\u70b9 Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u5199\u5165Hive\u8868 \u00b6 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u8282\u70b9\uff0c\u5e76\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u5176\u4e2d HDFS Connection \u8282\u70b9\u914d\u7f6e\u53c2\u8003\u4e0a\u8282\u4e2d\u5efa\u7acbHDFS\u8fde\u63a5\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e File Reader \u8282\u70b9\u4e2d\u9009\u62e9\u672c\u5730\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e Hive Loader \u8282\u70b9\u4e2d\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u7684\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868 Knime\u8fde\u63a5Spark \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5b89\u88c5spark-job-server \u00b6 \u6b64\u90e8\u5206\u53ef\u53c2\u8003KNIME\u5b98\u65b9\u6587\u6863 https://download.knime.org/store/3.6/knime_extension_for_apache_spark_2.3.0.pdf \u6253\u5f00 https://www.knime.com/knime-extension-for-apache-spark \uff0c\u6839\u636e\u96c6\u7fa4\u4ee5\u53ca\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u83b7\u53d6\u5bf9\u5e94\u7684 spark-job-server \u5b89\u88c5\u5305\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u4f8b\u5982 /opt \u76ee\u5f55\u4e0b\u3002 \u5bf9\u4e8eFusionInsight\u96c6\u7fa4\uff0cspark\u7248\u672c\u4e3a1.5\u548c2.1\uff0c\u6839\u636e\u8981\u4f7f\u7528\u7684spark\u7248\u672c\u9009\u62e9\u5bf9\u5e94\u7684spark-job-server\u8fdb\u884c\u5b89\u88c5\uff0c\u8fd9\u91cc\u4ee5spark2.1\u4e3a\u4f8b \u5bf9\u4e8e\u4f7f\u7528spark2x\u7684\u96c6\u7fa4\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5\u914d\u7f6e LINKNAME=spark2-job-server useradd -d /opt/${LINKNAME}/ -M -r -s /bin/false spark-job-server su -l -c \"hdfs dfs -mkdir -p /user/spark-job-server ; hdfs dfs -chown -R spark-job-server /user/spark-job-server\" hdfs cp /path/to/spark-job-server-xxx.tar.gz /opt cd /opt tar xzf spark-job-server-xxx.tar.gz ln -s spark-job-server-xxx ${LINKNAME} chown -R spark-job-server:spark-job-server ${LINKNAME} spark-job-server-xxx/ \u5bf9\u4e8e RHEL 6.x/CentOS 6.x \u64cd\u4f5c\u7cfb\u7edf\uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} chkconfig --levels 2345 ${LINKNAME} on \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a RHEL 7.x/CentOS 7.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} systemctl daemon-reload systemctl enable ${LINKNAME} \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 14.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d-ubuntu-sysv /etc/init.d/${LINKNAME} update-rc.d ${LINKNAME} start 20 2 3 4 5 . stop 20 0 1 6 . \u4fee\u6539 environment.conf \u6587\u4ef6,\u8bbe\u7f6e master = yarn-client \uff0c\u4ee5yarn-client\u6a21\u5f0f\u8fd0\u884cspark. \u4fee\u6539 settings.sh \u6587\u4ef6\uff0c\u8bbe\u7f6e SPARK_HOME=/opt/hadoopclient/Spark2x/spark \u914d\u7f6eKerberos\u5b89\u5168\u8ba4\u8bc1 \u00b6 \u5c06Kerberos\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u5e76\u6267\u884c\u4e00\u4e0b\u547d\u4ee4 chown spark-job-server:spark-job-server /path/to/keytab chmod go= /path/to/keytab \u5728 environment.conf \u6587\u4ef6\u4e2d\uff0c\u8fdb\u884c\u5982\u4e0b\u8bbe\u7f6e spark { jobserver { context-per-jvm = true } } shiro { authentication = on config.path = \"shiro.ini\" use-as-proxy-user = on } \u5728 setting.sh \u6587\u4ef6\u4e2d\uff0c\u7f16\u8f91\u4ee5\u4e0b\u51e0\u884c\uff0c\u914d\u7f6e\u5bf9\u5e94\u7684keytab\u6587\u4ef6\u8def\u5f84\u4ee5\u53ca\u7528\u6237principal export JOBSERVER_KEYTAB=/path/to/keytab export JOBSERVER_PRINCIPAL=user/host@REALM \u5728FusionInsight\u7684manager\u7ba1\u7406\u9875\u9762\uff0c\u4fee\u6539HDFS\u7684core-site.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4e3b\u9875\u9762\u9009\u62e9 \u670d\u52a1\u7ba1\u7406->HDFS->\u670d\u52a1\u914d\u7f6e ,\u53c2\u6570\u914d\u7f6e\u9009\u62e9 \u5168\u90e8\u670d\u52a1 \uff0c\u5728\u5de6\u4fa7\u9009\u62e9 HDFS->\u81ea\u5b9a\u4e49 ,\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570 hadoop.proxyuser.spark-job-server.hosts = * hadoop.proxyuser.spark-job-server.groups = * \u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1\u3002 \u542f\u52a8\u548c\u505c\u6b62Spark-job-server \u00b6 \u542f\u52a8Spark-job-server cd /etc/init.d ./spark2-job-server start \u542f\u52a8\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165http://ip:8090,\u53ef\u4ee5\u770b\u5230\u4ee5\u4e0b\u754c\u9762 \u505c\u6b62Spark-job-server cd /etc/init.d ./spark2-job-server stop \u5efa\u7acbSpark\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58,\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Create Spark Context \u8282\u70b9\uff0c\u53cc\u51fb\u540e\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u5728Context Settings\u9875\u9762 Spark version:\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c Context name\uff1a\u5efa\u7acb\u7684Spark Context \u540d\u5b57 \u5728Connection Settings\u9875\u9762\uff0cIP\u4e3aspark job server \u6240\u5728\u8282\u70b9IP Jobserver URL:http://ip:8090/ Authentication: None \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53ef\u70b9\u51fb\u83dc\u5355\u680f \u6309\u94ae\uff0c\u6d4b\u8bd5\u8fde\u63a5\u662f\u5426\u6709\u9519\uff0c\u82e5\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u8282\u70b9\u914d\u7f6e\u65e0\u8bef\u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00Jobserver URL\u4e2d\u914d\u7f6e\u7684\u5730\u5740\uff0c\u53ef\u4ee5\u8fdb\u5165Spark Job Server UI\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u5efa\u7acb\u7684Spark Context\uff0c\u663e\u793a\u5982\u4e0b\uff1a Spark\u5e94\u7528\u5b9e\u4f8b \u00b6 Spark\u5e94\u7528\u5b9e\u4f8b\u4e0b\u8f7d\u5730\u5740 https://www.knime.com/nodeguide/big-data/spark-executor Hive to Spark to Hive \u00b6 \u4e0b\u8f7d\u5b8c\u6210\u6253\u5f00\u5e94\u7528\u5b9e\u4f8b\uff0c\u914d\u7f6e HDFS Connection \uff0c File Reader \uff0c Hive Connector \uff0c Hive Loader \uff0c Create Spark Context \u548c Spark to Hive \u8282\u70b9\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u767b\u5f55\u8282\u70b9\uff0c\u6267\u884c beeline \u8fdb\u5165Hive\u754c\u9762\uff0c\u6267\u884c show tables; \u67e5\u770b\u5bfc\u5165\u7684\u8868. \u53ef\u4ee5\u770b\u5230\uff0c\u901a\u8fc7 Hive Loader \u8282\u70b9\u5bfc\u5165\u7684\u8868 contactdata \u4ee5\u53ca Spark to Hive \u8282\u70b9\u5bfc\u5165\u7684\u8868 sparktohivetable \u5747\u5df2\u5bfc\u5165Hive\u3002","title":"3.6.1 <--> 6.5"},{"location":"Data_Integration/Knime/#knime-fusioninsight","text":"","title":"Knime \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Knime/#_1","text":"Knime 3.6.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Spark) Knime 3.6.1 \u2194 FusionInsight HD 6.5 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Knime/#knime","text":"","title":"\u73af\u5883\u51c6\u5907\u4ee5\u53caKnime\u4e0b\u8f7d"},{"location":"Data_Integration/Knime/#_2","text":"\u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin;","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Knime/#knime_1","text":"\u5728Knime\u5b98\u7f51 https://www.knime.com/downloads/download-knime \u9009\u62e9\u5408\u9002\u7684\u5b89\u88c5\u5305\u8fdb\u884c\u4e0b\u8f7d.","title":"\u4e0b\u8f7dKnime"},{"location":"Data_Integration/Knime/#knime-extension","text":"\u5728\u83dc\u5355\u680f File->Install Knime extensions \u641c\u7d22 big data ,\u5728\u7ed3\u679c\u4e2d\u9009\u62e9 KNIME Big data Extensions ,\u7136\u540e next accept licence ,\u70b9\u51fb finish \u5f00\u59cb\u5b89\u88c5. \u5728\u53f3\u4e0b\u89d2\u53ef\u4ee5\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6 \u5b89\u88c5\u5b8c\u6210\u540e\u91cd\u542fKnime","title":"\u4e0b\u8f7dKnime extension"},{"location":"Data_Integration/Knime/#knime_2","text":"\u83b7\u53d6\u96c6\u7fa4\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6\uff0c\u4fdd\u5b58\u5728\u672c\u5730. \u5728Knime\u7684\u5b89\u88c5\u76ee\u5f55\u4e2d\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u201cknime.ini\u201d,\u5728\u672b\u5c3e\u6dfb\u52a0\u4e00\u884c `Djava.security.krb5.conf=path\\to\\krb5.conf` \u53cc\u51fb Knime.exe \uff0c\u542f\u52a8Knime \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Hadoop \uff0c\u5728 Hadoop Configuration \u4e2d\u586b\u5165\u672c\u5730\u4fdd\u5b58\u7684HDFS\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preference->KNIME->Big Data->Kerberos \uff0c\u586b\u5165kerberos\u8ba4\u8bc1\u7528\u6237\u540d\u548c\u672c\u5730keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5e76\u9009\u62e9 Enable Kerberos Logging ,\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002","title":"\u914d\u7f6eKnime"},{"location":"Data_Integration/Knime/#knimehdfs","text":"","title":"Knime\u8fde\u63a5HDFS"},{"location":"Data_Integration/Knime/#_3","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#hdfs","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 * \u5728Node Repository\u4e2d\u641c\u7d22 HDFS \u5c06 HDFS Connection \u8282\u70b9\u62d6\u5165\u5de5\u4f5c\u533a \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Host: NameNode\u4e3b\u8282\u70b9 Port: 25000 Authentication: Kerberos \u70b9\u51fb Test connection ,\u663e\u793a\u5982\u4e0b\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f - \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e","title":"\u5efa\u7acbHDFS\u8fde\u63a5"},{"location":"Data_Integration/Knime/#hdfs_1","text":"\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 Download \u8282\u70b9\uff0c\u5c06\u5176\u4e0e HDFS Connection \u76f8\u8fde \u53cc\u51fb Download \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4eceHDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u7684\u6587\u4ef6\u4ee5\u53ca\u6587\u4ef6\u7684\u672c\u5730\u4fdd\u5b58\u8def\u5f84 \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u67e5\u770b\u672c\u5730\u6587\u4ef6","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Knime/#hdfs_2","text":"\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u653e\u5728\u672c\u5730\u7684\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u4f8b\u5982 C:\\KnimeData \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 List Files , String to URI \u4ee5\u53ca Upload \u8282\u70b9\uff0c\u5c06\u5176\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u53cc\u51fb List Files \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4e0a\u4f20\u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53cc\u51fb Upload \u8282\u70b9\uff0c\u9009\u62e9\u5728HDFS\u4e2d\u6587\u4ef6\u4fdd\u5b58\u7684\u8def\u5f84\uff0c\u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770bHDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u6240\u4e0a\u4f20\u7684\u6587\u4ef6","title":"\u4e0a\u4f20\u6587\u4ef6\u81f3HDFS"},{"location":"Data_Integration/Knime/#knimehive","text":"","title":"Knime\u8fde\u63a5Hive"},{"location":"Data_Integration/Knime/#_4","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#hive","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58\u3002 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Hive Connector \u8282\u70b9 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u5982\u4e0b\u914d\u7f6e\uff1a Hostname: HIve\u4e3b\u8282\u70b9 Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e","title":"\u5efa\u7acbHive\u8fde\u63a5"},{"location":"Data_Integration/Knime/#hive_1","text":"\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u8282\u70b9\uff0c\u5e76\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5 \u5176\u4e2d HDFS Connection \u8282\u70b9\u914d\u7f6e\u53c2\u8003\u4e0a\u8282\u4e2d\u5efa\u7acbHDFS\u8fde\u63a5\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e File Reader \u8282\u70b9\u4e2d\u9009\u62e9\u672c\u5730\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e Hive Loader \u8282\u70b9\u4e2d\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u7684\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb Apply \u4fdd\u5b58\u914d\u7f6e \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868","title":"\u5199\u5165Hive\u8868"},{"location":"Data_Integration/Knime/#knimespark","text":"","title":"Knime\u8fde\u63a5Spark"},{"location":"Data_Integration/Knime/#_5","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#spark-job-server","text":"\u6b64\u90e8\u5206\u53ef\u53c2\u8003KNIME\u5b98\u65b9\u6587\u6863 https://download.knime.org/store/3.6/knime_extension_for_apache_spark_2.3.0.pdf \u6253\u5f00 https://www.knime.com/knime-extension-for-apache-spark \uff0c\u6839\u636e\u96c6\u7fa4\u4ee5\u53ca\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u83b7\u53d6\u5bf9\u5e94\u7684 spark-job-server \u5b89\u88c5\u5305\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u4f8b\u5982 /opt \u76ee\u5f55\u4e0b\u3002 \u5bf9\u4e8eFusionInsight\u96c6\u7fa4\uff0cspark\u7248\u672c\u4e3a1.5\u548c2.1\uff0c\u6839\u636e\u8981\u4f7f\u7528\u7684spark\u7248\u672c\u9009\u62e9\u5bf9\u5e94\u7684spark-job-server\u8fdb\u884c\u5b89\u88c5\uff0c\u8fd9\u91cc\u4ee5spark2.1\u4e3a\u4f8b \u5bf9\u4e8e\u4f7f\u7528spark2x\u7684\u96c6\u7fa4\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5\u914d\u7f6e LINKNAME=spark2-job-server useradd -d /opt/${LINKNAME}/ -M -r -s /bin/false spark-job-server su -l -c \"hdfs dfs -mkdir -p /user/spark-job-server ; hdfs dfs -chown -R spark-job-server /user/spark-job-server\" hdfs cp /path/to/spark-job-server-xxx.tar.gz /opt cd /opt tar xzf spark-job-server-xxx.tar.gz ln -s spark-job-server-xxx ${LINKNAME} chown -R spark-job-server:spark-job-server ${LINKNAME} spark-job-server-xxx/ \u5bf9\u4e8e RHEL 6.x/CentOS 6.x \u64cd\u4f5c\u7cfb\u7edf\uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} chkconfig --levels 2345 ${LINKNAME} on \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a RHEL 7.x/CentOS 7.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d /etc/init.d/${LINKNAME} systemctl daemon-reload systemctl enable ${LINKNAME} \u82e5\u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 14.x \uff0c\u6267\u884c\uff1a ln -s /opt/${LINKNAME}/spark-job-server-init.d-ubuntu-sysv /etc/init.d/${LINKNAME} update-rc.d ${LINKNAME} start 20 2 3 4 5 . stop 20 0 1 6 . \u4fee\u6539 environment.conf \u6587\u4ef6,\u8bbe\u7f6e master = yarn-client \uff0c\u4ee5yarn-client\u6a21\u5f0f\u8fd0\u884cspark. \u4fee\u6539 settings.sh \u6587\u4ef6\uff0c\u8bbe\u7f6e SPARK_HOME=/opt/hadoopclient/Spark2x/spark","title":"\u5b89\u88c5spark-job-server"},{"location":"Data_Integration/Knime/#kerberos","text":"\u5c06Kerberos\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8282\u70b9\uff0c\u5e76\u6267\u884c\u4e00\u4e0b\u547d\u4ee4 chown spark-job-server:spark-job-server /path/to/keytab chmod go= /path/to/keytab \u5728 environment.conf \u6587\u4ef6\u4e2d\uff0c\u8fdb\u884c\u5982\u4e0b\u8bbe\u7f6e spark { jobserver { context-per-jvm = true } } shiro { authentication = on config.path = \"shiro.ini\" use-as-proxy-user = on } \u5728 setting.sh \u6587\u4ef6\u4e2d\uff0c\u7f16\u8f91\u4ee5\u4e0b\u51e0\u884c\uff0c\u914d\u7f6e\u5bf9\u5e94\u7684keytab\u6587\u4ef6\u8def\u5f84\u4ee5\u53ca\u7528\u6237principal export JOBSERVER_KEYTAB=/path/to/keytab export JOBSERVER_PRINCIPAL=user/host@REALM \u5728FusionInsight\u7684manager\u7ba1\u7406\u9875\u9762\uff0c\u4fee\u6539HDFS\u7684core-site.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4e3b\u9875\u9762\u9009\u62e9 \u670d\u52a1\u7ba1\u7406->HDFS->\u670d\u52a1\u914d\u7f6e ,\u53c2\u6570\u914d\u7f6e\u9009\u62e9 \u5168\u90e8\u670d\u52a1 \uff0c\u5728\u5de6\u4fa7\u9009\u62e9 HDFS->\u81ea\u5b9a\u4e49 ,\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570 hadoop.proxyuser.spark-job-server.hosts = * hadoop.proxyuser.spark-job-server.groups = * \u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1\u3002","title":"\u914d\u7f6eKerberos\u5b89\u5168\u8ba4\u8bc1"},{"location":"Data_Integration/Knime/#spark-job-server_1","text":"\u542f\u52a8Spark-job-server cd /etc/init.d ./spark2-job-server start \u542f\u52a8\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165http://ip:8090,\u53ef\u4ee5\u770b\u5230\u4ee5\u4e0b\u754c\u9762 \u505c\u6b62Spark-job-server cd /etc/init.d ./spark2-job-server stop","title":"\u542f\u52a8\u548c\u505c\u6b62Spark-job-server"},{"location":"Data_Integration/Knime/#spark","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow ,\u547d\u540d\u540e\u4fdd\u5b58,\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Create Spark Context \u8282\u70b9\uff0c\u53cc\u51fb\u540e\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u5728Context Settings\u9875\u9762 Spark version:\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c Context name\uff1a\u5efa\u7acb\u7684Spark Context \u540d\u5b57 \u5728Connection Settings\u9875\u9762\uff0cIP\u4e3aspark job server \u6240\u5728\u8282\u70b9IP Jobserver URL:http://ip:8090/ Authentication: None \u70b9\u51fb Apply \uff0c\u4fdd\u5b58\u914d\u7f6e \u53ef\u70b9\u51fb\u83dc\u5355\u680f \u6309\u94ae\uff0c\u6d4b\u8bd5\u8fde\u63a5\u662f\u5426\u6709\u9519\uff0c\u82e5\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u8282\u70b9\u914d\u7f6e\u65e0\u8bef\u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00Jobserver URL\u4e2d\u914d\u7f6e\u7684\u5730\u5740\uff0c\u53ef\u4ee5\u8fdb\u5165Spark Job Server UI\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u5efa\u7acb\u7684Spark Context\uff0c\u663e\u793a\u5982\u4e0b\uff1a","title":"\u5efa\u7acbSpark\u8fde\u63a5"},{"location":"Data_Integration/Knime/#spark_1","text":"Spark\u5e94\u7528\u5b9e\u4f8b\u4e0b\u8f7d\u5730\u5740 https://www.knime.com/nodeguide/big-data/spark-executor","title":"Spark\u5e94\u7528\u5b9e\u4f8b"},{"location":"Data_Integration/Knime/#hive-to-spark-to-hive","text":"\u4e0b\u8f7d\u5b8c\u6210\u6253\u5f00\u5e94\u7528\u5b9e\u4f8b\uff0c\u914d\u7f6e HDFS Connection \uff0c File Reader \uff0c Hive Connector \uff0c Hive Loader \uff0c Create Spark Context \u548c Spark to Hive \u8282\u70b9\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1 \u767b\u5f55\u8282\u70b9\uff0c\u6267\u884c beeline \u8fdb\u5165Hive\u754c\u9762\uff0c\u6267\u884c show tables; \u67e5\u770b\u5bfc\u5165\u7684\u8868. \u53ef\u4ee5\u770b\u5230\uff0c\u901a\u8fc7 Hive Loader \u8282\u70b9\u5bfc\u5165\u7684\u8868 contactdata \u4ee5\u53ca Spark to Hive \u8282\u70b9\u5bfc\u5165\u7684\u8868 sparktohivetable \u5747\u5df2\u5bfc\u5165Hive\u3002","title":"Hive to Spark to Hive"},{"location":"Data_Integration/OceanSource/","text":"\u4e2d\u65b0\u8d5b\u514bOceanSource\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 OceanSource 1.0 \u2194 FusionInsight HD V100R002C80 (HBase/Hive/Kafka/ElasticSearch/Redis/GaussDB)","title":"1.0 <--> C80"},{"location":"Data_Integration/OceanSource/#oceansourcefusioninsight","text":"","title":"\u4e2d\u65b0\u8d5b\u514bOceanSource\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/OceanSource/#_1","text":"OceanSource 1.0 \u2194 FusionInsight HD V100R002C80 (HBase/Hive/Kafka/ElasticSearch/Redis/GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Oracle_GoldenGate/","text":"Oracle GoldenGate\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Oracle GoldenGate 12.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Flume/Kafka) \u73af\u5883\u4fe1\u606f \u00b6 \u8f6f\u4ef6\u4fe1\u606f \u00b6 Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20 \u786c\u4ef6\u4fe1\u606f \u00b6 \u6e90\u7aefOGG VM: 162.1.115.68 Redhat6.5 \uff08\u5305\u542bOracle DB12c\u7684\u6570\u636e\u5e93\uff09 \u76ee\u6807\u7aefOGG VM: 162.1.115.69 Redhat6.5\uff08\u5305\u542bHadoop\u7684\u5ba2\u6237\u7aef\uff09 \u62d3\u6734\u7ed3\u6784 \u00b6 \u6d4b\u8bd5\u62d3\u6734\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6d4b\u8bd5\u8868 \u00b6 \u6e90\u7aef\u6d4b\u8bd5\u8868\uff1a \u5728\u6e90\u7aefOracle\u7684PDBORCL\u6570\u636e\u5e93\u7684test\u7528\u6237\u4e0b\u521b\u5efatest1\u8868\uff0c\u5176\u4e2dID\u4e3a\u4e3b\u952e OGG for Oracle\u5b89\u88c5 \u00b6 \u524d\u7f6e\u6761\u4ef6\uff1a\u5b8c\u6210oracle12c\u6570\u636e\u5e93\u7684\u5b89\u88c5\uff08IP\uff1a162.1.115.68\uff09 \u8f6f\u4ef6\u7248\u672c\uff1alinuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Oracle \u00b6 \u5c06fbo_ggs_Linux_x64_shiphome.zip\u4e0a\u4f20\u81f3oracle\u5ba2\u6237\u7aef\uff08ip\uff1a162.1.115.68\uff09 /home/oracle \u76ee\u5f55\u4e0b\uff0c\u5207\u6362\u81f3oracle\u7528\u6237\uff0c\u89e3\u538b\u751f\u6210bo_ggs_Linux_x64_shiphome\u76ee\u5f55\u3002 \u5728 /home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1 \u76ee\u5f55\u4e0b\uff0c\u8fd0\u884c ./runInstaller \u5b89\u88c5\u6210\u529f\uff0c/home/orcle/OGG/\u662fOGG for Oracle\u7684\u5b89\u88c5\u76ee\u5f55\u3002 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u5207\u6362\u5230oracle\u7528\u6237 su - oracle vi .bash_profile \u6587\u4ef6.bash_profile\u5185\u5bb9\u5982\u4e0b\uff1a ```shell # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH= PATH: PATH: HOME/bin export PATH PATH= PATH: PATH: HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib ``` \u8fd0\u884cOGG \u6253\u5f00\u6570\u636e\u5e93\u5f52\u6863\u53ca\u5f00\u542f\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7 \u00b6 \u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: shutdown immediate ; startup mount ; alter database archivelog ; alter database open ; archive log list ; \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication ; alter system set enable_goldengate_replication = true scope = both ; \u914d\u7f6eDB12c PDB\u7684tnsname\u4fe1\u606f vi $ORACLE_HOME/network/admin/tnsnames.ora \uff1a \u5728\u6570\u636e\u5e93\u4e2d\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 \u00b6 \u4f7f\u7528 sqlplus / as sysdba \u767b\u9646\u6570\u636e\u5e93\u540e\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 create user c ## ogg identified by welcome1 ; grant dba to c ## ogg container = all ; grant create session , connect , resource to c ## ogg container = all ; grant alter any table to c ## ogg container = all ; grant alter system to c ## ogg container = all ; exec dbms_goldengate_auth . grant_admin_privilege ( 'c##ogg' , container => 'all' ); \u914d\u7f6eGoldenGate \u767b\u9646\u6570\u636e\u5e93\u7684\u522b\u540d \u00b6 \u5728GoldenGate\u4e2d\u521b\u5efa\u7528\u6237\u522b\u540d\uff0c\u7528\u4e8e\u767b\u5f55Oracle\u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u5e93\u65e5\u5fd7\uff1a add credentialstore ALTER CREDENTIALSTORE ADD USER c ## ogg PASSWORD welcome1 ALIAS ogg_src \u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u522b\u540dogg_src\u767b\u9646\u6570\u636e\u5e93\u4e86\uff1a dblogin useridalias ogg_src C##ogg\u662fOracle DB12c\u7684\u666e\u901a\u7528\u6237\uff0c\u53ef\u4ee5\u8bbf\u95ee\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\u3002 \u521b\u5efatest\u7528\u6237\u548ctest1\u8868 \u00b6 test\u7528\u6237\u662f\u57fa\u4e8epdborcl\u6570\u636e\u5e93\u5b9e\u4f8b\u7684\uff1a \u767b\u9646\u6570\u636e\u5e93 Sqlplus / as sysdba \u521b\u5efa\u7528\u6237 alter session set container = pdborcl ; alter database open ; create user test identified by welcome1 ; grant resource , connect to test ; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500 M UNIFORM SIZE 128 k ; alter user test quota unlimited on test ; alter user test quota unlimited on users ; \u521b\u5efa\u6d4b\u8bd5\u8868 conn test / welcome1 @ pdborcl ; create table test1 ( id number primary key , name varchar2 ( 50 )); \u914d\u7f6eGoldenGate\u6355\u83b7\u8fdb\u7a0b \u00b6 \u7f16\u8f91eora.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cedit param eora\u547d\u4ee4\uff1a GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume \u7f16\u8f91 diroby/eora.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/eora.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) \u4f7f\u7528oracle\u7528\u6237\u521b\u5efadiroby\u76ee\u5f55\uff1a cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby \u6ce8\u610f\u8fdb\u7a0b\u540deora\u548c\u6570\u636e\u6587\u4ef6dirdat/eo\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cobey diroby/eora.oby\u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0beora\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/eora.oby \u628a\u6355\u83b7\u8fdb\u7a0beora\u6ce8\u518c\u5230pdborcl\u6570\u636e\u5e93\u4e2d\uff1a GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) \u4e3apdborcl.test\u4e0b\u7684\u6240\u6709\u8868\u6dfb\u52a0\u8868\u7ea7\u9644\u52a0\u65e5\u5fd7\uff1a GGSCI> add schematrandata pdborcl.test allcols \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0beora: GGSCI> start eora \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HDFS\u5904\u7406\u3002 \u7f16\u8f91phdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phdfs \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phdfs.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phdfs.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phdfs.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phdfs**\u548c\u6570\u636e\u6587\u4ef6dirdat/rs\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phdfs.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphdfs\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phdfs.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphdfs: GGSCI> start phdfs \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HBASE\u5904\u7406\u3002 \u7f16\u8f91phbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phbase \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phbase.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phbase.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phbase.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phbase**\u548c\u6570\u636e\u6587\u4ef6dirdat/se\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phbase.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphbase\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phbase.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphbase: GGSCI> start phbase \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate FLUME\u5904\u7406\u3002 \u7f16\u8f91pflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pflume \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pflume.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pflume.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pflume.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pflume**\u548c\u6570\u636e\u6587\u4ef6dirdat/rf\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pflume.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpflume\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/pflume.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpflume: GGSCI> start pflume \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate Kafka\u5904\u7406\u3002 \u7f16\u8f91pkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pkafka \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pkafka.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pkafka.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pkafka.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pkafka**\u548c\u6570\u636e\u6587\u4ef6dirdat/rk\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pkafka.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpkafka\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/ pkafka.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpkafka: GGSCI> start pkafka \u67e5\u770bGoldenGate\u8fdb\u7a0b\u8fd0\u884c\u72b6\u6001 \u00b6 \u67e5\u770bGoldenGate\u8fdb\u7a0b\u72b6\u6001\uff1a(EORCL\u662f\u4e0eELK\u5bf9\u63a5\u7684\u8fdb\u7a0b) GGSCI> info all \u67e5\u770b\u67d0\u4e2a\u8fdb\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a GGSCI> info eora detail \u67e5\u770bGoldenGate\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a GGSCI> stats eora, latest \u67e5\u770bGoldenGate\u8fdb\u7a0b\u62a5\u544a\uff0c\u7528\u4e8e\u5b9a\u4f4d\u95ee\u9898\uff1a GGSCI> view report eora OGG for Bigdata\u5b89\u88c5 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728Bigdata\u5ba2\u6237\u7aef\u673a\u5668\u4e0a\uff08ip\uff1a162.1.115.69\uff09\u6309\u7167FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u3002\u5c06\u5ba2\u6237\u7aefJDK\u66ff\u6362\u62101.7\u7248\u672c\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5oracle JDK1.7 \u5c06krb5.conf\u653e\u5728/etc/\u76ee\u5f55\u4e0b \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Bigdata \u5c06122011_ggs_Adapters_Linux_x64.zip\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef/opt\u76ee\u5f55\u4e0b\uff1a unzip 122011_ggs_Adapters_Linux_x64.zip \u5c06\u89e3\u538b\u540e\u7684ggs_Adapters_Linux_x64.tar\u89e3\u538b\u5230/opt/OGG_HADOOP\u76ee\u5f55\u4e0b\uff1a \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u66f4\u6539\u73af\u5883\u53d8\u91cf\uff0c\u7f16\u8f91\u6839\u76ee\u5f55\u4e0b vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME = /usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH = $CLASSPATH : $JAVA_HOME /lib: $JAVA_HOME /jre/lib PATH = $JAVA_HOME /bin: $PATH : $HOME /bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib: $LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/local/lib: $LD_LIBRARY_PATH Source\u73af\u5883\u53d8\u91cf\uff0c source .bash_profile . \u5c06 /opt/OGG_HADOOP/AdapterExamples/big-data \u4e0b\u7684\u56db\u4e2a\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/OGG_HADOOP/dirprm \u76ee\u5f55\u4e0b\u3002 \u914d\u7f6eGoldenGate\u7ba1\u7406\u8fdb\u7a0b \u00b6 \u7f16\u8f91mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all \u914d\u7f6eGoldenGate HDFS \u590d\u5236\u8fdb\u7a0b \u00b6 \u7f16\u8f91rhdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhdfs \u547d\u4ee4\uff1a GGSCI> edit param rhdfs \u7f16\u8f91hdfs.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hdfs.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hdfs.props \u9700\u8981\u5728HDFS\u4e2d\u521b\u5efa/ogg1\u76ee\u5f55\u3002 \u5c06hdfs.keytab\u6587\u4ef6\u62f7\u8d1d\u5230/opt/OGG_HADOOP/dirprm\u76ee\u5f55\u4e2d\uff1a \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhdfs\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all \u914d\u7f6eGoldenGate HBase \u590d\u5236\u8fdb\u7a0b \u00b6 \u7f16\u8f91rhbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhbase \u547d\u4ee4\uff1a GGSCI> edit param rhbase \u7f16\u8f91hbase.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hbase.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hbase.props \u62f7\u8d1dhbase.keytab\u548cjaas.conf\u5230 /opt/OGG_HADOOP/dirprm/ \u4e0b\uff1a jaas.conf \u6587\u4ef6 \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhbase\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all \u914d\u7f6eGoldenGate Kafka \u590d\u5236\u8fdb\u7a0b \u00b6 \u521b\u5efakafka\u6d88\u606f\uff0c\u8fdb\u5165FusionInsight\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin Kafka\u521b\u5efa\u6d88\u606f\uff1a ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka\u67e5\u770b\u6d88\u606f\uff1a ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka\u7ed9\u6d88\u606f\u6388\u6743\uff1a ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test \u7f16\u8f91rkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rkafka \u547d\u4ee4\uff1a GGSCI> edit param rkafka \u7f16\u8f91kafka.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/kafka.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/kafka.props \u5176\u4e2d gg.handler.kafkahandler.BlockingSend \u5c5e\u6027\u63a7\u5236\u540c\u6b65\u548c\u5f02\u6b65\uff0c\u9ed8\u8ba4false\uff0c\u5f02\u6b65\u3002 GGSCI> shell vi dirprm/custom_kafka_producer.properties \u4fee\u6539Kafka\u91cc\u7684\u914d\u7f6e\uff0c\u5c06\u5982\u4e0b\u9009\u9879\u4fee\u6539\u4e3aTrue \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brkafka\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all \u914d\u7f6eGoldenGate Flume \u590d\u5236\u8fdb\u7a0b \u00b6 \u5b89\u88c5Flume\u5ba2\u6237\u7aef\uff0c\u914d\u7f6e\u975e\u52a0\u5bc6\u4f20\u8f93 \u914d\u7f6eServer\u7684\u914d\u7f6e\u6587\u4ef6properties.properties \u5bfc\u51fa\u7684properties.properties\u6587\u4ef6\uff0c\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a \u53ef\u4ee5\u5728HDFS\u4e2d\u589e\u52a0/ogg/flume\u76ee\u5f55 \u5c06\u6b64properties.properties\u6587\u4ef6\u4e0a\u4f20\u81f3FusionInsight\u3002 \u7f16\u8f91rflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rflume \u547d\u4ee4\uff1a GGSCI> edit param rflume \u7f16\u8f91flume.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/flume.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false \u63a7\u5236DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false \u76f8\u540cSCHAME\u6253\u5305 GGSCI> shell vi dirprm/custom-flume-rpc.properties \u62f7\u8d1dflume.keytab\u6587\u4ef6\u5230 /opt/OGG_HADOOP/dirprm/ \u76ee\u5f55\u4e0b \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brflume\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all \u6d4b\u8bd5\u7ed3\u679c \u00b6 Oracle\u7aef\u542f\u52a8\u6240\u6709\u7684\u4f20\u8f93\u8fdb\u7a0b \u00b6 \u786e\u4fdd\u6240\u6709\u4f20\u8f93\u8fdb\u7a0b\u5747\u5df2\u7ecf\u6b63\u5e38\u542f\u52a8 \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aInsert\u64cd\u4f5c \u00b6 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aUpdate\u64cd\u4f5c \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aDelete\u64cd\u4f5c \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0chadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"12.3 <--> C80"},{"location":"Data_Integration/Oracle_GoldenGate/#oracle-goldengatefusioninsight","text":"","title":"Oracle GoldenGate\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Oracle_GoldenGate/#_1","text":"Oracle GoldenGate 12.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Flume/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Oracle_GoldenGate/#_2","text":"","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_3","text":"Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20","title":"\u8f6f\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_4","text":"\u6e90\u7aefOGG VM: 162.1.115.68 Redhat6.5 \uff08\u5305\u542bOracle DB12c\u7684\u6570\u636e\u5e93\uff09 \u76ee\u6807\u7aefOGG VM: 162.1.115.69 Redhat6.5\uff08\u5305\u542bHadoop\u7684\u5ba2\u6237\u7aef\uff09","title":"\u786c\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_5","text":"\u6d4b\u8bd5\u62d3\u6734\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"\u62d3\u6734\u7ed3\u6784"},{"location":"Data_Integration/Oracle_GoldenGate/#_6","text":"\u6e90\u7aef\u6d4b\u8bd5\u8868\uff1a \u5728\u6e90\u7aefOracle\u7684PDBORCL\u6570\u636e\u5e93\u7684test\u7528\u6237\u4e0b\u521b\u5efatest1\u8868\uff0c\u5176\u4e2dID\u4e3a\u4e3b\u952e","title":"\u6d4b\u8bd5\u8868"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-oracle","text":"\u524d\u7f6e\u6761\u4ef6\uff1a\u5b8c\u6210oracle12c\u6570\u636e\u5e93\u7684\u5b89\u88c5\uff08IP\uff1a162.1.115.68\uff09 \u8f6f\u4ef6\u7248\u672c\uff1alinuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip","title":"OGG for Oracle\u5b89\u88c5"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-oracle_1","text":"\u5c06fbo_ggs_Linux_x64_shiphome.zip\u4e0a\u4f20\u81f3oracle\u5ba2\u6237\u7aef\uff08ip\uff1a162.1.115.68\uff09 /home/oracle \u76ee\u5f55\u4e0b\uff0c\u5207\u6362\u81f3oracle\u7528\u6237\uff0c\u89e3\u538b\u751f\u6210bo_ggs_Linux_x64_shiphome\u76ee\u5f55\u3002 \u5728 /home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1 \u76ee\u5f55\u4e0b\uff0c\u8fd0\u884c ./runInstaller \u5b89\u88c5\u6210\u529f\uff0c/home/orcle/OGG/\u662fOGG for Oracle\u7684\u5b89\u88c5\u76ee\u5f55\u3002","title":"\u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Oracle"},{"location":"Data_Integration/Oracle_GoldenGate/#_7","text":"\u5207\u6362\u5230oracle\u7528\u6237 su - oracle vi .bash_profile \u6587\u4ef6.bash_profile\u5185\u5bb9\u5982\u4e0b\uff1a ```shell # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH= PATH: PATH: HOME/bin export PATH PATH= PATH: PATH: HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib ``` \u8fd0\u884cOGG","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/Oracle_GoldenGate/#_8","text":"\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: shutdown immediate ; startup mount ; alter database archivelog ; alter database open ; archive log list ; \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication ; alter system set enable_goldengate_replication = true scope = both ; \u914d\u7f6eDB12c PDB\u7684tnsname\u4fe1\u606f vi $ORACLE_HOME/network/admin/tnsnames.ora \uff1a","title":"\u6253\u5f00\u6570\u636e\u5e93\u5f52\u6863\u53ca\u5f00\u542f\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg","text":"\u4f7f\u7528 sqlplus / as sysdba \u767b\u9646\u6570\u636e\u5e93\u540e\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 create user c ## ogg identified by welcome1 ; grant dba to c ## ogg container = all ; grant create session , connect , resource to c ## ogg container = all ; grant alter any table to c ## ogg container = all ; grant alter system to c ## ogg container = all ; exec dbms_goldengate_auth . grant_admin_privilege ( 'c##ogg' , container => 'all' );","title":"\u5728\u6570\u636e\u5e93\u4e2d\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate","text":"\u5728GoldenGate\u4e2d\u521b\u5efa\u7528\u6237\u522b\u540d\uff0c\u7528\u4e8e\u767b\u5f55Oracle\u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u5e93\u65e5\u5fd7\uff1a add credentialstore ALTER CREDENTIALSTORE ADD USER c ## ogg PASSWORD welcome1 ALIAS ogg_src \u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u522b\u540dogg_src\u767b\u9646\u6570\u636e\u5e93\u4e86\uff1a dblogin useridalias ogg_src C##ogg\u662fOracle DB12c\u7684\u666e\u901a\u7528\u6237\uff0c\u53ef\u4ee5\u8bbf\u95ee\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\u3002","title":"\u914d\u7f6eGoldenGate \u767b\u9646\u6570\u636e\u5e93\u7684\u522b\u540d"},{"location":"Data_Integration/Oracle_GoldenGate/#testtest1","text":"test\u7528\u6237\u662f\u57fa\u4e8epdborcl\u6570\u636e\u5e93\u5b9e\u4f8b\u7684\uff1a \u767b\u9646\u6570\u636e\u5e93 Sqlplus / as sysdba \u521b\u5efa\u7528\u6237 alter session set container = pdborcl ; alter database open ; create user test identified by welcome1 ; grant resource , connect to test ; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500 M UNIFORM SIZE 128 k ; alter user test quota unlimited on test ; alter user test quota unlimited on users ; \u521b\u5efa\u6d4b\u8bd5\u8868 conn test / welcome1 @ pdborcl ; create table test1 ( id number primary key , name varchar2 ( 50 ));","title":"\u521b\u5efatest\u7528\u6237\u548ctest1\u8868"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_1","text":"\u7f16\u8f91eora.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cedit param eora\u547d\u4ee4\uff1a GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume \u7f16\u8f91 diroby/eora.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/eora.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) \u4f7f\u7528oracle\u7528\u6237\u521b\u5efadiroby\u76ee\u5f55\uff1a cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby \u6ce8\u610f\u8fdb\u7a0b\u540deora\u548c\u6570\u636e\u6587\u4ef6dirdat/eo\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cobey diroby/eora.oby\u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0beora\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/eora.oby \u628a\u6355\u83b7\u8fdb\u7a0beora\u6ce8\u518c\u5230pdborcl\u6570\u636e\u5e93\u4e2d\uff1a GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) \u4e3apdborcl.test\u4e0b\u7684\u6240\u6709\u8868\u6dfb\u52a0\u8868\u7ea7\u9644\u52a0\u65e5\u5fd7\uff1a GGSCI> add schematrandata pdborcl.test allcols \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0beora: GGSCI> start eora","title":"\u914d\u7f6eGoldenGate\u6355\u83b7\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatephdfs","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HDFS\u5904\u7406\u3002 \u7f16\u8f91phdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phdfs \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phdfs.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phdfs.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phdfs.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phdfs**\u548c\u6570\u636e\u6587\u4ef6dirdat/rs\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phdfs.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphdfs\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phdfs.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphdfs: GGSCI> start phdfs","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatephbase","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HBASE\u5904\u7406\u3002 \u7f16\u8f91phbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phbase \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phbase.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phbase.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phbase.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phbase**\u548c\u6570\u636e\u6587\u4ef6dirdat/se\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phbase.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphbase\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phbase.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphbase: GGSCI> start phbase","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatepflume","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate FLUME\u5904\u7406\u3002 \u7f16\u8f91pflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pflume \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pflume.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pflume.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pflume.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pflume**\u548c\u6570\u636e\u6587\u4ef6dirdat/rf\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pflume.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpflume\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/pflume.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpflume: GGSCI> start pflume","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatepkafka","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate Kafka\u5904\u7406\u3002 \u7f16\u8f91pkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pkafka \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pkafka.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pkafka.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pkafka.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pkafka**\u548c\u6570\u636e\u6587\u4ef6dirdat/rk\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pkafka.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpkafka\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/ pkafka.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpkafka: GGSCI> start pkafka","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_2","text":"\u67e5\u770bGoldenGate\u8fdb\u7a0b\u72b6\u6001\uff1a(EORCL\u662f\u4e0eELK\u5bf9\u63a5\u7684\u8fdb\u7a0b) GGSCI> info all \u67e5\u770b\u67d0\u4e2a\u8fdb\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a GGSCI> info eora detail \u67e5\u770bGoldenGate\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a GGSCI> stats eora, latest \u67e5\u770bGoldenGate\u8fdb\u7a0b\u62a5\u544a\uff0c\u7528\u4e8e\u5b9a\u4f4d\u95ee\u9898\uff1a GGSCI> view report eora","title":"\u67e5\u770bGoldenGate\u8fdb\u7a0b\u8fd0\u884c\u72b6\u6001"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-bigdata","text":"","title":"OGG for Bigdata\u5b89\u88c5"},{"location":"Data_Integration/Oracle_GoldenGate/#_9","text":"\u4e0b\u8f7d\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728Bigdata\u5ba2\u6237\u7aef\u673a\u5668\u4e0a\uff08ip\uff1a162.1.115.69\uff09\u6309\u7167FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u3002\u5c06\u5ba2\u6237\u7aefJDK\u66ff\u6362\u62101.7\u7248\u672c\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5oracle JDK1.7 \u5c06krb5.conf\u653e\u5728/etc/\u76ee\u5f55\u4e0b \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Bigdata \u5c06122011_ggs_Adapters_Linux_x64.zip\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef/opt\u76ee\u5f55\u4e0b\uff1a unzip 122011_ggs_Adapters_Linux_x64.zip \u5c06\u89e3\u538b\u540e\u7684ggs_Adapters_Linux_x64.tar\u89e3\u538b\u5230/opt/OGG_HADOOP\u76ee\u5f55\u4e0b\uff1a \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u66f4\u6539\u73af\u5883\u53d8\u91cf\uff0c\u7f16\u8f91\u6839\u76ee\u5f55\u4e0b vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME = /usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH = $CLASSPATH : $JAVA_HOME /lib: $JAVA_HOME /jre/lib PATH = $JAVA_HOME /bin: $PATH : $HOME /bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib: $LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/local/lib: $LD_LIBRARY_PATH Source\u73af\u5883\u53d8\u91cf\uff0c source .bash_profile . \u5c06 /opt/OGG_HADOOP/AdapterExamples/big-data \u4e0b\u7684\u56db\u4e2a\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/OGG_HADOOP/dirprm \u76ee\u5f55\u4e0b\u3002","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_3","text":"\u7f16\u8f91mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all","title":"\u914d\u7f6eGoldenGate\u7ba1\u7406\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-hdfs","text":"\u7f16\u8f91rhdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhdfs \u547d\u4ee4\uff1a GGSCI> edit param rhdfs \u7f16\u8f91hdfs.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hdfs.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hdfs.props \u9700\u8981\u5728HDFS\u4e2d\u521b\u5efa/ogg1\u76ee\u5f55\u3002 \u5c06hdfs.keytab\u6587\u4ef6\u62f7\u8d1d\u5230/opt/OGG_HADOOP/dirprm\u76ee\u5f55\u4e2d\uff1a \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhdfs\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all","title":"\u914d\u7f6eGoldenGate HDFS \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-hbase","text":"\u7f16\u8f91rhbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhbase \u547d\u4ee4\uff1a GGSCI> edit param rhbase \u7f16\u8f91hbase.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hbase.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hbase.props \u62f7\u8d1dhbase.keytab\u548cjaas.conf\u5230 /opt/OGG_HADOOP/dirprm/ \u4e0b\uff1a jaas.conf \u6587\u4ef6 \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhbase\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all","title":"\u914d\u7f6eGoldenGate HBase \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-kafka","text":"\u521b\u5efakafka\u6d88\u606f\uff0c\u8fdb\u5165FusionInsight\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin Kafka\u521b\u5efa\u6d88\u606f\uff1a ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka\u67e5\u770b\u6d88\u606f\uff1a ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka\u7ed9\u6d88\u606f\u6388\u6743\uff1a ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test \u7f16\u8f91rkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rkafka \u547d\u4ee4\uff1a GGSCI> edit param rkafka \u7f16\u8f91kafka.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/kafka.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/kafka.props \u5176\u4e2d gg.handler.kafkahandler.BlockingSend \u5c5e\u6027\u63a7\u5236\u540c\u6b65\u548c\u5f02\u6b65\uff0c\u9ed8\u8ba4false\uff0c\u5f02\u6b65\u3002 GGSCI> shell vi dirprm/custom_kafka_producer.properties \u4fee\u6539Kafka\u91cc\u7684\u914d\u7f6e\uff0c\u5c06\u5982\u4e0b\u9009\u9879\u4fee\u6539\u4e3aTrue \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brkafka\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all","title":"\u914d\u7f6eGoldenGate Kafka \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-flume","text":"\u5b89\u88c5Flume\u5ba2\u6237\u7aef\uff0c\u914d\u7f6e\u975e\u52a0\u5bc6\u4f20\u8f93 \u914d\u7f6eServer\u7684\u914d\u7f6e\u6587\u4ef6properties.properties \u5bfc\u51fa\u7684properties.properties\u6587\u4ef6\uff0c\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a \u53ef\u4ee5\u5728HDFS\u4e2d\u589e\u52a0/ogg/flume\u76ee\u5f55 \u5c06\u6b64properties.properties\u6587\u4ef6\u4e0a\u4f20\u81f3FusionInsight\u3002 \u7f16\u8f91rflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rflume \u547d\u4ee4\uff1a GGSCI> edit param rflume \u7f16\u8f91flume.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/flume.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false \u63a7\u5236DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false \u76f8\u540cSCHAME\u6253\u5305 GGSCI> shell vi dirprm/custom-flume-rpc.properties \u62f7\u8d1dflume.keytab\u6587\u4ef6\u5230 /opt/OGG_HADOOP/dirprm/ \u76ee\u5f55\u4e0b \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brflume\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all","title":"\u914d\u7f6eGoldenGate Flume \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#_10","text":"","title":"\u6d4b\u8bd5\u7ed3\u679c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracle","text":"\u786e\u4fdd\u6240\u6709\u4f20\u8f93\u8fdb\u7a0b\u5747\u5df2\u7ecf\u6b63\u5e38\u542f\u52a8","title":"Oracle\u7aef\u542f\u52a8\u6240\u6709\u7684\u4f20\u8f93\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#oracleinsert","text":"su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aInsert\u64cd\u4f5c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracleupdate","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aUpdate\u64cd\u4f5c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracledelete","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0chadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aDelete\u64cd\u4f5c"},{"location":"Data_Integration/Pentaho/","text":"Pentaho\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Pentaho 7.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive) Pentaho 8.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive)","title":"8.0 <--> C60"},{"location":"Data_Integration/Pentaho/#pentahofusioninsight","text":"","title":"Pentaho\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Pentaho/#_1","text":"Pentaho 7.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive) Pentaho 8.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/SharePlex/","text":"SharePlex\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SharePlex 9.2.1 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) SharePlex 9.2.1 \u2194 FusionInsight HD 6.5 (Kafka) \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post \u00b6 \u6839\u636e\u5bf9\u63a5\u6a21\u5f0f\u4e0d\u540c\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u5bf9\u63a5\u65b9\u5f0f \u4f7f\u7528Kafka\u666e\u901a\u6a21\u5f0f\u5bf9\u63a5 \u4f7f\u7528Kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5 \u666e\u901a\u6a21\u5f0f\u5bf9\u63a5 \u00b6 \u4fee\u6539Kafka\u8ba4\u8bc1\u65b9\u5f0f \u5728FusionInsight\u670d\u52a1\u7aef\u4fee\u6539Kafka\u7684\u914d\u7f6e\u53c2\u6570 allow.everyone.if.no.acl.found \u4e3a True \uff0c\u5141\u8bb8\u4f7f\u752821005\u7684\u666e\u901a\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21005 target x.kafka set kafka security.protocol = PLAINTEXT target x.kafka set kafka topic = splex \u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5 \u00b6 \u4fee\u6539/etc/hosts\u6587\u4ef6 \u5728SharePlex Linux for Open Target\u4e0a\u4fee\u6539hadoop\u96c6\u7fa4\u6240\u6709\u7684\u4e3b\u673aIP\u5730\u5740\u90fd\u5bf9\u5e94\u5230 hadoop.hadoop.com ,\u4f7f\u5f97\u89e3\u6790\u5bf9\u5e94kafka service \u7684kerberos principal\u80fd\u591f\u6b63\u786e\u586b\u5199\u4e3a kafka/hadoop.hadoop.com@HADOOP.COM \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21007 target x.kafka set kafka sasl.kerberos.keytab = /home/quest/user.keytab target x.kafka set kafka sasl.kerberos.kinit.cmd = kinit -k -t \"%{sasl.kerberos.keytab}\" %{sasl.kerberos.principal} target x.kafka set kafka sasl.kerberos.min.time.before.relogin = 60000 target x.kafka set kafka sasl.kerberos.principal = developuser@HADOOP.COM target x.kafka set kafka sasl.kerberos.service.name = kafka target x.kafka set kafka sasl.mechanisms = GSSAPI target x.kafka set kafka security.protocol = SASL_PLAINTEXT target x.kafka set kafka threshold_size = 10000 target x.kafka set kafka topic = splex \u542f\u52a8kafka\u7684post\u8fdb\u7a0b \u00b6 \u6267\u884c start post \u6267\u884c show status \u9a8c\u8bc1Oracle\u7684\u64cd\u4f5c\u662f\u5426\u540c\u6b65\u5230Kafka \u00b6 \u5728Oracle\u7aef\u8fdb\u884c\u589e\u5220\u6539\u64cd\u4f5c \u67e5\u770bKafka\u7684Topic\u4e2d\u662f\u5426\u4e0a\u62a5\u5bf9\u5e94\u7684JSON\u6d88\u606f","title":"9.2.1 <--> 6.5"},{"location":"Data_Integration/SharePlex/#shareplexfusioninsight","text":"","title":"SharePlex\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/SharePlex/#_1","text":"SharePlex 9.2.1 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) SharePlex 9.2.1 \u2194 FusionInsight HD 6.5 (Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/SharePlex/#shareplexkafkapost","text":"\u6839\u636e\u5bf9\u63a5\u6a21\u5f0f\u4e0d\u540c\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u5bf9\u63a5\u65b9\u5f0f \u4f7f\u7528Kafka\u666e\u901a\u6a21\u5f0f\u5bf9\u63a5 \u4f7f\u7528Kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5","title":"\u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post"},{"location":"Data_Integration/SharePlex/#_2","text":"\u4fee\u6539Kafka\u8ba4\u8bc1\u65b9\u5f0f \u5728FusionInsight\u670d\u52a1\u7aef\u4fee\u6539Kafka\u7684\u914d\u7f6e\u53c2\u6570 allow.everyone.if.no.acl.found \u4e3a True \uff0c\u5141\u8bb8\u4f7f\u752821005\u7684\u666e\u901a\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21005 target x.kafka set kafka security.protocol = PLAINTEXT target x.kafka set kafka topic = splex","title":"\u666e\u901a\u6a21\u5f0f\u5bf9\u63a5"},{"location":"Data_Integration/SharePlex/#_3","text":"\u4fee\u6539/etc/hosts\u6587\u4ef6 \u5728SharePlex Linux for Open Target\u4e0a\u4fee\u6539hadoop\u96c6\u7fa4\u6240\u6709\u7684\u4e3b\u673aIP\u5730\u5740\u90fd\u5bf9\u5e94\u5230 hadoop.hadoop.com ,\u4f7f\u5f97\u89e3\u6790\u5bf9\u5e94kafka service \u7684kerberos principal\u80fd\u591f\u6b63\u786e\u586b\u5199\u4e3a kafka/hadoop.hadoop.com@HADOOP.COM \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21007 target x.kafka set kafka sasl.kerberos.keytab = /home/quest/user.keytab target x.kafka set kafka sasl.kerberos.kinit.cmd = kinit -k -t \"%{sasl.kerberos.keytab}\" %{sasl.kerberos.principal} target x.kafka set kafka sasl.kerberos.min.time.before.relogin = 60000 target x.kafka set kafka sasl.kerberos.principal = developuser@HADOOP.COM target x.kafka set kafka sasl.kerberos.service.name = kafka target x.kafka set kafka sasl.mechanisms = GSSAPI target x.kafka set kafka security.protocol = SASL_PLAINTEXT target x.kafka set kafka threshold_size = 10000 target x.kafka set kafka topic = splex","title":"\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5"},{"location":"Data_Integration/SharePlex/#kafkapost","text":"\u6267\u884c start post \u6267\u884c show status","title":"\u542f\u52a8kafka\u7684post\u8fdb\u7a0b"},{"location":"Data_Integration/SharePlex/#oraclekafka","text":"\u5728Oracle\u7aef\u8fdb\u884c\u589e\u5220\u6539\u64cd\u4f5c \u67e5\u770bKafka\u7684Topic\u4e2d\u662f\u5426\u4e0a\u62a5\u5bf9\u5e94\u7684JSON\u6d88\u606f","title":"\u9a8c\u8bc1Oracle\u7684\u64cd\u4f5c\u662f\u5426\u540c\u6b65\u5230Kafka"},{"location":"Data_Integration/TIBCO_BusinessWorks/","text":"TIBCO Business Works(BW) 5.13\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Tibco BW 5.13 \u2194 FusionInsight HD 6.5 (GaussDB) \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Tibco BW5.13\uff0c\u53c2\u8003Tibco\u5b98\u65b9\u6587\u6863 https://docs.tibco.com/pub/activematrix_businessworks/5.13.0/doc/pdf/TIB_BW_5.13.0_installation.pdf?id=0 \u5b89\u88c5\u5b8c\u6210\u540e\u542f\u52a8TIBCO Designer\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7a7a\u767d\u5de5\u7a0b \u8fde\u63a5GaussDB \u00b6 \u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u653e\u5728TIBCO\u5b89\u88c5\u76ee\u5f55 tpcl\\version\\jdbc \u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:\\tibco\\tpcl\\5.10\\jdbc \u76ee\u5f55\u4e0b \u5728TIBCO Designer\u4e2d\uff0c\u6dfb\u52a0 \u4e00\u4e2a\u65b0\u7684process\uff0c\u8fd9\u91cc\u53d6\u540d\u4e3a getTable \u5728\u5de5\u7a0b\u7684 Shared Resources \u4e2d\u6dfb\u52a0\u4e00\u4e2a JDBC Connection \uff0c\u547d\u540d\u4e3a Gauss Connection \uff0c\u5e76\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e JDBC Driver \u4e3apostgresql\u7684\u9a71\u52a8\u7c7b\u540d Database URL\u4e3apostgres\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u4e3ajdbc:postgresql://ip:port/postgresql,\u5bf9\u4e8eGaussDB\uff0c\u9ed8\u8ba4\u7aef\u53e3\u4e3a25308 UserName\u548cPassword\u4e3a\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801 \u70b9\u51fb Test Connection \uff0c\u5f39\u51fa\u8fde\u63a5\u6210\u529f\u7a97\u53e3 \u53cc\u51fb\u8fdb\u5165 getTable \uff0c\u4ece\u5de6\u4fa7\u7684activity\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u6d41\u7a0b \u5728 JDBC Query \u4e2d\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e,\u70b9\u51fb\u53f3\u4fa7\u641c\u7d22\u6807\u5fd7\uff0c\u5728\u5f39\u51fa\u7a97\u53e3\u4e2d\u9009\u62e9\u521a\u624d\u521b\u5efa\u7684 GAUSS COnnection ,\u8f93\u5165\u60f3\u8981\u6267\u884c\u7684SQL\u8bed\u53e5,\u8fd9\u91cc\u662f\u67e5\u8be2test\u8868\u4e2d\u7684\u6240\u6709\u5185\u5bb9 \u70b9\u51fb\u5de6\u4e0b\u89d2 fetch \uff0c\u4f1a\u5728Output\u9875\u7b7e\u4e2d\u770b\u5230\u8981\u67e5\u8be2\u7684\u8868\u7684\u5b57\u6bb5\u4fe1\u606f \u5728\u5de6\u4fa7 tester \u4e2d\u70b9\u51fb\u6267\u884c\u6309\u94ae\uff0c\u9009\u62e9 getTable \uff0c Load & Start Current \uff0c\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 * \u5728 JDBC Query \u7684output\u4e2d\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c ![](assets/TIBCO_BusinessWorks/1289b.png) \u5bf9\u4e8eJDBC\u7684\u5176\u4ed6\u64cd\u4f5c\uff0c\u4f8b\u5982 JDBC Update\uff0cCall Procedure \u914d\u7f6e\u7c7b\u4f3c\uff0c\u53ef\u53c2\u8003TIBCO\u7684\u5b98\u65b9\u6587\u6863 http://tutorialspedia.com/jdbc-call-procedure-tutorial/","title":"5.13 <--> 6.5"},{"location":"Data_Integration/TIBCO_BusinessWorks/#tibco-business-worksbw-513fusioninsight-hd","text":"","title":"TIBCO Business Works(BW) 5.13\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Integration/TIBCO_BusinessWorks/#_1","text":"Tibco BW 5.13 \u2194 FusionInsight HD 6.5 (GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/TIBCO_BusinessWorks/#_2","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Tibco BW5.13\uff0c\u53c2\u8003Tibco\u5b98\u65b9\u6587\u6863 https://docs.tibco.com/pub/activematrix_businessworks/5.13.0/doc/pdf/TIB_BW_5.13.0_installation.pdf?id=0 \u5b89\u88c5\u5b8c\u6210\u540e\u542f\u52a8TIBCO Designer\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7a7a\u767d\u5de5\u7a0b","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/TIBCO_BusinessWorks/#gaussdb","text":"\u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u653e\u5728TIBCO\u5b89\u88c5\u76ee\u5f55 tpcl\\version\\jdbc \u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:\\tibco\\tpcl\\5.10\\jdbc \u76ee\u5f55\u4e0b \u5728TIBCO Designer\u4e2d\uff0c\u6dfb\u52a0 \u4e00\u4e2a\u65b0\u7684process\uff0c\u8fd9\u91cc\u53d6\u540d\u4e3a getTable \u5728\u5de5\u7a0b\u7684 Shared Resources \u4e2d\u6dfb\u52a0\u4e00\u4e2a JDBC Connection \uff0c\u547d\u540d\u4e3a Gauss Connection \uff0c\u5e76\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e JDBC Driver \u4e3apostgresql\u7684\u9a71\u52a8\u7c7b\u540d Database URL\u4e3apostgres\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u4e3ajdbc:postgresql://ip:port/postgresql,\u5bf9\u4e8eGaussDB\uff0c\u9ed8\u8ba4\u7aef\u53e3\u4e3a25308 UserName\u548cPassword\u4e3a\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801 \u70b9\u51fb Test Connection \uff0c\u5f39\u51fa\u8fde\u63a5\u6210\u529f\u7a97\u53e3 \u53cc\u51fb\u8fdb\u5165 getTable \uff0c\u4ece\u5de6\u4fa7\u7684activity\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u6d41\u7a0b \u5728 JDBC Query \u4e2d\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e,\u70b9\u51fb\u53f3\u4fa7\u641c\u7d22\u6807\u5fd7\uff0c\u5728\u5f39\u51fa\u7a97\u53e3\u4e2d\u9009\u62e9\u521a\u624d\u521b\u5efa\u7684 GAUSS COnnection ,\u8f93\u5165\u60f3\u8981\u6267\u884c\u7684SQL\u8bed\u53e5,\u8fd9\u91cc\u662f\u67e5\u8be2test\u8868\u4e2d\u7684\u6240\u6709\u5185\u5bb9 \u70b9\u51fb\u5de6\u4e0b\u89d2 fetch \uff0c\u4f1a\u5728Output\u9875\u7b7e\u4e2d\u770b\u5230\u8981\u67e5\u8be2\u7684\u8868\u7684\u5b57\u6bb5\u4fe1\u606f \u5728\u5de6\u4fa7 tester \u4e2d\u70b9\u51fb\u6267\u884c\u6309\u94ae\uff0c\u9009\u62e9 getTable \uff0c Load & Start Current \uff0c\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 * \u5728 JDBC Query \u7684output\u4e2d\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c ![](assets/TIBCO_BusinessWorks/1289b.png) \u5bf9\u4e8eJDBC\u7684\u5176\u4ed6\u64cd\u4f5c\uff0c\u4f8b\u5982 JDBC Update\uff0cCall Procedure \u914d\u7f6e\u7c7b\u4f3c\uff0c\u53ef\u53c2\u8003TIBCO\u7684\u5b98\u65b9\u6587\u6863 http://tutorialspedia.com/jdbc-call-procedure-tutorial/","title":"\u8fde\u63a5GaussDB"},{"location":"Data_Integration/Talend_6.4.1/","text":"Talend\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) \u6ce8\uff1a\u56e0\u4e3aTalend 7.0.1\u7248\u672cbug\uff0cHive\u7ec4\u4ef6\u65e0\u6cd5\u5728\u7248\u672c7.0.1\u4e2d\u901a\u8fc7\uff0c\u5bf9\u63a5hive\u7ec4\u4ef6\u4f7f\u7528Talend 6.4.1\u7248\u672c \u5b89\u88c5Talend \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Talend 7.0.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5(\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6) \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cfJAVA_HOME,Path \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5411FusionInsight HD\u96c6\u7fa4\u7ba1\u7406\u5458\u83b7\u53d6\u96c6\u7fa4Kerberos\u7684krb5.conf\u6587\u4ef6,\u628a\u76f8\u5e94\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini,\u5e76\u653e\u5230 C:\\Windows \u76ee\u5f55\u4e0b\uff08Talend\u9ed8\u8ba4\u4ece\u6b64\u76ee\u5f55\u4e0b\u67e5\u627e\uff09 \u4e0b\u8f7dTOS\u5e76\u4fee\u6539TOS\u542f\u52a8\u53c2\u6570 \u5728 https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dTOS\uff0c\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8TOS_BD\uff0c\u8fd0\u884cTOS_BD-win-x86_64.exe \u5b89\u88c5\u5fc5\u9700\u7684\u7b2c\u4e09\u65b9\u5e93 Talend\u8fde\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HDFS\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 HDFS Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6dfb\u52a0tHDFSConnection\u7ec4\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\uff1a 1: \u9009\u62e9Cloudera\uff0c\u7248\u672c\u4e3aCloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - \u6d4b\u8bd5\u7ed3\u679c\uff1a HDFS Get \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSGet\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a /tmp/talend_test \u8def\u5f84\u4e0b\u5df2\u7ecf\u4f20\u5165\u6587\u4ef6 out.csv \uff0c C:/SOFT \u4e3a\u672c\u5730\u8f93\u51fa\u6587\u4ef6\u8def\u5f84 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5230\u672c\u5730\u8def\u5f84 C:/SOFT \u4e0b\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c HDFS Put \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSPut\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u672c\u5730\u76ee\u5f55 C:/SOFT \u4e0b\u521b\u5efa\u6587\u4ef6 HDFSPut.txt , \u5185\u5bb9\u5982\u4e0b\uff1a It is create on local PC. \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767b\u5f55\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a Talend\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 6.4.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 Hive Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bf9\u63a5Hive\u7ec4\u4ef6Talend\u7248\u672c\u9700\u89816.4.1 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fbDistritution\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u5bfc\u5165FusionInsight HD\u5ba2\u6237\u7aefHive\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 \u6d4b\u8bd5\u7ed3\u679c\uff1a Hive Create Table & Load \u64cd\u4f5c\u6b65\u9aa4 \u00b6 tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveCreateTable\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6e\u9700\u8981\u5bfc\u5165hive\u8868\u7684\u7ed3\u6784 tHiveLoad\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u63d0\u524d\u9700\u8981\u5411hdfs\u6587\u4ef6\u5b58\u50a8\u7cfb\u7edf /tmp/talend_test/ \u8def\u5f84\u4e0b\u4f20\u5165\u6587\u4ef6 out.csv out.csv \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b: \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5728\u96c6\u7fa4\u4e0a\u68c0\u67e5\u4f20\u5165\u7684\u8868 createdTableTalend Hive Input \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6d4b\u8bd5\u7ed3\u679c\uff1a Hive Row \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveRow\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u8fde\u63a5\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c Talend\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 HBase Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: \u7528eclipse\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\uff08\u6837\u4f8b\u4ee3\u7801\u8def\u5f84\u5982 C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example \uff09 \u5728Talend\u91cc\u63d2\u5165tHbaseConnection\u7ec4\u4ef6\uff0c\u70b9\u51fb\u7ec4\u4ef6\u8fdb\u884c\u8bbe\u7f6e \u9996\u5148\u70b9\u51fbtHBaseConnection\u56fe\u6807\u4e0b\u9762\u7684\u7ec4\u4ef6\u6309\u94ae\uff0c\u9009\u62e9\u7248\u672c\u4e3a Custom - Unsupported \u548c Hadoop 2 \uff0c\u518d\u70b9\u51fb\u7248\u672c\u65c1\u8fb9\u7684\u6309\u94ae\u5bfc\u5165jar\u5305\uff0c\u9700\u8981\u5bfc\u5165\u7684\u662f\u4e0a\u4e00\u6b65\u5bfc\u51fa\u7684hbase_loginUtil.jar\u4ee5\u53caFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801 hbase-example \u4e2d\u5f15\u7528\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 hbase-example \u6837\u4f8b\u4ee3\u7801\u4e2dlib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u5982\u4e0b\uff1a \u4f7f\u7528tLibraryLoad\u7ec4\u4ef6\u5bfc\u5165hbase_loginUtil.jar \u70b9\u51fb Advanced settings \u5728Import\u4e2d\u589e\u52a0 import com.huawei.hadoop.security.LoginUtil; tHBaseConnection\u914d\u7f6e\u5982\u4e0b: \u5f15\u5165tJava\u7ec4\u4ef6\u7528\u5b9a\u5236\u4ee3\u7801\u66ff\u4ee3Connection\u7ec4\u4ef6 \u4ee3\u7801\u5185\u5bb9\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - \u6d4b\u8bd5\u7ed3\u679c HBase Input Output \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tLibraryLoad\uff0ctHBaseConnection\uff0ctJava\u914d\u7f6e\u4e0d\u53d8 \u52a0\u5165tFileInputDelimited\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\uff0c\u6839\u636e\u9700\u8981\u5b58\u5165\u6587\u4ef6(out.csv)\u7684\u683c\u5f0f\u5b9a\u4e49\u5217\u548c\u7c7b\u578b out.csv \u6d4b\u8bd5\u6570\u636e\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u52a0\u5165tHBaseOutput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u7f16\u8f91\u8868\u7684\u67b6\u6784\uff1a tHBaseInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u540c\u6837\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u914d\u7f6e\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e \u6d4b\u8bd5\u7ed3\u679c \u68c0\u67e5\u96c6\u7fa4\u521b\u5efa\u7684HBase\u8868 hbaseInputOutputTest \u5728\u96c6\u7fa4\u4e0a\u4f7f\u7528\u4ee3\u7801 hbase shell scan 'hbaseInputOutputTest'","title":"7.0.1 <--> C80"},{"location":"Data_Integration/Talend_6.4.1/#talendfusioninsight","text":"","title":"Talend\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Talend_6.4.1/#_1","text":"Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) \u6ce8\uff1a\u56e0\u4e3aTalend 7.0.1\u7248\u672cbug\uff0cHive\u7ec4\u4ef6\u65e0\u6cd5\u5728\u7248\u672c7.0.1\u4e2d\u901a\u8fc7\uff0c\u5bf9\u63a5hive\u7ec4\u4ef6\u4f7f\u7528Talend 6.4.1\u7248\u672c","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#talend","text":"","title":"\u5b89\u88c5Talend"},{"location":"Data_Integration/Talend_6.4.1/#_2","text":"\u5b89\u88c5Talend 7.0.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5(\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6)","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cfJAVA_HOME,Path \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5411FusionInsight HD\u96c6\u7fa4\u7ba1\u7406\u5458\u83b7\u53d6\u96c6\u7fa4Kerberos\u7684krb5.conf\u6587\u4ef6,\u628a\u76f8\u5e94\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini,\u5e76\u653e\u5230 C:\\Windows \u76ee\u5f55\u4e0b\uff08Talend\u9ed8\u8ba4\u4ece\u6b64\u76ee\u5f55\u4e0b\u67e5\u627e\uff09 \u4e0b\u8f7dTOS\u5e76\u4fee\u6539TOS\u542f\u52a8\u53c2\u6570 \u5728 https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dTOS\uff0c\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8TOS_BD\uff0c\u8fd0\u884cTOS_BD-win-x86_64.exe \u5b89\u88c5\u5fc5\u9700\u7684\u7b2c\u4e09\u65b9\u5e93","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#talendhdfs","text":"","title":"Talend\u8fde\u63a5HDFS"},{"location":"Data_Integration/Talend_6.4.1/#_5","text":"Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HDFS\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#hdfs-connection","text":"\u6dfb\u52a0tHDFSConnection\u7ec4\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\uff1a 1: \u9009\u62e9Cloudera\uff0c\u7248\u672c\u4e3aCloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"HDFS Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hdfs-get","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSGet\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a /tmp/talend_test \u8def\u5f84\u4e0b\u5df2\u7ecf\u4f20\u5165\u6587\u4ef6 out.csv \uff0c C:/SOFT \u4e3a\u672c\u5730\u8f93\u51fa\u6587\u4ef6\u8def\u5f84 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5230\u672c\u5730\u8def\u5f84 C:/SOFT \u4e0b\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"HDFS Get \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hdfs-put","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSPut\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u672c\u5730\u76ee\u5f55 C:/SOFT \u4e0b\u521b\u5efa\u6587\u4ef6 HDFSPut.txt , \u5185\u5bb9\u5982\u4e0b\uff1a It is create on local PC. \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767b\u5f55\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"HDFS Put \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#talendhive","text":"","title":"Talend\u8fde\u63a5Hive"},{"location":"Data_Integration/Talend_6.4.1/#_7","text":"Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_8","text":"\u5df2\u7ecf\u5b8c\u6210Talend 6.4.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#hive-connection","text":"\u5bf9\u63a5Hive\u7ec4\u4ef6Talend\u7248\u672c\u9700\u89816.4.1 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fbDistritution\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u5bfc\u5165FusionInsight HD\u5ba2\u6237\u7aefHive\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"Hive Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hive-create-table-load","text":"tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveCreateTable\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6e\u9700\u8981\u5bfc\u5165hive\u8868\u7684\u7ed3\u6784 tHiveLoad\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u63d0\u524d\u9700\u8981\u5411hdfs\u6587\u4ef6\u5b58\u50a8\u7cfb\u7edf /tmp/talend_test/ \u8def\u5f84\u4e0b\u4f20\u5165\u6587\u4ef6 out.csv out.csv \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b: \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5728\u96c6\u7fa4\u4e0a\u68c0\u67e5\u4f20\u5165\u7684\u8868 createdTableTalend","title":"Hive Create Table &amp; Load \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hive-input","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"Hive Input \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hive-row","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveRow\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u8fde\u63a5\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"Hive Row \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#talendhbase","text":"","title":"Talend\u8fde\u63a5HBase"},{"location":"Data_Integration/Talend_6.4.1/#_9","text":"Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_10","text":"\u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#hbase-connection","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: \u7528eclipse\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\uff08\u6837\u4f8b\u4ee3\u7801\u8def\u5f84\u5982 C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example \uff09 \u5728Talend\u91cc\u63d2\u5165tHbaseConnection\u7ec4\u4ef6\uff0c\u70b9\u51fb\u7ec4\u4ef6\u8fdb\u884c\u8bbe\u7f6e \u9996\u5148\u70b9\u51fbtHBaseConnection\u56fe\u6807\u4e0b\u9762\u7684\u7ec4\u4ef6\u6309\u94ae\uff0c\u9009\u62e9\u7248\u672c\u4e3a Custom - Unsupported \u548c Hadoop 2 \uff0c\u518d\u70b9\u51fb\u7248\u672c\u65c1\u8fb9\u7684\u6309\u94ae\u5bfc\u5165jar\u5305\uff0c\u9700\u8981\u5bfc\u5165\u7684\u662f\u4e0a\u4e00\u6b65\u5bfc\u51fa\u7684hbase_loginUtil.jar\u4ee5\u53caFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801 hbase-example \u4e2d\u5f15\u7528\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 hbase-example \u6837\u4f8b\u4ee3\u7801\u4e2dlib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u5982\u4e0b\uff1a \u4f7f\u7528tLibraryLoad\u7ec4\u4ef6\u5bfc\u5165hbase_loginUtil.jar \u70b9\u51fb Advanced settings \u5728Import\u4e2d\u589e\u52a0 import com.huawei.hadoop.security.LoginUtil; tHBaseConnection\u914d\u7f6e\u5982\u4e0b: \u5f15\u5165tJava\u7ec4\u4ef6\u7528\u5b9a\u5236\u4ee3\u7801\u66ff\u4ee3Connection\u7ec4\u4ef6 \u4ee3\u7801\u5185\u5bb9\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - \u6d4b\u8bd5\u7ed3\u679c","title":"HBase Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hbase-input-output","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tLibraryLoad\uff0ctHBaseConnection\uff0ctJava\u914d\u7f6e\u4e0d\u53d8 \u52a0\u5165tFileInputDelimited\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\uff0c\u6839\u636e\u9700\u8981\u5b58\u5165\u6587\u4ef6(out.csv)\u7684\u683c\u5f0f\u5b9a\u4e49\u5217\u548c\u7c7b\u578b out.csv \u6d4b\u8bd5\u6570\u636e\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u52a0\u5165tHBaseOutput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u7f16\u8f91\u8868\u7684\u67b6\u6784\uff1a tHBaseInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u540c\u6837\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u914d\u7f6e\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e \u6d4b\u8bd5\u7ed3\u679c \u68c0\u67e5\u96c6\u7fa4\u521b\u5efa\u7684HBase\u8868 hbaseInputOutputTest \u5728\u96c6\u7fa4\u4e0a\u4f7f\u7528\u4ee3\u7801 hbase shell scan 'hbaseInputOutputTest'","title":"HBase Input Output \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/","text":"Talend\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Talend 7.2.1 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive) \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001HIVE\u3001HBASE\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u590d\u5236krb5.conf\u6587\u4ef6\u5e76\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002FusionInsight HD\u5ba2\u6237\u7aef\u89e3\u538b\u4e8e\u672c\u5730 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig Zookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff0c\u5982 C:\\developuser\\jaas.conf \uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u672c\u5730 C:\\Windows\\System32\\drivers\\etc\\hosts \u5df2\u6dfb\u52a0FusionInsight\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0ehostname\u7684\u6620\u5c04\u3002 \u672c\u5730\u5df2\u5b89\u88c5Hadoop\u670d\u52a1\uff08\u53ef\u4ece https://hadoop.apache.org/releases.html \u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\uff09\uff0c\u8be5\u9879\u53ef\u9009\u3002\u5982\u679c\u672c\u5730\u6ca1\u5b89\u88c5Hadoop\u670d\u52a1\uff0ctalend\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0eHadoop\u76f8\u5173\u7684\u9519\u8bef\u65e5\u5fd7\uff0c\u4f46\u4e0d\u5f71\u54cd\u5b9e\u9645\u8fd0\u884c\u7ed3\u679c\u3002 \u5b89\u88c5Talend \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Talend Open Studio for Big Data \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4ece https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dWindow\u7248\u7684Talend\u3002 \u89e3\u538b\u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u70b9\u51fbTOS_BD-win-x86_64.exe\u542f\u52a8Talend Open Studio for Big Data\u3002\u70b9\u51fb \u6211\u540c\u610f \u3002 \u70b9\u51fb \u5b8c\u6210 \uff0c\u9ed8\u8ba4\u521b\u5efaLocal_Project\u7684\u5de5\u7a0b\u3002 \u9009\u62e9\u5b89\u88c5\u5fc5\u987b\u7684\u7b2c\u4e09\u65b9\u5e93\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002 \u521b\u5efaHadoop\u670d\u52a1 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u521b\u5efa\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\teland\\config \u76ee\u5f55\u4e0b\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u521b\u5efaHadoop\u96c6\u7fa4 \u00b6 \u6253\u5f00 Talend Open Studio for Big Data \uff0c\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster \uff0c\u53f3\u952e Hadoop Cluster \u9009\u62e9 Create Hadoop Cluster \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201cFusionInsight\u201d\uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9 \u4ece\u672c\u5730\u6587\u4ef6\u5bfc\u5165\u914d\u7f6e \uff0c\u70b9\u51fb Next \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u76ee\u5f55 C:\\talend\\config \uff0c\u9ed8\u8ba4\u5168\u9009\uff0c\u70b9\u51fb Finish \u3002 \u201cDistribution\u201d\u9009\u62e9 Custom - Unsuported \uff0c\u70b9\u51fb\u4e0b\u62c9\u6846\u53f3\u8fb9\u7684 \u6309\u94ae\u5bfc\u5165HDFS\u3001HIVE\u3001HBASE\u76f8\u5173\u7684jar\u5305\u3002 \u70b9\u51fb Cancel \u53d6\u6d88\u81ea\u52a8\u5f39\u51fa\u7684\u201c\u5bfc\u5165\u81ea\u5b9a\u4e49\u7684\u5b9a\u4e49\u201d\u7a97\u53e3\u3002 \u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u70b9\u51fb \u6309\u94ae\u6dfb\u52a0HDFS\u76f8\u5173\u7684jar\u5305\u3002 \u9009\u62e9 \u5916\u90e8\u5e93 \uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\hdfs \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\uff0c\u70b9\u51fb OK \u5bfc\u5165jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common \u548c C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHive\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\jdbc \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHBase\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002\u5b8c\u6210\u540e\uff0c\u70b9\u51fb OK \u3002 \u914d\u7f6eKerberos\u8ba4\u8bc1\u3002\u201cCustom->Authentication\u201d\u9009\u62e9 Kerberos \u3002 \u52fe\u9009 Authentication->Enable Kerberos security \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Namenode Principal = hdfs/hadoop.hadoop.com@HADOOP.COM \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u5907\u6ce8\uff1a Namenode Principal\u7684\u53d6\u503c\u4e3ahdfs-site.xml\u7684dfs.namenode.kerberos.principal\u7684value\u503c\uff1b \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u7684\u53d6\u503c\u4e3ayarn-site.xml\u7684yarn.resourcemanager.principal\u7684value\u503c\uff1b \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53\u7684\u53d6\u503c\u4e3amapred-site.xml\u7684mapreduce.jobhistory.principal\u7684value\u503c\u3002 \u52fe\u9009 Authentication->Use a keytab to authenticate \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Principal = developuser Keytab = C:/developuser/user.keytab \u5907\u6ce8\uff1a Principal\u4e3aFusionInsight Manager\u7684\u7528\u6237\u540d\uff0cKeytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u51ed\u636e\u3002 \u914d\u7f6eHadoop\u5c5e\u6027\uff0c\u70b9\u51fb Hadoop\u5c5e\u6027 \u53f3\u8fb9\u7684 \u6309\u94ae\u3002 \u70b9\u51fb \u6309\u94ae\uff0c\u589e\u52a0\u4ee5\u4e0bHadoop\u5c5e\u6027\u3002\u589e\u52a0\u5b8c\u6bd5\uff0c\u70b9\u51fb OK \u3002 \u589e\u52a0core-site.xml\u7684hadoop.security.authentication\u548chadoop.rpc.protection\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\uff1b \u589e\u52a0hdfs-site.xml\u7684dfs.namenode.rpc-address.hacluster.*\uff0cdfs.ha.namenodes.hacluster\u3001dfs.nameservices\u3001dfs.client.failover.proxy.provider.hacluster\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\u3002 \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a hadoop.security.authentication = Kerberos hadoop.rpc.protection = privacy dfs.namenode.rpc-address.hacluster.141 = euleros-hd02:25000 dfs.namenode.rpc-address.hacluster.142 = euleros-hd03:25000 dfs.ha.namenodes.hacluster = 141,142 dfs.nameservices = hacluster dfs.client.failover.proxy.provider.hacluster = org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider \u786e\u8ba4\u9ed8\u8ba4\u52fe\u9009 \u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u5c5e\u6027 \uff0c\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \u3002 \u68c0\u67e5\u8fd4\u56de100%\uff0c\u5219Hadoop\u96c6\u7fa4\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb Close \u3002\u5982\u679c\u8fd4\u56de\u9519\u8bef\u65e5\u5fd7\uff0c\u5219\u6839\u636e\u9519\u8bef\u65e5\u5fd7\u63d0\u793a\u4fee\u6b63\u95ee\u9898\u540e\uff0c\u91cd\u65b0\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \uff0c\u76f4\u81f3\u68c0\u67e5\u8fd4\u56de100%\u3002 \u70b9\u51fb Finish \uff0c\u5219\u53ef\u5728 \u5143\u6570\u636e->Hadoop Cluster \u770b\u5230\u65b0\u5efa\u7684\u201cFusionInsight\u201d\u96c6\u7fa4\uff0c\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u3002 \u914d\u7f6eHIVE\u670d\u52a1 \u00b6 \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight->Hive(1)->FusionInsight_HIVE \uff0c\u53f3\u952e FusionInsight_HIVE \u9009\u62e9 Edit Hive \u3002 \u70b9\u51fb Next \u3002 \u9700\u8981\u66f4\u65b0\u7684\u914d\u7f6e\u5982\u4e0b\uff0c\u5176\u4f59\u7684\u4fdd\u6301\u4e0d\u53d8\u3002 hive\u6a21\u5f0f = Standalone hive\u670d\u52a1\u5668\u7248\u672c = Hive Server2 -- jdbc:hive2:// \u767b\u5f55\u540d = developuser \u5bc6\u7801 = Huawei@123 \u670d\u52a1\u5668 = 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23 \u7aef\u53e3 = 24002 DataBase = default \u9644\u52a0JDBC\u8bbe\u7f6e = ;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;user.principal=developuser;user.keytab=C:/developuser/user.keytab \u8bf4\u660e\uff1a\u4ee5\u4e0a\u4fe1\u606f\u53ef\u6839\u636eJDBC\u65b9\u5f0f\u8fde\u63a5Hive\u670d\u52a1\u7684\u914d\u7f6e\u586b\u5199\u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 Talend\u5bf9\u63a5FusionInsight HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight HDFS\u63a5\u53e3\uff0c\u5e76\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\uff0c\u6216\u8005\u5c06\u672c\u5730\u6587\u4ef6\u4e0a\u4f20\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHDFS\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 HDFS Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chdfsConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hdfsConnection \uff0c\u5728Palette\u9762\u677f\u8f93\u5165\u201chdfsConnection\u201d\u641c\u7d22\uff0c\u5c06\u641c\u7d22\u8fd4\u56de\u7684\u201ctHDFSConnection\u201d\u7ec4\u4ef6\u62d6\u81f3Disigner\u533a\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHDFSConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002\u5982\u679c\u63d0\u793a\u201c\u6b64\u7ec4\u4ef6tHDFSConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002\u201d\uff0c\u5219\u70b9\u51fb \u5b89\u88c5 \u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \u3002 \u7b49\u5f85\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HDFS\u6210\u529f\u3002 HDFS Get\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cgetFromHdfs.csv\u201d\uff0c\u5185\u5bb9\u968f\u610f\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsGet\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSGet\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \uff0c\u53f3\u952e\u9009\u62e9 \u89e6\u53d1\u5668->\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6 \uff0c\u8fde\u63a5\u81f3tHDFSGet_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSGet_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile (\u53ef\u9009\u62e9\u4efb\u610f\u7684\u672c\u5730\u76ee\u5f55)\uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u5728HDFS\u7684/tmp\u76ee\u5f55\u4e0b\u9700\u8981\u83b7\u53d6\u7684\u6587\u4ef6\u540d\u79f0 getFromHdfs.csv \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsGet\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4eceFusionInsight HDFS\u4e0b\u8f7d\u6587\u4ef6\u6210\u529f\u3002 getFromHdfs.csv\u5df2\u4e0b\u8f7d\u81f3\u672c\u5730 C:\\talend\\testFile \u3002 HDFS Put \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4ece\u672c\u5730\u4e0a\u4f20\u6587\u4ef6\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsPut\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSPut\u7ec4\u4ef6\uff0ctHDFSConnection_1\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884ctHDFSPut_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSPut_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u9700\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684\u6587\u4ef6\u540d\u79f0 putToHdfs.csv \u3002 \u8bf4\u660e\uff1a C:/talend/testFile/putToHdfs.csv \u4e3a\u672c\u5730\u5df2\u5b58\u5728\u6587\u4ef6\uff0c\u5185\u5bb9\u968f\u610f\u3002 ![](assets/Talend_7.2.1/61c635ee.png) \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsPut\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793a\u4eceTalend\u4e0a\u4f20\u6587\u4ef6putToHdfs.csv\u81f3FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u68c0\u67e5putToHdfs.csv\u5df2\u4e0a\u4f20\u81f3 /tmp \u76ee\u5f55\u3002 Talend\u5bf9\u63a5FusionInsight Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8868\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHive\u670d\u52a1\u7684Hadoop\u96c6\u7fa4\u548c\u5b8c\u6210Hadoop\u96c6\u7fa4\u7684Hive\u670d\u52a1\u914d\u7f6e\u3002 Hive Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chiveConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hiveConnection \uff0c\u52a0\u5165tHiveConnection\u3001tHiveClose\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight Hive\u6210\u529f\u3002 Hive Create Table \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06/tmp/putToHdfs.csv\u7684\u6570\u636e\u4f20\u5165\u8868talendHiveCreate\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cputToHdfs.csv\u201d\u3002 putToHdfs.csv\u7684\u5185\u5bb9\u5982\u4e0b(\u5305\u542b\u4e24\u5217\uff0c\u4e24\u5217\u4e4b\u95f4\u7528\u5206\u53f7\u9694\u5f00)\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chiveCreateTable\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveCreateTable\u3001tHiveLoad\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveCreateTable_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u8868 \uff0c\u201c\u683c\u5f0f\u201d\u9009\u62e9 \u6587\u672c\u6587\u4ef6 \uff0c\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveLoad_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u201c\u52a0\u8f7d\u64cd\u4f5c\u201d\u9009\u62e9 \u52a0\u8f7d \uff0c\u201c\u6587\u4ef6\u8def\u5f84\u201d\u8f93\u5165 /tmp/putToHdfs.csv \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveCreateTable\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4f7f\u7528Hive\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06putToHdfs.csv\u7684\u6570\u636e\u8f93\u5165\u5230\u8868talendHiveCreate\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \u3002 Hive Input \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveInput\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveInput\u3001tHiveClose\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"select * from talendHiveCreate\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveInput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u67e5\u8be2\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 Hive Row \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u63d2\u5165\u6570\u636e\u81f3Hive\u8868\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveRow\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveRow\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveRow_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"insert into talendHiveCreate values(123,'shenzhen')\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveRow\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u63d2\u5165\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5df2\u5305\u542b\u65b0\u589e\u7684\u6570\u636e\u3002 Talend\u5bf9\u63a5FusionInsight HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FusionInsight HBase\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8be2\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHBase\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 \u5df2\u5728IntelliJ IDEA\u4f7f\u7528 Import project from external model ~ Eclipse \u65b9\u5f0f\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \uff0c\u5e76\u4e14\u8c03\u6d4bTestMain.java\u901a\u8fc7\u3002 HBase Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\u3002 \u5728IntelliJ IDEA\u6253\u5f00 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \u5de5\u7a0b\uff0c\u9009\u62e9 File > Project Structure... \u83dc\u5355\u9879\u3002 \u9009\u62e9 Artifacts->Add->JAR->Empty \u3002 \u5bfc\u51fajar\u5305\u7684\u540d\u79f0\u8bbe\u7f6e\u4e3a hbase-loginUtil.jar \uff0c\u201cOutput directory\u201d\u9009\u62e9 C:\\talend\\testFile \uff0c\u53cc\u51fb\u201cAvailable Elements\u201d\u7684 'hbase-example' compile output \u5c06\u5b83\u52a0\u8f7d\u5230\u5de6\u8fb9\u5217\u8868\uff0c\u70b9\u51fb OK \u3002 \u9009\u4e2d\u201chbase-example\u201d\u5de5\u7a0bcom.huawei.hadoop.security\u7684LoginUtil.java\uff0c\u9009\u62e9 Build->Build Artifacts... \u9009\u62e9 hbase-loginUtil.jar->Build \u3002 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u5728\u672c\u5730 C:\\talend\\testFile \u4ea7\u751f\u201chbase-loginUtil.jar\u201d\u3002 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chbaseConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hbaseConnection \uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 C:\\talend\\testFile\\hbase-loginUtil.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseConnection_1\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HBASE \uff0c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u9009\u62e9 Hadoop 2 \u3002\u5982\u679c\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u5219\u70b9\u51fb\u5b89\u88c5\u3002 \u8bf4\u660e\uff1a\u5982\u679c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u7684\u4e0b\u62c9\u6846\u6ca1\u6709\u201cHadoop 2\u201d\uff0c\u53ea\u80fd\u9009\u62e9\u201cHadoop 1\u201d\u3002\u5219\u786e\u8ba4\u521b\u5efaHadoop\u96c6\u7fa4\u65f6\uff0c\u5bf9\u4e8eHBase\u670d\u52a1\uff0c\u662f\u5426\u5df2\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \uff0c\u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u8fd8\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u70b9\u51fb\u5b89\u88c5\uff0c\u5f39\u51fa\u662f jersey-client-1.9.jar \uff0c\u5219\u53ef\u4ee5\u5ffd\u7565\u5904\u7406\u3002\u4f5c\u4e1a\u8fd0\u884c\u540e\uff0c\u8be5\u63d0\u793a\u4f1a\u6d88\u5931\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctJava_1\u201d\u3002 \u5728\u201c\u57fa\u672c\u8bbe\u7f6e\u201d\u7684\u201c\u4ee3\u7801\u201d\u4e2d\u8f93\u5165HBase\u914d\u7f6e\u76f8\u5173\u7684\u4ee3\u7801\u3002 \u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); //\u8bbe\u7f6eKerberos\u8ba4\u8bc1\u7684\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84 System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); //\u589e\u52a0\u914d\u7f6e\u6587\u4ef6\uff0c\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u4f4d\u7f6e\u5237\u65b0 conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hbase-site.xml\")); //\u8f93\u51fa\u914d\u7f6e\u5c5e\u6027 System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); //\u767b\u5f55 LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); \u5728\u201ctJava_1\u201d\u7684\u201c\u9ad8\u7ea7\u8bbe\u7f6e\u201d\u7684\u201c\u5bfc\u5165\u201d\u8f93\u5165 import com.huawei.hadoop.security.LoginUtil; \uff0c \u70b9\u51fb\u9009\u4e2d\u201ctHBaseClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\u3002 HBase Input Output \u64cd\u4f5c\u6b65\u9aa4 \u00b6 Talend\u901a\u8fc7FusionInsight HBase\u63a5\u53e3\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u521b\u5efa\u8868talendHbaseCreate\uff0c\u5c06\u672c\u5730 C:/talend/testFile/putToHdfs.csv \u7684\u6570\u636e\u4f20\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u786e\u8ba4\u672c\u5730\u5df2\u5b58\u5728 C:/talend/testFile/putToHdfs.csv \u3002 putToHdfs.csv\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chbaseInputOutput\u201d\uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u3001tFileInputDelimited\u3001tHBaseOutput\u3001tHBaseInput\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose_1\u7ec4\u4ef6\u7684\u914d\u7f6e\u8bf7\u53c2\u8003\u201cHBase Connection \u64cd\u4f5c\u6b65\u9aa4\u201d\uff0ctLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctFileInputDelimited_1\u201d\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u8bbe\u8ba1schema\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u6587\u4ef6\u540d/\u6d41\u201d\u8f93\u5165 C:/talend/testFile/putToHdfs.csv \uff0c\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseOutput_1\u201d\u3002 \u5728\u201c\u9ad8\u7ea7\u914d\u7f6e\u201d\u4e2d\uff0c\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522b\u4e3aid\u548cname\uff0c\u5217\u540d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u5728\u201c\u57fa\u672c\u914d\u7f6e\u201d\u4e2d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u8868 \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseInputOutput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\uff0c\u4e14\u521b\u5efa\u8868talendHbaseCreate\u5e76\u5c06\u672c\u5730\u6587\u4ef6\u6570\u636e\u8f93\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5HBase\u8868\u201ctalendHbaseCreate\u201d\u3002 hbase shell scan 'hbaseInputOutputTest' FAQ \u00b6 \u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u8fd4\u56deClient cannot authenticate via:[TOKEN, KERBEROS] \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u4f7f\u7528Talend\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u7684\u7ec4\u4ef6\uff08\u4f8b\u5982tHDFSGet_1\uff09\u91c7\u7528\u201c\u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5\u201d\uff0c\u73b0\u6709\u8fde\u63a5tHDFSConnection_1\u7684\u5c5e\u6027\u7c7b\u578b\u662f\u201c\u5b58\u50a8\u5e93\u201d\u65f6\uff0c\u8fd0\u884c\u65f6\u8fd4\u56dejava.io.IOException: DestHost:destPort euleros-hd03:25000 , LocalHost:localPort user-PC/172.16.5.106:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\uff0c\u4e14\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u5931\u8d25\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 tHDFSConnection_1\u4f7f\u7528\u7684\u5b58\u50a8\u5e93FusionInsight_HDFS\u6240\u5c5e\u7684Hadoop\u96c6\u7fa4FusionInsight\u6ca1\u6709\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002\u9700\u8981\u4fee\u6539Hadoop\u96c6\u7fa4FusionInsight\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002 * \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight \uff0c\u53f3\u952e FusionInsight \u9009\u62e9 Edit Hadoop Cluster \u3002 ![](assets/Talend_7.2.1/33ef12f1.png) * \u52fe\u9009`\u4f7f\u7528\u81ea\u5b9a\u4e49Haddop\u914d\u7f6e`\u3002 ![](assets/Talend_7.2.1/b6962abf.png) * \u70b9\u51fb`Yes`\u3002 ![](assets/Talend_7.2.1/9e6ea1d7.png) \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\u8fd4\u56deCannot modify dfs.client.use.datanode.hostname at runtime\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\uff0c\u8fd4\u56de\u7c7b\u4f3c\u7684\u9519\u8bef\uff1aError while processing statement: Cannot modify dfs.client.use.datanode.hostname at runtime. It is not in list of params that are allowed to be modified at runtime\u3002\u53ef\u80fd\u6d89\u53ca\u7684\u6709\u4ee5\u4e0b\u4e09\u4e2a\u5c5e\u6027\uff1a dfs.client.use.datanode.hostname\u3001mapred.job.name\u3001hive.query.name \u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u767b\u5f55FusionInsight Manager\uff0c\u5728Hive\u670d\u52a1\u7684\u914d\u7f6e\u53c2\u6570hive.security.authorization.sqlstd.confwhitelist.append\u65b0\u589e |dfs\\.client\\.use\\.datanode\\.hostname|mapred\\.job\\.name|hive\\.query\\.name \uff0c\u7136\u540e\u91cd\u542fHive\u670d\u52a1\u3002","title":"7.2.1 <--> 6.5"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight","text":"","title":"Talend\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Talend_7.2.1/#_1","text":"Talend 7.2.1 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_2","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001HIVE\u3001HBASE\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u590d\u5236krb5.conf\u6587\u4ef6\u5e76\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002FusionInsight HD\u5ba2\u6237\u7aef\u89e3\u538b\u4e8e\u672c\u5730 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig Zookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff0c\u5982 C:\\developuser\\jaas.conf \uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u672c\u5730 C:\\Windows\\System32\\drivers\\etc\\hosts \u5df2\u6dfb\u52a0FusionInsight\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0ehostname\u7684\u6620\u5c04\u3002 \u672c\u5730\u5df2\u5b89\u88c5Hadoop\u670d\u52a1\uff08\u53ef\u4ece https://hadoop.apache.org/releases.html \u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\uff09\uff0c\u8be5\u9879\u53ef\u9009\u3002\u5982\u679c\u672c\u5730\u6ca1\u5b89\u88c5Hadoop\u670d\u52a1\uff0ctalend\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0eHadoop\u76f8\u5173\u7684\u9519\u8bef\u65e5\u5fd7\uff0c\u4f46\u4e0d\u5f71\u54cd\u5b9e\u9645\u8fd0\u884c\u7ed3\u679c\u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Talend_7.2.1/#talend","text":"","title":"\u5b89\u88c5Talend"},{"location":"Data_Integration/Talend_7.2.1/#_3","text":"\u5b89\u88c5Talend Open Studio for Big Data","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_4","text":"\u4ece https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dWindow\u7248\u7684Talend\u3002 \u89e3\u538b\u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u70b9\u51fbTOS_BD-win-x86_64.exe\u542f\u52a8Talend Open Studio for Big Data\u3002\u70b9\u51fb \u6211\u540c\u610f \u3002 \u70b9\u51fb \u5b8c\u6210 \uff0c\u9ed8\u8ba4\u521b\u5efaLocal_Project\u7684\u5de5\u7a0b\u3002 \u9009\u62e9\u5b89\u88c5\u5fc5\u987b\u7684\u7b2c\u4e09\u65b9\u5e93\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hadoop","text":"","title":"\u521b\u5efaHadoop\u670d\u52a1"},{"location":"Data_Integration/Talend_7.2.1/#_5","text":"\u521b\u5efa\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u7684Hadoop\u96c6\u7fa4","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_6","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\teland\\config \u76ee\u5f55\u4e0b\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#_7","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hadoop_1","text":"\u6253\u5f00 Talend Open Studio for Big Data \uff0c\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster \uff0c\u53f3\u952e Hadoop Cluster \u9009\u62e9 Create Hadoop Cluster \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201cFusionInsight\u201d\uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9 \u4ece\u672c\u5730\u6587\u4ef6\u5bfc\u5165\u914d\u7f6e \uff0c\u70b9\u51fb Next \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u76ee\u5f55 C:\\talend\\config \uff0c\u9ed8\u8ba4\u5168\u9009\uff0c\u70b9\u51fb Finish \u3002 \u201cDistribution\u201d\u9009\u62e9 Custom - Unsuported \uff0c\u70b9\u51fb\u4e0b\u62c9\u6846\u53f3\u8fb9\u7684 \u6309\u94ae\u5bfc\u5165HDFS\u3001HIVE\u3001HBASE\u76f8\u5173\u7684jar\u5305\u3002 \u70b9\u51fb Cancel \u53d6\u6d88\u81ea\u52a8\u5f39\u51fa\u7684\u201c\u5bfc\u5165\u81ea\u5b9a\u4e49\u7684\u5b9a\u4e49\u201d\u7a97\u53e3\u3002 \u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u70b9\u51fb \u6309\u94ae\u6dfb\u52a0HDFS\u76f8\u5173\u7684jar\u5305\u3002 \u9009\u62e9 \u5916\u90e8\u5e93 \uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\hdfs \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\uff0c\u70b9\u51fb OK \u5bfc\u5165jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common \u548c C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHive\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\jdbc \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHBase\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002\u5b8c\u6210\u540e\uff0c\u70b9\u51fb OK \u3002 \u914d\u7f6eKerberos\u8ba4\u8bc1\u3002\u201cCustom->Authentication\u201d\u9009\u62e9 Kerberos \u3002 \u52fe\u9009 Authentication->Enable Kerberos security \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Namenode Principal = hdfs/hadoop.hadoop.com@HADOOP.COM \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u5907\u6ce8\uff1a Namenode Principal\u7684\u53d6\u503c\u4e3ahdfs-site.xml\u7684dfs.namenode.kerberos.principal\u7684value\u503c\uff1b \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u7684\u53d6\u503c\u4e3ayarn-site.xml\u7684yarn.resourcemanager.principal\u7684value\u503c\uff1b \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53\u7684\u53d6\u503c\u4e3amapred-site.xml\u7684mapreduce.jobhistory.principal\u7684value\u503c\u3002 \u52fe\u9009 Authentication->Use a keytab to authenticate \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Principal = developuser Keytab = C:/developuser/user.keytab \u5907\u6ce8\uff1a Principal\u4e3aFusionInsight Manager\u7684\u7528\u6237\u540d\uff0cKeytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u51ed\u636e\u3002 \u914d\u7f6eHadoop\u5c5e\u6027\uff0c\u70b9\u51fb Hadoop\u5c5e\u6027 \u53f3\u8fb9\u7684 \u6309\u94ae\u3002 \u70b9\u51fb \u6309\u94ae\uff0c\u589e\u52a0\u4ee5\u4e0bHadoop\u5c5e\u6027\u3002\u589e\u52a0\u5b8c\u6bd5\uff0c\u70b9\u51fb OK \u3002 \u589e\u52a0core-site.xml\u7684hadoop.security.authentication\u548chadoop.rpc.protection\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\uff1b \u589e\u52a0hdfs-site.xml\u7684dfs.namenode.rpc-address.hacluster.*\uff0cdfs.ha.namenodes.hacluster\u3001dfs.nameservices\u3001dfs.client.failover.proxy.provider.hacluster\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\u3002 \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a hadoop.security.authentication = Kerberos hadoop.rpc.protection = privacy dfs.namenode.rpc-address.hacluster.141 = euleros-hd02:25000 dfs.namenode.rpc-address.hacluster.142 = euleros-hd03:25000 dfs.ha.namenodes.hacluster = 141,142 dfs.nameservices = hacluster dfs.client.failover.proxy.provider.hacluster = org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider \u786e\u8ba4\u9ed8\u8ba4\u52fe\u9009 \u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u5c5e\u6027 \uff0c\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \u3002 \u68c0\u67e5\u8fd4\u56de100%\uff0c\u5219Hadoop\u96c6\u7fa4\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb Close \u3002\u5982\u679c\u8fd4\u56de\u9519\u8bef\u65e5\u5fd7\uff0c\u5219\u6839\u636e\u9519\u8bef\u65e5\u5fd7\u63d0\u793a\u4fee\u6b63\u95ee\u9898\u540e\uff0c\u91cd\u65b0\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \uff0c\u76f4\u81f3\u68c0\u67e5\u8fd4\u56de100%\u3002 \u70b9\u51fb Finish \uff0c\u5219\u53ef\u5728 \u5143\u6570\u636e->Hadoop Cluster \u770b\u5230\u65b0\u5efa\u7684\u201cFusionInsight\u201d\u96c6\u7fa4\uff0c\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u3002","title":"\u521b\u5efaHadoop\u96c6\u7fa4"},{"location":"Data_Integration/Talend_7.2.1/#hive","text":"\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight->Hive(1)->FusionInsight_HIVE \uff0c\u53f3\u952e FusionInsight_HIVE \u9009\u62e9 Edit Hive \u3002 \u70b9\u51fb Next \u3002 \u9700\u8981\u66f4\u65b0\u7684\u914d\u7f6e\u5982\u4e0b\uff0c\u5176\u4f59\u7684\u4fdd\u6301\u4e0d\u53d8\u3002 hive\u6a21\u5f0f = Standalone hive\u670d\u52a1\u5668\u7248\u672c = Hive Server2 -- jdbc:hive2:// \u767b\u5f55\u540d = developuser \u5bc6\u7801 = Huawei@123 \u670d\u52a1\u5668 = 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23 \u7aef\u53e3 = 24002 DataBase = default \u9644\u52a0JDBC\u8bbe\u7f6e = ;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;user.principal=developuser;user.keytab=C:/developuser/user.keytab \u8bf4\u660e\uff1a\u4ee5\u4e0a\u4fe1\u606f\u53ef\u6839\u636eJDBC\u65b9\u5f0f\u8fde\u63a5Hive\u670d\u52a1\u7684\u914d\u7f6e\u586b\u5199\u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002","title":"\u914d\u7f6eHIVE\u670d\u52a1"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight-hdfs","text":"","title":"Talend\u5bf9\u63a5FusionInsight HDFS"},{"location":"Data_Integration/Talend_7.2.1/#_8","text":"Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight HDFS\u63a5\u53e3\uff0c\u5e76\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\uff0c\u6216\u8005\u5c06\u672c\u5730\u6587\u4ef6\u4e0a\u4f20\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_9","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHDFS\u670d\u52a1\u7684Hadoop\u96c6\u7fa4","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#hdfs-connection","text":"\u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chdfsConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hdfsConnection \uff0c\u5728Palette\u9762\u677f\u8f93\u5165\u201chdfsConnection\u201d\u641c\u7d22\uff0c\u5c06\u641c\u7d22\u8fd4\u56de\u7684\u201ctHDFSConnection\u201d\u7ec4\u4ef6\u62d6\u81f3Disigner\u533a\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHDFSConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002\u5982\u679c\u63d0\u793a\u201c\u6b64\u7ec4\u4ef6tHDFSConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002\u201d\uff0c\u5219\u70b9\u51fb \u5b89\u88c5 \u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \u3002 \u7b49\u5f85\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HDFS\u6210\u529f\u3002","title":"HDFS Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hdfs-get","text":"\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cgetFromHdfs.csv\u201d\uff0c\u5185\u5bb9\u968f\u610f\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsGet\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSGet\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \uff0c\u53f3\u952e\u9009\u62e9 \u89e6\u53d1\u5668->\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6 \uff0c\u8fde\u63a5\u81f3tHDFSGet_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSGet_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile (\u53ef\u9009\u62e9\u4efb\u610f\u7684\u672c\u5730\u76ee\u5f55)\uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u5728HDFS\u7684/tmp\u76ee\u5f55\u4e0b\u9700\u8981\u83b7\u53d6\u7684\u6587\u4ef6\u540d\u79f0 getFromHdfs.csv \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsGet\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4eceFusionInsight HDFS\u4e0b\u8f7d\u6587\u4ef6\u6210\u529f\u3002 getFromHdfs.csv\u5df2\u4e0b\u8f7d\u81f3\u672c\u5730 C:\\talend\\testFile \u3002","title":"HDFS Get\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hdfs-put","text":"\u4ece\u672c\u5730\u4e0a\u4f20\u6587\u4ef6\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsPut\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSPut\u7ec4\u4ef6\uff0ctHDFSConnection_1\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884ctHDFSPut_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSPut_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u9700\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684\u6587\u4ef6\u540d\u79f0 putToHdfs.csv \u3002 \u8bf4\u660e\uff1a C:/talend/testFile/putToHdfs.csv \u4e3a\u672c\u5730\u5df2\u5b58\u5728\u6587\u4ef6\uff0c\u5185\u5bb9\u968f\u610f\u3002 ![](assets/Talend_7.2.1/61c635ee.png) \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsPut\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793a\u4eceTalend\u4e0a\u4f20\u6587\u4ef6putToHdfs.csv\u81f3FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u68c0\u67e5putToHdfs.csv\u5df2\u4e0a\u4f20\u81f3 /tmp \u76ee\u5f55\u3002","title":"HDFS Put \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight-hive","text":"","title":"Talend\u5bf9\u63a5FusionInsight Hive"},{"location":"Data_Integration/Talend_7.2.1/#_10","text":"Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8868\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_11","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHive\u670d\u52a1\u7684Hadoop\u96c6\u7fa4\u548c\u5b8c\u6210Hadoop\u96c6\u7fa4\u7684Hive\u670d\u52a1\u914d\u7f6e\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#hive-connection","text":"\u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chiveConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hiveConnection \uff0c\u52a0\u5165tHiveConnection\u3001tHiveClose\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight Hive\u6210\u529f\u3002","title":"Hive Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hive-create-table","text":"\u4f7f\u7528Talend\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06/tmp/putToHdfs.csv\u7684\u6570\u636e\u4f20\u5165\u8868talendHiveCreate\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cputToHdfs.csv\u201d\u3002 putToHdfs.csv\u7684\u5185\u5bb9\u5982\u4e0b(\u5305\u542b\u4e24\u5217\uff0c\u4e24\u5217\u4e4b\u95f4\u7528\u5206\u53f7\u9694\u5f00)\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chiveCreateTable\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveCreateTable\u3001tHiveLoad\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveCreateTable_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u8868 \uff0c\u201c\u683c\u5f0f\u201d\u9009\u62e9 \u6587\u672c\u6587\u4ef6 \uff0c\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveLoad_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u201c\u52a0\u8f7d\u64cd\u4f5c\u201d\u9009\u62e9 \u52a0\u8f7d \uff0c\u201c\u6587\u4ef6\u8def\u5f84\u201d\u8f93\u5165 /tmp/putToHdfs.csv \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveCreateTable\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4f7f\u7528Hive\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06putToHdfs.csv\u7684\u6570\u636e\u8f93\u5165\u5230\u8868talendHiveCreate\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \u3002","title":"Hive Create Table \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hive-input","text":"\u4f7f\u7528Talend\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveInput\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveInput\u3001tHiveClose\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"select * from talendHiveCreate\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveInput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u67e5\u8be2\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002","title":"Hive Input \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hive-row","text":"\u4f7f\u7528Talend\u63d2\u5165\u6570\u636e\u81f3Hive\u8868\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveRow\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveRow\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveRow_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"insert into talendHiveCreate values(123,'shenzhen')\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveRow\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u63d2\u5165\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5df2\u5305\u542b\u65b0\u589e\u7684\u6570\u636e\u3002","title":"Hive Row \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight-hbase","text":"","title":"Talend\u5bf9\u63a5FusionInsight HBase"},{"location":"Data_Integration/Talend_7.2.1/#_12","text":"Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FusionInsight HBase\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8be2\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_13","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHBase\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 \u5df2\u5728IntelliJ IDEA\u4f7f\u7528 Import project from external model ~ Eclipse \u65b9\u5f0f\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \uff0c\u5e76\u4e14\u8c03\u6d4bTestMain.java\u901a\u8fc7\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#hbase-connection","text":"\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\u3002 \u5728IntelliJ IDEA\u6253\u5f00 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \u5de5\u7a0b\uff0c\u9009\u62e9 File > Project Structure... \u83dc\u5355\u9879\u3002 \u9009\u62e9 Artifacts->Add->JAR->Empty \u3002 \u5bfc\u51fajar\u5305\u7684\u540d\u79f0\u8bbe\u7f6e\u4e3a hbase-loginUtil.jar \uff0c\u201cOutput directory\u201d\u9009\u62e9 C:\\talend\\testFile \uff0c\u53cc\u51fb\u201cAvailable Elements\u201d\u7684 'hbase-example' compile output \u5c06\u5b83\u52a0\u8f7d\u5230\u5de6\u8fb9\u5217\u8868\uff0c\u70b9\u51fb OK \u3002 \u9009\u4e2d\u201chbase-example\u201d\u5de5\u7a0bcom.huawei.hadoop.security\u7684LoginUtil.java\uff0c\u9009\u62e9 Build->Build Artifacts... \u9009\u62e9 hbase-loginUtil.jar->Build \u3002 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u5728\u672c\u5730 C:\\talend\\testFile \u4ea7\u751f\u201chbase-loginUtil.jar\u201d\u3002 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chbaseConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hbaseConnection \uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 C:\\talend\\testFile\\hbase-loginUtil.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseConnection_1\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HBASE \uff0c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u9009\u62e9 Hadoop 2 \u3002\u5982\u679c\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u5219\u70b9\u51fb\u5b89\u88c5\u3002 \u8bf4\u660e\uff1a\u5982\u679c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u7684\u4e0b\u62c9\u6846\u6ca1\u6709\u201cHadoop 2\u201d\uff0c\u53ea\u80fd\u9009\u62e9\u201cHadoop 1\u201d\u3002\u5219\u786e\u8ba4\u521b\u5efaHadoop\u96c6\u7fa4\u65f6\uff0c\u5bf9\u4e8eHBase\u670d\u52a1\uff0c\u662f\u5426\u5df2\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \uff0c\u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u8fd8\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u70b9\u51fb\u5b89\u88c5\uff0c\u5f39\u51fa\u662f jersey-client-1.9.jar \uff0c\u5219\u53ef\u4ee5\u5ffd\u7565\u5904\u7406\u3002\u4f5c\u4e1a\u8fd0\u884c\u540e\uff0c\u8be5\u63d0\u793a\u4f1a\u6d88\u5931\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctJava_1\u201d\u3002 \u5728\u201c\u57fa\u672c\u8bbe\u7f6e\u201d\u7684\u201c\u4ee3\u7801\u201d\u4e2d\u8f93\u5165HBase\u914d\u7f6e\u76f8\u5173\u7684\u4ee3\u7801\u3002 \u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); //\u8bbe\u7f6eKerberos\u8ba4\u8bc1\u7684\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84 System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); //\u589e\u52a0\u914d\u7f6e\u6587\u4ef6\uff0c\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u4f4d\u7f6e\u5237\u65b0 conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hbase-site.xml\")); //\u8f93\u51fa\u914d\u7f6e\u5c5e\u6027 System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); //\u767b\u5f55 LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); \u5728\u201ctJava_1\u201d\u7684\u201c\u9ad8\u7ea7\u8bbe\u7f6e\u201d\u7684\u201c\u5bfc\u5165\u201d\u8f93\u5165 import com.huawei.hadoop.security.LoginUtil; \uff0c \u70b9\u51fb\u9009\u4e2d\u201ctHBaseClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\u3002","title":"HBase Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hbase-input-output","text":"Talend\u901a\u8fc7FusionInsight HBase\u63a5\u53e3\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u521b\u5efa\u8868talendHbaseCreate\uff0c\u5c06\u672c\u5730 C:/talend/testFile/putToHdfs.csv \u7684\u6570\u636e\u4f20\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u786e\u8ba4\u672c\u5730\u5df2\u5b58\u5728 C:/talend/testFile/putToHdfs.csv \u3002 putToHdfs.csv\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chbaseInputOutput\u201d\uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u3001tFileInputDelimited\u3001tHBaseOutput\u3001tHBaseInput\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose_1\u7ec4\u4ef6\u7684\u914d\u7f6e\u8bf7\u53c2\u8003\u201cHBase Connection \u64cd\u4f5c\u6b65\u9aa4\u201d\uff0ctLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctFileInputDelimited_1\u201d\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u8bbe\u8ba1schema\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u6587\u4ef6\u540d/\u6d41\u201d\u8f93\u5165 C:/talend/testFile/putToHdfs.csv \uff0c\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseOutput_1\u201d\u3002 \u5728\u201c\u9ad8\u7ea7\u914d\u7f6e\u201d\u4e2d\uff0c\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522b\u4e3aid\u548cname\uff0c\u5217\u540d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u5728\u201c\u57fa\u672c\u914d\u7f6e\u201d\u4e2d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u8868 \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseInputOutput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\uff0c\u4e14\u521b\u5efa\u8868talendHbaseCreate\u5e76\u5c06\u672c\u5730\u6587\u4ef6\u6570\u636e\u8f93\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5HBase\u8868\u201ctalendHbaseCreate\u201d\u3002 hbase shell scan 'hbaseInputOutputTest'","title":"HBase Input Output \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#faq","text":"\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u8fd4\u56deClient cannot authenticate via:[TOKEN, KERBEROS] \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u4f7f\u7528Talend\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u7684\u7ec4\u4ef6\uff08\u4f8b\u5982tHDFSGet_1\uff09\u91c7\u7528\u201c\u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5\u201d\uff0c\u73b0\u6709\u8fde\u63a5tHDFSConnection_1\u7684\u5c5e\u6027\u7c7b\u578b\u662f\u201c\u5b58\u50a8\u5e93\u201d\u65f6\uff0c\u8fd0\u884c\u65f6\u8fd4\u56dejava.io.IOException: DestHost:destPort euleros-hd03:25000 , LocalHost:localPort user-PC/172.16.5.106:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\uff0c\u4e14\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u5931\u8d25\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 tHDFSConnection_1\u4f7f\u7528\u7684\u5b58\u50a8\u5e93FusionInsight_HDFS\u6240\u5c5e\u7684Hadoop\u96c6\u7fa4FusionInsight\u6ca1\u6709\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002\u9700\u8981\u4fee\u6539Hadoop\u96c6\u7fa4FusionInsight\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002 * \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight \uff0c\u53f3\u952e FusionInsight \u9009\u62e9 Edit Hadoop Cluster \u3002 ![](assets/Talend_7.2.1/33ef12f1.png) * \u52fe\u9009`\u4f7f\u7528\u81ea\u5b9a\u4e49Haddop\u914d\u7f6e`\u3002 ![](assets/Talend_7.2.1/b6962abf.png) * \u70b9\u51fb`Yes`\u3002 ![](assets/Talend_7.2.1/9e6ea1d7.png) \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\u8fd4\u56deCannot modify dfs.client.use.datanode.hostname at runtime\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\uff0c\u8fd4\u56de\u7c7b\u4f3c\u7684\u9519\u8bef\uff1aError while processing statement: Cannot modify dfs.client.use.datanode.hostname at runtime. It is not in list of params that are allowed to be modified at runtime\u3002\u53ef\u80fd\u6d89\u53ca\u7684\u6709\u4ee5\u4e0b\u4e09\u4e2a\u5c5e\u6027\uff1a dfs.client.use.datanode.hostname\u3001mapred.job.name\u3001hive.query.name \u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u767b\u5f55FusionInsight Manager\uff0c\u5728Hive\u670d\u52a1\u7684\u914d\u7f6e\u53c2\u6570hive.security.authorization.sqlstd.confwhitelist.append\u65b0\u589e |dfs\\.client\\.use\\.datanode\\.hostname|mapred\\.job\\.name|hive\\.query\\.name \uff0c\u7136\u540e\u91cd\u542fHive\u670d\u52a1\u3002","title":"FAQ"},{"location":"Data_Integration/\u676d\u5dde\u5408\u4f17UTL/","text":"\u676d\u5dde\u5408\u4f17UTL\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 \u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive/Kafka)","title":"5.1 <--> C50"},{"location":"Data_Integration/\u676d\u5dde\u5408\u4f17UTL/#utlfusioninsight","text":"","title":"\u676d\u5dde\u5408\u4f17UTL\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/\u676d\u5dde\u5408\u4f17UTL/#_1","text":"\u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/","text":"\u6570\u636e\u5e93 \u00b6 Apache Druid 0.14.2 \u2194 C80 0.15.1 \u2194 C80 SAP VORA 2.0 \u2194 C70 2.1 \u2194 C70 \u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 C50","title":"Home"},{"location":"Database/#_1","text":"Apache Druid 0.14.2 \u2194 C80 0.15.1 \u2194 C80 SAP VORA 2.0 \u2194 C70 2.1 \u2194 C70 \u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 C50","title":"\u6570\u636e\u5e93"},{"location":"Database/SAP_VORA/","text":"SAP VORA\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SAP VORA 2.0 \u2194 FusionInsight HD V100R002C70SPC200 (Spark) SAP VORA 2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Spark)","title":"2.1 <--> C70"},{"location":"Database/SAP_VORA/#sap-vorafusioninsight","text":"","title":"SAP VORA\u5bf9\u63a5FusionInsight"},{"location":"Database/SAP_VORA/#_1","text":"SAP VORA 2.0 \u2194 FusionInsight HD V100R002C70SPC200 (Spark) SAP VORA 2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/apache_druid_0.14.2/","text":"Apache Druid 0.14.2 \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Druid 0.14.2 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8 \u73af\u5883\u63cf\u8ff0 \u00b6 FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.121 \u51c6\u5907\u5de5\u4f5c \u00b6 FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907 \u00b6 \u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5MySQL \u00b6 Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.121 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit \u5b89\u88c5\uff0c\u90e8\u7f72Druid \u00b6 \u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.14.2-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.14.2-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.121:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4 \u00b6 \u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid121/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid121/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0a\u8ff0\u6b65\u9aa4\u4fee\u6539\u540e\u7684common.runtime.properties\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u8def\u5f84\u4e0b \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.14.2-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar core.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.14.2-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar htrace-core4-4.0.1-incubating.jar jackson-core-asl-1.9.13.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-xc-1.9.13.jar javaluator-3.0.1.jar jaxb-api-2.2.2.jar jcip-annotations-1.0.jar jersey-client-1.9.jar jetty-6.1.26.jar jetty-sslengine-6.1.26.jar jetty-util-6.1.26.jar json-smart-1.1.1.jar jsp-api-2.1.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar nimbus-jose-jwt-3.9.jar objenesis-2.6.jar okhttp-2.4.0.jar okio-1.4.0.jar protobuf-java-2.5.0.jar rt.jar servlet-api-2.5.jar stax-api-1.0-2.jar xercesImpl-2.9.1.jar xml-apis-1.3.04.jar xmlenc-0.52.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid121/indexing-logs \u4ee5\u53ca /druid121/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task -f /opt/druid/apache-druid-0.14.2-incubating/wikipedia-index-hadoop.json \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21007 \u521b\u5efatopic wikipedia21007 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u5728druid\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a \u5c06\u8ba4\u8bc1\u4f7f\u7528\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5e76\u8986\u76d6\u5230druid\u4e3b\u673a\u7684/etc\u8def\u5f84\u4e0b\uff0cdruid\u9ed8\u8ba4\u4ece\u6b64\u8def\u5f84\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/conf/druid \u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid , \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u4e2d\u7684druid \u4f7f\u7528\u547d\u4ee4 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u96c6\u7fa4\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com\" \u52a0\u8f7d\u8fd0\u884cjava\u6574\u4f53jvm\u53c2\u6570\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u7ed3\u679c\uff1a \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21007\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21007\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21007,172.16.6.12:21007,172.16.6.10:21007\", \"kerberos.domain.name\": \"hadoop.hadoop.com\", \"security.protocol\": \"SASL_PLAINTEXT\", \"sasl.kerberos.service.name\": \"kafka\" } } } \u540e\u53f0\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4Kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8\u5b89\u5168\u6a21\u5f0fkafka producer bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic wikipedia21007 --producer.config config/producer.properties \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u5185\u5bb9\u5982\u4e0b {\"time\":\"2015-09-12T05:22:32.338Z\",\"channel\":\"#zh.wikipedia\",\"cityName\":null,\"comment\":\"/* \u6210\u7acb */\",\"countryIsoCode\":null,\"countryName\":null,\"isAnonymous\":false,\"isMinor\":false,\"isNew\":false,\"isRobot\":false,\"isUnpatrolled\":false,\"metroCode\":null,\"namespace\":\"Main\",\"page\":\"\u8056\u4f2f\u591a\u797f\u53f8\u9438\u5144\u5f1f\u6703\",\"regionIsoCode\":null,\"regionName\":null,\"user\":\"\u91d1\u8085\",\"delta\":675,\"added\":675,\"deleted\":0} \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT * FROM wikipedia21007","title":"0.14.2 <--> C80"},{"location":"Database/apache_druid_0.14.2/#apache-druid-0142-fusioninsight","text":"","title":"Apache Druid 0.14.2 \u5bf9\u63a5FusionInsight"},{"location":"Database/apache_druid_0.14.2/#_1","text":"Apache Druid 0.14.2 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/apache_druid_0.14.2/#_2","text":"\u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Database/apache_druid_0.14.2/#_3","text":"FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.121","title":"\u73af\u5883\u63cf\u8ff0"},{"location":"Database/apache_druid_0.14.2/#_4","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Database/apache_druid_0.14.2/#fi-hd","text":"\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6","title":"FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907"},{"location":"Database/apache_druid_0.14.2/#mysql","text":"Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.121 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit","title":"\u5b89\u88c5MySQL"},{"location":"Database/apache_druid_0.14.2/#druid","text":"\u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.14.2-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.14.2-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.121:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid","title":"\u5b89\u88c5\uff0c\u90e8\u7f72Druid"},{"location":"Database/apache_druid_0.14.2/#druidfi-hd","text":"\u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid121/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid121/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0a\u8ff0\u6b65\u9aa4\u4fee\u6539\u540e\u7684common.runtime.properties\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u8def\u5f84\u4e0b \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.14.2-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar core.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.14.2-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar htrace-core4-4.0.1-incubating.jar jackson-core-asl-1.9.13.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-xc-1.9.13.jar javaluator-3.0.1.jar jaxb-api-2.2.2.jar jcip-annotations-1.0.jar jersey-client-1.9.jar jetty-6.1.26.jar jetty-sslengine-6.1.26.jar jetty-util-6.1.26.jar json-smart-1.1.1.jar jsp-api-2.1.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar nimbus-jose-jwt-3.9.jar objenesis-2.6.jar okhttp-2.4.0.jar okio-1.4.0.jar protobuf-java-2.5.0.jar rt.jar servlet-api-2.5.jar stax-api-1.0-2.jar xercesImpl-2.9.1.jar xml-apis-1.3.04.jar xmlenc-0.52.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid121/indexing-logs \u4ee5\u53ca /druid121/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task -f /opt/druid/apache-druid-0.14.2-incubating/wikipedia-index-hadoop.json \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4"},{"location":"Database/apache_druid_0.14.2/#druidfi-hdkafka","text":"\u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f"},{"location":"Database/apache_druid_0.14.2/#druidfi-hdkafka_1","text":"\u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21007 \u521b\u5efatopic wikipedia21007 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u5728druid\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a \u5c06\u8ba4\u8bc1\u4f7f\u7528\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5e76\u8986\u76d6\u5230druid\u4e3b\u673a\u7684/etc\u8def\u5f84\u4e0b\uff0cdruid\u9ed8\u8ba4\u4ece\u6b64\u8def\u5f84\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/conf/druid \u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid , \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u4e2d\u7684druid \u4f7f\u7528\u547d\u4ee4 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u96c6\u7fa4\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com\" \u52a0\u8f7d\u8fd0\u884cjava\u6574\u4f53jvm\u53c2\u6570\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u7ed3\u679c\uff1a \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21007\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21007\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21007,172.16.6.12:21007,172.16.6.10:21007\", \"kerberos.domain.name\": \"hadoop.hadoop.com\", \"security.protocol\": \"SASL_PLAINTEXT\", \"sasl.kerberos.service.name\": \"kafka\" } } } \u540e\u53f0\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4Kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8\u5b89\u5168\u6a21\u5f0fkafka producer bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic wikipedia21007 --producer.config config/producer.properties \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u5185\u5bb9\u5982\u4e0b {\"time\":\"2015-09-12T05:22:32.338Z\",\"channel\":\"#zh.wikipedia\",\"cityName\":null,\"comment\":\"/* \u6210\u7acb */\",\"countryIsoCode\":null,\"countryName\":null,\"isAnonymous\":false,\"isMinor\":false,\"isNew\":false,\"isRobot\":false,\"isUnpatrolled\":false,\"metroCode\":null,\"namespace\":\"Main\",\"page\":\"\u8056\u4f2f\u591a\u797f\u53f8\u9438\u5144\u5f1f\u6703\",\"regionIsoCode\":null,\"regionName\":null,\"user\":\"\u91d1\u8085\",\"delta\":675,\"added\":675,\"deleted\":0} \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT * FROM wikipedia21007","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Database/apache_druid_0.15.1/","text":"Apache Druid 0.15.1 \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Druid 0.15.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8 \u73af\u5883\u63cf\u8ff0 \u00b6 FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.120 \u51c6\u5907\u5de5\u4f5c \u00b6 FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907 \u00b6 \u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5MySQL \u00b6 Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.120 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit \u5b89\u88c5\uff0c\u90e8\u7f72Druid \u00b6 \u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.15.1-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.15.1-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/start-micro-quickstart \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.120:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS \u00b6 \u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid120/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid120/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.15.1-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.15.1-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar avro-1.7.4.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.15.1-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.15.1-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.15.1-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid120/indexing-logs \u4ee5\u53ca /druid120/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/start-micro-quickstart \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task --file /opt/druid/apache-druid-0.15.1-incubating/wikipedia-index-hadoop.json --url http://172.16.2.120:8081 \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.120:8888/ \u70b9\u51fbsupervisor \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1aDruid Kafka\u7248\u672c\u4e3a2.1.0\uff0c FI HD\u7248\u672c\u6700\u9ad8\u4e3a1.1.0\uff0c\u7248\u672c\u4e0d\u9002\u914d\u6240\u4ee5\u6682\u4e0d\u652f\u6301kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5","title":"0.15.1 <--> C80"},{"location":"Database/apache_druid_0.15.1/#apache-druid-0151-fusioninsight","text":"","title":"Apache Druid 0.15.1 \u5bf9\u63a5FusionInsight"},{"location":"Database/apache_druid_0.15.1/#_1","text":"Apache Druid 0.15.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/apache_druid_0.15.1/#_2","text":"\u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Database/apache_druid_0.15.1/#_3","text":"FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.120","title":"\u73af\u5883\u63cf\u8ff0"},{"location":"Database/apache_druid_0.15.1/#_4","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Database/apache_druid_0.15.1/#fi-hd","text":"\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6","title":"FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907"},{"location":"Database/apache_druid_0.15.1/#mysql","text":"Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.120 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit","title":"\u5b89\u88c5MySQL"},{"location":"Database/apache_druid_0.15.1/#druid","text":"\u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.15.1-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.15.1-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/start-micro-quickstart \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.120:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid","title":"\u5b89\u88c5\uff0c\u90e8\u7f72Druid"},{"location":"Database/apache_druid_0.15.1/#druidfi-hdhdfs","text":"\u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid120/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid120/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.15.1-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.15.1-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar avro-1.7.4.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.15.1-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.15.1-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.15.1-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid120/indexing-logs \u4ee5\u53ca /druid120/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/start-micro-quickstart \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task --file /opt/druid/apache-druid-0.15.1-incubating/wikipedia-index-hadoop.json --url http://172.16.2.120:8081 \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS"},{"location":"Database/apache_druid_0.15.1/#druidfi-hdkafka","text":"\u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.120:8888/ \u70b9\u51fbsupervisor \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f"},{"location":"Database/apache_druid_0.15.1/#druidfi-hdkafka_1","text":"\u8bf4\u660e\uff1aDruid Kafka\u7248\u672c\u4e3a2.1.0\uff0c FI HD\u7248\u672c\u6700\u9ad8\u4e3a1.1.0\uff0c\u7248\u672c\u4e0d\u9002\u914d\u6240\u4ee5\u6682\u4e0d\u652f\u6301kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Database/\u676d\u5dde\u5408\u4f17UDB/","text":"\u676d\u5dde\u5408\u4f17UDB\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 \u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 FusionInsight HD V100R002C50 (GaussDB)","title":"6.1 <--> C50"},{"location":"Database/\u676d\u5dde\u5408\u4f17UDB/#udbfusioninsight","text":"","title":"\u676d\u5dde\u5408\u4f17UDB\u5bf9\u63a5FusionInsight"},{"location":"Database/\u676d\u5dde\u5408\u4f17UDB/#_1","text":"\u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 FusionInsight HD V100R002C50 (GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/","text":"\u96c6\u6210\u5f00\u53d1\u73af\u5883 \u00b6 DBeaver 4.0.8 \u2194 C60 4.2.1 \u2194 C70 6.1.4 \u2194 6.5 DbVisualizer 10.0.1 \u2194 C70 10.0.21 \u2194 6.5 9.5.7 \u2194 C60 HUE 4.0.1 \u2194 C70 Jupyter Notebook 2.4.4.0 \u2194 C70 2.7.16 \u2194 C80 2.7.16 \u2194 6.5 JupyterHub 1.0.0 \u2194 C80 RStudio 3.4.1 \u2194 C60 3.4.1 \u2194 C70 Squirrel 3.7.1 \u2194 C60 3.8.0 \u2194 C70 3.9.1 \u2194 6.5 Zeppelin 0.7.2 \u2194 C60 0.7.3 \u2194 C70 0.7.3 \u2194 C80 0.8.0 \u2194 C80","title":"Home"},{"location":"Development/#_1","text":"DBeaver 4.0.8 \u2194 C60 4.2.1 \u2194 C70 6.1.4 \u2194 6.5 DbVisualizer 10.0.1 \u2194 C70 10.0.21 \u2194 6.5 9.5.7 \u2194 C60 HUE 4.0.1 \u2194 C70 Jupyter Notebook 2.4.4.0 \u2194 C70 2.7.16 \u2194 C80 2.7.16 \u2194 6.5 JupyterHub 1.0.0 \u2194 C80 RStudio 3.4.1 \u2194 C60 3.4.1 \u2194 C70 Squirrel 3.7.1 \u2194 C60 3.8.0 \u2194 C70 3.9.1 \u2194 6.5 Zeppelin 0.7.2 \u2194 C60 0.7.3 \u2194 C70 0.7.3 \u2194 C80 0.8.0 \u2194 C80","title":"\u96c6\u6210\u5f00\u53d1\u73af\u5883"},{"location":"Development/DBeaver_4.2.1/","text":"DBeaver\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DBeaver 4.0.8 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DBeaver 4.2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL) \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DBeaver\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 Linux\u4e0bDBeaver\u8fde\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b89\u88c5\u597dLinux\uff08Redhat Linux Enterprise 6.5 64bit\uff09Desktop\u64cd\u4f5c\u7cfb\u7edf\uff1b \u5df2\u7ecf\u5b89\u88c5\u597d\u7684Linux\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5jdk1.8\uff0cDBeaver4.0.8\u9700\u8981jdk1.8\u4ee5\u4e0a\u7248\u672c tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf/etc/profile\uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH \u4e0b\u8f7d\u5730\u5740\uff1a http://dbeaver.jkiss.org/download/ , \u8f6f\u4ef6 dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \uff0c\u5b89\u88c5DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u4ea7\u54c1\u6587\u6863\u300b\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \uff0c\u5176\u4e2dFiber\u5ba2\u6237\u7aef\u76ee\u5f55 /opt/hadoopclient/Fiber/ \u3002 \u4fee\u6539Fiber\u7684\u914d\u7f6e\u6587\u4ef6 /opt/hadoopclient/Fiber/conf/fiber.xml \uff0c\u5c06\u5176\u4e2dhive\u3001spark\u3001phoenix\u7684\u8ba4\u8bc1\u65b9\u5f0f\u6539\u4e3a\u5b89\u5168\u6a21\u5f0fkeytab\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u6cd5\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 Hive JDBC\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>{HIVE_CLIENT_ZK_PRINCIPAL}</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Spark\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Phoenix\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> jaas.conf\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u6253\u5f00DBeaver\uff0c\u8fdb\u5165DBeaver\u7684\u5b89\u88c5\u76ee\u5f55\u6267\u884c ./dbeaver \uff0c\u542f\u52a8dbeaver \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef /opt/hadoopclient/Fiber/lib/ \u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027\uff1a \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection , \u7c7b\u578b\u9009\u62e9Fiber User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u914d\u7f6eDriver properties\u91cc\u9762\u7684defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\uff0c\u70b9\u51fbnext Network\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u8f93\u5165\u81ea\u5b9a\u4e49Connection name\u540e\uff0c\u70b9\u51fb finish , \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u94fe\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u94fe\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e Windows\u4e0bDBeaver\u8fde\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix \u524d\u63d0\u6761\u4ef6 \u00b6 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5b8c\u6210windows\u4e0a\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u64cd\u4f5c\u6b65\u9aa4 \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u53ef\u4ee5\u7528kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\uff0c\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002kinit\u8ba4\u8bc1\u7684\u6709\u6548\u671f\u662f24\u5c0f\u65f6\uff0ckeytab\u8ba4\u8bc1\u65b9\u5f0f\u957f\u671f\u6709\u6548 - \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e - \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\uff0c\u5e76\u5b89\u88c5 http://web.mit.edu/kerberos/dist/#kfw-4.0 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u89d2\u8272\u548c\u4eba\u673a\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u7cfb\u7edf\u8bbe\u7f6e -> \u6743\u9650\u7ba1\u7406 -> \u7528\u6237\u7ba1\u7406 -> \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\uff0c\u521b\u5efa\u7528\u6237\u201ctest\u201d \u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6 user.keytab \u4ee5\u53ca krb5.conf \u6587\u4ef6\uff0c\u628a krb5.conf \u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d(\u5982\uff1a test@HADOOP.COM )\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; \u4fee\u6539 fiber.xml \u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5\u524d\u786e\u8ba4kerberos\u8ba4\u8bc1\u6709\u6548 \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab\u548chbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5Fiber \u00b6 \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef\uff08 /opt/hadoopclient/Fiber/lib/ \uff09\u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027 \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u786e\u8ba4defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\u3002 Network\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u81ea\u5b9a\u4e49Connection name\uff0c\u70b9\u51fbfinish \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u8fde\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u8fde\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e DBeaver\u5bf9\u63a5Fiber\u529f\u80fd\u9a8c\u8bc1 \u00b6 Hive\u589e\u52a0\u67e5\u770b\u6570\u636e \u00b6 \u5c06JDBC\u7684defaultDrive\u5207\u6362\u81f3Hive Hive\u67e5\u8be2\u6570\u636e\uff1a\u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 \u67e5\u770b\u66f4\u65b0\u540e\u6570\u636e\uff1a Spark\u589e\u52a0\u67e5\u770b\u6570\u636e \u00b6 \u5c06JDBC \u7684defaultDriver\u5207\u6362\u81f3Spark Spark\u67e5\u8be2\u6570\u636e\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Spark\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3Spark\u7684JDBCServer(\u4e3b)\u5b9e\u4f8b\u6240\u5728\u7684\u8282\u70b9\u7684/opt/\u76ee\u5f55\u4e0b \u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info \u67e5\u770b\u7ed3\u679c\uff1a Phoenix\u589e\u5220\u6539\u67e5\u6570\u636e \u00b6 \u5c06JDBC \u7684defaultDrive\u5207\u6362\u81f3Phoenix Phoenix\u589e\u52a0\u6570\u636e \u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2 \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') \u67e5\u770b\u589e\u52a0\u7684\u6570\u636e\uff1a Phoenix\u5220\u9664\u6570\u636e \u9875\u9762\u4e0a\u5220\u9664\uff1a\u9009\u62e9\u5f85\u5220\u9664\u7684\u5217\uff0c\u7136\u540e\u70b9\u51fb\u4e0b\u65b9 \u5220\u9664 \u6309\u94ae\uff0c\u7136\u540e\u70b9\u51fb save \u6309\u94ae\uff1a \u811a\u672c\u5220\u9664\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae delete from TB_PHOENIX where ID=104; \u67e5\u770b\u8f93\u51fa\u540e\u7684\u6570\u636e Phoenix\u66f4\u65b0\u6570\u636e, \u7f16\u8f91\u66f4\u65b0\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') \u67e5\u770b\u66f4\u65b0\u540e\u7684\u6570\u636e\uff1a \u67e5\u770b\u6570\u636e\uff1a\u7f16\u8f91\u67e5\u8be2\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae\u3002 SELECT * FROM TB_PHOENIX","title":"4.2.1 <--> C70"},{"location":"Development/DBeaver_4.2.1/#dbeaverfusioninsight","text":"","title":"DBeaver\u5bf9\u63a5FusionInsight"},{"location":"Development/DBeaver_4.2.1/#_1","text":"DBeaver 4.0.8 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DBeaver 4.2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DBeaver_4.2.1/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DBeaver\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/DBeaver_4.2.1/#linuxdbeaverfiber","text":"","title":"Linux\u4e0bDBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver_4.2.1/#_3","text":"\u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_4.2.1/#_4","text":"\u5df2\u7ecf\u5b89\u88c5\u597dLinux\uff08Redhat Linux Enterprise 6.5 64bit\uff09Desktop\u64cd\u4f5c\u7cfb\u7edf\uff1b \u5df2\u7ecf\u5b89\u88c5\u597d\u7684Linux\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_4.2.1/#_5","text":"\u5b89\u88c5jdk1.8\uff0cDBeaver4.0.8\u9700\u8981jdk1.8\u4ee5\u4e0a\u7248\u672c tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf/etc/profile\uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH \u4e0b\u8f7d\u5730\u5740\uff1a http://dbeaver.jkiss.org/download/ , \u8f6f\u4ef6 dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \uff0c\u5b89\u88c5DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u4ea7\u54c1\u6587\u6863\u300b\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \uff0c\u5176\u4e2dFiber\u5ba2\u6237\u7aef\u76ee\u5f55 /opt/hadoopclient/Fiber/ \u3002 \u4fee\u6539Fiber\u7684\u914d\u7f6e\u6587\u4ef6 /opt/hadoopclient/Fiber/conf/fiber.xml \uff0c\u5c06\u5176\u4e2dhive\u3001spark\u3001phoenix\u7684\u8ba4\u8bc1\u65b9\u5f0f\u6539\u4e3a\u5b89\u5168\u6a21\u5f0fkeytab\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u6cd5\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 Hive JDBC\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>{HIVE_CLIENT_ZK_PRINCIPAL}</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Spark\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Phoenix\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> jaas.conf\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u6253\u5f00DBeaver\uff0c\u8fdb\u5165DBeaver\u7684\u5b89\u88c5\u76ee\u5f55\u6267\u884c ./dbeaver \uff0c\u542f\u52a8dbeaver \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef /opt/hadoopclient/Fiber/lib/ \u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027\uff1a \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection , \u7c7b\u578b\u9009\u62e9Fiber User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u914d\u7f6eDriver properties\u91cc\u9762\u7684defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\uff0c\u70b9\u51fbnext Network\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u8f93\u5165\u81ea\u5b9a\u4e49Connection name\u540e\uff0c\u70b9\u51fb finish , \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u94fe\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u94fe\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_4.2.1/#windowsdbeaverfiber","text":"","title":"Windows\u4e0bDBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver_4.2.1/#_6","text":"\u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_4.2.1/#_7","text":"Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5b8c\u6210windows\u4e0a\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_4.2.1/#_8","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u53ef\u4ee5\u7528kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\uff0c\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002kinit\u8ba4\u8bc1\u7684\u6709\u6548\u671f\u662f24\u5c0f\u65f6\uff0ckeytab\u8ba4\u8bc1\u65b9\u5f0f\u957f\u671f\u6709\u6548 - \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e - \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_4.2.1/#kinit","text":"\u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\uff0c\u5e76\u5b89\u88c5 http://web.mit.edu/kerberos/dist/#kfw-4.0 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u89d2\u8272\u548c\u4eba\u673a\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u7cfb\u7edf\u8bbe\u7f6e -> \u6743\u9650\u7ba1\u7406 -> \u7528\u6237\u7ba1\u7406 -> \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\uff0c\u521b\u5efa\u7528\u6237\u201ctest\u201d \u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6 user.keytab \u4ee5\u53ca krb5.conf \u6587\u4ef6\uff0c\u628a krb5.conf \u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d(\u5982\uff1a test@HADOOP.COM )\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; \u4fee\u6539 fiber.xml \u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5\u524d\u786e\u8ba4kerberos\u8ba4\u8bc1\u6709\u6548","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_4.2.1/#keytab","text":"\u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab\u548chbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_4.2.1/#dbeaverfiber","text":"\u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef\uff08 /opt/hadoopclient/Fiber/lib/ \uff09\u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027 \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u786e\u8ba4defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\u3002 Network\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u81ea\u5b9a\u4e49Connection name\uff0c\u70b9\u51fbfinish \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u8fde\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u8fde\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e","title":"DBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver_4.2.1/#dbeaverfiber_1","text":"","title":"DBeaver\u5bf9\u63a5Fiber\u529f\u80fd\u9a8c\u8bc1"},{"location":"Development/DBeaver_4.2.1/#hive","text":"\u5c06JDBC\u7684defaultDrive\u5207\u6362\u81f3Hive Hive\u67e5\u8be2\u6570\u636e\uff1a\u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 \u67e5\u770b\u66f4\u65b0\u540e\u6570\u636e\uff1a","title":"Hive\u589e\u52a0\u67e5\u770b\u6570\u636e"},{"location":"Development/DBeaver_4.2.1/#spark","text":"\u5c06JDBC \u7684defaultDriver\u5207\u6362\u81f3Spark Spark\u67e5\u8be2\u6570\u636e\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Spark\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3Spark\u7684JDBCServer(\u4e3b)\u5b9e\u4f8b\u6240\u5728\u7684\u8282\u70b9\u7684/opt/\u76ee\u5f55\u4e0b \u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"Spark\u589e\u52a0\u67e5\u770b\u6570\u636e"},{"location":"Development/DBeaver_4.2.1/#phoenix","text":"\u5c06JDBC \u7684defaultDrive\u5207\u6362\u81f3Phoenix Phoenix\u589e\u52a0\u6570\u636e \u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2 \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') \u67e5\u770b\u589e\u52a0\u7684\u6570\u636e\uff1a Phoenix\u5220\u9664\u6570\u636e \u9875\u9762\u4e0a\u5220\u9664\uff1a\u9009\u62e9\u5f85\u5220\u9664\u7684\u5217\uff0c\u7136\u540e\u70b9\u51fb\u4e0b\u65b9 \u5220\u9664 \u6309\u94ae\uff0c\u7136\u540e\u70b9\u51fb save \u6309\u94ae\uff1a \u811a\u672c\u5220\u9664\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae delete from TB_PHOENIX where ID=104; \u67e5\u770b\u8f93\u51fa\u540e\u7684\u6570\u636e Phoenix\u66f4\u65b0\u6570\u636e, \u7f16\u8f91\u66f4\u65b0\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') \u67e5\u770b\u66f4\u65b0\u540e\u7684\u6570\u636e\uff1a \u67e5\u770b\u6570\u636e\uff1a\u7f16\u8f91\u67e5\u8be2\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae\u3002 SELECT * FROM TB_PHOENIX","title":"Phoenix\u589e\u5220\u6539\u67e5\u6570\u636e"},{"location":"Development/DBeaver_6.1.4/","text":"DBeaver\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DBeaver 6.1.4 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) \u7b80\u4ecb \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> DBeaver\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 DBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002 \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin \u91cd\u542fDBeaver\u3002\u4fee\u6539dbeaver.ini\u540e\u9700\u8981\u91cd\u542fDBeaver\u624d\u751f\u6548\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Hive \u00b6 \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u3002 \u586b\u5199\u57fa\u672c\u4fe1\u606f\u5982\u4e0b\uff1a Driver Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1acom.huawei.fiber.FiberDriver URL Template\uff1ajdbc:fiber:// Default Port\uff1a2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category: Hadoop * \u70b9\u51fb Add File \uff0c\u589e\u52a0 C:\\ecotesting\\Fiber\\lib \u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb Connection properties \uff0c\u589e\u52a0\u4e24\u4e2a\u5c5e\u6027\u3002\u70b9\u51fb OK \u3002 defaultDriver = hive fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection \u3002\u70b9\u51fb Next \u3002 \u9009\u62e9 Fiber \uff0c\u70b9\u51fb Next \u3002 \u201cUser name\u201d\u548c\u201cPassword\u201d\u53ef\u4ee5\u4e0d\u586b\u5199\uff0c\u70b9\u51fb Connection details (name,type,...) \u3002 \u201cConnection name\u201d\u8f93\u5165 Hadoop - Fiber \u3002\u70b9\u51fb back \u3002 \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \u3002\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002\u70b9\u51fb Finish \u3002 \u6d4b\u8bd5hive\u8fde\u63a5\u3002 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\u3002 \u53cc\u51fb Database Navigator->Hadoop - Fiber \uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770bHive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411Hive\u8868test\u63d2\u5165\u6570\u636e \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 test \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS test (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test; DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x \u00b6 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a spark2x \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411\u8868test\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test; DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix \u00b6 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a phoenix \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \uff0cSQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u8868\u548c\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c\u811a\u672c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.TEST VALUES(1,'John'); UPSERT INTO MY_NS.TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.TEST VALUES(4,'Aurora'); \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Refresh \u5219\u53ef\u770b\u5230\u65b0\u5efa\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb MY_NS->tables->TEST \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770btest\u8868\u6570\u636e\u3002 SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u4fee\u6539\u811a\u672c\u5e76\u6267\u884c\u3002 UPSERT INTO MY_NS.TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u5220\u9664\u811a\u672c\u5e76\u6267\u884c\u3002 DELETE FROM MY_NS.TEST WHERE ID=4; SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u67e5\u8be2\u811a\u672c\u5e76\u6267\u884c\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 SELECT * FROM MY_NS.TEST; FAQ \u00b6 \u5bf9\u63a5Phoenix\u65f6\u8fd4\u56deDriver: Fiber? \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5Phoenix\u65f6\uff0c\u70b9\u51fb Test Connection \uff0c\u6ca1\u6709\u6b63\u786e\u8fd4\u56deServer\u548cDriver\u7684\u7248\u672c\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u786e\u8ba4\u662f\u5426\u5df2\u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin","title":"6.1.4 <--> 6.5"},{"location":"Development/DBeaver_6.1.4/#dbeaverfusioninsight","text":"","title":"DBeaver\u5bf9\u63a5FusionInsight"},{"location":"Development/DBeaver_6.1.4/#_1","text":"DBeaver 6.1.4 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DBeaver_6.1.4/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Development/DBeaver_6.1.4/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1);","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Development/DBeaver_6.1.4/#fiber","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_6.1.4/#_4","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_6.1.4/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_6.1.4/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_6.1.4/#kinit","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_6.1.4/#keytab","text":"\u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiber","text":"","title":"DBeaver\u5bf9\u63a5Fiber"},{"location":"Development/DBeaver_6.1.4/#_7","text":"DBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_6.1.4/#_8","text":"\u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002 \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin \u91cd\u542fDBeaver\u3002\u4fee\u6539dbeaver.ini\u540e\u9700\u8981\u91cd\u542fDBeaver\u624d\u751f\u6548\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_6.1.4/#_9","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiberhive","text":"\u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u3002 \u586b\u5199\u57fa\u672c\u4fe1\u606f\u5982\u4e0b\uff1a Driver Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1acom.huawei.fiber.FiberDriver URL Template\uff1ajdbc:fiber:// Default Port\uff1a2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category: Hadoop * \u70b9\u51fb Add File \uff0c\u589e\u52a0 C:\\ecotesting\\Fiber\\lib \u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb Connection properties \uff0c\u589e\u52a0\u4e24\u4e2a\u5c5e\u6027\u3002\u70b9\u51fb OK \u3002 defaultDriver = hive fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection \u3002\u70b9\u51fb Next \u3002 \u9009\u62e9 Fiber \uff0c\u70b9\u51fb Next \u3002 \u201cUser name\u201d\u548c\u201cPassword\u201d\u53ef\u4ee5\u4e0d\u586b\u5199\uff0c\u70b9\u51fb Connection details (name,type,...) \u3002 \u201cConnection name\u201d\u8f93\u5165 Hadoop - Fiber \u3002\u70b9\u51fb back \u3002 \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \u3002\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002\u70b9\u51fb Finish \u3002 \u6d4b\u8bd5hive\u8fde\u63a5\u3002 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\u3002 \u53cc\u51fb Database Navigator->Hadoop - Fiber \uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770bHive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411Hive\u8868test\u63d2\u5165\u6570\u636e \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 test \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS test (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Hive"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiberspark2x","text":"\u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a spark2x \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411\u8868test\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiberphoenix","text":"\u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a phoenix \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \uff0cSQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u8868\u548c\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c\u811a\u672c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.TEST VALUES(1,'John'); UPSERT INTO MY_NS.TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.TEST VALUES(4,'Aurora'); \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Refresh \u5219\u53ef\u770b\u5230\u65b0\u5efa\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb MY_NS->tables->TEST \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770btest\u8868\u6570\u636e\u3002 SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u4fee\u6539\u811a\u672c\u5e76\u6267\u884c\u3002 UPSERT INTO MY_NS.TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u5220\u9664\u811a\u672c\u5e76\u6267\u884c\u3002 DELETE FROM MY_NS.TEST WHERE ID=4; SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u67e5\u8be2\u811a\u672c\u5e76\u6267\u884c\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 SELECT * FROM MY_NS.TEST;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix"},{"location":"Development/DBeaver_6.1.4/#faq","text":"\u5bf9\u63a5Phoenix\u65f6\u8fd4\u56deDriver: Fiber? \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5Phoenix\u65f6\uff0c\u70b9\u51fb Test Connection \uff0c\u6ca1\u6709\u6b63\u786e\u8fd4\u56deServer\u548cDriver\u7684\u7248\u672c\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u786e\u8ba4\u662f\u5426\u5df2\u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin","title":"FAQ"},{"location":"Development/DbVisualizer_10.0.1/","text":"DbVisualizer\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DbVisualizer 9.5.7 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DbVisualizer 10.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL) \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DbVisualizer\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 DbVisualizer\u5b89\u88c5 \u00b6 DbVisualizer9.5.7\u9700\u8981jdk1.8\uff0c\u4e0b\u8f7d\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u4fee\u6539C:\\Windows\\System32\\drivers\\etc\\hosts\u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982C:\\Fiber\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dDbVisualizer\uff0c\u5730\u5740\uff1a http://www.dbvis.com/download/ \uff0c\u4e0b\u8f7d\u8f6f\u4ef6dbvis_windows-x64_9_5_7_jre.exe \u53cc\u51fbdbvis_windows-x64_9_5_7_jre.exe\u5b89\u88c5 DbVisualizer\u8fde\u63a5Fiber \u00b6 \u914d\u7f6eDbVisualizer\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001Spark\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00DbVisualizer9.5.7\uff0c\u70b9\u51fb Cancel \u83dc\u5355\u680f\u9009\u62e9 ToolsDriver Manager \u65b0\u5efadriver Name\uff1aFiber(\u81ea\u5b9a\u4e49) URL Format\uff1ajdbc:fiber:// User Specified\uff1a\u5c06C:\\Fiber\\lib\\\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Driver Class\uff1a\u52a0\u5165jar\u5305\u540e\u9009\u62e9com.huawei.fiber.FiberDriver \u83dc\u5355\u680f Database -> Create Database Connection \u9009\u62e9 Use Wizard {width=\"4.2in\" height=\"1.4in\"} \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982Fiber \u9009\u62e9Driver Fiber \u586b\u5199URL\uff1ajdbc:fiber:// \u70b9\u51fb Finish \u67e5\u8be2Hive\u8868\u6570\u636e \u00b6 \u6253\u5f00 Properties \u9762\u677f\uff0c\u586b\u5199defaultDriver\u548cfiberconfig\u5c5e\u6027\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\uff0c\u53ef\u4ee5\u5728\u5de6\u4fa7\u770b\u5230hive\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 \u67e5\u8be2SparkSQL\u4e2d\u7684\u6570\u636e \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aspark\uff1a\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aspark\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00Connection\u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230SparkSQL\u4e2d\u7684\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 \u67e5\u8be2Phoenix\u4e2d\u7684\u6570\u636e \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aphoenix\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230phoenix\u6570\u636e\u8868 \u67e5\u770bphoenix\u8868TB_PHOENIX\u4e2d\u7684\u6570\u636e\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e \u00b6 Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e\uff0c\u9700\u8981\u5728Fiber\u4e2dhbase\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u4e2d\u52a0\u5165\u5982\u4e0b\u53c2\u6570\uff0c\u5426\u5219\u4e0d\u4f1a\u81ea\u52a8Commit \u4fee\u6539Hbase-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \uff0c\u7136\u540e\u91cd\u542fDbVisualizer\u3002 <property> <name>phoenix.connection.autoCommit</name> <value>true</value> </property> Phoenix\u8868\u589e\u52a0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix\u8868\u5220\u9664\u6570\u636e delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix\u8868\u66f4\u65b0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix;","title":"9.5.7 <--> C60"},{"location":"Development/DbVisualizer_10.0.1/#dbvisualizerfusioninsight","text":"","title":"DbVisualizer\u5bf9\u63a5FusionInsight"},{"location":"Development/DbVisualizer_10.0.1/#_1","text":"DbVisualizer 9.5.7 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DbVisualizer 10.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.1/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DbVisualizer\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/DbVisualizer_10.0.1/#dbvisualizer","text":"DbVisualizer9.5.7\u9700\u8981jdk1.8\uff0c\u4e0b\u8f7d\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u4fee\u6539C:\\Windows\\System32\\drivers\\etc\\hosts\u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982C:\\Fiber\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dDbVisualizer\uff0c\u5730\u5740\uff1a http://www.dbvis.com/download/ \uff0c\u4e0b\u8f7d\u8f6f\u4ef6dbvis_windows-x64_9_5_7_jre.exe \u53cc\u51fbdbvis_windows-x64_9_5_7_jre.exe\u5b89\u88c5","title":"DbVisualizer\u5b89\u88c5"},{"location":"Development/DbVisualizer_10.0.1/#dbvisualizerfiber","text":"\u914d\u7f6eDbVisualizer\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001Spark\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00DbVisualizer9.5.7\uff0c\u70b9\u51fb Cancel \u83dc\u5355\u680f\u9009\u62e9 ToolsDriver Manager \u65b0\u5efadriver Name\uff1aFiber(\u81ea\u5b9a\u4e49) URL Format\uff1ajdbc:fiber:// User Specified\uff1a\u5c06C:\\Fiber\\lib\\\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Driver Class\uff1a\u52a0\u5165jar\u5305\u540e\u9009\u62e9com.huawei.fiber.FiberDriver \u83dc\u5355\u680f Database -> Create Database Connection \u9009\u62e9 Use Wizard {width=\"4.2in\" height=\"1.4in\"} \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982Fiber \u9009\u62e9Driver Fiber \u586b\u5199URL\uff1ajdbc:fiber:// \u70b9\u51fb Finish","title":"DbVisualizer\u8fde\u63a5Fiber"},{"location":"Development/DbVisualizer_10.0.1/#hive","text":"\u6253\u5f00 Properties \u9762\u677f\uff0c\u586b\u5199defaultDriver\u548cfiberconfig\u5c5e\u6027\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\uff0c\u53ef\u4ee5\u5728\u5de6\u4fa7\u770b\u5230hive\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2Hive\u8868\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.1/#sparksql","text":"\u5c06defaultDriver\u5207\u6362\u4e3aspark\uff1a\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aspark\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00Connection\u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230SparkSQL\u4e2d\u7684\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2SparkSQL\u4e2d\u7684\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.1/#phoenix","text":"\u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aphoenix\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230phoenix\u6570\u636e\u8868 \u67e5\u770bphoenix\u8868TB_PHOENIX\u4e2d\u7684\u6570\u636e\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2Phoenix\u4e2d\u7684\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.1/#phoenix_1","text":"Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e\uff0c\u9700\u8981\u5728Fiber\u4e2dhbase\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u4e2d\u52a0\u5165\u5982\u4e0b\u53c2\u6570\uff0c\u5426\u5219\u4e0d\u4f1a\u81ea\u52a8Commit \u4fee\u6539Hbase-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \uff0c\u7136\u540e\u91cd\u542fDbVisualizer\u3002 <property> <name>phoenix.connection.autoCommit</name> <value>true</value> </property> Phoenix\u8868\u589e\u52a0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix\u8868\u5220\u9664\u6570\u636e delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix\u8868\u66f4\u65b0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix;","title":"Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.21/","text":"DbVisualizer\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DbVisualizer 10.0.21 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) \u7b80\u4ecb \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a \u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab.file\u548chbase.myclient.principal\u3002 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.master.keytab.file</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DbVisualizer\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 DbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4ece http://www.dbvis.com/download/ \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684DbVisualizer\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Hive \u00b6 \u6253\u5f00DbVisualizer\uff0c\u70b9\u51fb Cancel \u3002 \u83dc\u5355\u680f\u9009\u62e9 Tools->Driver Manager \u3002 \u70b9\u51fb \u65b0\u5efadriver\u3002\u4fe1\u606f\u586b\u5199\u5b8c\u6bd5\u540e\uff0c\u5173\u95ed\u8be5\u7a97\u53e3\u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 URL Format\uff1ajdbc:fiber:// Dirver jar Files: \u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u83dc\u5355\u680f\u9009\u62e9 Database-> Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982 Fiber \uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9\u6570\u636e\u5e93Driver\u3002\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9 Fiber \u3002 \u586b\u5199 Database URL = jdbc:fiber:// \uff0c\u5176\u4f59\u7684\u53ef\u4e0d\u586b\u5199\u3002\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 \u9009\u62e9 Databases->Connections->Fiber \uff0c\u9009\u62e9 Properties \u9762\u677f\uff0c\u586b\u5199 defaultDriver = hive \u548c fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\u3002 Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student; DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x \u00b6 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a spark2x \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student; DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix \u00b6 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a phoenix \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb MY_NS->TABLE->SQL_TEST->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 FAQ \u00b6 \u754c\u9762\u67e5\u770b\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56deParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fde\u63a5\u4e0aHive\u540e\uff0c\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\u3002 An error occurred while performing the operation: Error while compiling statement: FAILED: ParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Generic->Delimited Identifiers \u3002\u5c06 Begin Identifier \u548c End Identifier \u7684\u5185\u5bb9\uff08\u4f8b\u5982\u53cc\u5f15\u53f7 \" \uff09\u6e05\u7a7a\u540e\uff0c\u70b9\u51fb Apply \u3002","title":"10.0.21 <--> 6.5"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfusioninsight","text":"","title":"DbVisualizer\u5bf9\u63a5FusionInsight"},{"location":"Development/DbVisualizer_10.0.21/#_1","text":"DbVisualizer 10.0.21 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.21/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Development/DbVisualizer_10.0.21/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1);","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Development/DbVisualizer_10.0.21/#fiber","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DbVisualizer_10.0.21/#_4","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.21/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DbVisualizer_10.0.21/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DbVisualizer_10.0.21/#kinit","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DbVisualizer_10.0.21/#keytab","text":"\u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a \u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab.file\u548chbase.myclient.principal\u3002 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.master.keytab.file</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiber","text":"","title":"DbVisualizer\u5bf9\u63a5Fiber"},{"location":"Development/DbVisualizer_10.0.21/#_7","text":"DbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.21/#_8","text":"\u4ece http://www.dbvis.com/download/ \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684DbVisualizer\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DbVisualizer_10.0.21/#_9","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiberhive","text":"\u6253\u5f00DbVisualizer\uff0c\u70b9\u51fb Cancel \u3002 \u83dc\u5355\u680f\u9009\u62e9 Tools->Driver Manager \u3002 \u70b9\u51fb \u65b0\u5efadriver\u3002\u4fe1\u606f\u586b\u5199\u5b8c\u6bd5\u540e\uff0c\u5173\u95ed\u8be5\u7a97\u53e3\u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 URL Format\uff1ajdbc:fiber:// Dirver jar Files: \u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u83dc\u5355\u680f\u9009\u62e9 Database-> Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982 Fiber \uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9\u6570\u636e\u5e93Driver\u3002\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9 Fiber \u3002 \u586b\u5199 Database URL = jdbc:fiber:// \uff0c\u5176\u4f59\u7684\u53ef\u4e0d\u586b\u5199\u3002\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 \u9009\u62e9 Databases->Connections->Fiber \uff0c\u9009\u62e9 Properties \u9762\u677f\uff0c\u586b\u5199 defaultDriver = hive \u548c fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\u3002 Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student;","title":"DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Hive"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiberspark2x","text":"\u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a spark2x \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student;","title":"DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiberphoenix","text":"\u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a phoenix \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb MY_NS->TABLE->SQL_TEST->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002","title":"DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix"},{"location":"Development/DbVisualizer_10.0.21/#faq","text":"\u754c\u9762\u67e5\u770b\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56deParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fde\u63a5\u4e0aHive\u540e\uff0c\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\u3002 An error occurred while performing the operation: Error while compiling statement: FAILED: ParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Generic->Delimited Identifiers \u3002\u5c06 Begin Identifier \u548c End Identifier \u7684\u5185\u5bb9\uff08\u4f8b\u5982\u53cc\u5f15\u53f7 \" \uff09\u6e05\u7a7a\u540e\uff0c\u70b9\u51fb Apply \u3002","title":"FAQ"},{"location":"Development/HUE/","text":"HUE\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 HUE 4.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Spark)","title":"4.0.1 <--> C70"},{"location":"Development/HUE/#huefusioninsight","text":"","title":"HUE\u5bf9\u63a5FusionInsight"},{"location":"Development/HUE/#_1","text":"HUE 4.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/JupyterHub/","text":"JupyterHub\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 JupyterHub 1.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x) \u8bf4\u660e\uff1a 1. \u76f8\u8f83\u4e8eJupyter Notebook, JupyterHub\u80fd\u591f\u652f\u6301\u591a\u7528\u6237\u8bbf\u95ee\uff0c\u53ef\u7528\u4e8e\u521b\u5efa\u3001\u7ba1\u7406\u3001\u4ee3\u7406\u591a\u4e2aJupyter Notebook \u5b9e\u4f8b\u3002\u5177\u6709\u6269\u5c55\u6027\u548c\u53ef\u5b9a\u5236\u6027\u3002JupyterHub\u9ed8\u8ba4\u4f7f\u7528python3\u5185\u6838\uff0c\u6240\u4ee5\u5728\u5b89\u88c5JupyterHub\u4e4b\u524d\u8981\u5148\u5b89\u88c5Anaconda3 FI HD\u96c6\u7fa4\u9ed8\u8ba4\u5b89\u88c5\u7684python\u7248\u672c\u4e3a2.x\uff0c\u4f7f\u7528pyspark\u65f6\u4f1a\u5728worker\u8282\u70b9\u8d77python\u8fdb\u7a0b\uff0c\u6240\u4ee5\u5982\u679c\u8981\u4f7f\u7528python3\u5185\u6838\uff0c\u9700\u8981\u5728\u8282\u70b9\u4e0a\u5b89\u88c5python3\u73af\u5883\u3002 JuypterHub\u4e3b\u673a\uff1a172.16.2.119 \u5bf9\u63a5\u96c6\u7fa4\uff1a172.16.6.10-12 \u5b89\u88c5Anaconda3 \u00b6 \u51c6\u5907Anaconda3\u5b89\u88c5\u5305\u5230/opt\u8def\u5f84\u4e0b: \u5b89\u88c5Anaconda3 \u4f7f\u7528\u547d\u4ee4\uff1abash Anaconda3-2019.07-Linux-x86_64.sh \u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e3a/opt/anaconda3 \u56de\u8f66\u5b89\u88c5(\u540e\u7eedjuypterhub \u591a\u7528\u6237\u4e0d\u652f\u6301root\u7528\u6237\u767b\u5f55\uff0c\u6240\u4ee5\u5b89\u88c5\u8def\u5f84\u4e0d\u8981\u653e\u5728/root\u8def\u5f84\u4e0b) \u5b8c\u6210\u5b89\u88c5\u540e\u5982\u679c\u9009yes\u8fdb\u884c\u521d\u59cb\u5316 \u4f1a\u81ea\u52a8\u5728 ~/.bashrc\u6587\u4ef6\u4e0b\u5199\u5165anaconda3\u7684\u521d\u59cb\u5316\u914d\u7f6e \u5c06\u66f4\u6539\u540e\u7684.bashrc\u91cd\u547d\u540d\u4e3a.bashrc.anaconda\uff0c \u8fd8\u662f\u4fdd\u6301.bashrc\u6587\u4ef6\u4e3a\u4e4b\u524d\u7684\u5185\u5bb9 \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u300a\u5b89\u88c5Anaconda3\u300b\u5728\u5bf9\u63a5FI HD\u96c6\u7fa4\u4e09\u4e2a\u8282\u70b9172.16.6.10-12\u90fd\u90e8\u7f72\u597danaconda3\uff0c\u5b89\u88c5\u8def\u5f84\u90fd\u4e3a /opt/anaconda3 \u5b89\u88c5JupyterHub \u00b6 \u5b89\u88c5\u76f8\u5173\u4f9d\u8d56 yum install gcc yum install openssl openssl-devel yum install sqlite-devel JupyterHub\u5b89\u88c5\u6b65\u9aa4\uff1a source ~/.bashrc.anaconda \u53ef\u4ee5\u4f7f\u7528\u6e05\u534e\u7684conda\u6e90\u52a0\u901f conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5JupyterHub conda install -c conda-forge jupyterhub \u5b89\u88c5JupyterHub\u76f8\u5173\u5305 npm install -g configurable-http-proxy \u589e\u52a0\u7528\u6237abc,\u7528\u4e8e\u767b\u5f55: useradd abc passwd abc \u8bbe\u7f6e\u7528\u6237abc\u7684\u767b\u9646\u5bc6\u7801\u4e3aHuawei@123 \u751f\u6210JupyterHub\u914d\u7f6e\u6587\u4ef6 mkdir /etc/jupyterhub jupyterhub --generate-config -f /etc/jupyterhub/jupyterhub_config.py vi /etc/jupyterhub/jupyterhub_config.py \u4fee\u6539\u5982\u4e0b\u53c2\u6570\uff1a c.JupyterHub.ip = '172.16.2.119' c.JupyterHub.port = 8001 c.Authenticator.whitelist = {'root','abc'} c.Authenticator.admin_users = {'root','abc'} c.JupyterHub.statsd_prefix = 'jupyterhub' - \u4fee\u6539/opt/anaconda3\u7684\u6240\u5c5e\u7528\u6237\uff0c\u4f7f\u5f97\u521b\u5efa\u7528\u6237abc\u6709\u6743\u9650 chown abc:abc -R /opt/anaconda3/ \u4f7f\u7528abc\u7528\u6237\u767b\u5f55\uff0c\u5c06root\u7528\u6237\u7684~/.bashrc.anaconda\u6587\u4ef6\u5185\u5bb9\u62f7\u8d1d\u5230abc\u7528\u6237\u7684 ~ \u8def\u5f84\u4e0b \u5b8c\u6210\u540e su - abc vi ~/.bashrc.anaconda \u5185\u5bb9\u5e94\u4e0eroot\u7528\u6237\u7684\u4e00\u6837\uff1a \u4ee5\u7528\u6237abc\u767b\u9646,\u4e4b\u540e\u542f\u52a8JupyterHub su - abc source ~/.bashrc.anaconda source /opt/101hdclient/hadoopclient/bigdata_env kinit jupyterhub --config=/etc/jupyterhub/jupyterhub.py --ip=172.16.2.119 --port=8001 --no-ssl \u6d4f\u89c8\u5668\u6253\u5f00JupyterHub\u7684Web UI\uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801(abc/Huawei@123) \u5bf9\u63a5Spark2x \u00b6 \u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u4e0b\u9762\u4e3a\u4ee3\u7801\u7247\u6bb5 import sys sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/') sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip') import os os.environ[\"PYSPARK_PYTHON\"]=\"/opt/anaconda3/bin/python3\" import pyspark from pyspark import SparkConf from pyspark import SparkContext # use this conf = pyspark.SparkConf().setMaster('yarn').setAppName('spark-wordcount_from172.16.2.119') sc = pyspark.SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print ('Nonempty lines', nonempty_lines.count()) words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print ('Top 100 words:') print (wordcounts.take(100)) sc.stop() \u8bf4\u660e\uff1a 1. /opt/101hdclient/hadoopclient/Spark2x/spark/python/ \u4e3aJupyterHub\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u8def\u5f84 2. /opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip \u4e3a\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u7684lib\u8def\u5f84\uff0c\u5177\u4f53\u7684zip\u5305\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e00\u81f4 3. /opt/anaconda3/bin/python3 \u4e3apython\u73af\u5883\u5b89\u88c5\u8def\u5f84 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u540e\u53f0yarn\u67e5\u770b\uff1a","title":"1.0.0 <--> C80"},{"location":"Development/JupyterHub/#jupyterhubfusioninsight","text":"","title":"JupyterHub\u5bf9\u63a5FusionInsight"},{"location":"Development/JupyterHub/#_1","text":"JupyterHub 1.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x) \u8bf4\u660e\uff1a 1. \u76f8\u8f83\u4e8eJupyter Notebook, JupyterHub\u80fd\u591f\u652f\u6301\u591a\u7528\u6237\u8bbf\u95ee\uff0c\u53ef\u7528\u4e8e\u521b\u5efa\u3001\u7ba1\u7406\u3001\u4ee3\u7406\u591a\u4e2aJupyter Notebook \u5b9e\u4f8b\u3002\u5177\u6709\u6269\u5c55\u6027\u548c\u53ef\u5b9a\u5236\u6027\u3002JupyterHub\u9ed8\u8ba4\u4f7f\u7528python3\u5185\u6838\uff0c\u6240\u4ee5\u5728\u5b89\u88c5JupyterHub\u4e4b\u524d\u8981\u5148\u5b89\u88c5Anaconda3 FI HD\u96c6\u7fa4\u9ed8\u8ba4\u5b89\u88c5\u7684python\u7248\u672c\u4e3a2.x\uff0c\u4f7f\u7528pyspark\u65f6\u4f1a\u5728worker\u8282\u70b9\u8d77python\u8fdb\u7a0b\uff0c\u6240\u4ee5\u5982\u679c\u8981\u4f7f\u7528python3\u5185\u6838\uff0c\u9700\u8981\u5728\u8282\u70b9\u4e0a\u5b89\u88c5python3\u73af\u5883\u3002 JuypterHub\u4e3b\u673a\uff1a172.16.2.119 \u5bf9\u63a5\u96c6\u7fa4\uff1a172.16.6.10-12","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/JupyterHub/#anaconda3","text":"\u51c6\u5907Anaconda3\u5b89\u88c5\u5305\u5230/opt\u8def\u5f84\u4e0b: \u5b89\u88c5Anaconda3 \u4f7f\u7528\u547d\u4ee4\uff1abash Anaconda3-2019.07-Linux-x86_64.sh \u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e3a/opt/anaconda3 \u56de\u8f66\u5b89\u88c5(\u540e\u7eedjuypterhub \u591a\u7528\u6237\u4e0d\u652f\u6301root\u7528\u6237\u767b\u5f55\uff0c\u6240\u4ee5\u5b89\u88c5\u8def\u5f84\u4e0d\u8981\u653e\u5728/root\u8def\u5f84\u4e0b) \u5b8c\u6210\u5b89\u88c5\u540e\u5982\u679c\u9009yes\u8fdb\u884c\u521d\u59cb\u5316 \u4f1a\u81ea\u52a8\u5728 ~/.bashrc\u6587\u4ef6\u4e0b\u5199\u5165anaconda3\u7684\u521d\u59cb\u5316\u914d\u7f6e \u5c06\u66f4\u6539\u540e\u7684.bashrc\u91cd\u547d\u540d\u4e3a.bashrc.anaconda\uff0c \u8fd8\u662f\u4fdd\u6301.bashrc\u6587\u4ef6\u4e3a\u4e4b\u524d\u7684\u5185\u5bb9 \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u300a\u5b89\u88c5Anaconda3\u300b\u5728\u5bf9\u63a5FI HD\u96c6\u7fa4\u4e09\u4e2a\u8282\u70b9172.16.6.10-12\u90fd\u90e8\u7f72\u597danaconda3\uff0c\u5b89\u88c5\u8def\u5f84\u90fd\u4e3a /opt/anaconda3","title":"\u5b89\u88c5Anaconda3"},{"location":"Development/JupyterHub/#jupyterhub","text":"\u5b89\u88c5\u76f8\u5173\u4f9d\u8d56 yum install gcc yum install openssl openssl-devel yum install sqlite-devel JupyterHub\u5b89\u88c5\u6b65\u9aa4\uff1a source ~/.bashrc.anaconda \u53ef\u4ee5\u4f7f\u7528\u6e05\u534e\u7684conda\u6e90\u52a0\u901f conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5JupyterHub conda install -c conda-forge jupyterhub \u5b89\u88c5JupyterHub\u76f8\u5173\u5305 npm install -g configurable-http-proxy \u589e\u52a0\u7528\u6237abc,\u7528\u4e8e\u767b\u5f55: useradd abc passwd abc \u8bbe\u7f6e\u7528\u6237abc\u7684\u767b\u9646\u5bc6\u7801\u4e3aHuawei@123 \u751f\u6210JupyterHub\u914d\u7f6e\u6587\u4ef6 mkdir /etc/jupyterhub jupyterhub --generate-config -f /etc/jupyterhub/jupyterhub_config.py vi /etc/jupyterhub/jupyterhub_config.py \u4fee\u6539\u5982\u4e0b\u53c2\u6570\uff1a c.JupyterHub.ip = '172.16.2.119' c.JupyterHub.port = 8001 c.Authenticator.whitelist = {'root','abc'} c.Authenticator.admin_users = {'root','abc'} c.JupyterHub.statsd_prefix = 'jupyterhub' - \u4fee\u6539/opt/anaconda3\u7684\u6240\u5c5e\u7528\u6237\uff0c\u4f7f\u5f97\u521b\u5efa\u7528\u6237abc\u6709\u6743\u9650 chown abc:abc -R /opt/anaconda3/ \u4f7f\u7528abc\u7528\u6237\u767b\u5f55\uff0c\u5c06root\u7528\u6237\u7684~/.bashrc.anaconda\u6587\u4ef6\u5185\u5bb9\u62f7\u8d1d\u5230abc\u7528\u6237\u7684 ~ \u8def\u5f84\u4e0b \u5b8c\u6210\u540e su - abc vi ~/.bashrc.anaconda \u5185\u5bb9\u5e94\u4e0eroot\u7528\u6237\u7684\u4e00\u6837\uff1a \u4ee5\u7528\u6237abc\u767b\u9646,\u4e4b\u540e\u542f\u52a8JupyterHub su - abc source ~/.bashrc.anaconda source /opt/101hdclient/hadoopclient/bigdata_env kinit jupyterhub --config=/etc/jupyterhub/jupyterhub.py --ip=172.16.2.119 --port=8001 --no-ssl \u6d4f\u89c8\u5668\u6253\u5f00JupyterHub\u7684Web UI\uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801(abc/Huawei@123)","title":"\u5b89\u88c5JupyterHub"},{"location":"Development/JupyterHub/#spark2x","text":"\u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u4e0b\u9762\u4e3a\u4ee3\u7801\u7247\u6bb5 import sys sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/') sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip') import os os.environ[\"PYSPARK_PYTHON\"]=\"/opt/anaconda3/bin/python3\" import pyspark from pyspark import SparkConf from pyspark import SparkContext # use this conf = pyspark.SparkConf().setMaster('yarn').setAppName('spark-wordcount_from172.16.2.119') sc = pyspark.SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print ('Nonempty lines', nonempty_lines.count()) words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print ('Top 100 words:') print (wordcounts.take(100)) sc.stop() \u8bf4\u660e\uff1a 1. /opt/101hdclient/hadoopclient/Spark2x/spark/python/ \u4e3aJupyterHub\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u8def\u5f84 2. /opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip \u4e3a\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u7684lib\u8def\u5f84\uff0c\u5177\u4f53\u7684zip\u5305\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e00\u81f4 3. /opt/anaconda3/bin/python3 \u4e3apython\u73af\u5883\u5b89\u88c5\u8def\u5f84 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u540e\u53f0yarn\u67e5\u770b\uff1a","title":"\u5bf9\u63a5Spark2x"},{"location":"Development/JupyterNotebook/","text":"Jupyter Notebook\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Jupyter Notebook 2.7.16 \u2194 FusionInsight HD V100R002C80SPC200 (Hive/Elk/Spark2x) Jupyter Notebook 2.7.16 \u2194 FusionInsight HD 6.5 (Hive/Elk/Spark2x) \u8bf4\u660e\uff1aJupyter Notebook\u7248\u672c \u57fa\u4e8eAnaconda Python\u5185\u6838\u7248\u672c \u5b89\u88c5Anaconda \u00b6 \u53c2\u8003Anaconda\u5b98\u65b9\u6587\u6863\u5b89\u88c5Linux\u5bf9\u5e94\u7684Anaconda\uff1a https://docs.anaconda.com/anaconda/install/linux/ \u4f7f\u7528\u547d\u4ee4 wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh \u4e0b\u8f7dlinux\u76f8\u5173\u7684\u5b89\u88c5\u5305 \u4f7f\u7528\u547d\u4ee4 bash Anaconda2-2019.03-Linux-x86_64.sh \u5f00\u59cb\u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e0d\u8981\u9009\u62e9\u9ed8\u8ba4\u4f4d\u7f6e\uff0c\u800c\u8bbe\u7f6e\u4e3a /opt/anaconda2 \u5b8c\u6210\u5b89\u88c5\u540e\u9009yes\u8fdb\u884c\u521d\u59cb\u5316, \u4f1a\u5c06\u521d\u59cb\u5316\u8bbe\u7f6e\u5199\u5165 ~/.bashrc \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 cp ~/.bashrc ~/.bashrc.anaconda \u5c06\u521d\u59cb\u5316\u4e4b\u540e\u7684 .basrc \u6587\u4ef6\u590d\u5236\uff0c\u91cd\u547d\u540d\u4e3a .bashrc.anaconda \uff0c \u5185\u5bb9\u5982\u4e0b\uff1a \u7ea2\u6846\u90e8\u5206\u4e3a\u5b89\u88c5anaconda\u540e\u52a0\u4e0a\u7684\u521d\u59cb\u5316\u914d\u7f6e \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \u7f16\u8f91 .bashrc \u6587\u4ef6\uff0c\u5c06conda\u521d\u59cb\u5316\u90e8\u5206\u5220\u6389\uff1a \u4f7f\u7528\u547d\u4ee4 source ~/.bashrc.anaconda \u52a0\u8f7d\u73af\u5883 \u4f7f\u7528\u547d\u4ee4 jupyter notebook --generate-config --allow-root \u751f\u6210jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 vi /root/.jupyter/jupyter_notebook_config.py \u4fee\u6539jupyter notebook\u7684\u914d\u7f6e\uff0c\u5177\u4f53\u5982\u4e0b\uff1a \u6539Ip\u4e3a\u672c\u673aIp \u6539\u7aef\u53e3\uff08\u5982\u679c\u88ab\u5360\u7528\uff09 \u4fdd\u5b58 \u5728jupyter notebook\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\uff0c\u5982\u679c\u5b8c\u6210\u53ef\u4ee5\u4e0d\u505a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u76f4\u63a5\u590d\u5236\u7c98\u8d34\u5bf9\u5e94\u7684\u5730\u5740\u8bbf\u95eejupyter notebook web UI\uff1a \u5bf9\u63a5Spark2x \u00b6 \u8bf4\u660e\uff1a\u4f7f\u7528pySpark\u63a5\u53e3\u5bf9\u63a5FI HD\u96c6\u7fa4Spark2x\u7ec4\u4ef6 \u4f7f\u7528\u4e0a\u4e00\u8282\u547d\u4ee4\u542f\u52a8jupyter notebook\u5e76\u8fdb\u5165weibUI \u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165python\u4ee3\u7801 from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.2.118') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) \u5e76\u4e14\u5728\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u4e0a\u67e5\u770b\u4efb\u52a1\uff1a \u5bf9\u63a5Hive \u00b6 \u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4Hive \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u5c06\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6user.keytab\u653e\u5230jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u7528\u4e8e\u8fde\u63a5Hive\u8ba4\u8bc1\u4f7f\u7528,\u5c06\u8ba4\u8bc1\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\u653e\u5230/etc/\u8def\u5f84\u4e0b \u5728jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dJVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype import os # this worked conn = jaydebeapi.connect( \"org.apache.hive.jdbc.HiveDriver\", [\"jdbc:hive2://172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/user.keytab\" , \"developuser\", \"Huawei@123\"], [ '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-collections-3.2.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-configuration-1.6.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-lang-2.6.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-logging-1.1.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-client-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-framework-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'guava-14.0.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-auth-2.7.2.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-common-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-mapreduce-client-core-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-exec-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-jdbc-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-metastore-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-serde-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-service-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-0.23-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpclient-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpcore-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'libthrift-0.9.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'log4j-1.2.17.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-api-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-log4j12-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'zookeeper-3.5.1.jar'] ) import pandas as pd sql = \"Select * From drill_iris\" df_hive = pd.read_sql(sql, conn) df_hive conn.close() \u8bf4\u660e\uff1ajaydebeapi.connect()\u4e3ajdbc\u8fde\u63a5\u65b9\u6cd5\uff0cjaydebeapi.connect(\"Driver Main Class\", [\"Connecting URL\", \"User\", \"Password\"], \"Path to JDBC driver\")\uff0c\u5bf9\u63a5hive\u9700\u8981\u5c06\u5ba2\u6237\u7aefhive jdbc\u6837\u4f8b\u4e2d\u6240\u6709\u7684jar\u5305\u90fd\u5bfc\u8fdb\u53bb \u5bf9\u63a5ELK \u00b6 \u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4ELK ELK\u914d\u7f6e \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237joe, \u5bc6\u7801\u4e3aBigdata@123\uff0c \u5e76\u8d4b\u4e88\u7528\u6237joe\u6240\u6709\u6743\u9650 \u521b\u5efaHDFS\u8868\u7a7a\u95f4 \u521b\u5efa\u6570\u636e\u5e93db_tpcds \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\uff0c\u63d2\u5165\u6570\u636e \u53c2\u8003ELK\u4ea7\u54c1\u6587\u6863\u300a\u8fdc\u7a0b\u8fde\u63a5\u6570\u636e\u5e93\u300b\u914d\u7f6eELK\u767d\u540d\u5355\uff0c\u80fd\u591f\u8bbf\u95eejupyter notebook\u4e3b\u673a \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u5c06ELK JDBC\u9a71\u52a8jar\u5305 gsjdbc4.jar \u653e\u5230jupyter notebook\u4e3b\u673a /opt \u8def\u5f84\u4e0b \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype jar = \"/opt/gsjdbc4.jar\" # location of the jdbc driver jar args='-Djava.class.path=%s' % jar jvm = jpype.getDefaultJVMPath() jpype.startJVM(jvm, args) # this worked conn = jaydebeapi.connect( 'org.postgresql.Driver', [\"jdbc:postgresql://172.16.6.10:25108/db_tpcds\" , \"joe\", \"Bigdata@123\"], \"/opt/gsjdbc4.jar\" ) import pandas as pd sql = \"Select * From hdfs_001\" df = pd.read_sql(sql, conn) df conn.close() F&Q \u00b6 \u5728\u4f7f\u7528pySpark\u7684\u65f6\u5019\u9047\u5230\u5982\u4e0b\u95ee\u9898\uff1a ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /opt/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 \u89e3\u51b3\u529e\u6cd5\uff1a \u8fd0\u884c sc.stop() \u8fde\u63a5ELK\u65f6\u5019\u62a5\u9519\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u914d\u7f6eELK\u767d\u540d\u5355","title":"2.7.16 <--> 6.5"},{"location":"Development/JupyterNotebook/#jupyter-notebookfusioninsight","text":"","title":"Jupyter Notebook\u5bf9\u63a5FusionInsight"},{"location":"Development/JupyterNotebook/#_1","text":"Jupyter Notebook 2.7.16 \u2194 FusionInsight HD V100R002C80SPC200 (Hive/Elk/Spark2x) Jupyter Notebook 2.7.16 \u2194 FusionInsight HD 6.5 (Hive/Elk/Spark2x) \u8bf4\u660e\uff1aJupyter Notebook\u7248\u672c \u57fa\u4e8eAnaconda Python\u5185\u6838\u7248\u672c","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/JupyterNotebook/#anaconda","text":"\u53c2\u8003Anaconda\u5b98\u65b9\u6587\u6863\u5b89\u88c5Linux\u5bf9\u5e94\u7684Anaconda\uff1a https://docs.anaconda.com/anaconda/install/linux/ \u4f7f\u7528\u547d\u4ee4 wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh \u4e0b\u8f7dlinux\u76f8\u5173\u7684\u5b89\u88c5\u5305 \u4f7f\u7528\u547d\u4ee4 bash Anaconda2-2019.03-Linux-x86_64.sh \u5f00\u59cb\u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e0d\u8981\u9009\u62e9\u9ed8\u8ba4\u4f4d\u7f6e\uff0c\u800c\u8bbe\u7f6e\u4e3a /opt/anaconda2 \u5b8c\u6210\u5b89\u88c5\u540e\u9009yes\u8fdb\u884c\u521d\u59cb\u5316, \u4f1a\u5c06\u521d\u59cb\u5316\u8bbe\u7f6e\u5199\u5165 ~/.bashrc \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 cp ~/.bashrc ~/.bashrc.anaconda \u5c06\u521d\u59cb\u5316\u4e4b\u540e\u7684 .basrc \u6587\u4ef6\u590d\u5236\uff0c\u91cd\u547d\u540d\u4e3a .bashrc.anaconda \uff0c \u5185\u5bb9\u5982\u4e0b\uff1a \u7ea2\u6846\u90e8\u5206\u4e3a\u5b89\u88c5anaconda\u540e\u52a0\u4e0a\u7684\u521d\u59cb\u5316\u914d\u7f6e \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \u7f16\u8f91 .bashrc \u6587\u4ef6\uff0c\u5c06conda\u521d\u59cb\u5316\u90e8\u5206\u5220\u6389\uff1a \u4f7f\u7528\u547d\u4ee4 source ~/.bashrc.anaconda \u52a0\u8f7d\u73af\u5883 \u4f7f\u7528\u547d\u4ee4 jupyter notebook --generate-config --allow-root \u751f\u6210jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 vi /root/.jupyter/jupyter_notebook_config.py \u4fee\u6539jupyter notebook\u7684\u914d\u7f6e\uff0c\u5177\u4f53\u5982\u4e0b\uff1a \u6539Ip\u4e3a\u672c\u673aIp \u6539\u7aef\u53e3\uff08\u5982\u679c\u88ab\u5360\u7528\uff09 \u4fdd\u5b58 \u5728jupyter notebook\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\uff0c\u5982\u679c\u5b8c\u6210\u53ef\u4ee5\u4e0d\u505a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u76f4\u63a5\u590d\u5236\u7c98\u8d34\u5bf9\u5e94\u7684\u5730\u5740\u8bbf\u95eejupyter notebook web UI\uff1a","title":"\u5b89\u88c5Anaconda"},{"location":"Development/JupyterNotebook/#spark2x","text":"\u8bf4\u660e\uff1a\u4f7f\u7528pySpark\u63a5\u53e3\u5bf9\u63a5FI HD\u96c6\u7fa4Spark2x\u7ec4\u4ef6 \u4f7f\u7528\u4e0a\u4e00\u8282\u547d\u4ee4\u542f\u52a8jupyter notebook\u5e76\u8fdb\u5165weibUI \u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165python\u4ee3\u7801 from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.2.118') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) \u5e76\u4e14\u5728\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u4e0a\u67e5\u770b\u4efb\u52a1\uff1a","title":"\u5bf9\u63a5Spark2x"},{"location":"Development/JupyterNotebook/#hive","text":"\u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4Hive \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u5c06\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6user.keytab\u653e\u5230jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u7528\u4e8e\u8fde\u63a5Hive\u8ba4\u8bc1\u4f7f\u7528,\u5c06\u8ba4\u8bc1\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\u653e\u5230/etc/\u8def\u5f84\u4e0b \u5728jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dJVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype import os # this worked conn = jaydebeapi.connect( \"org.apache.hive.jdbc.HiveDriver\", [\"jdbc:hive2://172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/user.keytab\" , \"developuser\", \"Huawei@123\"], [ '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-collections-3.2.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-configuration-1.6.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-lang-2.6.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-logging-1.1.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-client-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-framework-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'guava-14.0.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-auth-2.7.2.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-common-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-mapreduce-client-core-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-exec-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-jdbc-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-metastore-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-serde-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-service-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-0.23-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpclient-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpcore-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'libthrift-0.9.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'log4j-1.2.17.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-api-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-log4j12-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'zookeeper-3.5.1.jar'] ) import pandas as pd sql = \"Select * From drill_iris\" df_hive = pd.read_sql(sql, conn) df_hive conn.close() \u8bf4\u660e\uff1ajaydebeapi.connect()\u4e3ajdbc\u8fde\u63a5\u65b9\u6cd5\uff0cjaydebeapi.connect(\"Driver Main Class\", [\"Connecting URL\", \"User\", \"Password\"], \"Path to JDBC driver\")\uff0c\u5bf9\u63a5hive\u9700\u8981\u5c06\u5ba2\u6237\u7aefhive jdbc\u6837\u4f8b\u4e2d\u6240\u6709\u7684jar\u5305\u90fd\u5bfc\u8fdb\u53bb","title":"\u5bf9\u63a5Hive"},{"location":"Development/JupyterNotebook/#elk","text":"\u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4ELK ELK\u914d\u7f6e \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237joe, \u5bc6\u7801\u4e3aBigdata@123\uff0c \u5e76\u8d4b\u4e88\u7528\u6237joe\u6240\u6709\u6743\u9650 \u521b\u5efaHDFS\u8868\u7a7a\u95f4 \u521b\u5efa\u6570\u636e\u5e93db_tpcds \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\uff0c\u63d2\u5165\u6570\u636e \u53c2\u8003ELK\u4ea7\u54c1\u6587\u6863\u300a\u8fdc\u7a0b\u8fde\u63a5\u6570\u636e\u5e93\u300b\u914d\u7f6eELK\u767d\u540d\u5355\uff0c\u80fd\u591f\u8bbf\u95eejupyter notebook\u4e3b\u673a \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u5c06ELK JDBC\u9a71\u52a8jar\u5305 gsjdbc4.jar \u653e\u5230jupyter notebook\u4e3b\u673a /opt \u8def\u5f84\u4e0b \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype jar = \"/opt/gsjdbc4.jar\" # location of the jdbc driver jar args='-Djava.class.path=%s' % jar jvm = jpype.getDefaultJVMPath() jpype.startJVM(jvm, args) # this worked conn = jaydebeapi.connect( 'org.postgresql.Driver', [\"jdbc:postgresql://172.16.6.10:25108/db_tpcds\" , \"joe\", \"Bigdata@123\"], \"/opt/gsjdbc4.jar\" ) import pandas as pd sql = \"Select * From hdfs_001\" df = pd.read_sql(sql, conn) df conn.close()","title":"\u5bf9\u63a5ELK"},{"location":"Development/JupyterNotebook/#fq","text":"\u5728\u4f7f\u7528pySpark\u7684\u65f6\u5019\u9047\u5230\u5982\u4e0b\u95ee\u9898\uff1a ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /opt/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 \u89e3\u51b3\u529e\u6cd5\uff1a \u8fd0\u884c sc.stop() \u8fde\u63a5ELK\u65f6\u5019\u62a5\u9519\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u914d\u7f6eELK\u767d\u540d\u5355","title":"F&amp;Q"},{"location":"Development/Jupyter_Notebook/","text":"Jupyter_Notebook\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Jupyter Notebook 2.4.4.0 \u2194 FusionInsight HD V100R002C70SPC200 (pySpark) \u5b89\u88c5Jupyter notebook \u00b6 Jupyter notebook\u7684\u5b89\u88c5\u4f9d\u8d56\u4e8ePython\uff0c\u4e14\u6d89\u53ca\u5230\u8bb8\u591a\u5de5\u5177\u7684\u4f9d\u8d56\u5305\uff0c\u76f8\u4e92\u4e4b\u95f4\u8fd8\u5b58\u5728\u7248\u672c\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u8f83\u9ebb\u70e6\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u5b89\u88c5Anaconda\u5305\uff0c\u91cc\u9762\u5305\u542b\u4e86Python\u3001Jupyter Notebook\uff0c\u4ee5\u53ca\u4f17\u591a\u7684\u79d1\u5b66\u5de5\u5177\u5305\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u5b89\u88c5Anaconda \u4eceAnaconda\u5b98\u7f51\u4e0b\u8f7d\u5e76\u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh \u751f\u6210Jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 jupyter notebook --generate-config --allow-root \u4fee\u6539Jupyter notebook\u7684\u914d\u7f6eIPc.NotebookApp.ip\u4e3a\u672c\u673aIP\u5730\u5740 vi /root/.jupyter/jupyter_notebook_config.py \u542f\u52a8Jupyter notebook:: jupyter notebook --allow-root \u51fa\u73b0\u5982\u4e0b\u63d0\u793a\u8868\u793aJupyter notebook\u542f\u52a8\u6210\u529f [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 \u4f7f\u7528 Ctrl+C \u53ef\u4ee5\u9000\u51faJupyter notebook \u5b89\u88c5FusionInsight Client \u00b6 \u53c2\u8003FusionInsight\u7684\u4ea7\u54c1\u6587\u6863\u5b8c\u6210Linux\u4e0b\u7684FusionInsight\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5b89\u88c5\u5230 /opt/hadoopclient \u76ee\u5f55 \u5b8c\u6210Kerberos\u8ba4\u8bc1 \u00b6 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /opt/hadoopclient/ source bigdata_env kinit sparkuser \u5bfc\u5165ipython\u76f8\u5173\u73af\u5883\u53d8\u91cf \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165\u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u4e24\u884c\u6dfb\u52a0\u5230 /opt/hadoopclient/bigdata_env\u6587\u4ef6 \uff0c\u540e\u7eedsource bigdata_env\u65f6\u53ef\u4ee5\u81ea\u52a8\u5c06\u73af\u5883\u53d8\u91cf\u5bfc\u5165 export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" Jupyter notebook\u4e2d\u4f7f\u7528pyspark\u8fdb\u884c\u5206\u6790 \u00b6 \u6267\u884cpyspark\u4f1a\u81ea\u52a8\u542f\u52a8Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 \u6253\u5f00\u4e0a\u8ff0\u94fe\u63a5\uff0c\u53ef\u4ee5\u8fdb\u884c\u6570\u636e\u5206\u6790 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10-1.2.0\") sqlContext <- sparkRSQL.init(sc) flightsDF <- read.df(sqlContext, \"/user/sparkuser/flights.csv\", source = \"com.databricks.spark.csv\", header = \"true\") destDF <- select(flightsDF, \"dest\", \"cancelled\") groupBy(flightsDF, flightsDF$date) %>% summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10) Jupyter notebook\u4e2d\u4f7f\u7528R\u8bed\u8a00\u8fdb\u884c\u5206\u6790 \u00b6 TBD","title":"2.4.4.0 <--> C70"},{"location":"Development/Jupyter_Notebook/#jupyter_notebookfusioninsight","text":"","title":"Jupyter_Notebook\u5bf9\u63a5FusionInsight"},{"location":"Development/Jupyter_Notebook/#_1","text":"Jupyter Notebook 2.4.4.0 \u2194 FusionInsight HD V100R002C70SPC200 (pySpark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Jupyter_Notebook/#jupyter-notebook","text":"Jupyter notebook\u7684\u5b89\u88c5\u4f9d\u8d56\u4e8ePython\uff0c\u4e14\u6d89\u53ca\u5230\u8bb8\u591a\u5de5\u5177\u7684\u4f9d\u8d56\u5305\uff0c\u76f8\u4e92\u4e4b\u95f4\u8fd8\u5b58\u5728\u7248\u672c\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u8f83\u9ebb\u70e6\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u5b89\u88c5Anaconda\u5305\uff0c\u91cc\u9762\u5305\u542b\u4e86Python\u3001Jupyter Notebook\uff0c\u4ee5\u53ca\u4f17\u591a\u7684\u79d1\u5b66\u5de5\u5177\u5305\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u5b89\u88c5Anaconda \u4eceAnaconda\u5b98\u7f51\u4e0b\u8f7d\u5e76\u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh \u751f\u6210Jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 jupyter notebook --generate-config --allow-root \u4fee\u6539Jupyter notebook\u7684\u914d\u7f6eIPc.NotebookApp.ip\u4e3a\u672c\u673aIP\u5730\u5740 vi /root/.jupyter/jupyter_notebook_config.py \u542f\u52a8Jupyter notebook:: jupyter notebook --allow-root \u51fa\u73b0\u5982\u4e0b\u63d0\u793a\u8868\u793aJupyter notebook\u542f\u52a8\u6210\u529f [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 \u4f7f\u7528 Ctrl+C \u53ef\u4ee5\u9000\u51faJupyter notebook","title":"\u5b89\u88c5Jupyter notebook"},{"location":"Development/Jupyter_Notebook/#fusioninsight-client","text":"\u53c2\u8003FusionInsight\u7684\u4ea7\u54c1\u6587\u6863\u5b8c\u6210Linux\u4e0b\u7684FusionInsight\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5b89\u88c5\u5230 /opt/hadoopclient \u76ee\u5f55","title":"\u5b89\u88c5FusionInsight Client"},{"location":"Development/Jupyter_Notebook/#kerberos","text":"\u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /opt/hadoopclient/ source bigdata_env kinit sparkuser","title":"\u5b8c\u6210Kerberos\u8ba4\u8bc1"},{"location":"Development/Jupyter_Notebook/#ipython","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165\u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u4e24\u884c\u6dfb\u52a0\u5230 /opt/hadoopclient/bigdata_env\u6587\u4ef6 \uff0c\u540e\u7eedsource bigdata_env\u65f6\u53ef\u4ee5\u81ea\u52a8\u5c06\u73af\u5883\u53d8\u91cf\u5bfc\u5165 export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\"","title":"\u5bfc\u5165ipython\u76f8\u5173\u73af\u5883\u53d8\u91cf"},{"location":"Development/Jupyter_Notebook/#jupyter-notebookpyspark","text":"\u6267\u884cpyspark\u4f1a\u81ea\u52a8\u542f\u52a8Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 \u6253\u5f00\u4e0a\u8ff0\u94fe\u63a5\uff0c\u53ef\u4ee5\u8fdb\u884c\u6570\u636e\u5206\u6790 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10-1.2.0\") sqlContext <- sparkRSQL.init(sc) flightsDF <- read.df(sqlContext, \"/user/sparkuser/flights.csv\", source = \"com.databricks.spark.csv\", header = \"true\") destDF <- select(flightsDF, \"dest\", \"cancelled\") groupBy(flightsDF, flightsDF$date) %>% summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10)","title":"Jupyter notebook\u4e2d\u4f7f\u7528pyspark\u8fdb\u884c\u5206\u6790"},{"location":"Development/Jupyter_Notebook/#jupyter-notebookr","text":"TBD","title":"Jupyter notebook\u4e2d\u4f7f\u7528R\u8bed\u8a00\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/","text":"RSutdio\u5bf9\u63a5FusionInsight Spark \u00b6 \u9002\u7528\u573a\u666f \u00b6 RStudio 3.4.1 \u2194 FusionInsight HD V100R002C60U10 (SparkR) RStudio 3.4.1 \u2194 FusionInsight HD V100R002C70SPC100 (SparkR) \u5bf9\u63a5\u65b9\u5f0f \u00b6 RStudio\u4e0eSpark\u96c6\u6210\u6709\u4e24\u79cd\u65b9\u5f0f\uff1a \u901a\u8fc7RStudio\u5b98\u65b9\u53d1\u5e03\u7684sparklyr\u4e0eSpark\u8fdb\u884c\u96c6\u6210 \u901a\u8fc7Apache Spark\u793e\u533a\u53d1\u5e03\u7684SparkR\u8fdb\u884c\u96c6\u6210 \u672c\u6587\u6863\u5305\u542b\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5\u7684\u6b65\u9aa4, \u76f8\u5173\u5bf9\u63a5\u6b65\u9aa4\u5982\u4e0b\uff1a \u5b89\u88c5R \u5b89\u88c5RStudio Server \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x) \u5b89\u88c5R \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728RStudio\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u672c\u6587\u4f7f\u7528\u7684RStudio\u8282\u70b9\u4e3aRedhat7.1\uff0cFusionInsight\u96c6\u7fa4\u8282\u70b9\u4e3aRedhat6.6 \u914d\u7f6eredhat\u7684yum\u6e90\uff0c\u56fd\u5185\u53ef\u4ee5\u914d\u7f6e aliyun\u7684\u6e90 \u6216\u8005 163\u7684\u6e90 \u914d\u7f6eEPEL\u7684\u6e90 \u5b89\u88c5R-3.4.1 \u914d\u7f6ealiyun\u7684\u6e90 \u00b6 \u914d\u7f6e\u597dRedhat7.1\u7684yum\u6e90 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache \u914d\u7f6e163\u7684\u6e90 \u00b6 \u914d\u7f6e\u597dRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 \u00b6 \u5b89\u88c5EPEL\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u5b89\u88c5R-3.4.1 \u00b6 \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u5b89\u88c5RStudio Server \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm \u4f7f\u7528 vi /etc/rstudio/rserver.conf \u4fee\u6539RStudio\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u6307\u5b9aRStudio Server\u4f7f\u7528\u7684R\u7684\u8def\u5f84 rsession-which-r=/usr/bin/R \u91cd\u542frstudio-server\u540e\uff0c\u67e5\u770b\u670d\u52a1\u662f\u5426\u6b63\u5e38 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server \u670d\u52a1\u6b63\u5e38\u542f\u52a8\u5982\u4e0b \u7531\u4e8eRStudio Server\u4e0d\u5141\u8bb8\u4f7f\u7528root\u7528\u6237\u767b\u9646\uff0c\u9700\u8981\u65b0\u5efa\u4e00\u4e2a\u666e\u901a\u7528\u6237\u7528\u4e8eWeb\u754c\u9762\u7684\u767b\u9646 useradd -d /home/test -m test passwd test \u7528\u6237\u65b0\u5efa\u5b8c\u6210\u540e\uff0c\u5173\u95ed\u9632\u706b\u5899\uff0c\u7136\u540e\u4f7f\u7528\u672c\u673aip:8787\u7aef\u53e3\u8bbf\u95eeRStudio Server\uff0c\u4f7f\u7528\u65b0\u5efa\u7684test\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 sudo systemctl stop firewalld \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u00b6 \u767b\u5f55FusionInsight Manager\u7cfb\u7edf\uff0c\u5355\u51fb \u670d\u52a1\u7ba1\u7406 \uff0c\u5728\u83dc\u5355\u680f\u4e2d\u5355\u51fb \u4e0b\u8f7d\u5ba2\u6237\u7aef , \u5ba2\u6237\u7aef\u7c7b\u578b\u52fe\u9009 \u5b8c\u6574\u5ba2\u6237\u7aef , \u662f\u5426\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e2d\u751f\u6210\u5ba2\u6237\u7aef\u6587\u4ef6\u9009\u62e9 \u5426 \u4f7f\u7528WinSCP\u5de5\u5177\u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684\u8f6f\u4ef6\u5305\u4e0a\u4f20\u5230Linux\u670d\u52a1\u5668\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u5207\u6362\u5230\u65b0\u5efa\u7684test\u7528\u6237 su test \u89e3\u538b\u8f6f\u4ef6\u5305\u3002\u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u3002\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u89e3\u538b\u5b89\u88c5\u5305\u5230\u672c\u5730\u76ee\u5f55 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar \u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u5ba2\u6237\u7aef\u5230\u6307\u5b9a\u76ee\u5f55\uff08\u7edd\u5bf9\u8def\u5f84\uff09\uff0c\u4f8b\u5982\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient \u5ba2\u6237\u7aef\u5c06\u88ab\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55\u4e2d \u68c0\u67e5\u5ba2\u6237\u7aef\u8282\u70b9\u4e0eFusionInsight\u96c6\u7fa4\u65f6\u95f4\u540c\u6b65\uff08\u5dee\u8ddd\u4e0d\u80fd\u8d85\u8fc75\u5206\u949f\uff09 \u68c0\u67e5SparkR\u662f\u5426\u53ef\u7528 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u6267\u884c sparkR \u542f\u52a8SparkR, \u6b63\u5e38\u542f\u52a8\u51fa\u73b0\u4ee5\u4e0b\u754c\u9762 \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u00b6 \u4f7f\u7528\u65b0\u5efa\u7684\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u754c\u9762\u4e2d\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u521d\u59cb\u5316SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10:1.2.0\") sqlContext <- sparkRSQL.init(sc) \u521d\u59cb\u5316\u6210\u529f\u540e\u5982\u4e0b\u56fe \u5728Yarn\u7684ResourceManager\u754c\u9762\u53ef\u4ee5\u770b\u5230sparkuser\u5728\u96c6\u7fa4\u542f\u52a8\u4e86\u4e00\u4e2aSparkR\u7684\u5e94\u7528 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u00b6 R DataFrame \u8f6c\u5316\u4e3aSparkR DataFrame df <- createDataFrame(sqlContext, faithful) head(df) \u901a\u8fc7JSON\u6587\u4ef6\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790\u5904\u7406 \u5c06\u6d4b\u8bd5\u6570\u636eput\u5230HDFS\u4e2d wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ \u6267\u884c\u6587\u4ef6\u52a0\u8f7d\u5206\u6790 people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") head(people) printSchema(people) \u4eceHive\u8868\u4e2d\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790 hiveContext <- sparkRHive.init(sc) results <- sql(hiveContext, \"SELECT * FROM employees\") head(results) DataFrame Operations Selecting rows, columns df <- createDataFrame(sqlContext, faithful) df head(select(df, df$eruptions)) head(select(df, \"eruptions\")) head(filter(df, df$waiting < 50)) Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting)) head(arrange(waiting_counts, desc(waiting_counts$count))) Operating on Columns df$waiting_secs <- df$waiting * 60 head(df) Running SQL Queries from SparkR people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") registerTempTable(people, \"people\") teenagers <- sql(sqlContext, \"SELECT name FROM people WHERE age >= 13 AND age <= 19\") head(teenagers) Machine Learning df <- createDataFrame(sqlContext, iris) model <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\") summary(model) predictions <- predict(model, newData = df) head(select(predictions, \"Sepal_Length\", \"prediction\")) \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u00b6 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u4e2d\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5b89\u88c5\u6240\u9700\u7684library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") \u901a\u8fc7spark_connect\u8fde\u63a5spark\u96c6\u7fa4 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") \u8fd9\u91cc\u5982\u679cSPARK_HOME\u6307\u5411/home/test/hadoopclient/Spark/spark\uff0c\u540c\u65f6\u8bbe\u7f6eversion\u4e3a1.6.1\uff0c\u5219\u4f1a\u5bf9\u63a5\u4e0a1.5.1\u7684Spark sparklyr\u5b98\u65b9\u652f\u6301\u662f1.6.1\u4ee5\u4e0a\u7684Spark\uff0c\u8fd9\u91cc\u5f3a\u5236\u6307\u5b9aversion\u4e3a1.6.1\uff0c\u4e3b\u8981\u529f\u80fd\u5747\u6b63\u5e38\uff0c\u90e8\u5206Spark1.6.1\u652f\u6301\u800c1.5.1\u4e0d\u652f\u6301\u7684\u7279\u6027\u6267\u884c\u4f1a\u5931\u8d25 \u542f\u52a8\u6210\u529f\u540e\uff0c\u5728FusionInsgiht\u7684Yarn\u7684ResourceManager\u9875\u9762\u53ef\u4ee5\u770b\u5230sparklyr\u7684\u4efb\u52a1\u5df2\u7ecf\u542f\u52a8 \u5728RStudio\u7684Spark\u9762\u677f\u5237\u65b0\u4e00\u4e0b\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u6709hive\u7684\u8868 \u9009\u62e9hive\u8868\u53f3\u8fb9\u7684\u6570\u636e\u56fe\u8868\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u00b6 Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc <- spark_connect(master = \"yarn-client\", version = \"1.6.1\", spark_home = \"/home/test/hadoopclient/Spark/spark\") Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl <- copy_to(sc, babynames, \"babynames\") applicants_tbl <- copy_to(sc, applicants, \"applicants\") birthsYearly <- applicants_tbl %>% mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl <- filteredNames_tbl %>% group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl <- yearlyNames_tbl %>% filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly <- yearlyNames_tbl %>% inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl <- yearlyNames_tbl %>% filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly <- yearlyNames_tbl %>% inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName <- babynames_tbl %>% mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE) \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x) \u00b6 Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) ** Assess model performance** Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { sdf_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- sdf_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights. FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u00b6 \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167 \u914d\u7f6eEPEL\u7684\u6e90\u5b89\u88c5R \u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u5b89\u88c5sparklyr\u62a5\u9519configuration failed for package \u2018openssl\u2019 \u00b6 \u64cd\u4f5c\u7cfb\u7edf\u9700\u8981\u6267\u884c yum install openssl-devel \u5b89\u88c5openssl-devel \u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/sparkuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports ;","title":"3.4.1 <--> C70"},{"location":"Development/RStudio/#rsutdiofusioninsight-spark","text":"","title":"RSutdio\u5bf9\u63a5FusionInsight Spark"},{"location":"Development/RStudio/#_1","text":"RStudio 3.4.1 \u2194 FusionInsight HD V100R002C60U10 (SparkR) RStudio 3.4.1 \u2194 FusionInsight HD V100R002C70SPC100 (SparkR)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/RStudio/#_2","text":"RStudio\u4e0eSpark\u96c6\u6210\u6709\u4e24\u79cd\u65b9\u5f0f\uff1a \u901a\u8fc7RStudio\u5b98\u65b9\u53d1\u5e03\u7684sparklyr\u4e0eSpark\u8fdb\u884c\u96c6\u6210 \u901a\u8fc7Apache Spark\u793e\u533a\u53d1\u5e03\u7684SparkR\u8fdb\u884c\u96c6\u6210 \u672c\u6587\u6863\u5305\u542b\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5\u7684\u6b65\u9aa4, \u76f8\u5173\u5bf9\u63a5\u6b65\u9aa4\u5982\u4e0b\uff1a \u5b89\u88c5R \u5b89\u88c5RStudio Server \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x)","title":"\u5bf9\u63a5\u65b9\u5f0f"},{"location":"Development/RStudio/#r","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728RStudio\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u672c\u6587\u4f7f\u7528\u7684RStudio\u8282\u70b9\u4e3aRedhat7.1\uff0cFusionInsight\u96c6\u7fa4\u8282\u70b9\u4e3aRedhat6.6 \u914d\u7f6eredhat\u7684yum\u6e90\uff0c\u56fd\u5185\u53ef\u4ee5\u914d\u7f6e aliyun\u7684\u6e90 \u6216\u8005 163\u7684\u6e90 \u914d\u7f6eEPEL\u7684\u6e90 \u5b89\u88c5R-3.4.1","title":"\u5b89\u88c5R"},{"location":"Development/RStudio/#aliyun","text":"\u914d\u7f6e\u597dRedhat7.1\u7684yum\u6e90 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache","title":"\u914d\u7f6ealiyun\u7684\u6e90"},{"location":"Development/RStudio/#163","text":"\u914d\u7f6e\u597dRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache","title":"\u914d\u7f6e163\u7684\u6e90"},{"location":"Development/RStudio/#epel","text":"\u5b89\u88c5EPEL\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache","title":"\u914d\u7f6eEPEL\u7684\u6e90"},{"location":"Development/RStudio/#r-341","text":"\u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a","title":"\u5b89\u88c5R-3.4.1"},{"location":"Development/RStudio/#rstudio-server","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm \u4f7f\u7528 vi /etc/rstudio/rserver.conf \u4fee\u6539RStudio\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u6307\u5b9aRStudio Server\u4f7f\u7528\u7684R\u7684\u8def\u5f84 rsession-which-r=/usr/bin/R \u91cd\u542frstudio-server\u540e\uff0c\u67e5\u770b\u670d\u52a1\u662f\u5426\u6b63\u5e38 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server \u670d\u52a1\u6b63\u5e38\u542f\u52a8\u5982\u4e0b \u7531\u4e8eRStudio Server\u4e0d\u5141\u8bb8\u4f7f\u7528root\u7528\u6237\u767b\u9646\uff0c\u9700\u8981\u65b0\u5efa\u4e00\u4e2a\u666e\u901a\u7528\u6237\u7528\u4e8eWeb\u754c\u9762\u7684\u767b\u9646 useradd -d /home/test -m test passwd test \u7528\u6237\u65b0\u5efa\u5b8c\u6210\u540e\uff0c\u5173\u95ed\u9632\u706b\u5899\uff0c\u7136\u540e\u4f7f\u7528\u672c\u673aip:8787\u7aef\u53e3\u8bbf\u95eeRStudio Server\uff0c\u4f7f\u7528\u65b0\u5efa\u7684test\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 sudo systemctl stop firewalld","title":"\u5b89\u88c5RStudio Server"},{"location":"Development/RStudio/#fusioninsight","text":"\u767b\u5f55FusionInsight Manager\u7cfb\u7edf\uff0c\u5355\u51fb \u670d\u52a1\u7ba1\u7406 \uff0c\u5728\u83dc\u5355\u680f\u4e2d\u5355\u51fb \u4e0b\u8f7d\u5ba2\u6237\u7aef , \u5ba2\u6237\u7aef\u7c7b\u578b\u52fe\u9009 \u5b8c\u6574\u5ba2\u6237\u7aef , \u662f\u5426\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e2d\u751f\u6210\u5ba2\u6237\u7aef\u6587\u4ef6\u9009\u62e9 \u5426 \u4f7f\u7528WinSCP\u5de5\u5177\u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684\u8f6f\u4ef6\u5305\u4e0a\u4f20\u5230Linux\u670d\u52a1\u5668\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u5207\u6362\u5230\u65b0\u5efa\u7684test\u7528\u6237 su test \u89e3\u538b\u8f6f\u4ef6\u5305\u3002\u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u3002\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u89e3\u538b\u5b89\u88c5\u5305\u5230\u672c\u5730\u76ee\u5f55 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar \u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u5ba2\u6237\u7aef\u5230\u6307\u5b9a\u76ee\u5f55\uff08\u7edd\u5bf9\u8def\u5f84\uff09\uff0c\u4f8b\u5982\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient \u5ba2\u6237\u7aef\u5c06\u88ab\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55\u4e2d \u68c0\u67e5\u5ba2\u6237\u7aef\u8282\u70b9\u4e0eFusionInsight\u96c6\u7fa4\u65f6\u95f4\u540c\u6b65\uff08\u5dee\u8ddd\u4e0d\u80fd\u8d85\u8fc75\u5206\u949f\uff09 \u68c0\u67e5SparkR\u662f\u5426\u53ef\u7528 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u6267\u884c sparkR \u542f\u52a8SparkR, \u6b63\u5e38\u542f\u52a8\u51fa\u73b0\u4ee5\u4e0b\u754c\u9762","title":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef"},{"location":"Development/RStudio/#sparkrrstudio","text":"\u4f7f\u7528\u65b0\u5efa\u7684\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u754c\u9762\u4e2d\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u521d\u59cb\u5316SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10:1.2.0\") sqlContext <- sparkRSQL.init(sc) \u521d\u59cb\u5316\u6210\u529f\u540e\u5982\u4e0b\u56fe \u5728Yarn\u7684ResourceManager\u754c\u9762\u53ef\u4ee5\u770b\u5230sparkuser\u5728\u96c6\u7fa4\u542f\u52a8\u4e86\u4e00\u4e2aSparkR\u7684\u5e94\u7528","title":"\u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/#rstudiosparkr","text":"R DataFrame \u8f6c\u5316\u4e3aSparkR DataFrame df <- createDataFrame(sqlContext, faithful) head(df) \u901a\u8fc7JSON\u6587\u4ef6\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790\u5904\u7406 \u5c06\u6d4b\u8bd5\u6570\u636eput\u5230HDFS\u4e2d wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ \u6267\u884c\u6587\u4ef6\u52a0\u8f7d\u5206\u6790 people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") head(people) printSchema(people) \u4eceHive\u8868\u4e2d\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790 hiveContext <- sparkRHive.init(sc) results <- sql(hiveContext, \"SELECT * FROM employees\") head(results) DataFrame Operations Selecting rows, columns df <- createDataFrame(sqlContext, faithful) df head(select(df, df$eruptions)) head(select(df, \"eruptions\")) head(filter(df, df$waiting < 50)) Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting)) head(arrange(waiting_counts, desc(waiting_counts$count))) Operating on Columns df$waiting_secs <- df$waiting * 60 head(df) Running SQL Queries from SparkR people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") registerTempTable(people, \"people\") teenagers <- sql(sqlContext, \"SELECT name FROM people WHERE age >= 13 AND age <= 19\") head(teenagers) Machine Learning df <- createDataFrame(sqlContext, iris) model <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\") summary(model) predictions <- predict(model, newData = df) head(select(predictions, \"Sepal_Length\", \"prediction\"))","title":"\u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790"},{"location":"Development/RStudio/#rstudio-sparklyrspark","text":"\u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u4e2d\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5b89\u88c5\u6240\u9700\u7684library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") \u901a\u8fc7spark_connect\u8fde\u63a5spark\u96c6\u7fa4 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") \u8fd9\u91cc\u5982\u679cSPARK_HOME\u6307\u5411/home/test/hadoopclient/Spark/spark\uff0c\u540c\u65f6\u8bbe\u7f6eversion\u4e3a1.6.1\uff0c\u5219\u4f1a\u5bf9\u63a5\u4e0a1.5.1\u7684Spark sparklyr\u5b98\u65b9\u652f\u6301\u662f1.6.1\u4ee5\u4e0a\u7684Spark\uff0c\u8fd9\u91cc\u5f3a\u5236\u6307\u5b9aversion\u4e3a1.6.1\uff0c\u4e3b\u8981\u529f\u80fd\u5747\u6b63\u5e38\uff0c\u90e8\u5206Spark1.6.1\u652f\u6301\u800c1.5.1\u4e0d\u652f\u6301\u7684\u7279\u6027\u6267\u884c\u4f1a\u5931\u8d25 \u542f\u52a8\u6210\u529f\u540e\uff0c\u5728FusionInsgiht\u7684Yarn\u7684ResourceManager\u9875\u9762\u53ef\u4ee5\u770b\u5230sparklyr\u7684\u4efb\u52a1\u5df2\u7ecf\u542f\u52a8 \u5728RStudio\u7684Spark\u9762\u677f\u5237\u65b0\u4e00\u4e0b\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u6709hive\u7684\u8868 \u9009\u62e9hive\u8868\u53f3\u8fb9\u7684\u6570\u636e\u56fe\u8868\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"\u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/#sparklyrsparkbabynames","text":"Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc <- spark_connect(master = \"yarn-client\", version = \"1.6.1\", spark_home = \"/home/test/hadoopclient/Spark/spark\") Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl <- copy_to(sc, babynames, \"babynames\") applicants_tbl <- copy_to(sc, applicants, \"applicants\") birthsYearly <- applicants_tbl %>% mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl <- filteredNames_tbl %>% group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl <- yearlyNames_tbl %>% filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly <- yearlyNames_tbl %>% inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl <- yearlyNames_tbl %>% filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly <- yearlyNames_tbl %>% inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName <- babynames_tbl %>% mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE)","title":"\u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6"},{"location":"Development/RStudio/#sparklyrsparkspark2x","text":"Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) ** Assess model performance** Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { sdf_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- sdf_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights.","title":"\u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x)"},{"location":"Development/RStudio/#faq","text":"","title":"FAQ"},{"location":"Development/RStudio/#fusioninsightr","text":"\u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167 \u914d\u7f6eEPEL\u7684\u6e90\u5b89\u88c5R \u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5","title":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R"},{"location":"Development/RStudio/#sparklyrconfiguration-failed-for-package-openssl","text":"\u64cd\u4f5c\u7cfb\u7edf\u9700\u8981\u6267\u884c yum install openssl-devel \u5b89\u88c5openssl-devel","title":"\u5b89\u88c5sparklyr\u62a5\u9519configuration failed for package \u2018openssl\u2019"},{"location":"Development/RStudio/#sparklyr","text":"\u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/sparkuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports ;","title":"\u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e"},{"location":"Development/Squirrel_3.8.0/","text":"Squirrel\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Squirrel 3.7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) Squirrel 3.8.0 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL) \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86Squirrel\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 Squirrel\u5b89\u88c5 \u00b6 \u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u3002 \u4fee\u6539 C:\\Windows\\System32\\drivers\\etc\\hosts \u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f\u3002 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dSquirrel\uff0c\u5730\u5740\uff1a http://www.squirrelsql.org/#installation \uff0c\u9009\u62e9Install jar of SQuirreL 3.7.1 for Windows/Linux/others\uff0c\u4e0b\u8f7d\u8f6f\u4ef6squirrel-sql-3.7.1-standard.jar \u53cc\u51fbsquirrel-sql-3.7.1-standard.jar\u5b89\u88c5 \u5728\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u8981\u5b89\u88c5\u54ea\u4e9b\u73af\u5883\uff0c\u4f7f\u7528\u7684\u6570\u636e\u5e93\u63d2\u4ef6\uff0c\u8bed\u8a00\u5305\u3002 Squirrel\u8fde\u63a5Fiber \u00b6 \u4f7f\u7528SQuirreL SQL Client\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9Drivers\uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path\uff1a\u5c06Fiber/lib\u4e0b\u7684jar\u5305\u90fd\u6dfb\u52a0\u8fdb\u6765 ClassName\uff1acom.huawei.fiber.FiberDriver \u53ef\u4ee5\u770b\u5230\u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002 \u5bf9\u63a5Hive \u00b6 \u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name\uff1atest Password\uff1a\u5bc6\u7801 \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u70b9\u51fb Connect \u67e5\u770bhive\u4e2d\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a \u5bf9\u63a5SparkSQL \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aspark\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fbFiber\uff0c\u70b9\u51fb Connet \uff0c\u5c06driver\u5207\u6362\u4e3aspark \u53ef\u4ee5\u770b\u5230\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Spark\u589e\u52a0\u6570\u636e \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a \u5bf9\u63a5Phoenix \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \uff0c\u5c06driver\u5207\u6362\u4e3aphoenix \u53ef\u4ee5\u770b\u5230\u6570\u636ephoenix\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5411phoenix\u8868\u4e2d\u589e\u52a0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') \u67e5\u8be2\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5220\u9664\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 delete from TB_PHOENIX where ID=109; \u67e5\u770b\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u66f4\u65b0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') \u67e5\u770b\u7ed3\u679c","title":"3.8.0 <--> C70"},{"location":"Development/Squirrel_3.8.0/#squirrelfusioninsight","text":"","title":"Squirrel\u5bf9\u63a5FusionInsight"},{"location":"Development/Squirrel_3.8.0/#_1","text":"Squirrel 3.7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) Squirrel 3.8.0 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Squirrel_3.8.0/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86Squirrel\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/Squirrel_3.8.0/#squirrel","text":"\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u3002 \u4fee\u6539 C:\\Windows\\System32\\drivers\\etc\\hosts \u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f\u3002 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dSquirrel\uff0c\u5730\u5740\uff1a http://www.squirrelsql.org/#installation \uff0c\u9009\u62e9Install jar of SQuirreL 3.7.1 for Windows/Linux/others\uff0c\u4e0b\u8f7d\u8f6f\u4ef6squirrel-sql-3.7.1-standard.jar \u53cc\u51fbsquirrel-sql-3.7.1-standard.jar\u5b89\u88c5 \u5728\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u8981\u5b89\u88c5\u54ea\u4e9b\u73af\u5883\uff0c\u4f7f\u7528\u7684\u6570\u636e\u5e93\u63d2\u4ef6\uff0c\u8bed\u8a00\u5305\u3002","title":"Squirrel\u5b89\u88c5"},{"location":"Development/Squirrel_3.8.0/#squirrelfiber","text":"\u4f7f\u7528SQuirreL SQL Client\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9Drivers\uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path\uff1a\u5c06Fiber/lib\u4e0b\u7684jar\u5305\u90fd\u6dfb\u52a0\u8fdb\u6765 ClassName\uff1acom.huawei.fiber.FiberDriver \u53ef\u4ee5\u770b\u5230\u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002","title":"Squirrel\u8fde\u63a5Fiber"},{"location":"Development/Squirrel_3.8.0/#hive","text":"\u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name\uff1atest Password\uff1a\u5bc6\u7801 \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u70b9\u51fb Connect \u67e5\u770bhive\u4e2d\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"\u5bf9\u63a5Hive"},{"location":"Development/Squirrel_3.8.0/#sparksql","text":"\u5c06defaultDriver\u5207\u6362\u4e3aspark\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fbFiber\uff0c\u70b9\u51fb Connet \uff0c\u5c06driver\u5207\u6362\u4e3aspark \u53ef\u4ee5\u770b\u5230\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Spark\u589e\u52a0\u6570\u636e \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"\u5bf9\u63a5SparkSQL"},{"location":"Development/Squirrel_3.8.0/#phoenix","text":"\u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \uff0c\u5c06driver\u5207\u6362\u4e3aphoenix \u53ef\u4ee5\u770b\u5230\u6570\u636ephoenix\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5411phoenix\u8868\u4e2d\u589e\u52a0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') \u67e5\u8be2\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5220\u9664\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 delete from TB_PHOENIX where ID=109; \u67e5\u770b\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u66f4\u65b0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') \u67e5\u770b\u7ed3\u679c","title":"\u5bf9\u63a5Phoenix"},{"location":"Development/Squirrel_3.9.1/","text":"SQuirreL\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Squirrel 3.9.1 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) \u7b80\u4ecb \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001SQuirreL\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cSQuirreL\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a \u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab.file\u548chbase.myclient.principal\u3002 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.master.keytab.file</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> SQuirreL\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 SQuirreL SQL Client\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4ece http://www.squirrelsql.org/#installation \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684SQuirreL\u8f6f\u4ef6\uff08\u4f8b\u5982\uff1aInstall jar of SQuirreL 3.9.1 for Windows/Linux/others\uff09\u3002\u53cc\u51fb\u4e0b\u8f7d\u7684jar\u5305\u542f\u52a8\u5b89\u88c5\uff0c\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Hive \u00b6 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9 Drivers \uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml;defaultDriver=hive Extra Class Path: \u70b9\u51fbAdd\u6309\u94ae\u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\uff0c\u70b9\u51fbList Drivers\u6309\u94ae\u7136\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002 \u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u3002\u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name: developuser\uff08\u53ef\u4e0d\u586b\u5199\uff09 Password\uff1a\u5bc6\u7801\uff08\u53ef\u4e0d\u586b\u5199\uff09 \u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u70b9\u51fb Connect \u3002 \u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u5411Hive\u8868SQL_TEST\u63d2\u5165\u6570\u636e \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 SQL_TEST \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS SQL_TEST (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST; SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x \u00b6 \u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a spark2x \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u5982\u679c\u8fd4\u56de\u201cSession startup time hint\u201d\uff0c\u5219\u70b9\u51fb Close \u3002 \u5982\u679c\u6ca1\u6709\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \uff0c\u70b9\u51fb \u6309\u94ae\u201cRefresh Object Tree and Database Meta Data Cache\u201d\uff0c\u5219\u4f1a\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u3002\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u8868SQL_TEST\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST; SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix \u00b6 \u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a phoenix \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb Objects->Fiber->MY_NS->TABLE->SQL_TEST->Content \u67e5\u770bSQL_TEST\u8868\u6570\u636e\u3002","title":"3.9.1 <--> 6.5"},{"location":"Development/Squirrel_3.9.1/#squirrelfusioninsight","text":"","title":"SQuirreL\u5bf9\u63a5FusionInsight"},{"location":"Development/Squirrel_3.9.1/#_1","text":"Squirrel 3.9.1 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Squirrel_3.9.1/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001SQuirreL\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cSQuirreL\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Development/Squirrel_3.9.1/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1);","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Development/Squirrel_3.9.1/#fiber","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/Squirrel_3.9.1/#_4","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Squirrel_3.9.1/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Squirrel_3.9.1/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Squirrel_3.9.1/#kinit","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/Squirrel_3.9.1/#keytab","text":"\u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a \u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab.file\u548chbase.myclient.principal\u3002 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.master.keytab.file</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/Squirrel_3.9.1/#squirrelfiber","text":"","title":"SQuirreL\u5bf9\u63a5Fiber"},{"location":"Development/Squirrel_3.9.1/#_7","text":"SQuirreL SQL Client\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Squirrel_3.9.1/#_8","text":"\u4ece http://www.squirrelsql.org/#installation \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684SQuirreL\u8f6f\u4ef6\uff08\u4f8b\u5982\uff1aInstall jar of SQuirreL 3.9.1 for Windows/Linux/others\uff09\u3002\u53cc\u51fb\u4e0b\u8f7d\u7684jar\u5305\u542f\u52a8\u5b89\u88c5\uff0c\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Squirrel_3.9.1/#_9","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Squirrel_3.9.1/#squirrelfiberhive","text":"\u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9 Drivers \uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml;defaultDriver=hive Extra Class Path: \u70b9\u51fbAdd\u6309\u94ae\u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\uff0c\u70b9\u51fbList Drivers\u6309\u94ae\u7136\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002 \u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u3002\u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name: developuser\uff08\u53ef\u4e0d\u586b\u5199\uff09 Password\uff1a\u5bc6\u7801\uff08\u53ef\u4e0d\u586b\u5199\uff09 \u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u70b9\u51fb Connect \u3002 \u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u5411Hive\u8868SQL_TEST\u63d2\u5165\u6570\u636e \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 SQL_TEST \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS SQL_TEST (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST;","title":"SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Hive"},{"location":"Development/Squirrel_3.9.1/#squirrelfiberspark2x","text":"\u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a spark2x \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u5982\u679c\u8fd4\u56de\u201cSession startup time hint\u201d\uff0c\u5219\u70b9\u51fb Close \u3002 \u5982\u679c\u6ca1\u6709\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \uff0c\u70b9\u51fb \u6309\u94ae\u201cRefresh Object Tree and Database Meta Data Cache\u201d\uff0c\u5219\u4f1a\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u3002\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u8868SQL_TEST\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST;","title":"SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x"},{"location":"Development/Squirrel_3.9.1/#squirrelfiberphoenix","text":"\u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a phoenix \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb Objects->Fiber->MY_NS->TABLE->SQL_TEST->Content \u67e5\u770bSQL_TEST\u8868\u6570\u636e\u3002","title":"SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix"},{"location":"Development/Zeppelin_0.7.2/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.7.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive/Spark/SparkR) \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.7.2 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06\u8f6f\u4ef6\u5305zeppelin-0.7.2-bin-all.tgz\u4e0a\u4f20\u81f3/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.2-bin-all\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.7.2-bin-all.tgz \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/jdk1.7.0_51/ \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\uff0coverwrite\u9009\u62e9n \u5728/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u65b0\u5efa\u76ee\u5f55zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar \u5c06/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u4e0eFusionInsight\u51b2\u7a81\u768438\u4e2ajar\u5305\u79fb\u52a8\u5230zeppelin_hbase_jar\u76ee\u5f55\u4e2d commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar \u6700\u7ec8/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u6709152\u4e2ajar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b\u7684\u539f\u6709\u7684\u76f8\u5173\u7684jar\u5305\u5220\u9664 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684\u4ee5\u4e0bjar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u7684jackson\u7684\u76f8\u5173jar\u5305\u5220\u9664 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684jackson\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar \u5c06\u6b65\u9aa41\u548c\u6b65\u9aa42\u6240\u6709\u4ecespark\u5ba2\u6237\u7aef\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.2-bin-all/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"0.7.2 <--> C60"},{"location":"Development/Zeppelin_0.7.2/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.7.2/#_1","text":"Zeppelin 0.7.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive/Spark/SparkR)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#zeppelin","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.7.2/#_2","text":"\u5b89\u88c5Zeppelin0.7.2","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_4","text":"\u5c06\u8f6f\u4ef6\u5305zeppelin-0.7.2-bin-all.tgz\u4e0a\u4f20\u81f3/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.2-bin-all\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.7.2-bin-all.tgz \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/jdk1.7.0_51/ \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.7.2/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.7.2/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_10","text":"\u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\uff0coverwrite\u9009\u62e9n \u5728/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u65b0\u5efa\u76ee\u5f55zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar \u5c06/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u4e0eFusionInsight\u51b2\u7a81\u768438\u4e2ajar\u5305\u79fb\u52a8\u5230zeppelin_hbase_jar\u76ee\u5f55\u4e2d commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar \u6700\u7ec8/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u6709152\u4e2ajar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.7.2/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_12","text":"\u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_13","text":"\u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b\u7684\u539f\u6709\u7684\u76f8\u5173\u7684jar\u5305\u5220\u9664 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684\u4ee5\u4e0bjar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u7684jackson\u7684\u76f8\u5173jar\u5305\u5220\u9664 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684jackson\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar \u5c06\u6b65\u9aa41\u548c\u6b65\u9aa42\u6240\u6709\u4ecespark\u5ba2\u6237\u7aef\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.2-bin-all/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.7.2/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_15","text":"\u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#faq","text":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"FAQ"},{"location":"Development/Zeppelin_0.7.3/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C70SPC100 (HBase/Hive/Spark/SparkR) Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR) \u7f16\u8bd1Zeppelin \u00b6 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u5b89\u88c5bower npm install -g bower \u914d\u7f6ebower\u5141\u8bb8root\u7528\u6237\u6267\u884c echo '{ \"allow_root\": true }' > /root/.bowerrc \u6267\u884c bower -v \u83b7\u53d6Zeppelin0.7.3\u7684\u7248\u672c git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 \u4fee\u6539scala\u7248\u672c\uff0c\u9002\u914dFusionInsight_HD_V100R002C70SPC100\u7684Hadoop\u7248\u672c \u5728zeppelin\u4ee3\u7801\u6839\u76ee\u5f55\u6267\u884c vi ./dev/change_scala_version.sh \uff0c\u4fee\u6539\u4e0b\u56fe\u7684SCALA_LIB_VERSION\u4e3a2.11.8 \u6267\u884c\u547d\u4ee4\u5b8c\u6210scala\u7248\u672c\u7684\u4fee\u6539 ./dev/change_scala_version.sh 2.11 \u6267\u884c vi pom.xml \u6587\u4ef6\u7684\u4fee\u6539 \u4e3a0.9.3 \u6267\u884c vi hbase/pom.xml \u4fee\u6539hbase\u7248\u672c\u548chadoop\u7248\u672c \u7f16\u8bd1Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests \u7f16\u8bd1\u5b8c\u6210\u540e\u5728 zeppelin-distribution/target \u76ee\u5f55\u4e0b\u751f\u6210 zeppelin-0.7.3.tar.gz \u6587\u4ef6 \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.7.3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06\u7f16\u8bd1\u597d\u7684zeppelin-0.7.3.tar.gz\u4e0a\u4f20\u653e\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.3\u76ee\u5f55\u3002 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.3/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.3/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684sparkSQL\u8bed\u53e5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.3/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"0.7.3 <--> C80"},{"location":"Development/Zeppelin_0.7.3/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.7.3/#_1","text":"Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C70SPC100 (HBase/Hive/Spark/SparkR) Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#zeppelin","text":"\u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u5b89\u88c5bower npm install -g bower \u914d\u7f6ebower\u5141\u8bb8root\u7528\u6237\u6267\u884c echo '{ \"allow_root\": true }' > /root/.bowerrc \u6267\u884c bower -v \u83b7\u53d6Zeppelin0.7.3\u7684\u7248\u672c git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 \u4fee\u6539scala\u7248\u672c\uff0c\u9002\u914dFusionInsight_HD_V100R002C70SPC100\u7684Hadoop\u7248\u672c \u5728zeppelin\u4ee3\u7801\u6839\u76ee\u5f55\u6267\u884c vi ./dev/change_scala_version.sh \uff0c\u4fee\u6539\u4e0b\u56fe\u7684SCALA_LIB_VERSION\u4e3a2.11.8 \u6267\u884c\u547d\u4ee4\u5b8c\u6210scala\u7248\u672c\u7684\u4fee\u6539 ./dev/change_scala_version.sh 2.11 \u6267\u884c vi pom.xml \u6587\u4ef6\u7684\u4fee\u6539 \u4e3a0.9.3 \u6267\u884c vi hbase/pom.xml \u4fee\u6539hbase\u7248\u672c\u548chadoop\u7248\u672c \u7f16\u8bd1Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests \u7f16\u8bd1\u5b8c\u6210\u540e\u5728 zeppelin-distribution/target \u76ee\u5f55\u4e0b\u751f\u6210 zeppelin-0.7.3.tar.gz \u6587\u4ef6","title":"\u7f16\u8bd1Zeppelin"},{"location":"Development/Zeppelin_0.7.3/#zeppelin_1","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.7.3/#_2","text":"\u5b89\u88c5Zeppelin0.7.3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_4","text":"\u5c06\u7f16\u8bd1\u597d\u7684zeppelin-0.7.3.tar.gz\u4e0a\u4f20\u653e\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.3\u76ee\u5f55\u3002 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.7.3/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.3/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.7.3/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_10","text":"\u5c06 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.7.3/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_12","text":"\u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_13","text":"\u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.3/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684sparkSQL\u8bed\u53e5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.7.3/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_15","text":"\u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#faq","text":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.3/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"FAQ"},{"location":"Development/Zeppelin_0.8.0/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.8.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR/ELK) \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.8.0 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5Zeppelin 0.8.0,\u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf zeppelin-0.8.0-bin-all.tgz \u5b89\u88c5\u751f\u6210zeppelin-0.8.0-bin-all\u76ee\u5f55\u3002 \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 - \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all \u76ee\u5f55\u4e0b \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin \u91cd\u542fzeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u4f7f\u7528\u8d26\u6237developuser\u767b\u9646zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 502:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser/ \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b libfb303-0.9.3.jar \u548c libthrift-0.9.3.jar \u4e24\u4e2ajar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep \u8def\u5f84\u4e0b \u786e\u4fdd /usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter \u8def\u5f84\u4e0b\u6709\u4e14\u4ec5\u6709 libthrift-0.9.3.jar \u8fd9\u4e2a\u7248\u672c\u7684jar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u5982\u679c\u9047\u5230\u6e90yum-plugin-fastestmirror\u65e0\u6cd5\u4e0b\u8f7d\u65f6\uff0c\u53ef\u5728\u7f51\u5740 https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror \u4e0b\u9009\u62e9\u76f8\u5e94\u7684\u7248\u672c\u4ee3\u66ff\u4e0b\u8f7d\u5b89\u88c5 \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b Zeppelin\u8fde\u63a5Apache Livy \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u8fde\u63a5Livy \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8Livy\u670d\u52a1 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9livy\uff0c\u70b9\u51fb edit \u7f16\u8f91zeppelin.livy.url\u7684\u503c\u4e3a http://172.21.3.43:8998 \uff08\u53ef\u4ee5\u4e0d\u66f4\u6539\uff09\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982livy_connection_test \u5728Zeppelin\u4e2d\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cPySpark\u6837\u4f8b\u4ee3\u7801 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cSparkR\u6837\u4f8b\u4ee3\u7801 %livy.sparkr hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") Zeppelin\u8fde\u63a5FusionInsight Elk \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5FusionInsight Elk \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bElk\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55FusionInsight Elk, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93\uff0c \u6d4b\u8bd5\u6570\u636e\u8868 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25108 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**joe**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER joe WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO joe; \u521b\u5efaHDFS\u8868\u7a7a\u95f4\u3002 CREATE TABLESPACE hdfs_tablespace LOCATION '/srv/BigData/hadoop/hdfs_tablespace' WITH (filesystem = 'HDFS', cfgpath = '/opt/huawei/Bigdata/mppdb/conf', storepath = '/user/elk/tablespace/ hdfs_tablespace'); \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE TABLESPACE \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE DATABASE \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25108 -U joe -W Bigdata@123 \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\u3002 CREATE TABLE hdfs_001(id int,name varchar2(20) ) WITH (orientation=orc,version=0.12,compression=no) TABLESPACE hdfs_tablespace; \u4f7f\u7528INSERT\u547d\u4ee4\u63d2\u5165\u6570\u636e\u3002 \u63d2\u5165\u4e00\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'); \u63d2\u5165\u591a\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'),(2, 'Marketing'), (2, 'Purchasing'); \u68c0\u67e5\u7ed3\u679c Select * from hdfs_001 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6e\u96c6\u7fa4Elk\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 joe \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.52.190 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all joe 172.16.52.190/32 sha256\" \u4f7f\u7528\u201cjoe\u201d\u7528\u6237\u524d\uff0c\u9700\u5148\u672c\u5730\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u5e76\u5728\u6570\u636e\u5e93\u4e2d\u4f7f\u7528\u5982\u4e0b\u8bed\u53e5\u5efa\u7acb\u201cjoe\u201d\u7528\u6237\u3002 -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 joe \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.52.190/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u6267\u884c\u5982\u4e0b\u547d\u4ee4\u91cd\u542f\u96c6\u7fa4\u3002 gs_om -t stop && gs_om -t start \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 FusionInsight elk \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230Elk\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1aGauss200-OLAP-V100R007C10-REDHAT-64bit-Jdbc.tar.gz \u9a71\u52a8\u7c7b\uff1aorg.postgresql.Driver \u5177\u4f53\u4f4d\u7f6e\u4e3a\uff1aC:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\Elk \u9a71\u52a8jar\u5305\u7684\u540d\u5b57\u53eb gsjdbc4.jar \u5c06\u627e\u5230\u7684\u8fd9\u4e2a gsjdbc4.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/zeppelin-0.8.0-bin-all/interpreter/jdbc \u8def\u5f84\u4e0b\uff0c \u5e76\u4e14\u4f7f\u7528 \u4e0b\u9762\u547d\u4ee4\u66f4\u6539\u9a71\u52a8\u6743\u9650\u3002 chown 502:wheel gsjdbc4.jar chmod 755 gsjdbc4.jar \u542f\u52a8Zeppelin, \u914d\u7f6e JDBC interpreter\u5982\u4e0b: 1: default.driver = org.postgresql.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = joe \u68c0\u67e5\u7ed3\u679c\uff1a","title":"0.8.0 <--> C80"},{"location":"Development/Zeppelin_0.8.0/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.8.0/#_1","text":"Zeppelin 0.8.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR/ELK)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#zeppelin","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.8.0/#_2","text":"\u5b89\u88c5Zeppelin0.8.0","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_4","text":"\u5b89\u88c5Zeppelin 0.8.0,\u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf zeppelin-0.8.0-bin-all.tgz \u5b89\u88c5\u751f\u6210zeppelin-0.8.0-bin-all\u76ee\u5f55\u3002 \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 - \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all \u76ee\u5f55\u4e0b \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin \u91cd\u542fzeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u4f7f\u7528\u8d26\u6237developuser\u767b\u9646zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.8.0/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 502:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser/ \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.8.0/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_10","text":"\u5c06 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.8.0/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_12","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_13","text":"\u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b libfb303-0.9.3.jar \u548c libthrift-0.9.3.jar \u4e24\u4e2ajar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep \u8def\u5f84\u4e0b \u786e\u4fdd /usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter \u8def\u5f84\u4e0b\u6709\u4e14\u4ec5\u6709 libthrift-0.9.3.jar \u8fd9\u4e2a\u7248\u672c\u7684jar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark)","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.8.0/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_15","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u5982\u679c\u9047\u5230\u6e90yum-plugin-fastestmirror\u65e0\u6cd5\u4e0b\u8f7d\u65f6\uff0c\u53ef\u5728\u7f51\u5740 https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror \u4e0b\u9009\u62e9\u76f8\u5e94\u7684\u7248\u672c\u4ee3\u66ff\u4e0b\u8f7d\u5b89\u88c5 \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinapache-livy","text":"","title":"Zeppelin\u8fde\u63a5Apache Livy"},{"location":"Development/Zeppelin_0.8.0/#_17","text":"Zeppelin\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u8fde\u63a5Livy","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_18","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_19","text":"\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8Livy\u670d\u52a1 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9livy\uff0c\u70b9\u51fb edit \u7f16\u8f91zeppelin.livy.url\u7684\u503c\u4e3a http://172.21.3.43:8998 \uff08\u53ef\u4ee5\u4e0d\u66f4\u6539\uff09\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982livy_connection_test \u5728Zeppelin\u4e2d\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cPySpark\u6837\u4f8b\u4ee3\u7801 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cSparkR\u6837\u4f8b\u4ee3\u7801 %livy.sparkr hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\")","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinfusioninsight-elk","text":"","title":"Zeppelin\u8fde\u63a5FusionInsight Elk"},{"location":"Development/Zeppelin_0.8.0/#_20","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5FusionInsight Elk","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_21","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bElk\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_22","text":"\u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55FusionInsight Elk, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93\uff0c \u6d4b\u8bd5\u6570\u636e\u8868 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25108 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**joe**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER joe WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO joe; \u521b\u5efaHDFS\u8868\u7a7a\u95f4\u3002 CREATE TABLESPACE hdfs_tablespace LOCATION '/srv/BigData/hadoop/hdfs_tablespace' WITH (filesystem = 'HDFS', cfgpath = '/opt/huawei/Bigdata/mppdb/conf', storepath = '/user/elk/tablespace/ hdfs_tablespace'); \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE TABLESPACE \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE DATABASE \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25108 -U joe -W Bigdata@123 \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\u3002 CREATE TABLE hdfs_001(id int,name varchar2(20) ) WITH (orientation=orc,version=0.12,compression=no) TABLESPACE hdfs_tablespace; \u4f7f\u7528INSERT\u547d\u4ee4\u63d2\u5165\u6570\u636e\u3002 \u63d2\u5165\u4e00\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'); \u63d2\u5165\u591a\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'),(2, 'Marketing'), (2, 'Purchasing'); \u68c0\u67e5\u7ed3\u679c Select * from hdfs_001 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6e\u96c6\u7fa4Elk\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 joe \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.52.190 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all joe 172.16.52.190/32 sha256\" \u4f7f\u7528\u201cjoe\u201d\u7528\u6237\u524d\uff0c\u9700\u5148\u672c\u5730\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u5e76\u5728\u6570\u636e\u5e93\u4e2d\u4f7f\u7528\u5982\u4e0b\u8bed\u53e5\u5efa\u7acb\u201cjoe\u201d\u7528\u6237\u3002 -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 joe \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.52.190/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u6267\u884c\u5982\u4e0b\u547d\u4ee4\u91cd\u542f\u96c6\u7fa4\u3002 gs_om -t stop && gs_om -t start \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 FusionInsight elk \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230Elk\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1aGauss200-OLAP-V100R007C10-REDHAT-64bit-Jdbc.tar.gz \u9a71\u52a8\u7c7b\uff1aorg.postgresql.Driver \u5177\u4f53\u4f4d\u7f6e\u4e3a\uff1aC:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\Elk \u9a71\u52a8jar\u5305\u7684\u540d\u5b57\u53eb gsjdbc4.jar \u5c06\u627e\u5230\u7684\u8fd9\u4e2a gsjdbc4.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/zeppelin-0.8.0-bin-all/interpreter/jdbc \u8def\u5f84\u4e0b\uff0c \u5e76\u4e14\u4f7f\u7528 \u4e0b\u9762\u547d\u4ee4\u66f4\u6539\u9a71\u52a8\u6743\u9650\u3002 chown 502:wheel gsjdbc4.jar chmod 755 gsjdbc4.jar \u542f\u52a8Zeppelin, \u914d\u7f6e JDBC interpreter\u5982\u4e0b: 1: default.driver = org.postgresql.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = joe \u68c0\u67e5\u7ed3\u679c\uff1a","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/","text":"\u5176\u4ed6 \u00b6 Apache Livy 0.5.0 \u2194 C80 0.6.0 \u2194 6.5 GIS Tools for Hadoop 1.0 \u2194 C60 IBM WAS 8.5.5.9 \u2194 C50 Kibana 6.1.3 \u2194 C80 Logstash 6.4.2 \u2194 C80 NeoKylin 6.9 \u2194 C70 7.2 \u2194 C70 elasticsearch-head 1.0 \u2194 C80 filebeat 6.5.1 \u2194 C80","title":"Home"},{"location":"Other/#_1","text":"Apache Livy 0.5.0 \u2194 C80 0.6.0 \u2194 6.5 GIS Tools for Hadoop 1.0 \u2194 C60 IBM WAS 8.5.5.9 \u2194 C50 Kibana 6.1.3 \u2194 C80 Logstash 6.4.2 \u2194 C80 NeoKylin 6.9 \u2194 C70 7.2 \u2194 C70 elasticsearch-head 1.0 \u2194 C80 filebeat 6.5.1 \u2194 C80","title":"\u5176\u4ed6"},{"location":"Other/Apache_Livy_0_5_0/","text":"Apache Livy\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Livy 0.5.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x) \u5b89\u88c5Livy \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5 Apache Livy 0.5.0-incubating\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5Apache Livy 0.5.0-incubating\uff0c\u5728\u7f51\u5740 https://livy.incubator.apache.org/download/ \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 unzip livy-0.5.0-incubating-bin.zip \u89e3\u538b\u751f\u6210livy-0.5.0-incubating-bin\u76ee\u5f55 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser \u76ee\u5f55\u4e0b \u5728 /usr/livy/livy-0.5.0-incubating-bin/conf \u8def\u5f84\u4e0b\u65b0\u5efalivy\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"livy\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u914d\u7f6eLivy\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH \u7f16\u8f91livy.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true livy.server.launch.kerberos.keytab=/opt/user.keytab livy.server.launch.kerberos.principal=livy@HADOOP.COM \u7f16\u8f91livy-client.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf \u52a0\u5165\u5982\u672c\u673aip\u5730\u5740\uff1a livy.rsc.rpc.server.address =172.16.52.190 - \u7f16\u8f91livy-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 \u7f16\u8f91spark-blacklist.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf \u6ce8\u9500\u6389\u5982\u4e0b\u5185\u5bb9\uff1a spark.master spark.submit.deployMode \u542f\u52a8\u548c\u505c\u6b62Livy\uff0c\u5728\u8def\u5f84 /usr/livy/livy-0.5.0-incubating-bin \u4e0b bin/livy-server start \u542f\u52a8\u6210\u529f\u540e\u53ef\u4ee5\u5728http://172.16.52.190:8998\u8bbf\u95ee\u5230Livy\u670d\u52a1\u5668\uff1a \u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801\uff0c\u5305\u62ecSpark Shell\uff0cPySpark\uff0cSparkR \u6837\u4f8b\u4ee3\u7801\u53c2\u8003\u7f51\u5740 https://livy.incubator.apache.org/examples/ \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u5df2\u5b8c\u6210Anaconda\u548cR\u5728\u5ba2\u6237\u7aef\u4e3b\u673a\u4e0a\u7684\u5b89\u88c5\u3002 \u82e5\u6ca1\u6709\u5b89\u88c5Anaconda\u548cR\uff0c\u8bf7\u53c2\u8003Zeppelin0.8.0\u5bf9\u63a5FusionInsight HD V100R002C80SPC200 (Spark2.x)\u6307\u5bfc\u6587\u6863\u4e2d\u8fde\u63a5Spark\u548cSparkR\u90e8\u5206\u76f8\u5173\u5185\u5bb9 \u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u8f93\u5165\u547d\u4ee4 python \u542f\u52a8Anaconda \u8f93\u5165\u5982\u4e0bpython\u4ee3\u7801\u542f\u52a8\u4e00\u4e2aLivy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) \u5f53\u4e00\u4e2asession\u5b8c\u6210\u542f\u52a8\u540e\uff0c \u5b83\u5c06\u4f1a\u53d8\u4e3a\u95f2\u7f6e\u72b6\u6001 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() \u4e0b\u9762\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u7b80\u5355JSON\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u6765\u6267\u884cScala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u4e5f\u53ef\u4ee5\u5728\u7ec8\u7aef\u770b\u5230\u4ee5JSON\u683c\u5f0f\u8fd4\u56de\u7684\u7ed3\u679c \u66f4\u65b0Scala\u518d\u6b21\u8fd0\u884c data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES) \"\"\") } r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers) \u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3apyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session1 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cPython\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/1/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session1\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers) \u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3asparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session2 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cR\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/2/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session2\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers)","title":"0.5.0 <--> C80"},{"location":"Other/Apache_Livy_0_5_0/#apache-livyfusioninsight","text":"","title":"Apache Livy\u5bf9\u63a5FusionInsight"},{"location":"Other/Apache_Livy_0_5_0/#_1","text":"Apache Livy 0.5.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Apache_Livy_0_5_0/#livy","text":"","title":"\u5b89\u88c5Livy"},{"location":"Other/Apache_Livy_0_5_0/#_2","text":"\u5b89\u88c5 Apache Livy 0.5.0-incubating\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Other/Apache_Livy_0_5_0/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Apache_Livy_0_5_0/#_4","text":"\u5b89\u88c5Apache Livy 0.5.0-incubating\uff0c\u5728\u7f51\u5740 https://livy.incubator.apache.org/download/ \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 unzip livy-0.5.0-incubating-bin.zip \u89e3\u538b\u751f\u6210livy-0.5.0-incubating-bin\u76ee\u5f55 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser \u76ee\u5f55\u4e0b \u5728 /usr/livy/livy-0.5.0-incubating-bin/conf \u8def\u5f84\u4e0b\u65b0\u5efalivy\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"livy\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u914d\u7f6eLivy\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH \u7f16\u8f91livy.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true livy.server.launch.kerberos.keytab=/opt/user.keytab livy.server.launch.kerberos.principal=livy@HADOOP.COM \u7f16\u8f91livy-client.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf \u52a0\u5165\u5982\u672c\u673aip\u5730\u5740\uff1a livy.rsc.rpc.server.address =172.16.52.190 - \u7f16\u8f91livy-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 \u7f16\u8f91spark-blacklist.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf \u6ce8\u9500\u6389\u5982\u4e0b\u5185\u5bb9\uff1a spark.master spark.submit.deployMode \u542f\u52a8\u548c\u505c\u6b62Livy\uff0c\u5728\u8def\u5f84 /usr/livy/livy-0.5.0-incubating-bin \u4e0b bin/livy-server start \u542f\u52a8\u6210\u529f\u540e\u53ef\u4ee5\u5728http://172.16.52.190:8998\u8bbf\u95ee\u5230Livy\u670d\u52a1\u5668\uff1a","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_5_0/#livy_1","text":"","title":"\u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801"},{"location":"Other/Apache_Livy_0_5_0/#_5","text":"\u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801\uff0c\u5305\u62ecSpark Shell\uff0cPySpark\uff0cSparkR \u6837\u4f8b\u4ee3\u7801\u53c2\u8003\u7f51\u5740 https://livy.incubator.apache.org/examples/","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Other/Apache_Livy_0_5_0/#_6","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u5df2\u5b8c\u6210Anaconda\u548cR\u5728\u5ba2\u6237\u7aef\u4e3b\u673a\u4e0a\u7684\u5b89\u88c5\u3002 \u82e5\u6ca1\u6709\u5b89\u88c5Anaconda\u548cR\uff0c\u8bf7\u53c2\u8003Zeppelin0.8.0\u5bf9\u63a5FusionInsight HD V100R002C80SPC200 (Spark2.x)\u6307\u5bfc\u6587\u6863\u4e2d\u8fde\u63a5Spark\u548cSparkR\u90e8\u5206\u76f8\u5173\u5185\u5bb9","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Apache_Livy_0_5_0/#spark","text":"\u8f93\u5165\u547d\u4ee4 python \u542f\u52a8Anaconda \u8f93\u5165\u5982\u4e0bpython\u4ee3\u7801\u542f\u52a8\u4e00\u4e2aLivy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) \u5f53\u4e00\u4e2asession\u5b8c\u6210\u542f\u52a8\u540e\uff0c \u5b83\u5c06\u4f1a\u53d8\u4e3a\u95f2\u7f6e\u72b6\u6001 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() \u4e0b\u9762\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u7b80\u5355JSON\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u6765\u6267\u884cScala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u4e5f\u53ef\u4ee5\u5728\u7ec8\u7aef\u770b\u5230\u4ee5JSON\u683c\u5f0f\u8fd4\u56de\u7684\u7ed3\u679c \u66f4\u65b0Scala\u518d\u6b21\u8fd0\u884c data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES) \"\"\") } r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_5_0/#pyspark","text":"\u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3apyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session1 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cPython\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/1/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session1\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_5_0/#sparkr","text":"\u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3asparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session2 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cR\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/2/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session2\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_6_0/","text":"Apache Livy\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Livy 0.6.0 \u2194 FusionInsight HD 6.5 (Spark2x) \u90e8\u7f72\u5bf9\u5916\u9a8c\u8bc1\u7684livy\u670d\u52a1\u5e76\u4f7f\u7528session\uff0cbatch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1 \u00b6 \u573a\u666f\u8bf4\u660e \u00b6 \u590d\u6742\u573a\u666f\u4e0b\u9700\u8981\u5bf9\u63d0\u4ea4\u4efb\u52a1\u7684\u7528\u6237\u8fdb\u884c\u8bbf\u95ee\u63a7\u5236\uff0clivy\u670d\u52a1\u652f\u6301\u5bf9\u5916\u90e8\u8bbf\u95ee\u63d0\u4f9bkerberos SPNEGO\u8ba4\u8bc1\uff0c\u4e0b\u9762\u4e3a\u5177\u4f53\u7684\u6d4b\u8bd5\u573a\u666f \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a172.16.6.10 - 12\uff0c\u4e09\u8282\u70b9\u90e8\u7f72 Apache Livy\u670d\u52a1\u7aef\uff1a 172.16.2.118\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u53c2\u8003\u4e0a\u4e00\u7ae0\u5b8c\u6210Livy\u7684\u4e0b\u8f7d\uff0c\u5b89\u88c5 \u5ba2\u6237\u7aef\uff1a172.16.2.119\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u4f7f\u7528curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u8bf7\u6c42\uff0c\u9700\u8981\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u68c0\u67e5curl\u662f\u5426\u652f\u6301SPNEGO\u8ba4\u8bc1 \u5728\u6b64\u573a\u666f\u4e2d\u4f7f\u7528\u4e24\u4e2a\u8ba4\u8bc1\u7528\u6237\uff0c\u7528\u6237developuser\u548c\u7528\u6237livy \u7528\u6237livy\u4e3alivy\u670d\u52a1\u5b9e\u9645\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\u8bf7\u6c42\u6240\u9700\u8981\u7528\u5230\u7684\u7528\u6237 \u7528\u6237developuser\u662f\u5ba2\u6237\u7aef\u5411Livy\u670d\u52a1\u7aef\u63d0\u4ea4\u4efb\u52a1\u6240\u7528\u7684\u7528\u6237 \u6574\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u5b9e\u9645\u4e0a\u662f\u4ee3\u7406\u7528\u6237developuser\u4ee5\u7528\u6237livy\u7684\u540d\u4e49\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\uff0c\u4f46\u662f\u6267\u884c\u4efb\u52a1\u4e4b\u524d\u7528\u6237developuser\u9700\u8981\u901a\u8fc7FI HD\u96c6\u7fa4\u7684kerberos\u8ba4\u8bc1\uff0c\u901a\u8fc7\u8fd9\u6837\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0Apache Livy\u670d\u52a1\u7aef\u8bbf\u95ee\u63a7\u5236 Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u00b6 \u767b\u5f55FI HD manager\u521b\u5efa\u6d4b\u8bd5\u4e2d\u9700\u8981\u7528\u5230\u7684\u7528\u6237developuser, livy\u3002 \u5e76\u4e14\u5c06\u7528\u6237livy\u7684\u8ba4\u8bc1\u4fe1\u606f\u4e0b\u8f7d\u4e0b\u6765\uff08user.keytab, krb5.conf\uff09 \u4f7f\u7528FI HD\u5ba2\u6237\u7aef\u767b\u5f55kadmin\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\u7528\u4e8eFI HD\u5bf9Livy HTTP\u670d\u52a1\u7684Kerberos\u8ba4\u8bc1,\u5176\u540d\u79f0\u4e3a\u201cHTTP/host-172-16-2-118\u201d,\u5176\u4e2dhost-172-16-2-118\u4e3aApache Livy\u90e8\u7f72\u7684\u8282\u70b9\u7684\u4e3b\u673a\u540d\u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u5c06\u751f\u6210\u7684http2.keytab(keytab\u6587\u4ef6\u540d\u53ef\u81ea\u5b9a\u4e49)\u8ba4\u8bc1\u6587\u4ef6\u4f20\u5230livy\u670d\u52a1\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528kinit -kt\u547d\u4ee4\u68c0\u67e5\u8ba4\u8bc1\u662f\u5426\u6210\u529f kinit -kt /opt/http2.keytab HTTP/host-172-16-2-118@HADOOP.COM \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4kdestroy\u6e05\u9664\u7f13\u5b58\u7684\u7968\u636e \u767b\u5f55\u9700\u8981\u5bf9\u63a5\u96c6\u7fa4\uff0c\u70b9\u51fb\u670d\u52a1\u7ba1\u7406 -> Yarn -> \u670d\u52a1\u914d\u7f6e -> \u9009\u62e9\u5168\u90e8\u914d\u7f6e -> \u81ea\u5b9a\u4e49\uff0c \u5728\u5bf9\u5e94\u53c2\u6570\u6587\u4ef6\u4e3acore-site.xml\u4e0b\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a hadoop.proxyuser.livy.hosts = * hadoop.proxyuser.livy.groups = * \u53c2\u7167\u4e0a\u9762\u7684\u540c\u6837\u65b9\u6cd5\u5bf9hdfs\u670d\u52a1\uff0c hive\u670d\u52a1\u7684 core-site.xml \u6587\u4ef6\u589e\u52a0\u76f8\u540c\u7684\u914d\u7f6e\uff1a \u5b8c\u6210\u540e\u91cd\u542f\u76f8\u5173\u670d\u52a1 \u5ba2\u6237\u7aef\u76f8\u5173\u68c0\u67e5 \u00b6 \u4f7f\u7528curl -V\u547d\u4ee4\u68c0\u67e5\u5ba2\u6237\u7aefcurl\u547d\u4ee4\u662f\u5426\u652f\u6301Kerberos Spnego \u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5\u5ba2\u6237\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f Livy\u670d\u52a1\u7aef\u914d\u7f6e \u00b6 \u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5Livy\u670d\u52a1\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u68c0\u67e5livy.conf\u6587\u4ef6\u914d\u7f6e \u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u662f\uff1a livy.file.local-dir-whitelist=/opt/ \u914d\u7f6e\u53c2\u6570\u662f\u4f7f\u7528livy batch\u65b9\u5f0f\u672c\u5730\u63d0\u4ea4\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u5c06\u672c\u5730\u8def\u5f84\u6253\u5f00\u767d\u540d\u5355 launch.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5b9e\u9645\u540cFI HD\u96c6\u7fa4\u4ea4\u4e92\u6240\u9700\u8981\u7684\u914d\u7f6e auth.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5bf9\u5916\u4f7f\u7528kerberos\u8fdb\u884c\u8bbf\u95ee\u7ba1\u63a7\u6240\u9700\u8981\u7684\u914d\u7f6e \u68c0\u67e5livy-client.conf\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5livy-env.sh\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5spark-blacklist.conf\u6587\u4ef6\u914d\u7f6e \u5728log4j.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u5982\u4e0b\u4e00\u6761\u6765\u8c03\u6574\u65e5\u5fd7\u7ea7\u522b\uff08\u53ef\u9009\uff09 log4j.logger.org.eclipse.jetty=DEBUG \u4f7f\u7528Livy session\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1 \u00b6 livy session\u65b9\u5f0f\u5bf9\u5e94spark console\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u5177\u4f53\u7684\u4ee3\u7801\u7684\u65b9\u5f0f\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u767b\u5f55livy\u670d\u52a1\u7aef\u4f7f\u7528 bin/livy-server start \u542f\u52a8livy\u670d\u52a1 \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770blivy\u662f\u5426\u542f\u52a8\u6210\u529f \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728livy\u4e2d\u8d77\u4e00\u4e2apyspark\u7684session curl --negotiate -k -v -u developuser : -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/sessions \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e3asession/0\u63d0\u4ea4\u4e00\u6bb5\u4ee3\u7801 curl --negotiate -k -v -u developuser : -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://host-172-16-2-118:8998/sessions/0/statements \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u7ed3\u679c\uff1a curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0/statements | python -m json.tool \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5173\u95edsession curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0 -X DELETE \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u53e6\u5916\u5728\u5ba2\u6237\u7aef(172.16.2.119)\u5b8c\u6210curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u4e4b\u540e\u53ef\u4ee5\u4f7f\u7528klist\u67e5\u770b\u7968\u636e\u4fe1\u606f\uff1a \u53ef\u4ee5\u770b\u5230\u4f1a\u589e\u52a0\u8ba4\u8bc1\u7968\u636eHTTP/host-172-16-2-118@HADOOP.COM \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b1 \u00b6 Livy batch\u65b9\u5f0f\u5bf9\u5e94spark-submit\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u4e00\u4e2a\u7f16\u8bd1\u597d\u7684jar\u5305\uff0c\u6216\u8005\u662f\u5199\u597d\u7684py\u6587\u4ef6\u7b49\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2ajar\u5305\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u6d4b\u8bd5jar\u5305spark-examples_2.11-2.1.0.jar\u5e76\u4f20\u5230livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b2 \u00b6 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2apy\u6587\u4ef6\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u521b\u5efapy2.py\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u81f3Livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b\uff0c\u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) spark.stop() \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/pi2.py\" }' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b3 \u00b6 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4hdfs\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u4fee\u6539livy.conf\u6587\u4ef6\u914d\u7f6e\u4e3a\uff1a \u5728\u5bf9\u63a5FI HD\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\u4e0a\u4f20jar\u5305 \u91cd\u542fLivy \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b4 \u00b6 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4\u672c\u5730\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u56e0\u4e3a\u4f7f\u7528yarn cluster\u672c\u5730\u63d0\u4ea4jar\u5305\u6a21\u5f0f\uff0c\u4e8b\u5148\u5e76\u4e0d\u77e5\u9053worker\u5728\u54ea\u4e2a\u96c6\u7fa4\u8282\u70b9\uff0c\u6240\u4ee5\u5c06jar\u5305spark-examples_2.11-2.1.0.jar\u5206\u522b\u653e\u5230\u5404\u96c6\u7fa4\u8282\u70b9\u7684/home\u8def\u5f84\u4e0b\uff1a \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a Windows\u8df3\u677f\u673a\u914d\u7f6eKerberos Spnego\u8bbf\u95eelivy web ui \u00b6 Windows\u8df3\u677f\u673a\uff08172.16.2.111\uff09\u8bbf\u95eeLivy web ui\u7684\u8ba4\u8bc1\u539f\u7406\u540c\u4e0a\u6587\u5ba2\u6237\u7aef\uff08172.16.2.119\uff09\u4f7f\u7528curl\u547d\u4ee4\u8bbf\u95eeLivy\u670d\u52a1\u7aef\uff0c\u63d0\u4ea4spark\u4efb\u52a1\u4e00\u6837 \u53c2\u8003\u4ea7\u54c1\u6587\u6863 -> \u5e94\u7528\u5f00\u53d1\u6307\u5357 -> \u5b89\u5168\u6a21\u5f0f -> Spark2x\u5f00\u53d1\u6307\u5357 -> \u73af\u5883\u51c6\u5907 -> \u51c6\u5907HiveODBC\u5f00\u53d1\u73af\u5883 -> Windows\u73af\u5883 -> \u64cd\u4f5c\u6b65\u9aa4\u7b2c1\u5230\u7b2c4\u6b65 \u5b8c\u6210MIT Kerberos\u7684\u5b89\u88c5\u914d\u7f6e \u914d\u7f6eJCE \u5230java\u5b98\u7f51\u4e0a\u4e0b\u8f7dJava Cryptography Extension (JCE)\uff0c\u7136\u540e\u89e3\u538b\u5230%JAVA_HOME%/jre/lib/security\u4e2d\u66ff\u6362\u76f8\u5e94\u7684\u6587\u4ef6\u3002 \u68c0\u67e5livy\u670d\u52a1\u7aef\u4e3b\u673a\u540d\u662f\u5426\u52a0\u5165hosts\u6587\u4ef6\uff1a \u914d\u7f6eFirefox windows\u4e0bFirefox\u9700\u8981\u901a\u8fc7\u8bbf\u95eeabout:config \u9875\u9762\u8c03\u6574\u4ee5\u4e0b\u53c2\u6570\uff1a network.negotiate-auth.trusted-uris \u5141\u8bb8\u4f7f\u7528gssapi\u94fe\u63a5\u9a8c\u8bc1\u7684\u5730\u5740 network.auth.use-sspi \u5173\u95edsspi\u9a8c\u8bc1\u534f\u8bae \u4f7f\u7528MIT Kerberos\u5b8c\u6210\u8ba4\u8bc1 \u767b\u5f55Livy\u7684web ui\u5730\u5740\u4e3ahttp://host-172-16-2-118:8998/ui \u4f7f\u7528\u4e4b\u524d\u4e00\u4e2a\u6837\u4f8b\u63d0\u4ea4\u4efb\u52a1\u5e76\u5728Livy web ui\u68c0\u67e5 \u68c0\u67e5MIT Kerberos\u751f\u6210\u7684\u670d\u52a1\u7968\u636e","title":"0.6.0 <--> 6.5"},{"location":"Other/Apache_Livy_0_6_0/#apache-livyfusioninsight","text":"","title":"Apache Livy\u5bf9\u63a5FusionInsight"},{"location":"Other/Apache_Livy_0_6_0/#_1","text":"Apache Livy 0.6.0 \u2194 FusionInsight HD 6.5 (Spark2x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Apache_Livy_0_6_0/#livysessionbatch","text":"","title":"\u90e8\u7f72\u5bf9\u5916\u9a8c\u8bc1\u7684livy\u670d\u52a1\u5e76\u4f7f\u7528session\uff0cbatch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1"},{"location":"Other/Apache_Livy_0_6_0/#_2","text":"\u590d\u6742\u573a\u666f\u4e0b\u9700\u8981\u5bf9\u63d0\u4ea4\u4efb\u52a1\u7684\u7528\u6237\u8fdb\u884c\u8bbf\u95ee\u63a7\u5236\uff0clivy\u670d\u52a1\u652f\u6301\u5bf9\u5916\u90e8\u8bbf\u95ee\u63d0\u4f9bkerberos SPNEGO\u8ba4\u8bc1\uff0c\u4e0b\u9762\u4e3a\u5177\u4f53\u7684\u6d4b\u8bd5\u573a\u666f \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a172.16.6.10 - 12\uff0c\u4e09\u8282\u70b9\u90e8\u7f72 Apache Livy\u670d\u52a1\u7aef\uff1a 172.16.2.118\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u53c2\u8003\u4e0a\u4e00\u7ae0\u5b8c\u6210Livy\u7684\u4e0b\u8f7d\uff0c\u5b89\u88c5 \u5ba2\u6237\u7aef\uff1a172.16.2.119\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u4f7f\u7528curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u8bf7\u6c42\uff0c\u9700\u8981\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u68c0\u67e5curl\u662f\u5426\u652f\u6301SPNEGO\u8ba4\u8bc1 \u5728\u6b64\u573a\u666f\u4e2d\u4f7f\u7528\u4e24\u4e2a\u8ba4\u8bc1\u7528\u6237\uff0c\u7528\u6237developuser\u548c\u7528\u6237livy \u7528\u6237livy\u4e3alivy\u670d\u52a1\u5b9e\u9645\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\u8bf7\u6c42\u6240\u9700\u8981\u7528\u5230\u7684\u7528\u6237 \u7528\u6237developuser\u662f\u5ba2\u6237\u7aef\u5411Livy\u670d\u52a1\u7aef\u63d0\u4ea4\u4efb\u52a1\u6240\u7528\u7684\u7528\u6237 \u6574\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u5b9e\u9645\u4e0a\u662f\u4ee3\u7406\u7528\u6237developuser\u4ee5\u7528\u6237livy\u7684\u540d\u4e49\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\uff0c\u4f46\u662f\u6267\u884c\u4efb\u52a1\u4e4b\u524d\u7528\u6237developuser\u9700\u8981\u901a\u8fc7FI HD\u96c6\u7fa4\u7684kerberos\u8ba4\u8bc1\uff0c\u901a\u8fc7\u8fd9\u6837\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0Apache Livy\u670d\u52a1\u7aef\u8bbf\u95ee\u63a7\u5236","title":"\u573a\u666f\u8bf4\u660e"},{"location":"Other/Apache_Livy_0_6_0/#kerberos","text":"\u767b\u5f55FI HD manager\u521b\u5efa\u6d4b\u8bd5\u4e2d\u9700\u8981\u7528\u5230\u7684\u7528\u6237developuser, livy\u3002 \u5e76\u4e14\u5c06\u7528\u6237livy\u7684\u8ba4\u8bc1\u4fe1\u606f\u4e0b\u8f7d\u4e0b\u6765\uff08user.keytab, krb5.conf\uff09 \u4f7f\u7528FI HD\u5ba2\u6237\u7aef\u767b\u5f55kadmin\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\u7528\u4e8eFI HD\u5bf9Livy HTTP\u670d\u52a1\u7684Kerberos\u8ba4\u8bc1,\u5176\u540d\u79f0\u4e3a\u201cHTTP/host-172-16-2-118\u201d,\u5176\u4e2dhost-172-16-2-118\u4e3aApache Livy\u90e8\u7f72\u7684\u8282\u70b9\u7684\u4e3b\u673a\u540d\u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u5c06\u751f\u6210\u7684http2.keytab(keytab\u6587\u4ef6\u540d\u53ef\u81ea\u5b9a\u4e49)\u8ba4\u8bc1\u6587\u4ef6\u4f20\u5230livy\u670d\u52a1\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528kinit -kt\u547d\u4ee4\u68c0\u67e5\u8ba4\u8bc1\u662f\u5426\u6210\u529f kinit -kt /opt/http2.keytab HTTP/host-172-16-2-118@HADOOP.COM \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4kdestroy\u6e05\u9664\u7f13\u5b58\u7684\u7968\u636e \u767b\u5f55\u9700\u8981\u5bf9\u63a5\u96c6\u7fa4\uff0c\u70b9\u51fb\u670d\u52a1\u7ba1\u7406 -> Yarn -> \u670d\u52a1\u914d\u7f6e -> \u9009\u62e9\u5168\u90e8\u914d\u7f6e -> \u81ea\u5b9a\u4e49\uff0c \u5728\u5bf9\u5e94\u53c2\u6570\u6587\u4ef6\u4e3acore-site.xml\u4e0b\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a hadoop.proxyuser.livy.hosts = * hadoop.proxyuser.livy.groups = * \u53c2\u7167\u4e0a\u9762\u7684\u540c\u6837\u65b9\u6cd5\u5bf9hdfs\u670d\u52a1\uff0c hive\u670d\u52a1\u7684 core-site.xml \u6587\u4ef6\u589e\u52a0\u76f8\u540c\u7684\u914d\u7f6e\uff1a \u5b8c\u6210\u540e\u91cd\u542f\u76f8\u5173\u670d\u52a1","title":"Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e"},{"location":"Other/Apache_Livy_0_6_0/#_3","text":"\u4f7f\u7528curl -V\u547d\u4ee4\u68c0\u67e5\u5ba2\u6237\u7aefcurl\u547d\u4ee4\u662f\u5426\u652f\u6301Kerberos Spnego \u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5\u5ba2\u6237\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f","title":"\u5ba2\u6237\u7aef\u76f8\u5173\u68c0\u67e5"},{"location":"Other/Apache_Livy_0_6_0/#livy","text":"\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5Livy\u670d\u52a1\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u68c0\u67e5livy.conf\u6587\u4ef6\u914d\u7f6e \u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u662f\uff1a livy.file.local-dir-whitelist=/opt/ \u914d\u7f6e\u53c2\u6570\u662f\u4f7f\u7528livy batch\u65b9\u5f0f\u672c\u5730\u63d0\u4ea4\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u5c06\u672c\u5730\u8def\u5f84\u6253\u5f00\u767d\u540d\u5355 launch.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5b9e\u9645\u540cFI HD\u96c6\u7fa4\u4ea4\u4e92\u6240\u9700\u8981\u7684\u914d\u7f6e auth.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5bf9\u5916\u4f7f\u7528kerberos\u8fdb\u884c\u8bbf\u95ee\u7ba1\u63a7\u6240\u9700\u8981\u7684\u914d\u7f6e \u68c0\u67e5livy-client.conf\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5livy-env.sh\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5spark-blacklist.conf\u6587\u4ef6\u914d\u7f6e \u5728log4j.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u5982\u4e0b\u4e00\u6761\u6765\u8c03\u6574\u65e5\u5fd7\u7ea7\u522b\uff08\u53ef\u9009\uff09 log4j.logger.org.eclipse.jetty=DEBUG","title":"Livy\u670d\u52a1\u7aef\u914d\u7f6e"},{"location":"Other/Apache_Livy_0_6_0/#livy-session","text":"livy session\u65b9\u5f0f\u5bf9\u5e94spark console\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u5177\u4f53\u7684\u4ee3\u7801\u7684\u65b9\u5f0f\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u767b\u5f55livy\u670d\u52a1\u7aef\u4f7f\u7528 bin/livy-server start \u542f\u52a8livy\u670d\u52a1 \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770blivy\u662f\u5426\u542f\u52a8\u6210\u529f \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728livy\u4e2d\u8d77\u4e00\u4e2apyspark\u7684session curl --negotiate -k -v -u developuser : -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/sessions \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e3asession/0\u63d0\u4ea4\u4e00\u6bb5\u4ee3\u7801 curl --negotiate -k -v -u developuser : -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://host-172-16-2-118:8998/sessions/0/statements \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u7ed3\u679c\uff1a curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0/statements | python -m json.tool \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5173\u95edsession curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0 -X DELETE \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u53e6\u5916\u5728\u5ba2\u6237\u7aef(172.16.2.119)\u5b8c\u6210curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u4e4b\u540e\u53ef\u4ee5\u4f7f\u7528klist\u67e5\u770b\u7968\u636e\u4fe1\u606f\uff1a \u53ef\u4ee5\u770b\u5230\u4f1a\u589e\u52a0\u8ba4\u8bc1\u7968\u636eHTTP/host-172-16-2-118@HADOOP.COM","title":"\u4f7f\u7528Livy session\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch1","text":"Livy batch\u65b9\u5f0f\u5bf9\u5e94spark-submit\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u4e00\u4e2a\u7f16\u8bd1\u597d\u7684jar\u5305\uff0c\u6216\u8005\u662f\u5199\u597d\u7684py\u6587\u4ef6\u7b49\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2ajar\u5305\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u6d4b\u8bd5jar\u5305spark-examples_2.11-2.1.0.jar\u5e76\u4f20\u5230livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b1"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch2","text":"\u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2apy\u6587\u4ef6\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u521b\u5efapy2.py\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u81f3Livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b\uff0c\u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) spark.stop() \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/pi2.py\" }' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b2"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch3","text":"\u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4hdfs\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u4fee\u6539livy.conf\u6587\u4ef6\u914d\u7f6e\u4e3a\uff1a \u5728\u5bf9\u63a5FI HD\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\u4e0a\u4f20jar\u5305 \u91cd\u542fLivy \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b3"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch4","text":"\u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4\u672c\u5730\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u56e0\u4e3a\u4f7f\u7528yarn cluster\u672c\u5730\u63d0\u4ea4jar\u5305\u6a21\u5f0f\uff0c\u4e8b\u5148\u5e76\u4e0d\u77e5\u9053worker\u5728\u54ea\u4e2a\u96c6\u7fa4\u8282\u70b9\uff0c\u6240\u4ee5\u5c06jar\u5305spark-examples_2.11-2.1.0.jar\u5206\u522b\u653e\u5230\u5404\u96c6\u7fa4\u8282\u70b9\u7684/home\u8def\u5f84\u4e0b\uff1a \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b4"},{"location":"Other/Apache_Livy_0_6_0/#windowskerberos-spnegolivy-web-ui","text":"Windows\u8df3\u677f\u673a\uff08172.16.2.111\uff09\u8bbf\u95eeLivy web ui\u7684\u8ba4\u8bc1\u539f\u7406\u540c\u4e0a\u6587\u5ba2\u6237\u7aef\uff08172.16.2.119\uff09\u4f7f\u7528curl\u547d\u4ee4\u8bbf\u95eeLivy\u670d\u52a1\u7aef\uff0c\u63d0\u4ea4spark\u4efb\u52a1\u4e00\u6837 \u53c2\u8003\u4ea7\u54c1\u6587\u6863 -> \u5e94\u7528\u5f00\u53d1\u6307\u5357 -> \u5b89\u5168\u6a21\u5f0f -> Spark2x\u5f00\u53d1\u6307\u5357 -> \u73af\u5883\u51c6\u5907 -> \u51c6\u5907HiveODBC\u5f00\u53d1\u73af\u5883 -> Windows\u73af\u5883 -> \u64cd\u4f5c\u6b65\u9aa4\u7b2c1\u5230\u7b2c4\u6b65 \u5b8c\u6210MIT Kerberos\u7684\u5b89\u88c5\u914d\u7f6e \u914d\u7f6eJCE \u5230java\u5b98\u7f51\u4e0a\u4e0b\u8f7dJava Cryptography Extension (JCE)\uff0c\u7136\u540e\u89e3\u538b\u5230%JAVA_HOME%/jre/lib/security\u4e2d\u66ff\u6362\u76f8\u5e94\u7684\u6587\u4ef6\u3002 \u68c0\u67e5livy\u670d\u52a1\u7aef\u4e3b\u673a\u540d\u662f\u5426\u52a0\u5165hosts\u6587\u4ef6\uff1a \u914d\u7f6eFirefox windows\u4e0bFirefox\u9700\u8981\u901a\u8fc7\u8bbf\u95eeabout:config \u9875\u9762\u8c03\u6574\u4ee5\u4e0b\u53c2\u6570\uff1a network.negotiate-auth.trusted-uris \u5141\u8bb8\u4f7f\u7528gssapi\u94fe\u63a5\u9a8c\u8bc1\u7684\u5730\u5740 network.auth.use-sspi \u5173\u95edsspi\u9a8c\u8bc1\u534f\u8bae \u4f7f\u7528MIT Kerberos\u5b8c\u6210\u8ba4\u8bc1 \u767b\u5f55Livy\u7684web ui\u5730\u5740\u4e3ahttp://host-172-16-2-118:8998/ui \u4f7f\u7528\u4e4b\u524d\u4e00\u4e2a\u6837\u4f8b\u63d0\u4ea4\u4efb\u52a1\u5e76\u5728Livy web ui\u68c0\u67e5 \u68c0\u67e5MIT Kerberos\u751f\u6210\u7684\u670d\u52a1\u7968\u636e","title":"Windows\u8df3\u677f\u673a\u914d\u7f6eKerberos Spnego\u8bbf\u95eelivy web ui"},{"location":"Other/Elasticsearch_Related/","text":"FusionInsight HD ES\u7ec4\u4ef6\u4e0e\u5468\u8fb9\u751f\u6001\u5bf9\u63a5 \u00b6 \u751f\u6001\u7b80\u4ecb \u00b6 Kibana: \u53ef\u6269\u5c55\u7684\u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u7ba1\u7406\u6574\u4e2a\u751f\u6001\u7ec4\u4ef6\uff08elasticsearch, logstash, beats\uff09\u4ee5\u53ca\u6570\u636e Elasticsearch: \u517c\u6709\u641c\u7d22\u5f15\u64ce\u548cNoSQL\u6570\u636e\u5e93\u529f\u80fd\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u57fa\u4e8eJAVA/Lucene\u6784\u5efa\uff0c\u5f00\u6e90\u3001\u5206\u5e03\u5f0f\u3001\u652f\u6301RESTful\u8bf7\u6c42 Logstash: \u5f00\u6e90\u7684\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u591a\u4e2a\u6e90\u5934\u6536\u96c6\u6570\u636e\uff0c\u4f20\u5230Elasticsearch\uff0c\u80fd\u591f\u548cElasticsearch\u4ea7\u751f\u534f\u540c\u6548\u5e94 beats: \u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u642c\u8fd0\u5de5\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u670d\u52a1\u5668\u4e0a\u5c06\u6570\u636e\u4f20\u8f93\u5230Logstash\u6216\u8005Elasticsearch elasticsearch-head: \u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u67e5\u8be2Elasticsearch\u4e2d\u7684\u6570\u636e \u6ce8\uff1a FusionInsight HD\u7684Elasticsearch\u7ec4\u4ef6\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u4f46\u662f\u76f8\u5173\u7684\u5468\u8fb9\u751f\u6001Kibana\uff0cLogstash\uff0cbeats\uff0c elasticseach-head\u4e3a\u5f00\u6e90\uff0c\u6682\u65f6\u65e0\u6cd5\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u6545\u91c7\u7528\u5b89\u5168FI HD\u96c6\u7fa4\u7684\u975e\u5b89\u5168ES\u7ec4\u4ef6\u8fdb\u884c\u5bf9\u63a5 Logstash\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Logstash 6.4.2 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dlogstash 6.4.2, \u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**logstash-6.4.2.zip**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt/logstash \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 unzip logstash-6.4.2.zip \u89e3\u538b\u5b89\u88c5\u5305 \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2/config \u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u914d\u7f6e\u6587\u4ef6 logstash-Simple.conf ,\u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ stdin{ } } output { stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"hellow_world\" #user => \"elastic\" #password => \"changeme\" } } \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2 \u4e0b\uff0c\u6267\u884c\u547d\u4ee4 bin/logstash -f config/logstash-Simple.conf \u6839\u636e\u4e4b\u524d\u7684 logstash-Simple.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash, \u7136\u540e\u5728\u7ec8\u7aef\u624b\u52a8\u8f93\u5165\u6570\u5b571\u52306\uff1a \u767b\u5f55elasticsearch-head\u670d\u52a1\u5668\u67e5\u770b\u7ed3\u679c\uff08\u5bf9\u63a5\u6b65\u9aa4\u53c2\u89c1\u540e\u6587\uff09 Kibana\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kibana 6.1.3 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dKibana 6.1.3, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Kibana 6.1.3**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf kibana-6.1.3-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 vi /opt/kibana-6.1.3-linux-x86_64/config/kibana.yml \u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\u9009\u9879: server.port: 5601 server.host: \"172.16.52.190\" server.name: \"LinuxTest\" elasticsearch.url: \"http://172.21.3.101:24100\" \u4f7f\u7528 bin/kibana \u542f\u52a8kibana \u8bbf\u95eekibana\u767b\u5f55\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u683c\u5f0f\u4e3ahttp://Kibana\u670d\u52a1IP\u5730\u5740:5601 \u9009\u62e9**Dev Tools** \u4f7f\u7528 GET _cat/indices \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4ES\u4e2d\u7684indices \u4f7f\u7528 GET hellow_world/_search \u547d\u4ee4\u67e5\u770b\u7ed3\u679c elasticsearch-head\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 elasticsearch-head 1.0 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u5728FusionInsight Manager\uff0c\u9009\u62e9\u670d\u52a1\u7ba1\u7406->Elasticsearch\u670d\u52a1\u914d\u7f6e->\u670d\u52a1\u914d\u7f6e(\u9009\u62e9\u5168\u90e8\u914d\u7f6e)->\u81ea\u5b9a\u4e49\uff0c\u5728elasticsearch.yml\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4e0b\u9762\u4e24\u4e2a\u914d\u7f6e\u9879\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1 http.cors.enabled = true http.cors.allow-origin = \"*\" \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u4e14\u5b89\u88c5**elasticsearch-head**\u5230\u4e3b\u673a\u4e0a,\u5e76\u4e14\u542f\u52a8**elasticsearch-head**\u670d\u52a1 git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm install npm run start \u8bbf\u95eeelasticsearch-head\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u4e3ahttp://elasticsearch-head\u670d\u52a1IP\u5730\u5740:9100, \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u4e3a http://172.21.3.101:24100/ ,\u70b9\u51fb\u8fde\u63a5: \u67e5\u770b\u7ed3\u679c \u5728FI HD\u96c6\u7fa4\u4e0a\u90e8\u7f72beats \u00b6 \u9002\u7528\u573a\u666f \u00b6 filebeat 6.5.1 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4e0b\u8f7dFilebeat 6.5.1, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Filebeat 6.5.1**\u4f7f\u7528WinSCP\u5bfc\u5165FI HD\u96c6\u7fa4\u8282\u70b9\uff08172.21.3.103\uff09\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf filebeat-6.5.1-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 cd /opt/filebeat-6.5.1-linux-x86_64 \u8fdb\u5165filebeat\u5b89\u88c5\u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 filebeat_new.yml ,\u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-host3.log path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5046\"] \u6ce8\uff1a\u7aef\u53e35046\u53ef\u4ee5\u81ea\u5df1\u6307\u5b9a\uff0c\u4e0d\u51b2\u7a81\u5373\u53ef \u4f7f\u7528\u547d\u4ee4 ./filebeat -e -c filebeat_new.yml \u542f\u52a8filebeat \u5e94\u7528\u573a\u666f\u4e3e\u4f8b\u8bf4\u660e \u00b6 \u573a\u666f\u7b80\u4ecb \u00b6 \u5206\u522b\u5728FI HD\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72filebeat\u5b9e\u65f6\u83b7\u53d6\u4e24\u53f0\u670d\u52a1\u5668\u7684\u7684\u65e5\u5fd7\u6587\u4ef6\uff08/var/log/ipmitool.fi.log\uff09\uff0c\u901a\u8fc7logstash\u7ba1\u9053\u83b7\u53d6\u5e76\u8fc7\u6ee4\u5143\u65e5\u5fd7\u6587\u4ef6\u4e3a\u591a\u4e2a\u5b57\u6bb5\uff0c\u5e76\u4f20\u5230FI HD Elasticsearch\u7ec4\u4ef6\u4e0a\uff0c\u6700\u540e\u901a\u8fc7Kibana\u6765\u67e5\u770b\u83b7\u53d6\u7684\u65e5\u5fd7\u6587\u4ef6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210Kibana\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210logstash\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210filebeat\u7684\u5b89\u88c5 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9996\u5148\u767b\u9646FusionInsight HD\u96c6\u7fa4\u8282\u70b9172.21.3.102\u548c172.21.3.103\u4e0a \u53c2\u8003\u4e4b\u524d\u7684\u6b65\u9aa4\u5206\u522b\u90e8\u7f72filebeat\u5230172.21.3.102\u548c172.21.3.103\u4e0a\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u521b\u5efafilebeat\u914d\u7f6e\u6587\u4ef6 filebeat_new_host2.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5045\"] \u914d\u7f6e\u6587\u4ef6 filebeat_new_host3.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5047\"] \u767b\u9646\u5b89\u88c5logstash\u7684\u4e3b\u673a\uff0c\u914d\u7f6e\u4e00\u4e2a\u65b0\u7684\u542f\u52a8\u6587\u4ef6 logstash-beats-ipmitool.conf \u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ beats{ port => \"5045\" } beats{ port => \"5047\" } } filter{ grok{ match =>{\"message\" => \"(?<object>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<object2>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<status>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<number>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|%{GREEDYDATA:additional_info}\"} } } output{ stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"ipmitool_log\" } } \u542f\u52a8kibana \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-beats-ipmitool.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash \u5206\u522b\u542f\u52a8172.21.3.102\u548c172.21.3.103\u4e0a\u7684filebeat\u6765\u83b7\u53d6\u65e5\u5fd7\u6587\u4ef6 \u767b\u9646kibana\u7f51\u9875\u754c\u9762\uff0c\u9009\u62e9**Management**\u4e0b\u9762\u7684**Index Patterns** \u5728**Create index pattern**\u4e0b\u9009\u62e9\u521a\u521a\u7531logstash\u4f20\u8f93\u521b\u5efa\u7684index ipmitool_log \uff0c \u70b9\u51fb Next step \u5728step 2\u4e2d\u9009\u62e9**@timestamp**, \u70b9\u51fb**Create index pattern** \u5728 Discover \u90e8\u5206\u9009\u62e9\u521a\u521a\u751f\u6210\u7684index pattern ipmitool_log \u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u65e5\u5fd7\u7684\u60c5\u51b5 \u53ef\u6839\u636e\u4e0d\u540c\u7684\u5b57\u6bb5\u60c5\u51b5\u6765\u6574\u4f53\u4e86\u89e3\u65e5\u5fd7\u60c5\u51b5 \u5b8c\u6210","title":"6.5.1 <--> C80"},{"location":"Other/Elasticsearch_Related/#fusioninsight-hd-es","text":"","title":"FusionInsight HD ES\u7ec4\u4ef6\u4e0e\u5468\u8fb9\u751f\u6001\u5bf9\u63a5"},{"location":"Other/Elasticsearch_Related/#_1","text":"Kibana: \u53ef\u6269\u5c55\u7684\u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u7ba1\u7406\u6574\u4e2a\u751f\u6001\u7ec4\u4ef6\uff08elasticsearch, logstash, beats\uff09\u4ee5\u53ca\u6570\u636e Elasticsearch: \u517c\u6709\u641c\u7d22\u5f15\u64ce\u548cNoSQL\u6570\u636e\u5e93\u529f\u80fd\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u57fa\u4e8eJAVA/Lucene\u6784\u5efa\uff0c\u5f00\u6e90\u3001\u5206\u5e03\u5f0f\u3001\u652f\u6301RESTful\u8bf7\u6c42 Logstash: \u5f00\u6e90\u7684\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u591a\u4e2a\u6e90\u5934\u6536\u96c6\u6570\u636e\uff0c\u4f20\u5230Elasticsearch\uff0c\u80fd\u591f\u548cElasticsearch\u4ea7\u751f\u534f\u540c\u6548\u5e94 beats: \u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u642c\u8fd0\u5de5\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u670d\u52a1\u5668\u4e0a\u5c06\u6570\u636e\u4f20\u8f93\u5230Logstash\u6216\u8005Elasticsearch elasticsearch-head: \u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u67e5\u8be2Elasticsearch\u4e2d\u7684\u6570\u636e \u6ce8\uff1a FusionInsight HD\u7684Elasticsearch\u7ec4\u4ef6\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u4f46\u662f\u76f8\u5173\u7684\u5468\u8fb9\u751f\u6001Kibana\uff0cLogstash\uff0cbeats\uff0c elasticseach-head\u4e3a\u5f00\u6e90\uff0c\u6682\u65f6\u65e0\u6cd5\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u6545\u91c7\u7528\u5b89\u5168FI HD\u96c6\u7fa4\u7684\u975e\u5b89\u5168ES\u7ec4\u4ef6\u8fdb\u884c\u5bf9\u63a5","title":"\u751f\u6001\u7b80\u4ecb"},{"location":"Other/Elasticsearch_Related/#logstashfusioninsight-hd-es","text":"","title":"Logstash\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_2","text":"Logstash 6.4.2 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_4","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dlogstash 6.4.2, \u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**logstash-6.4.2.zip**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt/logstash \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 unzip logstash-6.4.2.zip \u89e3\u538b\u5b89\u88c5\u5305 \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2/config \u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u914d\u7f6e\u6587\u4ef6 logstash-Simple.conf ,\u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ stdin{ } } output { stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"hellow_world\" #user => \"elastic\" #password => \"changeme\" } } \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2 \u4e0b\uff0c\u6267\u884c\u547d\u4ee4 bin/logstash -f config/logstash-Simple.conf \u6839\u636e\u4e4b\u524d\u7684 logstash-Simple.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash, \u7136\u540e\u5728\u7ec8\u7aef\u624b\u52a8\u8f93\u5165\u6570\u5b571\u52306\uff1a \u767b\u5f55elasticsearch-head\u670d\u52a1\u5668\u67e5\u770b\u7ed3\u679c\uff08\u5bf9\u63a5\u6b65\u9aa4\u53c2\u89c1\u540e\u6587\uff09","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#kibanafusioninsight-hd-es","text":"","title":"Kibana\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_5","text":"Kibana 6.1.3 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_6","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_7","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dKibana 6.1.3, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Kibana 6.1.3**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf kibana-6.1.3-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 vi /opt/kibana-6.1.3-linux-x86_64/config/kibana.yml \u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\u9009\u9879: server.port: 5601 server.host: \"172.16.52.190\" server.name: \"LinuxTest\" elasticsearch.url: \"http://172.21.3.101:24100\" \u4f7f\u7528 bin/kibana \u542f\u52a8kibana \u8bbf\u95eekibana\u767b\u5f55\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u683c\u5f0f\u4e3ahttp://Kibana\u670d\u52a1IP\u5730\u5740:5601 \u9009\u62e9**Dev Tools** \u4f7f\u7528 GET _cat/indices \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4ES\u4e2d\u7684indices \u4f7f\u7528 GET hellow_world/_search \u547d\u4ee4\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#elasticsearch-headfusioninsight-hd-es","text":"","title":"elasticsearch-head\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_8","text":"elasticsearch-head 1.0 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_9","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_10","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u5728FusionInsight Manager\uff0c\u9009\u62e9\u670d\u52a1\u7ba1\u7406->Elasticsearch\u670d\u52a1\u914d\u7f6e->\u670d\u52a1\u914d\u7f6e(\u9009\u62e9\u5168\u90e8\u914d\u7f6e)->\u81ea\u5b9a\u4e49\uff0c\u5728elasticsearch.yml\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4e0b\u9762\u4e24\u4e2a\u914d\u7f6e\u9879\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1 http.cors.enabled = true http.cors.allow-origin = \"*\" \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u4e14\u5b89\u88c5**elasticsearch-head**\u5230\u4e3b\u673a\u4e0a,\u5e76\u4e14\u542f\u52a8**elasticsearch-head**\u670d\u52a1 git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm install npm run start \u8bbf\u95eeelasticsearch-head\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u4e3ahttp://elasticsearch-head\u670d\u52a1IP\u5730\u5740:9100, \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u4e3a http://172.21.3.101:24100/ ,\u70b9\u51fb\u8fde\u63a5: \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#fi-hdbeats","text":"","title":"\u5728FI HD\u96c6\u7fa4\u4e0a\u90e8\u7f72beats"},{"location":"Other/Elasticsearch_Related/#_11","text":"filebeat 6.5.1 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_12","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_13","text":"\u4e0b\u8f7dFilebeat 6.5.1, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Filebeat 6.5.1**\u4f7f\u7528WinSCP\u5bfc\u5165FI HD\u96c6\u7fa4\u8282\u70b9\uff08172.21.3.103\uff09\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf filebeat-6.5.1-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 cd /opt/filebeat-6.5.1-linux-x86_64 \u8fdb\u5165filebeat\u5b89\u88c5\u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 filebeat_new.yml ,\u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-host3.log path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5046\"] \u6ce8\uff1a\u7aef\u53e35046\u53ef\u4ee5\u81ea\u5df1\u6307\u5b9a\uff0c\u4e0d\u51b2\u7a81\u5373\u53ef \u4f7f\u7528\u547d\u4ee4 ./filebeat -e -c filebeat_new.yml \u542f\u52a8filebeat","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#_14","text":"","title":"\u5e94\u7528\u573a\u666f\u4e3e\u4f8b\u8bf4\u660e"},{"location":"Other/Elasticsearch_Related/#_15","text":"\u5206\u522b\u5728FI HD\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72filebeat\u5b9e\u65f6\u83b7\u53d6\u4e24\u53f0\u670d\u52a1\u5668\u7684\u7684\u65e5\u5fd7\u6587\u4ef6\uff08/var/log/ipmitool.fi.log\uff09\uff0c\u901a\u8fc7logstash\u7ba1\u9053\u83b7\u53d6\u5e76\u8fc7\u6ee4\u5143\u65e5\u5fd7\u6587\u4ef6\u4e3a\u591a\u4e2a\u5b57\u6bb5\uff0c\u5e76\u4f20\u5230FI HD Elasticsearch\u7ec4\u4ef6\u4e0a\uff0c\u6700\u540e\u901a\u8fc7Kibana\u6765\u67e5\u770b\u83b7\u53d6\u7684\u65e5\u5fd7\u6587\u4ef6","title":"\u573a\u666f\u7b80\u4ecb"},{"location":"Other/Elasticsearch_Related/#_16","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210Kibana\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210logstash\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210filebeat\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_17","text":"\u9996\u5148\u767b\u9646FusionInsight HD\u96c6\u7fa4\u8282\u70b9172.21.3.102\u548c172.21.3.103\u4e0a \u53c2\u8003\u4e4b\u524d\u7684\u6b65\u9aa4\u5206\u522b\u90e8\u7f72filebeat\u5230172.21.3.102\u548c172.21.3.103\u4e0a\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u521b\u5efafilebeat\u914d\u7f6e\u6587\u4ef6 filebeat_new_host2.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5045\"] \u914d\u7f6e\u6587\u4ef6 filebeat_new_host3.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5047\"] \u767b\u9646\u5b89\u88c5logstash\u7684\u4e3b\u673a\uff0c\u914d\u7f6e\u4e00\u4e2a\u65b0\u7684\u542f\u52a8\u6587\u4ef6 logstash-beats-ipmitool.conf \u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ beats{ port => \"5045\" } beats{ port => \"5047\" } } filter{ grok{ match =>{\"message\" => \"(?<object>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<object2>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<status>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<number>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|%{GREEDYDATA:additional_info}\"} } } output{ stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"ipmitool_log\" } } \u542f\u52a8kibana \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-beats-ipmitool.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash \u5206\u522b\u542f\u52a8172.21.3.102\u548c172.21.3.103\u4e0a\u7684filebeat\u6765\u83b7\u53d6\u65e5\u5fd7\u6587\u4ef6 \u767b\u9646kibana\u7f51\u9875\u754c\u9762\uff0c\u9009\u62e9**Management**\u4e0b\u9762\u7684**Index Patterns** \u5728**Create index pattern**\u4e0b\u9009\u62e9\u521a\u521a\u7531logstash\u4f20\u8f93\u521b\u5efa\u7684index ipmitool_log \uff0c \u70b9\u51fb Next step \u5728step 2\u4e2d\u9009\u62e9**@timestamp**, \u70b9\u51fb**Create index pattern** \u5728 Discover \u90e8\u5206\u9009\u62e9\u521a\u521a\u751f\u6210\u7684index pattern ipmitool_log \u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u65e5\u5fd7\u7684\u60c5\u51b5 \u53ef\u6839\u636e\u4e0d\u540c\u7684\u5b57\u6bb5\u60c5\u51b5\u6765\u6574\u4f53\u4e86\u89e3\u65e5\u5fd7\u60c5\u51b5 \u5b8c\u6210","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/FUSE/","text":"FUSE\u5bf9\u63a5FusionInsight HDFS \u00b6 \u9002\u7528\u573a\u666f \u00b6 FUSE 2.8.3 \u2194 FusionInsight HD V100R002C60U20\uff08HDFS\uff09 \u8bf4\u660e \u00b6 \u901a\u8fc7\u4f7f\u7528FUSE\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c06\u8fdc\u7aef\u7684HDFS\u6587\u4ef6\u7cfb\u7edfmount\u5230\u672c\u7aef\u7684Linux\u7cfb\u7edf\u4e2d\u4f7f\u7528\u3002FusionInsight HD\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\u3002 \u914d\u7f6e\u5bf9\u63a5 \u00b6 \u5b89\u88c5jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf /etc/profile \uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf export JAVA_HOME = /opt/jdk1.8.0_112 export CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar export PATH = $JAVA_HOME /bin: $PATH source /etc/profile \u5b89\u88c5rpm\u5305 yum install fuse fuse-devel fuse-libs \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u4ea7\u54c1\u6587\u6863\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u4f8b\u5982\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \u4e0b\u8f7dHadoop-2.7.2\u6e90\u7801hadoop-2.7.2-src.tar.gz\uff0c\u7f16\u8bd1fuse_dfs\uff0c\u5c06\u7f16\u8bd1\u597d\u7684fuse_dfs\u62f7\u8d1d\u5230/opt\u76ee\u5f55\u4e0b \u5c06\u6e90\u7801\u4e0b\u7684fuse_dfs_wrapper.sh\u811a\u672c\u62f7\u8d1d\u81f3 /opt \u76ee\u5f55\u4e0b\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u4f5c\u5982\u4e0b\u4fee\u6539(\u4fee\u6539HADOOP_PREFIX\u548cJAVA_HOME\u7684\u914d\u7f6e)\uff1a export HADOOP_PREFIX = /opt/hadoopclient/HDFS/hadoop if [ \" $OS_ARCH \" = \"\" ] ; then export OS_ARCH = amd64 fi if [ \" $JAVA_HOME \" = \"\" ] ; then export JAVA_HOME = /opt/jdk1.8.0_112 fi if [ \" $LD_LIBRARY_PATH \" = \"\" ] ; then export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX /share ] ; then export HADOOP_PREFIX = $HADOOP_PREFIX for f in ${ HADOOP_PREFIX } /share/hadoop/hdfs/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in $HADOOP_PREFIX /share/hadoop/hdfs/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/*.jar ; do export CLASSPATH = $CLASSPATH : $f done export PATH = /opt: $PATH export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib: $LD_LIBRARY_PATH fi fuse_dfs $@ \u66f4\u6539\u6587\u4ef6\u6743\u9650 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u672c\u5730\u521b\u5efamount\u76ee\u5f55 mkdir \u2013p /mnt/hdfs \u6267\u884c\u6302\u8f7d\u811a\u672c\uff0c\u5176\u4e2d162-1-95-196\u662fHDFS\u7684NameNode(hacluster,\u4e3b)\u7684\u4e3b\u673a\u540d ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ \u67e5\u770b/mnt/hdfs\u76ee\u5f55 \u5982\u679c\u9700\u8981\u5378\u8f7d\u6302\u8f7d\u70b9\uff0c\u6267\u884cumount /mnt/hdfs\u5373\u53ef \u9a8c\u8bc1\u5bf9\u63a5 \u00b6 hdfsclient\u5199 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient\u8bfb hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse\u5199 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse\u8bfb dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null","title":"FUSE\u5bf9\u63a5FusionInsight HDFS"},{"location":"Other/FUSE/#fusefusioninsight-hdfs","text":"","title":"FUSE\u5bf9\u63a5FusionInsight HDFS"},{"location":"Other/FUSE/#_1","text":"FUSE 2.8.3 \u2194 FusionInsight HD V100R002C60U20\uff08HDFS\uff09","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/FUSE/#_2","text":"\u901a\u8fc7\u4f7f\u7528FUSE\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c06\u8fdc\u7aef\u7684HDFS\u6587\u4ef6\u7cfb\u7edfmount\u5230\u672c\u7aef\u7684Linux\u7cfb\u7edf\u4e2d\u4f7f\u7528\u3002FusionInsight HD\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\u3002","title":"\u8bf4\u660e"},{"location":"Other/FUSE/#_3","text":"\u5b89\u88c5jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf /etc/profile \uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf export JAVA_HOME = /opt/jdk1.8.0_112 export CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar export PATH = $JAVA_HOME /bin: $PATH source /etc/profile \u5b89\u88c5rpm\u5305 yum install fuse fuse-devel fuse-libs \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u4ea7\u54c1\u6587\u6863\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u4f8b\u5982\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \u4e0b\u8f7dHadoop-2.7.2\u6e90\u7801hadoop-2.7.2-src.tar.gz\uff0c\u7f16\u8bd1fuse_dfs\uff0c\u5c06\u7f16\u8bd1\u597d\u7684fuse_dfs\u62f7\u8d1d\u5230/opt\u76ee\u5f55\u4e0b \u5c06\u6e90\u7801\u4e0b\u7684fuse_dfs_wrapper.sh\u811a\u672c\u62f7\u8d1d\u81f3 /opt \u76ee\u5f55\u4e0b\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u4f5c\u5982\u4e0b\u4fee\u6539(\u4fee\u6539HADOOP_PREFIX\u548cJAVA_HOME\u7684\u914d\u7f6e)\uff1a export HADOOP_PREFIX = /opt/hadoopclient/HDFS/hadoop if [ \" $OS_ARCH \" = \"\" ] ; then export OS_ARCH = amd64 fi if [ \" $JAVA_HOME \" = \"\" ] ; then export JAVA_HOME = /opt/jdk1.8.0_112 fi if [ \" $LD_LIBRARY_PATH \" = \"\" ] ; then export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX /share ] ; then export HADOOP_PREFIX = $HADOOP_PREFIX for f in ${ HADOOP_PREFIX } /share/hadoop/hdfs/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in $HADOOP_PREFIX /share/hadoop/hdfs/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/*.jar ; do export CLASSPATH = $CLASSPATH : $f done export PATH = /opt: $PATH export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib: $LD_LIBRARY_PATH fi fuse_dfs $@ \u66f4\u6539\u6587\u4ef6\u6743\u9650 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u672c\u5730\u521b\u5efamount\u76ee\u5f55 mkdir \u2013p /mnt/hdfs \u6267\u884c\u6302\u8f7d\u811a\u672c\uff0c\u5176\u4e2d162-1-95-196\u662fHDFS\u7684NameNode(hacluster,\u4e3b)\u7684\u4e3b\u673a\u540d ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ \u67e5\u770b/mnt/hdfs\u76ee\u5f55 \u5982\u679c\u9700\u8981\u5378\u8f7d\u6302\u8f7d\u70b9\uff0c\u6267\u884cumount /mnt/hdfs\u5373\u53ef","title":"\u914d\u7f6e\u5bf9\u63a5"},{"location":"Other/FUSE/#_4","text":"hdfsclient\u5199 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient\u8bfb hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse\u5199 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse\u8bfb dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null","title":"\u9a8c\u8bc1\u5bf9\u63a5"},{"location":"Other/GIS_Tools/","text":"GIS Tools for Hadoop\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 GIS Tools for Hadoop 1.0 \u2194 FusionInsight HD V100R002C60U20 (Hive/MapReduce) aggregation-hive \u00b6 \u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive \u4e2d\u5173\u4e8e\u96c6\u6210Hive\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2aHiveAdmin\u89d2\u8272\uff0c\u5177\u4f53\u8bf7\u53c2\u52a0\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efaHive\u89d2\u8272 \u7ae0\u8282\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237 testuser \u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7FusionInsight HD\u7684\u5ba2\u6237\u7aef\u4e0a\u4f20\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u5c06\u76ee\u5f55gis-tools-for-hadoop-master\u76f4\u63a5\u653e\u5230HDFS\u7684\u6839\u76ee\u5f55\u4e0b\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master \u4fee\u6539\u6267\u884chive\u793a\u4f8b\u7684sql\u6587\u4ef6\uff0c\u4fee\u6539\u540e\u7684\u6587\u4ef6\u5982\u4e0b set role admin ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / esri - geometry - api . jar ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / spatial - sdk - hadoop . jar ; reload function ; DROP TABLE earthquakes ; DROP TABLE counties ; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point' ; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains' ; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING , latitude DOUBLE , longitude DOUBLE , depth DOUBLE , magnitude DOUBLE , magtype string , mbstations string , gap string , distance string , rms string , source string , eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data' ; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string , Perimeter string , State string , County string , Name string , BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data' ; SELECT counties . name , count ( * ) cnt FROM counties JOIN earthquakes WHERE ST_Contains ( counties . boundaryshape , ST_Point ( earthquakes . longitude , earthquakes . latitude )) GROUP BY counties . name ORDER BY cnt desc ; \u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u6267\u884c\u4fee\u6539\u540e\u7684sql\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql \u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff0c\u4e0eGIS\u5f00\u6e90\u7f51\u7ad9\u63cf\u8ff0\u4e00\u81f4 aggregation-mr \u00b6 \u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr \u4e2d\u5173\u4e8e\u96c6\u6210MR\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctestuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh \u5982\u4e0b\uff0c\u5176\u4e2d26004\u4e3ayarn\u914d\u7f6e\u7684yarn.resourcemanager.port\u7aef\u53e3 #!/bin/bash NAME_NODE_URL = hdfs://hacluster JOB_TRACKER_URL = 162 .1.93.103:26004 SAMPLE_DIR = /tmp/gistest JOB_DIR = $SAMPLE_DIR /job LIB_DIR = $SAMPLE_DIR /lib DATA_DIR = $SAMPLE_DIR /data OUTPUT_DIR = $SAMPLE_DIR /output \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh \u7684\u6267\u884c\u6743\u9650\uff0c\u5e76\u6267\u884c source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh \u6267\u884c\u5b8c\u6bd5\u5f97\u5230\u5982\u4e0b\u7ed3\u679c\u6587\u4ef6result.txt","title":"1.0 <--> C60"},{"location":"Other/GIS_Tools/#gis-tools-for-hadoopfusioninsight","text":"","title":"GIS Tools for Hadoop\u5bf9\u63a5FusionInsight"},{"location":"Other/GIS_Tools/#_1","text":"GIS Tools for Hadoop 1.0 \u2194 FusionInsight HD V100R002C60U20 (Hive/MapReduce)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/GIS_Tools/#aggregation-hive","text":"\u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive \u4e2d\u5173\u4e8e\u96c6\u6210Hive\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2aHiveAdmin\u89d2\u8272\uff0c\u5177\u4f53\u8bf7\u53c2\u52a0\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efaHive\u89d2\u8272 \u7ae0\u8282\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237 testuser \u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7FusionInsight HD\u7684\u5ba2\u6237\u7aef\u4e0a\u4f20\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u5c06\u76ee\u5f55gis-tools-for-hadoop-master\u76f4\u63a5\u653e\u5230HDFS\u7684\u6839\u76ee\u5f55\u4e0b\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master \u4fee\u6539\u6267\u884chive\u793a\u4f8b\u7684sql\u6587\u4ef6\uff0c\u4fee\u6539\u540e\u7684\u6587\u4ef6\u5982\u4e0b set role admin ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / esri - geometry - api . jar ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / spatial - sdk - hadoop . jar ; reload function ; DROP TABLE earthquakes ; DROP TABLE counties ; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point' ; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains' ; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING , latitude DOUBLE , longitude DOUBLE , depth DOUBLE , magnitude DOUBLE , magtype string , mbstations string , gap string , distance string , rms string , source string , eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data' ; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string , Perimeter string , State string , County string , Name string , BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data' ; SELECT counties . name , count ( * ) cnt FROM counties JOIN earthquakes WHERE ST_Contains ( counties . boundaryshape , ST_Point ( earthquakes . longitude , earthquakes . latitude )) GROUP BY counties . name ORDER BY cnt desc ; \u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u6267\u884c\u4fee\u6539\u540e\u7684sql\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql \u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff0c\u4e0eGIS\u5f00\u6e90\u7f51\u7ad9\u63cf\u8ff0\u4e00\u81f4","title":"aggregation-hive"},{"location":"Other/GIS_Tools/#aggregation-mr","text":"\u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr \u4e2d\u5173\u4e8e\u96c6\u6210MR\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctestuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh \u5982\u4e0b\uff0c\u5176\u4e2d26004\u4e3ayarn\u914d\u7f6e\u7684yarn.resourcemanager.port\u7aef\u53e3 #!/bin/bash NAME_NODE_URL = hdfs://hacluster JOB_TRACKER_URL = 162 .1.93.103:26004 SAMPLE_DIR = /tmp/gistest JOB_DIR = $SAMPLE_DIR /job LIB_DIR = $SAMPLE_DIR /lib DATA_DIR = $SAMPLE_DIR /data OUTPUT_DIR = $SAMPLE_DIR /output \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh \u7684\u6267\u884c\u6743\u9650\uff0c\u5e76\u6267\u884c source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh \u6267\u884c\u5b8c\u6bd5\u5f97\u5230\u5982\u4e0b\u7ed3\u679c\u6587\u4ef6result.txt","title":"aggregation-mr"},{"location":"Other/IBM_WAS/","text":"IBM WAS\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM WAS 8.5.5.9 \u2194 FusionInsight HD V100R002C50 (IBM_JDK)","title":"8.5.5.9 <--> C50"},{"location":"Other/IBM_WAS/#ibm-wasfusioninsight","text":"","title":"IBM WAS\u5bf9\u63a5FusionInsight"},{"location":"Other/IBM_WAS/#_1","text":"IBM WAS 8.5.5.9 \u2194 FusionInsight HD V100R002C50 (IBM_JDK)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/NeoKylin/","text":"NeoKylin\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 NeoKylin 6.9 \u2194 FusionInsight HD V100R002C70SPC200 (OS) NeoKylin 7.2 \u2194 FusionInsight HD V100R002C70SPC200 (OS)","title":"7.2 <--> C70"},{"location":"Other/NeoKylin/#neokylinfusioninsight","text":"","title":"NeoKylin\u5bf9\u63a5FusionInsight"},{"location":"Other/NeoKylin/#_1","text":"NeoKylin 6.9 \u2194 FusionInsight HD V100R002C70SPC200 (OS) NeoKylin 7.2 \u2194 FusionInsight HD V100R002C70SPC200 (OS)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/","text":"SQL\u5206\u6790 \u00b6 Apache Drill 1.15.0 \u2194 C80 Apache Kylin 1.6.0 \u2194 C60 2.1.0 \u2194 C70 2.3.1 \u2194 C80 2.6.1 \u2194 6.5 Kyligence Analytics Platform 2.2 \u2194 C60 2.3 \u2194 C60 2.4 \u2194 C70 2.5 \u2194 C70 3.0 \u2194 C80 Presto 0.155 \u2194 C60 0.184 \u2194 C70 0.196 \u2194 C80 0.210 \u2194 C80 0.210 \u2194 6.5","title":"Home"},{"location":"SQL_Analytics/#sql","text":"Apache Drill 1.15.0 \u2194 C80 Apache Kylin 1.6.0 \u2194 C60 2.1.0 \u2194 C70 2.3.1 \u2194 C80 2.6.1 \u2194 6.5 Kyligence Analytics Platform 2.2 \u2194 C60 2.3 \u2194 C60 2.4 \u2194 C70 2.5 \u2194 C70 3.0 \u2194 C80 Presto 0.155 \u2194 C60 0.184 \u2194 C70 0.196 \u2194 C80 0.210 \u2194 C80 0.210 \u2194 6.5","title":"SQL\u5206\u6790"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/","text":"Apache Dril\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Drill 1.15.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/HBase/Kafka) \u8bf4\u660e \u00b6 Apache Drill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.2.123 FI HD V100R002C80SPC200\u96c6\u7fa4\uff1a 172.16.6.10-12 \u5b89\u88c5Apache Drill \u00b6 \u4e0b\u8f7dApache Drill wget http://apache.mirrors.hoobly.com/drill/drill-1.15.0/apache-drill-1.15.0.tar.gz \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.15.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.15.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.2.123:8047 \u6765\u67e5\u770bwebUI\u754c\u9762 \u5bf9\u63a5HDFS \u00b6 \u786e\u4fddApache Drill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.6.12:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.6.10:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a \u5bf9\u63a5HIVE \u00b6 \u8bf4\u660e\uff1aapache drill 1.15\u7248\u672c\u7684hive\u7248\u672c\u4e3a2.3.2, FI HD V100R002C80SPC200\u7248\u672chive\u7248\u672c\u4e3a1.3.0\uff0c\u7248\u672c\u4e0d\u5339\u914d\uff0c\u4e0d\u80fd\u5bf9\u63a5\u6210\u529f\uff0c\u9700\u8981\u4f7f\u7528drill 1.12.0\u7248\u672c\uff0c\u6545\u672c\u8282\u6240\u8ff0\u4f7f\u7528\u7684drill\u7248\u672c\u90fd\u4e3a1.12.0 ApacheDrill 1.12.0\u5bf9\u63a5 FI HD V100R002C80SPC200 Hive\u670d\u52a1\u524d\u63d0\u6761\u4ef6\uff1a \u4e0b\u8f7dApacheDrill 1.12.0\u7684\u5b89\u88c5\u5305apache-drill-1.12.0.tar.gz\uff0c \u53c2\u8003\u4e0a\u8ff0\u300a\u5b89\u88c5Apache Drill\u300b\u7ae0\u8282\u5b8c\u6210drill\u5b89\u88c5 \u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.12.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u7531\u4e8edrill 1.12.0\u7248\u672c\u652f\u6301\u5bf9\u63a5ha\u6a21\u5f0f\u7684\u5927\u6570\u636e\u96c6\u7fa4\uff0c\u4e3a\u51cf\u5c11\u4f9d\u8d56\u76f8\u5173\u9519\u8bef\u53d1\u751f\uff0c\u66f4\u6539\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS\u670d\u52a1\u4e2ddfs.client.failover.proxy.provider.hacluster\u914d\u7f6e\u9879\u7684\u503c\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\uff0c\u5e76\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1 \u5b8c\u6210\u540e\u91cd\u65b0\u4e0b\u8f7d\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 \u5230drill\u5b89\u88c5\u8def\u5f84/jars\u4e0b\u4f7f\u7528\u547d\u4ee4 mkdir hd_jars \u65b0\u5efa\u4e00\u4e2a\u540d\u4e3ahd_jars\u7684\u8def\u5f84 \u5230FI HD\u5ba2\u6237\u7aef\u4e0b\u627e\u5230Jar\u5305 hadoop-yarn-api-2.7.2.jar\uff0c \u5e76\u5c06\u6b64jar\u5305\u62f7\u8d1d\u5230hd_jars\u8def\u5f84\u4e0b cp /opt/hadoopclient/HDFS/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar /opt/drill/apache-drill-1.12.0/jars/hd_jars/ \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh\u6587\u4ef6\u5982\u4e0b\uff1a \u5728 3rdparty\u8def\u5f84\u5bfc\u5165\u524d\u52a0\u5165\u4e00\u884c\uff1a CP=\"$CP:$DRILL_HOME/jars/hd_jars/*\" \u5c06\u91cd\u65b0\u4e0b\u8f7d\u7684\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff08core-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff09\uff1a \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.2.11\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab scp /opt/huawei/Bigdata/components/FusionInsight_HD_V100R002C80SPC200/Hive/hive.keytab root@172.16.2.123:/opt \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"enabled\": true, \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.6.10:21088,thrift://172.16.6.11:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.6.12:25000\", \"inputDirectories\": \"hdfs://172.16.6.12:25000\" } } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 use huaweihive; \u4f7f\u7528hive\u8fde\u63a5\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a \u5bf9\u63a5kafka \u00b6 \u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic \u5bf9\u63a5HBase \u00b6 \u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u5c06HDFS\u5ba2\u6237\u7aef\u5305\u542b\u7684core-site.xml,hdfs-site.xml,yarn-site.xml\u6587\u4ef6\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5176\u4e2dhdfs-site.xml\u6587\u4ef6\u9700\u8981\u505a\u5982\u4e0b\u66f4\u6539\uff1a \u5220\u9664hdfs-site.xml\u6587\u4ef6\u4e2d\u7684\u5982\u4e0b\u53c2\u6570\uff1a <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.6.10,172.16.6.11\uff0c172.16.6.12\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from ImportTable; \u67e5\u770bhbase\u8868\uff1a","title":"1.15.0 <--> C80"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#apache-drilfusioninsight","text":"","title":"Apache Dril\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#_1","text":"Apache Drill 1.15.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/HBase/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#_2","text":"Apache Drill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.2.123 FI HD V100R002C80SPC200\u96c6\u7fa4\uff1a 172.16.6.10-12","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#apache-drill","text":"\u4e0b\u8f7dApache Drill wget http://apache.mirrors.hoobly.com/drill/drill-1.15.0/apache-drill-1.15.0.tar.gz \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.15.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.15.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.2.123:8047 \u6765\u67e5\u770bwebUI\u754c\u9762","title":"\u5b89\u88c5Apache Drill"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#hdfs","text":"\u786e\u4fddApache Drill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.6.12:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.6.10:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a","title":"\u5bf9\u63a5HDFS"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#hive","text":"\u8bf4\u660e\uff1aapache drill 1.15\u7248\u672c\u7684hive\u7248\u672c\u4e3a2.3.2, FI HD V100R002C80SPC200\u7248\u672chive\u7248\u672c\u4e3a1.3.0\uff0c\u7248\u672c\u4e0d\u5339\u914d\uff0c\u4e0d\u80fd\u5bf9\u63a5\u6210\u529f\uff0c\u9700\u8981\u4f7f\u7528drill 1.12.0\u7248\u672c\uff0c\u6545\u672c\u8282\u6240\u8ff0\u4f7f\u7528\u7684drill\u7248\u672c\u90fd\u4e3a1.12.0 ApacheDrill 1.12.0\u5bf9\u63a5 FI HD V100R002C80SPC200 Hive\u670d\u52a1\u524d\u63d0\u6761\u4ef6\uff1a \u4e0b\u8f7dApacheDrill 1.12.0\u7684\u5b89\u88c5\u5305apache-drill-1.12.0.tar.gz\uff0c \u53c2\u8003\u4e0a\u8ff0\u300a\u5b89\u88c5Apache Drill\u300b\u7ae0\u8282\u5b8c\u6210drill\u5b89\u88c5 \u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.12.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u7531\u4e8edrill 1.12.0\u7248\u672c\u652f\u6301\u5bf9\u63a5ha\u6a21\u5f0f\u7684\u5927\u6570\u636e\u96c6\u7fa4\uff0c\u4e3a\u51cf\u5c11\u4f9d\u8d56\u76f8\u5173\u9519\u8bef\u53d1\u751f\uff0c\u66f4\u6539\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS\u670d\u52a1\u4e2ddfs.client.failover.proxy.provider.hacluster\u914d\u7f6e\u9879\u7684\u503c\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\uff0c\u5e76\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1 \u5b8c\u6210\u540e\u91cd\u65b0\u4e0b\u8f7d\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 \u5230drill\u5b89\u88c5\u8def\u5f84/jars\u4e0b\u4f7f\u7528\u547d\u4ee4 mkdir hd_jars \u65b0\u5efa\u4e00\u4e2a\u540d\u4e3ahd_jars\u7684\u8def\u5f84 \u5230FI HD\u5ba2\u6237\u7aef\u4e0b\u627e\u5230Jar\u5305 hadoop-yarn-api-2.7.2.jar\uff0c \u5e76\u5c06\u6b64jar\u5305\u62f7\u8d1d\u5230hd_jars\u8def\u5f84\u4e0b cp /opt/hadoopclient/HDFS/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar /opt/drill/apache-drill-1.12.0/jars/hd_jars/ \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh\u6587\u4ef6\u5982\u4e0b\uff1a \u5728 3rdparty\u8def\u5f84\u5bfc\u5165\u524d\u52a0\u5165\u4e00\u884c\uff1a CP=\"$CP:$DRILL_HOME/jars/hd_jars/*\" \u5c06\u91cd\u65b0\u4e0b\u8f7d\u7684\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff08core-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff09\uff1a \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.2.11\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab scp /opt/huawei/Bigdata/components/FusionInsight_HD_V100R002C80SPC200/Hive/hive.keytab root@172.16.2.123:/opt \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"enabled\": true, \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.6.10:21088,thrift://172.16.6.11:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.6.12:25000\", \"inputDirectories\": \"hdfs://172.16.6.12:25000\" } } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 use huaweihive; \u4f7f\u7528hive\u8fde\u63a5\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a","title":"\u5bf9\u63a5HIVE"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#kafka","text":"\u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic","title":"\u5bf9\u63a5kafka"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#hbase","text":"\u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u5c06HDFS\u5ba2\u6237\u7aef\u5305\u542b\u7684core-site.xml,hdfs-site.xml,yarn-site.xml\u6587\u4ef6\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5176\u4e2dhdfs-site.xml\u6587\u4ef6\u9700\u8981\u505a\u5982\u4e0b\u66f4\u6539\uff1a \u5220\u9664hdfs-site.xml\u6587\u4ef6\u4e2d\u7684\u5982\u4e0b\u53c2\u6570\uff1a <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.6.10,172.16.6.11\uff0c172.16.6.12\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from ImportTable; \u67e5\u770bhbase\u8868\uff1a","title":"\u5bf9\u63a5HBase"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/","text":"Apache Kylin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 1.6.0 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 162.2.200.200 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm \u7f16\u8bd1Kylin \u00b6 \u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-1.6.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u914d\u59571.0.2\u7684HBase\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-1.6.0\u57fa\u4e8eHBase1.0.2\u7248\u672c\u7684\u6e90\u7801 https://github.com/apache/kylin/tree/yang21-hbase102 \u5f97\u5230kylin-yang21-hbase102.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u7f16\u8bd1\u6253\u5305 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305 \u542f\u52a8Kylin \u00b6 \u89e3\u538b\u4e8c\u8fdb\u5236\u5305 \u00b6 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test_cn Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u7f16\u8f91find-hive-dependency.sh\uff1a vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'` \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u6784\u5efaCube \u00b6 \u9009\u62e9learn_kylin\u5de5\u7a0b\uff0c\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"1.6.0 <--> C60"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#apache-kylinfusioninsight-hd","text":"","title":"Apache Kylin\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_1","text":"Apache Kylin 1.6.0 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 162.2.200.200 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin","text":"\u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-1.6.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u914d\u59571.0.2\u7684HBase\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-1.6.0\u57fa\u4e8eHBase1.0.2\u7248\u672c\u7684\u6e90\u7801 https://github.com/apache/kylin/tree/yang21-hbase102 \u5f97\u5230kylin-yang21-hbase102.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u7f16\u8bd1\u6253\u5305 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305","title":"\u7f16\u8bd1Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_1","text":"","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_4","text":"\u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt","title":"\u89e3\u538b\u4e8c\u8fdb\u5236\u5305"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_5","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test_cn Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_2","text":"\u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u7f16\u8f91find-hive-dependency.sh\uff1a vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'`","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_3","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#cube","text":"\u9009\u62e9learn_kylin\u5de5\u7a0b\uff0c\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_6","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/","text":"Apache Kylin2.1.0\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.1.0 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm \u7f16\u8bd1Kylin \u00b6 \u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-2.1.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-2.1.0\u57fa\u4e8eHBase1.1.1\u7248\u672c\u7684\u6e90\u7801\u7801 https://github.com/apache/kylin/tree/2.1.x \u5f97\u5230kylin-2.1.x.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u4fee\u6539kylin\u6e90\u7801 \u4fee\u6539HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java \u4fee\u6539pom.xml vi /opt/kylin-2.1.x/pom.xml \u5c06HBase\u3001Hive\u3001Hadoop\u7248\u672c\u6539\u6210\u4e0eFusionInsight HD\u5bf9\u5e94\u7684\u7248\u672c \u7f16\u8bd1\u6253\u5305 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305 \u542f\u52a8Kylin \u00b6 \u89e3\u538b\u4e8c\u8fdb\u5236\u5305 \u00b6 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u9700\u8981\u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"2.1.0 <--> C70"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#apache-kylin210fusioninsight-hd","text":"","title":"Apache Kylin2.1.0\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_1","text":"Apache Kylin 2.1.0 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin","text":"\u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-2.1.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-2.1.0\u57fa\u4e8eHBase1.1.1\u7248\u672c\u7684\u6e90\u7801\u7801 https://github.com/apache/kylin/tree/2.1.x \u5f97\u5230kylin-2.1.x.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u4fee\u6539kylin\u6e90\u7801 \u4fee\u6539HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java \u4fee\u6539pom.xml vi /opt/kylin-2.1.x/pom.xml \u5c06HBase\u3001Hive\u3001Hadoop\u7248\u672c\u6539\u6210\u4e0eFusionInsight HD\u5bf9\u5e94\u7684\u7248\u672c \u7f16\u8bd1\u6253\u5305 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305","title":"\u7f16\u8bd1Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_1","text":"","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_4","text":"\u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt","title":"\u89e3\u538b\u4e8c\u8fdb\u5236\u5305"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_5","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_2","text":"\u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u9700\u8981\u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_3","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_6","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/","text":"Apache Kylin2.3.1\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.3.1 \u2194 FusionInsight HD V100R002C80SPC100 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient \u5b89\u88c5JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm \u4e0b\u8f7dKylin \u00b6 Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.3.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin \u4e0b\u8f7d\u89e3\u538bKylin \u00b6 \u4e0b\u8f7dKylin-2.3.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz \u4e0a\u4f20apache-kylin-2.3.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt \u914d\u7f6eKylin \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh \u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879 \u00b6 \u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..* \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"2.3.1 <--> C80"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#apache-kylin231fusioninsight-hd","text":"","title":"Apache Kylin2.3.1\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_1","text":"Apache Kylin 2.3.1 \u2194 FusionInsight HD V100R002C80SPC100 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient \u5b89\u88c5JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin","text":"Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.3.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin","title":"\u4e0b\u8f7dKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_1","text":"\u4e0b\u8f7dKylin-2.3.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz \u4e0a\u4f20apache-kylin-2.3.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt","title":"\u4e0b\u8f7d\u89e3\u538bKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_2","text":"","title":"\u914d\u7f6eKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#fusioninsighthive","text":"\u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..*","title":"\u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_3","text":"\u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_4","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_5","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/","text":"Apache Kylin2.6.1\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.6.1 \u2194 FusionInsight HD 6.5 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e\uff0c\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65\u3002 server 172.21.3.101 nomodify notrap nopeer noquery \u8bf4\u660e\uff1a172.21.3.101\u4e3aFusionInsight HD\u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u7684IP\u3002 \u91cd\u542fNTP\u670d\u52a1\uff0c\u5e76\u540c\u6b65\u672c\u673a\u548c\u96c6\u7fa4\u7684\u65f6\u95f4\u3002 systemctl restart ntpd systemctl stop ntpd ntpdate 172.16.4.21 \u8bf4\u660e\uff1a\u9700\u8981\u5148\u505c\u6b62ntpd\u670d\u52a1\u540e\uff0c\u518d\u6267\u884cntpdate\u540c\u6b65\u65f6\u95f4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u9519\u8bef\u201cthe NTP socket is in use, exiting\u201d\u3002\u53e6\u5916172.16.4.21\u96c6\u7fa4\u7684nptd\u670d\u52a1\u5fc5\u987b\u662f\u542f\u52a8\u7684\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55\u3002 ./install.sh /opt/hadoopclient \u4e0b\u8f7dKylin \u00b6 Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-**-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u4f8b\u5982apache-kylin-2.6.1-hbase1x-bin.tar.gz\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin\u3002 \u4e0b\u8f7d\u89e3\u538bKylin \u00b6 \u4e0b\u8f7dKylin\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\u3002\u4f8b\u5982Kylin-2.6.1\u4e0b\u8f7d\u8def\u5f84\u4e3a https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-2.6.1/apache-kylin-2.6.1-bin-hbase1x.tar.gz \u3002\u5176\u4ed6\u7248\u672c\u4e0b\u8f7d\u8def\u5f84\u7c7b\u540c\u3002 \u4e0a\u4f20\u5b89\u88c5\u5305\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.6.1-hbase1x-bin.tar.gz \u914d\u7f6eKylin \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.6.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.6.1-bin-hbase1x/bin ./check-env.sh \u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879 \u00b6 \u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\u5e76\u91cd\u542fHive\u670d\u52a1\u4ee5\u53ca\u53d7\u5f71\u54cd\u7684\u670d\u52a1\u3002 |mapreduce\\.job\\..*|dfs\\..* \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff0c\u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389\u3002 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4hive_exec_path\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84 /opt/huawei/Bigdata/ \uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 vi find-hive-dependency.sh \u8bf4\u660e\uff1a\u5728\u7ed9hive_exec_path\u8d4b\u503c\u4e4b\u540e\uff0c\u4f7f\u7528\u4e4b\u524d\uff0c\u589e\u52a0 hive_exec_path=\"$HCAT_HOME\" \u3002\u5426\u5219\u6267\u884c./kylin.sh start\u542f\u52a8kylin\u65f6\u8fd4\u56de\u9519\u8bef\"Couldn't find hive executable jar. Please check if hive executable jar exists in HIVE_LIB folder.\" \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2 FAQ \u00b6 \u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6784\u5efaCube\u540e\uff0c\u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs\u3002 \u67e5\u770b\u65e5\u5fd7/opt/apache-kylin-2.6.3-bin-hbase1x/logs/kylin.log\u9519\u8bef\u5982\u4e0b\uff1a 2019-08-20 21:19:07,958 INFO [http-bio-7070-exec-7] ipc.RpcClientImpl:824 : RPC Server Kerberos principal name for service=ClientService is hbase/hadoop.hadoop.com@HADOOP.COM 2019-08-20 21:19:07,979 DEBUG [http-bio-7070-exec-7] badquery.BadQueryHistoryManager:65 : Loaded 0 Bad Query(s) 2019-08-20 21:19:08,016 ERROR [http-bio-7070-exec-8] controller.BasicController:63 : org.springframework.web.util.NestedServletException: Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: org/apache/directory/api/util/Strings at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:982) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ... ... Caused by: java.lang.ClassNotFoundException: org.apache.directory.api.util.Strings at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1928) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1771) ... 89 more \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a\u7f3a\u5c11\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\u3002 * \u4ece<http://www.java2s.com/Code/JarDownload/api-util/api-util-1.0.0-m18.jar.zip>\u4e0b\u8f7d\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\uff0c\u5e76\u653e\u5230/opt/apache-kylin-2.6.3-bin-hbase1x/lib\u76ee\u5f55\u4e0b\u3002 ![](assets/Apache_Kylin_2.6.1/1cde4e36.png) * \u91cd\u542fkylin\u3002 ``` cd /opt/apache-kylin-2.6.3-bin-hbase1x/bin ./kylin.sh stop ./kylin.sh start ```","title":"2.6.1 <--> 6.5"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#apache-kylin261fusioninsight-hd","text":"","title":"Apache Kylin2.6.1\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_1","text":"Apache Kylin 2.6.1 \u2194 FusionInsight HD 6.5 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_3","text":"\u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e\uff0c\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65\u3002 server 172.21.3.101 nomodify notrap nopeer noquery \u8bf4\u660e\uff1a172.21.3.101\u4e3aFusionInsight HD\u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u7684IP\u3002 \u91cd\u542fNTP\u670d\u52a1\uff0c\u5e76\u540c\u6b65\u672c\u673a\u548c\u96c6\u7fa4\u7684\u65f6\u95f4\u3002 systemctl restart ntpd systemctl stop ntpd ntpdate 172.16.4.21 \u8bf4\u660e\uff1a\u9700\u8981\u5148\u505c\u6b62ntpd\u670d\u52a1\u540e\uff0c\u518d\u6267\u884cntpdate\u540c\u6b65\u65f6\u95f4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u9519\u8bef\u201cthe NTP socket is in use, exiting\u201d\u3002\u53e6\u5916172.16.4.21\u96c6\u7fa4\u7684nptd\u670d\u52a1\u5fc5\u987b\u662f\u542f\u52a8\u7684\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55\u3002 ./install.sh /opt/hadoopclient","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin","text":"Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-**-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u4f8b\u5982apache-kylin-2.6.1-hbase1x-bin.tar.gz\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin\u3002","title":"\u4e0b\u8f7dKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_1","text":"\u4e0b\u8f7dKylin\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\u3002\u4f8b\u5982Kylin-2.6.1\u4e0b\u8f7d\u8def\u5f84\u4e3a https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-2.6.1/apache-kylin-2.6.1-bin-hbase1x.tar.gz \u3002\u5176\u4ed6\u7248\u672c\u4e0b\u8f7d\u8def\u5f84\u7c7b\u540c\u3002 \u4e0a\u4f20\u5b89\u88c5\u5305\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.6.1-hbase1x-bin.tar.gz","title":"\u4e0b\u8f7d\u89e3\u538bKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_2","text":"","title":"\u914d\u7f6eKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.6.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.6.1-bin-hbase1x/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#fusioninsighthive","text":"\u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\u5e76\u91cd\u542fHive\u670d\u52a1\u4ee5\u53ca\u53d7\u5f71\u54cd\u7684\u670d\u52a1\u3002 |mapreduce\\.job\\..*|dfs\\..*","title":"\u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_3","text":"\u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff0c\u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389\u3002 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4hive_exec_path\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84 /opt/huawei/Bigdata/ \uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 vi find-hive-dependency.sh \u8bf4\u660e\uff1a\u5728\u7ed9hive_exec_path\u8d4b\u503c\u4e4b\u540e\uff0c\u4f7f\u7528\u4e4b\u524d\uff0c\u589e\u52a0 hive_exec_path=\"$HCAT_HOME\" \u3002\u5426\u5219\u6267\u884c./kylin.sh start\u542f\u52a8kylin\u65f6\u8fd4\u56de\u9519\u8bef\"Couldn't find hive executable jar. Please check if hive executable jar exists in HIVE_LIB folder.\"","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_4","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_5","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#faq","text":"\u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6784\u5efaCube\u540e\uff0c\u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs\u3002 \u67e5\u770b\u65e5\u5fd7/opt/apache-kylin-2.6.3-bin-hbase1x/logs/kylin.log\u9519\u8bef\u5982\u4e0b\uff1a 2019-08-20 21:19:07,958 INFO [http-bio-7070-exec-7] ipc.RpcClientImpl:824 : RPC Server Kerberos principal name for service=ClientService is hbase/hadoop.hadoop.com@HADOOP.COM 2019-08-20 21:19:07,979 DEBUG [http-bio-7070-exec-7] badquery.BadQueryHistoryManager:65 : Loaded 0 Bad Query(s) 2019-08-20 21:19:08,016 ERROR [http-bio-7070-exec-8] controller.BasicController:63 : org.springframework.web.util.NestedServletException: Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: org/apache/directory/api/util/Strings at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:982) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ... ... Caused by: java.lang.ClassNotFoundException: org.apache.directory.api.util.Strings at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1928) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1771) ... 89 more \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a\u7f3a\u5c11\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\u3002 * \u4ece<http://www.java2s.com/Code/JarDownload/api-util/api-util-1.0.0-m18.jar.zip>\u4e0b\u8f7d\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\uff0c\u5e76\u653e\u5230/opt/apache-kylin-2.6.3-bin-hbase1x/lib\u76ee\u5f55\u4e0b\u3002 ![](assets/Apache_Kylin_2.6.1/1cde4e36.png) * \u91cd\u542fkylin\u3002 ``` cd /opt/apache-kylin-2.6.3-bin-hbase1x/bin ./kylin.sh stop ./kylin.sh start ```","title":"FAQ"},{"location":"SQL_Analytics/Kyligence/","text":"Kyligence Analytics Platform\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kyligence Analytics Platform 2.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.3 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.4 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 2.5 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 3.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Yarn) \u8bf4\u660e \u00b6 \u53c2\u8003 \u5b98\u65b9\u4ea7\u54c1\u6587\u6863 \u7684 \u5feb\u901f\u5b89\u88c5 \u7ae0\u8282","title":"3.0 <--> C80"},{"location":"SQL_Analytics/Kyligence/#kyligence-analytics-platformfusioninsight","text":"","title":"Kyligence Analytics Platform\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Kyligence/#_1","text":"Kyligence Analytics Platform 2.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.3 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.4 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 2.5 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 3.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Yarn)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Kyligence/#_2","text":"\u53c2\u8003 \u5b98\u65b9\u4ea7\u54c1\u6587\u6863 \u7684 \u5feb\u901f\u5b89\u88c5 \u7ae0\u8282","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.155/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto 0.155 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive) \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eHive Connector \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.155 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.155\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.155\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.155 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.155.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log \u68c0\u67e5FusionInsight Manager\u4e2dHDFS\u670d\u52a1\u914d\u7f6e\u4e2dhadoop.rpc.protection\u7684\u914d\u7f6e\uff0c\u5fc5\u987b\u8bbe\u7f6e\u4e3aauthentacation\u3002 \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.155-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.155/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar \u53c2\u8003 https://prestodb.io/docs/0.155/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"0.155 <--> C60"},{"location":"SQL_Analytics/Presto_0.155/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.155/#_1","text":"Presto 0.155 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.155/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.155/#hive-connector","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.155 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.155\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.155\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.155 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.155.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log \u68c0\u67e5FusionInsight Manager\u4e2dHDFS\u670d\u52a1\u914d\u7f6e\u4e2dhadoop.rpc.protection\u7684\u914d\u7f6e\uff0c\u5fc5\u987b\u8bbe\u7f6e\u4e3aauthentacation\u3002","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.155/#presto-clihive","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.155-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.155/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.155/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar \u53c2\u8003 https://prestodb.io/docs/0.155/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.184/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto 0.184 \u2194 FusionInsight HD V100R002C70SPC100 (HDFS/Hive) Presto 0.196 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/Hive) \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eHive Connector \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.184 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C70SPC100\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.184\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.184/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ \u6309\u7167\u4e0b\u56fe\u4fee\u6539hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.184\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.184 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.184.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.184-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.184/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar \u53c2\u8003 https://prestodb.io/docs/0.184/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"0.196 <--> C80"},{"location":"SQL_Analytics/Presto_0.184/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.184/#_1","text":"Presto 0.184 \u2194 FusionInsight HD V100R002C70SPC100 (HDFS/Hive) Presto 0.196 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.184/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.184/#hive-connector","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.184 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C70SPC100\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.184\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.184/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ \u6309\u7167\u4e0b\u56fe\u4fee\u6539hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.184\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.184 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.184.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.184/#presto-clihive","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.184-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.184/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.184/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar \u53c2\u8003 https://prestodb.io/docs/0.184/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto 0.210 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/ElasticSearch) Presto 0.210 \u2194 FusionInsight HD 6.5 (HDFS/Hive) \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u8fdb\u884c\u5bf9\u63a5,\u5728Presto0.210\u7248\u672c\u4e2d\u652f\u6301\u4e0eFusionInsight\u7684ES\u8fdb\u884c\u5bf9\u63a5\u3002 \u83b7\u53d6\u5e76\u914d\u7f6epresto server \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.210/presto-server-0.210.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.210 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C80SPC200\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto-0.210\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool -genkeypair -alias testuser -keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d,\u5e76\u4e14\u8981\u5ffd\u7565\u5927\u5c0f\u5199\uff0c\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u9009\u62e9\u7528\u6237\u7ec4(hadoop\u548chive\u7ec4)\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.210/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.210/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.210/etc/catalog/ \u5c06hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4fee\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.210\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.210 \u83b7\u53d6Presto CLI\u542f\u52a8\u5305 \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.210/presto-cli-0.210-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.210-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u914d\u7f6eHive Connector \u00b6 \u8fdb\u5165\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://172.21.3.115:21088,thrift://172.21.3.116:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.210/etc/catalog/core-site.xml,/opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP\"\u7684\u503c\u4fee\u6539\u4e3a\u56fa\u5b9a\u7684\"auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.210.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.210/plugin/hive-hadoop2/presto-hive-0.210.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.210/bin/launcher start tailf /var/presto/data/var/log/server.log \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982/opt\uff1b \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.210/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.210/presto-jdbc-0.210.jar \u53c2\u8003 https://prestodb.io/docs/0.210/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://172.21.3.48:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from adult limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u914d\u7f6eElasticSearch Connector \u00b6 presto\u548cES\u5b98\u65b9\u90fd\u6ca1\u6709\u7ed9\u51fa\u9002\u914d\u7684\u6587\u6863\u4ecb\u7ecd\uff0c\u8fd9\u91cc\u6211\u4eec\u91c7\u7528\u5f00\u6e90\u7684\u9002\u914d\u5305\u8fdb\u884c\u9002\u914d\u3002\u5728https://github.com/harbby/presto-connectors \u4e0b\u8f7d\u9002\u914d\u5305\u6e90\u7801\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\uff0c\u89e3\u538b\u3002 \u4fee\u6539presto-elasticsearch-connectors\u6e90\u7801\u4ee5\u53ca\u914d\u7f6e \u8fdb\u5165/opt/presto-connectors-master/presto-elasticsearch6/src/main/java/com/facebook/presto/elasticsearch6/functions\u76ee\u5f55\uff0c\u53c2\u8003\u4e0b\u56fe\uff0c\u4fee\u6539MatchQueryFunction.java\u6587\u4ef6\uff0c\u6dfb\u52a0\u6784\u9020\u51fd\u6570\uff0c\u5c06\u51fd\u6570\u58f0\u660e\u4e3apublic \u8fdb\u5165presto-connectors-master\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-connectors-master vi pom.xml \u5728\u6700\u540e\u7684' '\u4e4b\u524d\uff0c\u6dfb\u52a0\u4ee5\u4e0bplugin\u4f9d\u8d56 <build> <pluginManagement> <plugins> <plugin> <groupId>pl.project13.maven</groupId> <artifactId>git-commit-id-plugin</artifactId> <configuration> <skip>true</skip> </configuration> </plugin> </plugins> </pluginManagement> </build> \uff08\u53ef\u9009\u64cd\u4f5c\uff09\u5728modules\u4e2d\uff0c\u53bb\u6389\u9664presto-base-elasticsearch\u548cpresto-elasticsearch6\u4ee5\u5916\u7684module\uff0c\u5176\u4ed6\u7684module\u8fd9\u91cc\u5e76\u4e0d\u9700\u8981\uff0c\u53ef\u4ee5\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4 \u8fdb\u5165presto-elasticsearch6\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-elasticsearch6 vi pom.xml \u5c06'elasticsearch.version'\u4fee\u6539\u4e3a6.1.3 \u5c06'elasticsearch-x-content'\u548c'elasticsearch-core'\u7684\u4f9d\u8d56\u6ce8\u91ca\u6389 \u56de\u5230presto-connectors-master\u76ee\u5f55\uff0c\u7f16\u8bd1presto-connectors mvn clean install -DskipTests \u7f16\u8bd1\u6210\u529f\u540e\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u83b7\u53d6'presto-connectors-master/presto-elasticsearch6/target'\u76ee\u5f55\u4e0b\u7684'presto-elasticsearch6-0.210'\u6587\u4ef6\u5939\uff0c\u5c06\u5176\u590d\u5236\u5230presto-server\u7684plugin\u76ee\u5f55\u4e0b cp -r /opt/presto-connectors-master/presto-elasticsearch6/target/presto-elasticsearch6-0.210 /opt/presto-server-0.210/plugin \u767b\u5f55\u96c6\u7fa4manager\u7ba1\u7406\u9875\u9762\uff0c\u5728'\u670d\u52a1\u7ba1\u7406->Elasticsearch \u670d\u52a1\u914d\u7f6e'\u9875\u9762\uff0c\u9009\u62e9\u5168\u90e8\u914d\u7f6e\uff0c\u641c\u7d22'port'\u5173\u952e\u8bcd,\u67e5\u770b'SERVER_PORT'\u914d\u7f6e\u4e3a24100\uff0c'TRANSPORT_TCP_PORT'\u914d\u7f6e\u4e3a24101 \u5728\u96c6\u7fa4\u5ba2\u6237\u7aef\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_evn curl -XGET http://172.21.3.115:24100/_cluster/health?pretty \u5176\u4e2d172.21.3.115\u662felasticsearch\u96c6\u7fa4\u8282\u70b9\uff0c24100\u4e3aSERVER_PORT\uff0c\u770b\u5230\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c { \"cluster_name\" : \"elasticsearch_cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 33, \"active_shards\" : 66, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } \u5728/opt/presto-server-0.210/etc/catalog\u76ee\u5f55\u4e0b\u521b\u5efaes.properties\u6587\u4ef6 connector.name=elasticsearch6 elasticsearch.cluster.name=elasticsearch_cluster elasticsearch.transport.hosts=172.21.3.115:24101 \u5176\u4e2d'elasticsearch.cluster.name'\u662f\u521a\u624d\u83b7\u53d6\u7684ES\u96c6\u7fa4\u7684\u540d\u5b57\uff0c'elasticsearch.transport.hosts'\u4e3aEsNode\u8282\u70b9IP\uff0c\u7aef\u53e3\u4e3a'TRANSPORT_TCP_PORT' \u91cd\u542fpresto-server sh /opt/presto-server-0.210/bin/launcher restart \u901a\u8fc7Presto CLI\u8fde\u63a5ElasticSearch \u00b6 \u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55,\u4f8b\u5982/opt \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog es \\ --schema default \\ --; catalog\u540e\u9762\u7684es\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684es.properties\u7684\u6587\u4ef6\u540d\u5339\u914d \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\u67e5\u8be2ES\u4e2d\u7684\u7d22\u5f15\u4fe1\u606f \u5f53\u524dconnector\u652f\u6301show,create,select,insert,drop\u64cd\u4f5c\uff0c\u6682\u4e0d\u652f\u6301delete,update,alter\u7b49\u64cd\u4f5c","title":"0.210 <--> 6.5"},{"location":"SQL_Analytics/Presto_0.210/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.210/#_1","text":"Presto 0.210 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/ElasticSearch) Presto 0.210 \u2194 FusionInsight HD 6.5 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.210/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u8fdb\u884c\u5bf9\u63a5,\u5728Presto0.210\u7248\u672c\u4e2d\u652f\u6301\u4e0eFusionInsight\u7684ES\u8fdb\u884c\u5bf9\u63a5\u3002","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.210/#presto-server","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.210/presto-server-0.210.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.210 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C80SPC200\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto-0.210\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool -genkeypair -alias testuser -keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d,\u5e76\u4e14\u8981\u5ffd\u7565\u5927\u5c0f\u5199\uff0c\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u9009\u62e9\u7528\u6237\u7ec4(hadoop\u548chive\u7ec4)\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.210/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.210/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.210/etc/catalog/ \u5c06hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4fee\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.210\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.210","title":"\u83b7\u53d6\u5e76\u914d\u7f6epresto server"},{"location":"SQL_Analytics/Presto_0.210/#presto-cli","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.210/presto-cli-0.210-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.210-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h","title":"\u83b7\u53d6Presto CLI\u542f\u52a8\u5305"},{"location":"SQL_Analytics/Presto_0.210/#hive-connector","text":"\u8fdb\u5165\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://172.21.3.115:21088,thrift://172.21.3.116:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.210/etc/catalog/core-site.xml,/opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP\"\u7684\u503c\u4fee\u6539\u4e3a\u56fa\u5b9a\u7684\"auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.210.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.210/plugin/hive-hadoop2/presto-hive-0.210.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.210/bin/launcher start tailf /var/presto/data/var/log/server.log","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.210/#presto-clihive","text":"\u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982/opt\uff1b \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.210/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.210/presto-jdbc-0.210.jar \u53c2\u8003 https://prestodb.io/docs/0.210/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://172.21.3.48:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from adult limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/#elasticsearch-connector","text":"presto\u548cES\u5b98\u65b9\u90fd\u6ca1\u6709\u7ed9\u51fa\u9002\u914d\u7684\u6587\u6863\u4ecb\u7ecd\uff0c\u8fd9\u91cc\u6211\u4eec\u91c7\u7528\u5f00\u6e90\u7684\u9002\u914d\u5305\u8fdb\u884c\u9002\u914d\u3002\u5728https://github.com/harbby/presto-connectors \u4e0b\u8f7d\u9002\u914d\u5305\u6e90\u7801\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\uff0c\u89e3\u538b\u3002 \u4fee\u6539presto-elasticsearch-connectors\u6e90\u7801\u4ee5\u53ca\u914d\u7f6e \u8fdb\u5165/opt/presto-connectors-master/presto-elasticsearch6/src/main/java/com/facebook/presto/elasticsearch6/functions\u76ee\u5f55\uff0c\u53c2\u8003\u4e0b\u56fe\uff0c\u4fee\u6539MatchQueryFunction.java\u6587\u4ef6\uff0c\u6dfb\u52a0\u6784\u9020\u51fd\u6570\uff0c\u5c06\u51fd\u6570\u58f0\u660e\u4e3apublic \u8fdb\u5165presto-connectors-master\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-connectors-master vi pom.xml \u5728\u6700\u540e\u7684' '\u4e4b\u524d\uff0c\u6dfb\u52a0\u4ee5\u4e0bplugin\u4f9d\u8d56 <build> <pluginManagement> <plugins> <plugin> <groupId>pl.project13.maven</groupId> <artifactId>git-commit-id-plugin</artifactId> <configuration> <skip>true</skip> </configuration> </plugin> </plugins> </pluginManagement> </build> \uff08\u53ef\u9009\u64cd\u4f5c\uff09\u5728modules\u4e2d\uff0c\u53bb\u6389\u9664presto-base-elasticsearch\u548cpresto-elasticsearch6\u4ee5\u5916\u7684module\uff0c\u5176\u4ed6\u7684module\u8fd9\u91cc\u5e76\u4e0d\u9700\u8981\uff0c\u53ef\u4ee5\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4 \u8fdb\u5165presto-elasticsearch6\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-elasticsearch6 vi pom.xml \u5c06'elasticsearch.version'\u4fee\u6539\u4e3a6.1.3 \u5c06'elasticsearch-x-content'\u548c'elasticsearch-core'\u7684\u4f9d\u8d56\u6ce8\u91ca\u6389 \u56de\u5230presto-connectors-master\u76ee\u5f55\uff0c\u7f16\u8bd1presto-connectors mvn clean install -DskipTests \u7f16\u8bd1\u6210\u529f\u540e\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u83b7\u53d6'presto-connectors-master/presto-elasticsearch6/target'\u76ee\u5f55\u4e0b\u7684'presto-elasticsearch6-0.210'\u6587\u4ef6\u5939\uff0c\u5c06\u5176\u590d\u5236\u5230presto-server\u7684plugin\u76ee\u5f55\u4e0b cp -r /opt/presto-connectors-master/presto-elasticsearch6/target/presto-elasticsearch6-0.210 /opt/presto-server-0.210/plugin \u767b\u5f55\u96c6\u7fa4manager\u7ba1\u7406\u9875\u9762\uff0c\u5728'\u670d\u52a1\u7ba1\u7406->Elasticsearch \u670d\u52a1\u914d\u7f6e'\u9875\u9762\uff0c\u9009\u62e9\u5168\u90e8\u914d\u7f6e\uff0c\u641c\u7d22'port'\u5173\u952e\u8bcd,\u67e5\u770b'SERVER_PORT'\u914d\u7f6e\u4e3a24100\uff0c'TRANSPORT_TCP_PORT'\u914d\u7f6e\u4e3a24101 \u5728\u96c6\u7fa4\u5ba2\u6237\u7aef\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_evn curl -XGET http://172.21.3.115:24100/_cluster/health?pretty \u5176\u4e2d172.21.3.115\u662felasticsearch\u96c6\u7fa4\u8282\u70b9\uff0c24100\u4e3aSERVER_PORT\uff0c\u770b\u5230\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c { \"cluster_name\" : \"elasticsearch_cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 33, \"active_shards\" : 66, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } \u5728/opt/presto-server-0.210/etc/catalog\u76ee\u5f55\u4e0b\u521b\u5efaes.properties\u6587\u4ef6 connector.name=elasticsearch6 elasticsearch.cluster.name=elasticsearch_cluster elasticsearch.transport.hosts=172.21.3.115:24101 \u5176\u4e2d'elasticsearch.cluster.name'\u662f\u521a\u624d\u83b7\u53d6\u7684ES\u96c6\u7fa4\u7684\u540d\u5b57\uff0c'elasticsearch.transport.hosts'\u4e3aEsNode\u8282\u70b9IP\uff0c\u7aef\u53e3\u4e3a'TRANSPORT_TCP_PORT' \u91cd\u542fpresto-server sh /opt/presto-server-0.210/bin/launcher restart","title":"\u914d\u7f6eElasticSearch Connector"},{"location":"SQL_Analytics/Presto_0.210/#presto-clielasticsearch","text":"\u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55,\u4f8b\u5982/opt \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog es \\ --schema default \\ --; catalog\u540e\u9762\u7684es\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684es.properties\u7684\u6587\u4ef6\u540d\u5339\u914d \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\u67e5\u8be2ES\u4e2d\u7684\u7d22\u5f15\u4fe1\u606f \u5f53\u524dconnector\u652f\u6301show,create,select,insert,drop\u64cd\u4f5c\uff0c\u6682\u4e0d\u652f\u6301delete,update,alter\u7b49\u64cd\u4f5c","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5ElasticSearch"}]}